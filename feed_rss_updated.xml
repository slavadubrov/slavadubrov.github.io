<?xml version="1.0" encoding="UTF-8" ?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel><title>Shared Intelligence: Tips &amp; Tricks in Machine Learning</title><link>https://slavadubrov.github.io/</link><atom:link href="https://slavadubrov.github.io/feed_rss_updated.xml" rel="self" type="application/rss+xml" /><language>en</language> <pubDate>Sat, 10 May 2025 12:56:23 -0000</pubDate> <lastBuildDate>Sat, 10 May 2025 12:56:23 -0000</lastBuildDate> <ttl>1440</ttl> <generator>MkDocs RSS plugin - v1.17.1</generator> <image> <url>None</url> <title>Shared Intelligence: Tips & Tricks in Machine Learning</title><link>https://slavadubrov.github.io/</link> </image> <item> <title>Quick-Guide on managing Python on macOS with uv</title> <category>guide</category> <category>python</category> <category>tooling</category> <description>&lt;h1&gt;Quick-Guide on managing Python like an AI Engineer on macOS with &lt;strong&gt;uv&lt;/strong&gt;&lt;/h1&gt;&lt;h2&gt;TL;DR BashÂ Cheatâ€‘sheet&lt;/h2&gt;&lt;p&gt;&lt;code&gt;bashbrew install uv # install tooluv python install 3.12 # grab interpreteruv python pin # lock version for repouv venv # create .venvuv pip install numpy pandas # ML staplesuv run train.py # run with correct interpreteruv self upgrade # update uv itself&lt;/code&gt;&lt;/p&gt;&lt;hr&gt;</description><link>https://slavadubrov.github.io/blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/</guid> </item> <item> <title>Quick-Guide on setting up a MacBook for AI Engineering</title> <category>guide</category> <category>macos</category> <category>tooling</category> <description>&lt;h1&gt;Quick-Guide on setting up a MacBook for AI Engineering&lt;/h1&gt;&lt;p&gt;Here&#39;s my distilled, 10â€‘step workflow to transform a vanilla macOS install into a ready to-go AI engineering working station.&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/</guid> </item> <item> <title>Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</title> <category>Deep Learning</category> <category>Distributed Training</category> <category>GPU</category> <category>LLM</category> <category>Parallelism</category> <description>&lt;h1&gt;Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025&lt;/h1&gt;&lt;p&gt;The race to build bigger, better language models continues at breakneck speed. Today&#39;s state-of-the-art models require massive computing resources that no single GPU can handle. Whether you&#39;re training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.&lt;/p&gt;&lt;p&gt;This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face&#39;s &lt;a href=&#34;https://huggingface.co/spaces/nanotron/ultrascale-playbook&#34;&gt;Ultra-Scale Playbook&lt;/a&gt;.&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/</guid> </item> <item> <title>MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</title> <category>infrastructure</category> <category>llmops</category> <category>mlops</category> <description>&lt;h1&gt;MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond&lt;/h1&gt;&lt;p&gt;The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models - from giant language models (LLMs) like GPT-4 to image diffusion models like Stable Diffusion. As a result, the way we build and operate ML systems (MLOps) looks very different today than it did just a few years ago. In this post, we&#39;ll explore how ML infrastructure and MLOps practices have evolved - contrasting the &#34;classic&#34; era of MLOps with the modern paradigms emerging to support foundation models. We&#39;ll highlight what&#39;s changed, what new patterns and workflows have emerged.&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/</guid> </item> <item> <title>Quick-Guide on ~/.zprofile vs ~/.zshrc ðŸš€</title> <category>guide</category> <category>macos</category> <category>tooling</category> <description>&lt;h1&gt;Quick-Guide on ~/.zprofile vs ~/.zshrc ðŸš€&lt;/h1&gt;&lt;h2&gt;TL;DR âš¡&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt; â†’ one-shot, login-shell initialization (think &#34;environment/bootstrap&#34;) ðŸ”§&lt;/li&gt;&lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt; â†’ every interactive prompt (think &#34;daily driving experience&#34;) ðŸŽ®&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Use both in tandem: keep your environment reliable with &lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt;, and your shell pleasant and tweakable with &lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt; âœ¨&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/</guid> </item> <item> <title>Quick-Guide on `pyproject.toml`</title> <category>guide</category> <category>python</category> <description>&lt;h1&gt;Quick-Guide on &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/h1&gt;&lt;h2&gt;TL;DR&lt;/h2&gt;&lt;p&gt;Think of &lt;code&gt;pyproject.toml&lt;/code&gt; as the &lt;strong&gt;&lt;code&gt;package.json&lt;/code&gt; for Python&lt;/strong&gt;. Whether you prefer &lt;code&gt;.venv&lt;/code&gt;, &lt;code&gt;pyenv&lt;/code&gt;, or &lt;code&gt;uv&lt;/code&gt;, putting all your project&#39;s metadata, dependencies, and tooling into one tidy TOML file simplifies development and boosts collaboration.&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/05/08/quick-guide-on-pyprojecttoml/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/08/quick-guide-on-pyprojecttoml/</guid> </item> <item> <title>Quick-guide on Local Stable-Diffusion Toolkits for macOS</title> <category>genai</category> <category>guide</category> <category>macos</category> <category>tools</category> <description>&lt;h1&gt;Quick-guide on Local Stable-Diffusion Toolkits for macOS&lt;/h1&gt;&lt;p&gt;Running generative-AI models on-device means zero cloud costs, no upload limits, and full control of your checkpoints. Below is a quick guide to five of the most popular macOS-ready front-ends and launchers.&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/</guid> </item> <item> <title>Quick-guide on Running LLMs Locally on macOS</title> <category>guide</category> <category>llm</category> <category>macos</category> <category>tools</category> <description>&lt;h1&gt;Quick-guide on Running LLMs Locally on macOS&lt;/h1&gt;&lt;p&gt;This guide compares the five most popular local toolchains, complete with download links, quick overviews, and pros &amp;amp; cons. A comparison table follows for easy reference.&lt;/p&gt;</description><link>https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/</link> <pubDate>Sat, 10 May 2025 12:56:04 +0000</pubDate><source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/</guid> </item> </channel></rss>