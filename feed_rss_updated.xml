<?xml version="1.0" encoding="UTF-8" ?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel> <title>Shared Intelligence: Tips &amp; Tricks in Machine Learning</title><link>https://slavadubrov.github.io/</link><atom:link href="https://slavadubrov.github.io/feed_rss_updated.xml" rel="self" type="application/rss+xml" /> <language>en</language> <pubDate>Thu, 23 Oct 2025 21:51:30 -0000</pubDate> <lastBuildDate>Thu, 23 Oct 2025 21:51:30 -0000</lastBuildDate> <ttl>1440</ttl> <generator>MkDocs RSS plugin - v1.17.4</generator> <image> <url>None</url> <title>Shared Intelligence: Tips & Tricks in Machine Learning</title> <link>https://slavadubrov.github.io/</link> </image> <item> <title>Quick-Guide on managing Python on macOS with uv</title> <category>guide</category> <category>python</category> <category>tooling</category> <description>&lt;h1&gt;Quick-Guide on managing Python like an AI Engineer on macOS with &lt;strong&gt;uv&lt;/strong&gt;&lt;/h1&gt; &lt;h2&gt;TL;DR Bash Cheatâ€‘sheet&lt;/h2&gt; &lt;p&gt;```bash brew install uv # install tool uv python install 3.12 # grab interpreter&lt;/p&gt; &lt;h1&gt;New project workflow (modern)&lt;/h1&gt; &lt;p&gt;uv init # create new project with pyproject.toml uv add pandas numpy # add dependencies uv run train.py # run with correct interpreter&lt;/p&gt; &lt;h1&gt;Classical project workflow (requirements.txt)&lt;/h1&gt; &lt;p&gt;uv venv # create .venv uv pip install -r requirements.txt # install from requirements uv run train.py # run script&lt;/p&gt; &lt;p&gt;brew upgrade uv # update uv itself (Homebrew install) ```&lt;/p&gt; &lt;hr&gt;</description> <link>https://slavadubrov.github.io/blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/</guid> </item> <item> <title>Quick-Guide on setting up a MacBook for AI Engineering</title> <category>guide</category> <category>macos</category> <category>tooling</category> <description>&lt;h1&gt;Quick-Guide on setting up a MacBook for AI Engineering&lt;/h1&gt; &lt;p&gt;Here&#39;s my distilled, 10â€‘step workflow to transform a vanilla macOS install into a ready to-go AI engineering working station.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/</guid> </item> <item> <title>Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</title> <category>Deep Learning</category> <category>Distributed Training</category> <category>GPU</category> <category>LLM</category> <category>Parallelism</category> <description>&lt;h1&gt;Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025&lt;/h1&gt; &lt;p&gt;The race to build bigger, better language models continues at breakneck speed. Today&#39;s state-of-the-art models require massive computing resources that no single GPU can handle. Whether you&#39;re training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.&lt;/p&gt; &lt;p&gt;This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face&#39;s &lt;a href=&#34;https://huggingface.co/spaces/nanotron/ultrascale-playbook&#34;&gt;Ultra-Scale Playbook&lt;/a&gt;.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/</guid> </item> <item> <title>MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</title> <category>infrastructure</category> <category>llmops</category> <category>mlops</category> <description>&lt;h1&gt;MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond&lt;/h1&gt; &lt;p&gt;The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models. From giant language models like GPT-4 to image diffusion models like Stable Diffusion, these powerful models have fundamentally changed how we build and operate ML systems. &lt;/p&gt; &lt;p&gt;In this post, I&#39;ll explore how ML infrastructure and MLOps practices have evolved to support foundation models. We&#39;ll contrast the &#34;classic&#34; era of MLOps with modern paradigms, examine what&#39;s changed, and look at the new patterns and workflows that have emerged.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/</guid> </item> <item> <title>Quick-Guide on ~/.zprofile vs ~/.zshrc ðŸš€</title> <category>guide</category> <category>macos</category> <category>tooling</category> <description>&lt;h1&gt;Quick-Guide on ~/.zprofile vs ~/.zshrc ðŸš€&lt;/h1&gt; &lt;h2&gt;TL;DR âš¡&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt; â†’ runs once when you start a login shell (think &#34;environment setup&#34;) ðŸ”§&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt; â†’ runs every time you open an interactive shell (think &#34;daily experience&#34;) ðŸŽ®&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Use both strategically: put your PATH and environment variables in &lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt;, and your aliases, functions, and prompt customizations in &lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt; âœ¨&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/</guid> </item> <item> <title>Quick-Guide on `pyproject.toml`</title> <category>guide</category> <category>python</category> <description>&lt;h1&gt;Quick-Guide on &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/h1&gt; &lt;h2&gt;TL;DR&lt;/h2&gt; &lt;p&gt;Think of &lt;code&gt;pyproject.toml&lt;/code&gt; as the &lt;strong&gt;&lt;code&gt;package.json&lt;/code&gt; for Python&lt;/strong&gt;. It&#39;s a single configuration file that holds your project&#39;s metadata, dependencies, and tool settings. Whether you use &lt;code&gt;.venv&lt;/code&gt;, &lt;code&gt;pyenv&lt;/code&gt;, or &lt;code&gt;uv&lt;/code&gt;, this one file simplifies development and makes collaboration smoother.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/08/quick-guide-on-pyprojecttoml/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/08/quick-guide-on-pyprojecttoml/</guid> </item> <item> <title>Quick-guide on Local Stable-Diffusion Toolkits for macOS</title> <category>genai</category> <category>guide</category> <category>macos</category> <category>tools</category> <description>&lt;h1&gt;Quick-guide on Local Stable-Diffusion Toolkits for macOS&lt;/h1&gt; &lt;p&gt;Running generative-AI models on-device means zero cloud costs, no upload limits, and full control of your checkpoints. Whether you&#39;re generating portraits, concept art, or iterating on product designs, keeping everything local gives you privacy and unlimited generations.&lt;/p&gt; &lt;p&gt;Below is a practical guide to five of the most popular macOS-ready front-ends. Each tool wraps the same underlying Stable Diffusion models but offers different trade-offs between simplicity and power.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/</guid> </item> <item> <title>Quick-guide on Running LLMs Locally on macOS</title> <category>guide</category> <category>llm</category> <category>macos</category> <category>tools</category> <description>&lt;h1&gt;Quick-guide on Running LLMs Locally on macOS&lt;/h1&gt; &lt;p&gt;Running large language models locally on your Mac means faster responses, complete privacy, and no API bills. But which tool should you pick?&lt;/p&gt; &lt;p&gt;This guide breaks down the five most popular options - from dead-simple menu bar apps to full-control command-line tools. Each comes with download links, what makes it special, and honest trade-offs.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/</guid> </item> <item> <title>Choosing the Right Open-Source LLM Variant &amp; File Format</title> <category>guide</category> <category>llm</category> <description>&lt;h1&gt;Choosing the Right Open-Source LLM Variant &amp;amp; File Format&lt;/h1&gt; &lt;hr&gt; &lt;h2&gt;Why do open-source LLMs have so many confusing names?&lt;/h2&gt; &lt;p&gt;You&#39;ve probably seen model names like &lt;code&gt;Llama-3.1-8B-Instruct.Q4_K_M.gguf&lt;/code&gt; or &lt;code&gt;Mistral-7B-v0.3-A3B.awq&lt;/code&gt; and wondered what all those suffixes mean. The short answer: &lt;strong&gt;they tell you two critical things.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open-source LLMs vary along &lt;strong&gt;two independent dimensions&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model variant&lt;/strong&gt; â€“ the suffix in the name (&lt;code&gt;-Instruct&lt;/code&gt;, &lt;code&gt;-Distill&lt;/code&gt;, &lt;code&gt;-A3B&lt;/code&gt;, etc.) describes &lt;em&gt;how&lt;/em&gt; the model was trained and &lt;em&gt;what&lt;/em&gt; it&#39;s optimized for.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File format&lt;/strong&gt; â€“ the extension (&lt;code&gt;.gguf&lt;/code&gt;, &lt;code&gt;.gptq&lt;/code&gt;, &lt;code&gt;.awq&lt;/code&gt;, etc.) describes &lt;em&gt;how&lt;/em&gt; the weights are stored and &lt;em&gt;where&lt;/em&gt; they run best (CPU, GPU, mobile, etc.).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Think of it like this: the model variant is the recipe, and the file format is the container. Same recipe, different containers for different kitchens.&lt;/p&gt; &lt;p&gt;&lt;code&gt;mermaid graph LR A[Model Release] --&amp;gt; B[Model Variant&amp;lt;br/&amp;gt;What it does] A --&amp;gt; C[File Format&amp;lt;br/&amp;gt;Where it runs] B --&amp;gt; D[Base&amp;lt;br/&amp;gt;Instruct&amp;lt;br/&amp;gt;Distill&amp;lt;br/&amp;gt;QAT&amp;lt;br/&amp;gt;MoE] C --&amp;gt; E[GGUF&amp;lt;br/&amp;gt;GPTQ&amp;lt;br/&amp;gt;AWQ&amp;lt;br/&amp;gt;EXL2&amp;lt;br/&amp;gt;Safetensors] D --&amp;gt; F[Pick based on&amp;lt;br/&amp;gt;your use case] E --&amp;gt; G[Pick based on&amp;lt;br/&amp;gt;your hardware]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Understanding both dimensions helps you avoid downloading 20 GB of the wrong model at midnight and then spending hours debugging CUDA errors.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/</guid> </item> <item> <title>Building a Custom FeatureStoreLite MCP Server Using uv</title> <category>guide</category> <category>mcp</category> <description>&lt;h1&gt;Building a Custom FeatureStoreLite MCP Server Using uv&lt;/h1&gt; &lt;p&gt;&lt;em&gt;A step-by-step guide that shows how to create your own lightweight feature store MCP server from scratch using FastMCP, run it through &lt;/em&gt;&lt;em&gt;uv&lt;/em&gt;&lt;em&gt;, and integrate it with Claude Desktop. This is a practical example of building a useful MCP server that ML engineers can actually use.&lt;/em&gt;&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/</guid> </item> <item> <title>Context Engineering in the Agenticâ€‘AI Era â€” and How to Cook It</title> <author>Viacheslav Dubrov</author> <category>agents</category> <category>ai-engineering</category> <category>context-layer</category> <category>guardrails</category> <category>memory</category> <category>rag</category> <category>retrieval</category> <description>A practical guide to designing, evaluating, and shipping the context layer (a.k.a. context engineering) for agentic AI systems â€” with diagrams, patterns, and a starter config.</description> <link>https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/</guid> </item> <item> <title>Domain-driven design for AI agents: a beginner-friendly guide</title> <author>Viacheslav Dubrov</author> <category>agents</category> <category>ai-engineering</category> <category>architecture</category> <category>domain-driven-design</category> <description>Learn how domain-driven design (DDD) keeps AI agents aligned with the business domain, with practical patterns, code snippets, and tooling tips.</description> <link>https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/</guid> </item> <item> <title>LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes</title> <category>Deployment</category> <category>Inference</category> <category>Kubernetes</category> <category>LLM</category> <category>LoRA</category> <description>&lt;h1&gt;LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes&lt;/h1&gt; &lt;p&gt;Serving dozens of fine-tuned large language models used to mean provisioning one GPU per model. LoRAX (LoRA eXchange) flips that math on its head: keep a single base model in memory and hot-swap lightweight LoRA adapters per request.&lt;/p&gt; &lt;p&gt;This guide shows you how LoRAX achieves near-constant cost per token regardless of how many fine-tunes you&#39;re serving, when it makes sense to use it, how to deploy it on Kubernetes with Helm, and how to call it through REST, Python, and OpenAI-compatible APIs.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/</link> <pubDate>Thu, 23 Oct 2025 21:51:12 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Shared Intelligence: Tips & Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/</guid> </item> </channel> </rss>