<?xml version="1.0" encoding="UTF-8" ?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel> <title>Edge of Context: Tips and Tricks in Machine Learning</title><description>ML &amp; engineering notes</description><link>https://slavadubrov.github.io/</link><atom:link href="https://slavadubrov.github.io/feed_rss_updated.xml" rel="self" type="application/rss+xml" /> <language>en</language> <pubDate>Mon, 05 Jan 2026 20:45:44 -0000</pubDate> <lastBuildDate>Mon, 05 Jan 2026 20:45:44 -0000</lastBuildDate> <ttl>1440</ttl> <generator>MkDocs RSS plugin - v1.17.4</generator> <image> <url>https://slavadubrov.github.io/assets/logo-eoc.svg</url> <title>Edge of Context: Tips and Tricks in Machine Learning</title> <link>https://slavadubrov.github.io/</link> </image> <item> <title>The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production</title> <author>Viacheslav Dubrov</author> <category>ai-engineering</category> <category>axolotl</category> <category>fine-tuning</category> <category>llm</category> <category>lora</category> <category>peft</category> <category>qlora</category> <category>unsloth</category> <description>&lt;h1 id=&#34;the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production&#34;&gt;The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production&lt;/h1&gt; &lt;p&gt;Fine-tuning has become the secret weapon for building specialized AI applications. While general-purpose models like GPT-4 and Claude excel at broad tasks, fine-tuning transforms them into laser-focused experts for your specific domain. This guide walks you through everything you need to know—from understanding when to fine-tune to deploying your custom model.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;strategic-decision-fine-tuning-vs-alternatives&#34;&gt;Strategic Decision: Fine-Tuning vs Alternatives&lt;/h2&gt; &lt;p&gt;Before investing GPU hours and engineering time, you need to answer a fundamental question: &lt;strong&gt;is fine-tuning the right solution for your problem?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Decision Flowchart&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/decision_flowchart.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;fine-tuning-vs-rag&#34;&gt;Fine-Tuning vs RAG&lt;/h3&gt; &lt;p&gt;Do not fine-tune just to add &#34;knowledge.&#34; Here&#39;s when to use each approach:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Feature&lt;/th&gt; &lt;th&gt;Fine-Tuning&lt;/th&gt; &lt;th&gt;RAG (Retrieval-Augmented Generation)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Core Function&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Alters internal weights to teach skills, styles, or behaviors&lt;/td&gt; &lt;td&gt;Provides external, up-to-date context at inference time&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Best For&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;• Specific conversational styles&lt;br&gt;• Complex instruction following&lt;br&gt;• Domain-specific reasoning&lt;/td&gt; &lt;td&gt;• Rapidly changing data (news, stock prices)&lt;br&gt;• Reducing hallucinations (grounding)&lt;br&gt;• Citing sources&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Knowledge Handling&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Internalizes patterns, not facts&lt;/td&gt; &lt;td&gt;Retrieves facts from external knowledge base&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Update Frequency&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Requires retraining for updates&lt;/td&gt; &lt;td&gt;Updates immediately with new documents&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;fine-tuning-vs-prompt-engineering&#34;&gt;Fine-Tuning vs Prompt Engineering&lt;/h3&gt; &lt;p&gt;Modern LLMs are remarkably responsive to well-crafted prompts. Before fine-tuning, exhaust prompt engineering options:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Aspect&lt;/th&gt; &lt;th&gt;Fine-Tuning&lt;/th&gt; &lt;th&gt;Prompt Engineering&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Setup Cost&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;High (data curation, GPU compute, iteration)&lt;/td&gt; &lt;td&gt;Low (iterative prompt refinement)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Locked after training&lt;/td&gt; &lt;td&gt;Change anytime without retraining&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Format/Style&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Best for complex, consistent output formats&lt;/td&gt; &lt;td&gt;Good for simple formatting with few-shot examples&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Lower (no long system prompts)&lt;/td&gt; &lt;td&gt;Higher (context tax on every request)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Best For&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Complex behaviors, distillation, cost at scale&lt;/td&gt; &lt;td&gt;Rapid iteration, changing requirements&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;blockquote&gt; &lt;p&gt;[!TIP] &lt;strong&gt;Try Prompting First&lt;/strong&gt; Start with few-shot examples in your prompt. If you don&#39;t get consistent behavior, try SGR before jumping to fine-tuning.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;fine-tuning-vs-schema-guided-reasoning-sgr&#34;&gt;Fine-Tuning vs Schema-Guided Reasoning (SGR)&lt;/h3&gt; &lt;p&gt;Libraries like &lt;code&gt;xgrammar&lt;/code&gt; and &lt;code&gt;outlines&lt;/code&gt; constrain model outputs at inference time using Finite State Machines. They work with base models out of the box—&lt;strong&gt;no training required&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SGR isn&#39;t just about structured outputs.&lt;/strong&gt; Its primary value is &lt;strong&gt;consistency and reliability&lt;/strong&gt;. When prompting alone produces inconsistent results, SGR enforces deterministic output patterns without the cost and complexity of fine-tuning.&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Aspect&lt;/th&gt; &lt;th&gt;SGR (No Fine-Tuning)&lt;/th&gt; &lt;th&gt;Fine-Tuning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Immediate—define schema, deploy&lt;/td&gt; &lt;td&gt;Requires data curation, GPU compute, iteration&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Consistency&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Guaranteed structure, reliable patterns&lt;/td&gt; &lt;td&gt;Learned behavior (may still vary)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Change schema anytime without retraining&lt;/td&gt; &lt;td&gt;Locked after training&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Slight overhead (model may &#34;fight&#34; schema)&lt;/td&gt; &lt;td&gt;Lower (model naturally outputs format)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Best For&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Structured outputs, consistent behavior&lt;/td&gt; &lt;td&gt;Complex reasoning, deep behavioral changes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Start with prompting&lt;/strong&gt; — Simple few-shot examples for basic formatting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add SGR if inconsistent&lt;/strong&gt; — Use &lt;code&gt;xgrammar&lt;/code&gt; or &lt;code&gt;outlines&lt;/code&gt; to guarantee output structure and reliability&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tune as last resort&lt;/strong&gt; — Only when you need deep behavioral changes that schema constraints can&#39;t achieve&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;quick-reference-matching-problems-to-solutions&#34;&gt;Quick Reference: Matching Problems to Solutions&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Challenge&lt;/th&gt; &lt;th&gt;Best Solution&lt;/th&gt; &lt;th&gt;Why?&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Missing knowledge&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;RAG&lt;/td&gt; &lt;td&gt;Models hallucinate facts. Retrieval provides grounded, up-to-date context&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Wrong format/tone&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Prompt Engineering&lt;/td&gt; &lt;td&gt;Modern models follow style instructions well via few-shot examples&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Inconsistent outputs&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;SGR (xgrammar)&lt;/td&gt; &lt;td&gt;Guaranteed structure and reliability without training&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Complex behavioral changes&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Fine-Tuning (SFT)&lt;/td&gt; &lt;td&gt;Deep persona, reasoning patterns, or multi-step workflows&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Safety/preference&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Alignment (DPO)&lt;/td&gt; &lt;td&gt;When outputs are correct but don&#39;t match preferences&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Latency/cost at scale&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Distillation (SFT)&lt;/td&gt; &lt;td&gt;Train smaller student model on larger teacher&#39;s outputs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Reduce model size&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Quantization&lt;/td&gt; &lt;td&gt;No training—compress weights (FP16→INT4) for faster inference&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;the-economic-case&#34;&gt;The Economic Case&lt;/h3&gt; &lt;p&gt;Fine-tuning shines in &lt;strong&gt;high-volume, stable-requirement scenarios&lt;/strong&gt;. Consider this: a robust RAG system might require 2,000 tokens of context on every call (system prompt + retrieved docs + few-shot examples). That&#39;s your &#34;context tax&#34; on every request.&lt;/p&gt; &lt;p&gt;A fine-tuned model can internalize those instructions, reducing your prompt from 2,000 tokens to 50. At scale, this pays for the training compute within weeks.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[!TIP] &lt;strong&gt;The Hybrid Approach&lt;/strong&gt; The industry sweet spot is often a fine-tuned smaller model (8B params) combined with lightweight RAG for facts. This often outperforms prompting a massive model (70B+) in both accuracy and cost.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&#34;types-of-fine-tuning&#34;&gt;Types of Fine-Tuning&lt;/h2&gt; &lt;p&gt;Before diving into the lifecycle, understand the three main approaches to fine-tuning:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Fine-Tuning Types&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/finetuning_types.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;1-continued-pre-training-unsupervised&#34;&gt;1. Continued Pre-training (Unsupervised)&lt;/h3&gt; &lt;p&gt;Continued pre-training extends the base model&#39;s knowledge by training on additional raw text &lt;strong&gt;without labels&lt;/strong&gt;. The model simply learns to predict the next token, just like during original pre-training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your domain has specialized vocabulary the base model doesn&#39;t know (medical, legal, financial)&lt;/li&gt; &lt;li&gt;You have large amounts of domain text but no labeled examples&lt;/li&gt; &lt;li&gt;The base model struggles with domain-specific terminology&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Training on millions of clinical notes so the model understands medical abbreviations, drug names, and clinical workflows.&lt;/p&gt; &lt;h3 id=&#34;2-supervised-fine-tuning-sft&#34;&gt;2. Supervised Fine-Tuning (SFT)&lt;/h3&gt; &lt;p&gt;SFT trains on labeled &lt;strong&gt;(input, output) pairs&lt;/strong&gt;. You show the model exactly what output you expect for each input.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You have a specific task with clear input/output format&lt;/li&gt; &lt;li&gt;Quality labeled data is available (even small amounts)&lt;/li&gt; &lt;li&gt;You need consistent, predictable behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Training on (SQL query description, SQL code) pairs for Text-to-SQL conversion.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Get all users who signed up last month&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;SELECT * FROM users WHERE signup_date &amp;gt;= DATE_SUB(NOW(), INTERVAL 1 MONTH)&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;3-instruction-tuning&#34;&gt;3. Instruction Tuning&lt;/h3&gt; &lt;p&gt;Instruction tuning is a &lt;strong&gt;special case of SFT&lt;/strong&gt; designed to make models follow diverse natural language instructions. The training data consists of (instruction, response) pairs across many different tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want a general-purpose assistant (like ChatGPT or Claude)&lt;/li&gt; &lt;li&gt;The model needs to handle varied, open-ended requests&lt;/li&gt; &lt;li&gt;You&#39;re building a chat interface&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Training on thousands of diverse instructions like &#34;Summarize this article,&#34; &#34;Write a poem about X,&#34; &#34;Explain Y in simple terms.&#34;&lt;/p&gt; &lt;h3 id=&#34;comparison&#34;&gt;Comparison&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Aspect&lt;/th&gt; &lt;th&gt;Continued Pre-training&lt;/th&gt; &lt;th&gt;SFT&lt;/th&gt; &lt;th&gt;Instruction Tuning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Raw text&lt;/td&gt; &lt;td&gt;(input, output) pairs&lt;/td&gt; &lt;td&gt;(instruction, response) pairs&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Labels&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;None (unsupervised)&lt;/td&gt; &lt;td&gt;Task-specific&lt;/td&gt; &lt;td&gt;Diverse tasks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Domain knowledge&lt;/td&gt; &lt;td&gt;Specific task behavior&lt;/td&gt; &lt;td&gt;Follow any instruction&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Data Volume&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Large (millions of tokens)&lt;/td&gt; &lt;td&gt;Small-Medium (500-10k examples)&lt;/td&gt; &lt;td&gt;Medium-Large (10k-100k examples)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;blockquote&gt; &lt;p&gt;[!NOTE] &lt;strong&gt;Most Common Approach&lt;/strong&gt; In practice, most practitioners use &lt;strong&gt;SFT&lt;/strong&gt; for specific tasks or &lt;strong&gt;Instruction Tuning&lt;/strong&gt; for chat applications. Continued pre-training is rarer because it requires massive amounts of domain text and is computationally expensive.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&#34;the-7-stage-fine-tuning-lifecycle&#34;&gt;The 7-Stage Fine-Tuning Lifecycle&lt;/h2&gt; &lt;p&gt;Fine-tuning isn&#39;t a single action—it&#39;s a structured lifecycle. Understanding this pipeline is critical for success.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;7-Stage Pipeline&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/pipeline_7_stages.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Each stage builds on the previous one:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt; — Clean, deduplicate, and format your data (highest leverage step)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Selection&lt;/strong&gt; — Choose the right base model and load weights&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Setup&lt;/strong&gt; — Configure hardware, hyperparameters, and optimization strategy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-Tuning&lt;/strong&gt; — Run SFT, DPO, or ORPO training&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt; — Benchmark performance and validate quality&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt; — Export and serve your model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt; — Track performance, maintain, and iterate&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;[!WARNING] &lt;strong&gt;Data is the Foundation&lt;/strong&gt; Stage 1 (Dataset Preparation) is the highest leverage step. Flaws in your data cannot be fixed by algorithms later. &lt;strong&gt;Quality over quantity&lt;/strong&gt; — 500-1,000 carefully curated examples often outperform 50,000 noisy ones.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-1-data-preparation-data-is-the-foundation&#34;&gt;Stage 1: Data Preparation — &#34;Data is the Foundation&#34;&lt;/h2&gt; &lt;p&gt;This is where most fine-tuning projects succeed or fail. The industry has moved far beyond simple &#34;clean and format&#34; scripts.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Data Pipeline&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/data_pipeline.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;the-5-stage-data-pipeline&#34;&gt;The 5-Stage Data Pipeline&lt;/h3&gt; &lt;p&gt;Modern production pipelines use tools like &lt;strong&gt;DataTrove&lt;/strong&gt; (Hugging Face) and &lt;strong&gt;Distilabel&lt;/strong&gt; (Argilla) rather than custom scripts:&lt;/p&gt; &lt;h4 id=&#34;1-ingestion-filtering&#34;&gt;1. Ingestion &amp;amp; Filtering&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: Remove &#34;refusals&#34; (e.g., &#34;I cannot answer that&#34;), broken UTF-8, non-target languages&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Trafilatura (extraction) + FastText (language ID)&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;2-pii-scrubbing-enterprise-critical&#34;&gt;2. PII Scrubbing (Enterprise Critical)&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: Detect and redact emails, IP addresses, phone numbers before training&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Microsoft Presidio or scrubadub&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt;: Training on customer PII is a critical security failure&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;3-deduplication-minhash-lsh&#34;&gt;3. Deduplication (MinHash LSH)&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: Remove near-duplicates to prevent memorization&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: DataTrove (industry standard for terabyte-scale processing)&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;4-synthetic-augmentation-the-2025-secret&#34;&gt;4. Synthetic Augmentation (The 2025 Secret)&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: Use a stronger &#34;teacher&#34; model (GPT-4o, DeepSeek-V3) to rewrite raw data into high-quality instruction-response pairs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Distilabel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: This step often provides the biggest quality boost&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;5-formatting&#34;&gt;5. Formatting&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: Convert to standard formats (Alpaca or ShareGPT)&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;data-format-examples&#34;&gt;Data Format Examples&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Alpaca Format&lt;/strong&gt; (Instruction-Following):&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;instruction&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Summarize the following text.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;The text to be summarized...&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;This is the summary.&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;ShareGPT/ChatML Format&lt;/strong&gt; (Conversational):&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;conversations&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;from&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Hello, who are you?&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;from&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;assistant&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;I am a helpful AI assistant.&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;key-principles&#34;&gt;Key Principles&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quality over Quantity&lt;/strong&gt;: 500-1,000 carefully curated examples often outperform 50,000 noisy ones&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cleanliness&lt;/strong&gt;: Remove irrelevant information, normalize text, ensure consistent formatting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Balance&lt;/strong&gt;: Ensure representation across different topics to prevent bias&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enterprise Critical&lt;/strong&gt;: PII scrubbing (Stage 2) is mandatory for production systems&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-2-model-selection-hardware-requirements&#34;&gt;Stage 2: Model Selection &amp;amp; Hardware Requirements&lt;/h2&gt; &lt;p&gt;Choosing the right base model and understanding hardware constraints is critical for project success.&lt;/p&gt; &lt;h3 id=&#34;hardware-requirements-by-model-size&#34;&gt;Hardware Requirements by Model Size&lt;/h3&gt; &lt;p&gt;The physics of fine-tuning impose strict memory constraints:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Tier&lt;/th&gt; &lt;th&gt;Hardware Config&lt;/th&gt; &lt;th&gt;Capability&lt;/th&gt; &lt;th&gt;Use Case&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Enterprise Standard&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8x NVIDIA H100 (80GB)&lt;/td&gt; &lt;td&gt;Full Fine-Tuning&lt;/td&gt; &lt;td&gt;Training 70B models with long context (32k+) at max speed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Minimum Viable (Pro)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;4x NVIDIA A100 (80GB)&lt;/td&gt; &lt;td&gt;QLoRA / LoRA&lt;/td&gt; &lt;td&gt;Fine-tuning Qwen 72B or Llama 70B in 4-bit&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Local R&amp;amp;D&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;4x RTX 6000 Ada (48GB)&lt;/td&gt; &lt;td&gt;QLoRA&lt;/td&gt; &lt;td&gt;On-prem workstation for data privacy requirements&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Hobbyist/Indie&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;1-2x RTX 3090/4090 (24GB)&lt;/td&gt; &lt;td&gt;QLoRA&lt;/td&gt; &lt;td&gt;Fine-tune 7B-32B models with parameter-efficient methods&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;blockquote&gt; &lt;p&gt;[!TIP] &lt;strong&gt;Consumer GPUs Are More Capable Than You Think&lt;/strong&gt; With QLoRA and optimized frameworks like Unsloth, a single RTX 4090 (24GB) can fine-tune models up to 32B parameters. RTX 3090s remain excellent value for 7B-13B model training. The 24GB VRAM sweet spot makes these cards highly capable for serious fine-tuning work.&lt;/p&gt; &lt;p&gt;[!NOTE] &lt;strong&gt;Why H100s?&lt;/strong&gt; It&#39;s not just VRAM—it&#39;s &lt;strong&gt;FP8 precision&lt;/strong&gt;. H100s support native FP8 training, which effectively doubles memory capacity and throughput compared to A100s. For long-context models (128k tokens), FP8 on H100s is often the only way to fit reasonable batch sizes.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;memory-calculations&#34;&gt;Memory Calculations&lt;/h3&gt; &lt;p&gt;To fine-tune a 72B model, you need to store:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Weights&lt;/strong&gt; (16-bit): ~144 GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gradients &amp;amp; Optimizer States&lt;/strong&gt;: ~2-3x the model size (depending on optimizer, e.g., AdamW)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Activations&lt;/strong&gt;: Scales with context length (e.g., 32k tokens)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is why QLoRA (4-bit quantization + LoRA adapters) is essential for most teams.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-3-training-methods-peft-lora&#34;&gt;Stage 3: Training Methods (PEFT &amp;amp; LoRA)&lt;/h2&gt; &lt;h3 id=&#34;full-fine-tuning-vs-peft&#34;&gt;Full Fine-Tuning vs PEFT&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Full Fine-Tuning (FFT)&lt;/strong&gt; updates all model weights. For a 7B model at 16-bit precision, you need roughly 112GB of VRAM just for training. This is prohibitive for most teams.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Parameter-Efficient Fine-Tuning (PEFT)&lt;/strong&gt; changes the game by updating only a small subset of parameters while freezing the rest.&lt;/p&gt; &lt;h3 id=&#34;lora-the-industry-standard&#34;&gt;LoRA: The Industry Standard&lt;/h3&gt; &lt;p&gt;LoRA (Low-Rank Adaptation) is the foundational PEFT technique. The key insight: weight changes during fine-tuning have low &#34;intrinsic rank.&#34;&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;LoRA Architecture&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/lora_architecture.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Instead of updating a massive weight matrix &lt;strong&gt;W&lt;/strong&gt; (dimension d×d), LoRA learns two smaller matrices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt; (d × r) — down-projection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;B&lt;/strong&gt; (r × d) — up-projection&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The update becomes: &lt;strong&gt;ΔW = B × A&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;With rank &lt;code&gt;r=16&lt;/code&gt;, this reduces trainable parameters by &lt;strong&gt;~10,000x&lt;/strong&gt;, dropping VRAM from 120GB to 16GB.&lt;/p&gt; &lt;h3 id=&#34;peft-methods-compared&#34;&gt;PEFT Methods Compared&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Method&lt;/th&gt; &lt;th&gt;How It Works&lt;/th&gt; &lt;th&gt;Memory Savings&lt;/th&gt; &lt;th&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;LoRA&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Low-rank matrices injected into frozen weights&lt;/td&gt; &lt;td&gt;~10x&lt;/td&gt; &lt;td&gt;General fine-tuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;LoRA + 4-bit base model quantization&lt;/td&gt; &lt;td&gt;~20x&lt;/td&gt; &lt;td&gt;Consumer GPUs (16-24GB)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DoRA&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;LoRA with magnitude/direction decomposition&lt;/td&gt; &lt;td&gt;~10x&lt;/td&gt; &lt;td&gt;When LoRA hits performance ceiling&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HFT&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Freezes half parameters per training round&lt;/td&gt; &lt;td&gt;~2x&lt;/td&gt; &lt;td&gt;Balance between FFT and PEFT&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h4 id=&#34;when-to-use-which&#34;&gt;When to Use Which?&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: Start here. It&#39;s fast, memory-efficient, and widely supported&lt;/li&gt; &lt;li&gt;&lt;strong&gt;QLoRA&lt;/strong&gt;: When you need to fine-tune 70B models on consumer hardware&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DoRA&lt;/strong&gt;: When you need to match full fine-tuning quality on complex reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HFT&lt;/strong&gt;: When you need better performance than LoRA but can&#39;t afford full fine-tuning&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;dora-weight-decomposed-lora&#34;&gt;DoRA: Weight-Decomposed LoRA&lt;/h3&gt; &lt;p&gt;DoRA (Weight-Decomposed Low-Rank Adaptation) is a novel technique that bridges the performance gap between standard LoRA and full fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;DoRA Architecture&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/dora_architecture.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of treating weights as a single entity, DoRA decomposes pre-trained weights into two components:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Magnitude&lt;/strong&gt; — Trainable scalar per column (controls &#34;strength&#34;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Direction&lt;/strong&gt; — Updated with LoRA matrices (controls &#34;what&#34;)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The update becomes: &lt;strong&gt;W&#39; = m × (V + B × A)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;m&lt;/code&gt; = magnitude (trainable)&lt;/li&gt; &lt;li&gt;&lt;code&gt;V&lt;/code&gt; = direction (W / ||W||)&lt;/li&gt; &lt;li&gt;&lt;code&gt;B × A&lt;/code&gt; = LoRA update to direction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it outperforms standard LoRA:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Richer parameter updates while maintaining efficiency&lt;/li&gt; &lt;li&gt;Achieves learning outcomes closer to full fine-tuning&lt;/li&gt; &lt;li&gt;Same memory efficiency (~10x savings)&lt;/li&gt; &lt;li&gt;Particularly effective on complex reasoning tasks&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;half-fine-tuning-hft&#34;&gt;Half Fine-Tuning (HFT)&lt;/h3&gt; &lt;p&gt;HFT offers a unique balance between full fine-tuning and PEFT methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Methodology&lt;/strong&gt;: Freezes half of the model&#39;s parameters during each fine-tuning round while updating the other half&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strategy&lt;/strong&gt;: The frozen and active halves vary across rounds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benefit&lt;/strong&gt;: Retains foundational knowledge (frozen params) while acquiring new skills (active params)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use case&lt;/strong&gt;: When LoRA is insufficient but full fine-tuning is too expensive&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;adapter-merging-for-multi-task-learning&#34;&gt;Adapter Merging for Multi-Task Learning&lt;/h3&gt; &lt;p&gt;Instead of fine-tuning a monolithic model for multiple tasks, train separate small adapter modules for each function while keeping the base LLM frozen.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Merging Methods:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Concatenation&lt;/strong&gt; — Combines adapter parameters, increasing rank (fast, simple)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Linear Combination&lt;/strong&gt; — Weighted sum of adapters (more control)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SVD&lt;/strong&gt; — Matrix decomposition for merging (versatile but slower)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Example use case:&lt;/strong&gt; One adapter for summarization, another for translation, merged into a single multi-task model.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-4-fine-tuning-preference-alignment&#34;&gt;Stage 4: Fine-Tuning &amp;amp; Preference Alignment&lt;/h2&gt; &lt;p&gt;When SFT isn&#39;t enough—the model technically answers correctly but in &#34;wrong&#34; ways (too verbose, unsafe, wrong tone)—you need preference alignment.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Alignment Methods&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/alignment_methods.svg&#34; /&gt;&lt;/p&gt; &lt;h4 id=&#34;traditional-approach-rlhf-with-ppo&#34;&gt;Traditional Approach: RLHF with PPO&lt;/h4&gt; &lt;p&gt;The old standard was a complex 3-stage pipeline:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;SFT&lt;/strong&gt; — Learn the task&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reward Model&lt;/strong&gt; — Train on human preferences (chosen vs rejected)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PPO (Proximal Policy Optimization)&lt;/strong&gt; — Reinforcement learning to optimize policy&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Complex to implement and manage&lt;/li&gt; &lt;li&gt;Computationally expensive (training multiple models)&lt;/li&gt; &lt;li&gt;Unstable training (hyperparameter sensitive)&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;modern-streamlined-dpo&#34;&gt;Modern Streamlined: DPO&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;DPO (Direct Preference Optimization)&lt;/strong&gt; simplifies preference alignment by eliminating the explicit reward model and RL training loop. While DPO optimizes the same objective as RLHF (reward maximization with KL-divergence constraint), it achieves this through a reparameterized supervised learning objective rather than explicit reinforcement learning:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;prompt&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Explain quantum computing&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;chosen&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Quantum computing uses qubits...&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Preferred response&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;rejected&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Well, it&amp;#39;s complicated...&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Non-preferred response&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simpler implementation (no explicit reward model or RL training loop)&lt;/li&gt; &lt;li&gt;More stable training (supervised learning approach)&lt;/li&gt; &lt;li&gt;Less compute required&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The PPO vs DPO Debate:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Recent research suggests the debate isn&#39;t settled:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DPO may yield biased solutions in some scenarios&lt;/li&gt; &lt;li&gt;Well-tuned PPO can still achieve state-of-the-art results, particularly in complex tasks like code generation&lt;/li&gt; &lt;li&gt;PPO&#39;s explicit reward signal provides more granular guidance for specialized tasks&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;newest-single-stage-orpo&#34;&gt;Newest Single-Stage: ORPO&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;ORPO (Odds-Ratio Preference Optimization)&lt;/strong&gt; is the 2025 recommendation for most use cases. It combines SFT and preference alignment into a &lt;strong&gt;single training stage&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ORPO uses a combined loss function that simultaneously:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Maximizes likelihood&lt;/strong&gt; of the chosen response (learning the task)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Penalizes&lt;/strong&gt; the rejected response using an odds-ratio term (learning preferences)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Key Hyperparameters:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;trl&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ORPOConfig&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ORPOConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8e-6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Very low, as recommended by the ORPO paper&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Controls strength of preference penalty&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# ... other params&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: Use very low values (8e-6) as recommended by the ORPO paper&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Beta&lt;/strong&gt;: Controls the strength of the preference penalty (typically 0.1)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ Single training stage (no separate SFT needed)&lt;/li&gt; &lt;li&gt;✅ No reward model required&lt;/li&gt; &lt;li&gt;✅ Fastest path to production&lt;/li&gt; &lt;li&gt;✅ Simpler than DPO (fewer hyperparameters)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;When to use what:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ORPO&lt;/strong&gt;: Start here for most use cases (fastest, simplest)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DPO&lt;/strong&gt;: When you need more control over the alignment process&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;: Only for specialized tasks requiring explicit reward signals (e.g., code generation)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;fine-tuning-frameworks&#34;&gt;Fine-Tuning Frameworks&lt;/h2&gt; &lt;p&gt;The ecosystem has consolidated around four major tools:&lt;/p&gt; &lt;h3 id=&#34;unsloth-speed-efficiency-champion&#34;&gt;Unsloth — Speed &amp;amp; Efficiency Champion&lt;/h3&gt; &lt;p&gt;Unsloth uses HuggingFace packages (&lt;code&gt;trl&lt;/code&gt; and &lt;code&gt;transformers&lt;/code&gt;) under the hood but adds additional optimizations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Custom Triton kernels&lt;/strong&gt; — Hand-written GPU kernels for attention, RoPE, and cross-entropy that bypass PyTorch overhead&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory-efficient backpropagation&lt;/strong&gt; — Recomputes activations during backward pass instead of storing them&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fused operations&lt;/strong&gt; — Combines multiple operations (layer norm + linear, etc.) into single GPU calls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;4-bit quantization integration&lt;/strong&gt; — Seamless QLoRA with optimized dequantization&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Import Order Matters&lt;/strong&gt; Because Unsloth patches HuggingFace packages, you &lt;strong&gt;must&lt;/strong&gt; import and initialize Unsloth&#39;s &lt;code&gt;FastLanguageModel&lt;/code&gt; &lt;strong&gt;before&lt;/strong&gt; importing &lt;code&gt;trl&lt;/code&gt; or &lt;code&gt;transformers&lt;/code&gt;. Incorrect import order will cause failures.&lt;/p&gt; &lt;/blockquote&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# ✅ Correct order&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;unsloth&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastLanguageModel&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Must be first!&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;trl&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SFTTrainer&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;transformers&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TrainingArguments&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# ❌ Wrong order - will fail&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;trl&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SFTTrainer&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;unsloth&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastLanguageModel&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Too late!&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Single-GPU training, prototyping, Colab notebooks, anyone paying for GPU hours.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key advantage:&lt;/strong&gt; Custom Triton kernels make it 2-5x faster than standard HuggingFace implementations.&lt;/p&gt; &lt;h3 id=&#34;axolotl-config-driven-production&#34;&gt;Axolotl — Config-Driven Production&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# config.yaml - no code required&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;base_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;meta-llama/Meta-Llama-3-8B&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;adapter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;qlora&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;lora_r&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;lora_alpha&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;16&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;data/my_data.jsonl&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;alpaca&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;sample_packing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;true&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Run with: &lt;code&gt;accelerate launch -m axolotl.cli.train config.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Production pipelines, multi-GPU clusters, reproducible experiments.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key advantage:&lt;/strong&gt; YAML configs are version-controllable and shareable.&lt;/p&gt; &lt;h3 id=&#34;framework-comparison&#34;&gt;Framework Comparison&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Feature&lt;/th&gt; &lt;th&gt;Unsloth&lt;/th&gt; &lt;th&gt;Axolotl&lt;/th&gt; &lt;th&gt;TRL&lt;/th&gt; &lt;th&gt;Torchtune&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Strength&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Speed &amp;amp; efficiency&lt;/td&gt; &lt;td&gt;Multi-GPU scale&lt;/td&gt; &lt;td&gt;Ecosystem&lt;/td&gt; &lt;td&gt;PyTorch native&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Fastest (2-5x)&lt;/td&gt; &lt;td&gt;High&lt;/td&gt; &lt;td&gt;Moderate&lt;/td&gt; &lt;td&gt;High&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Multi-GPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Growing&lt;/td&gt; &lt;td&gt;Excellent&lt;/td&gt; &lt;td&gt;Good&lt;/td&gt; &lt;td&gt;Excellent&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Config&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Python&lt;/td&gt; &lt;td&gt;YAML&lt;/td&gt; &lt;td&gt;Python&lt;/td&gt; &lt;td&gt;Python&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Best for&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Local/Colab&lt;/td&gt; &lt;td&gt;Clusters&lt;/td&gt; &lt;td&gt;Research&lt;/td&gt; &lt;td&gt;PyTorch purists&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;practical-demo-fine-tuning-with-unsloth&#34;&gt;Practical Demo: Fine-Tuning with Unsloth&lt;/h2&gt; &lt;p&gt;Let&#39;s walk through a complete example using my &lt;a href=&#34;https://github.com/slavadubrov/unsloth-finetune-demo&#34;&gt;unsloth-finetune-demo&lt;/a&gt; repository. This demo fine-tunes Nemotron-Nano for function calling.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Training Pipeline&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/training_pipeline.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Clone and setup&lt;/span&gt; git&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;clone&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;https://github.com/slavadubrov/unsloth-finetune-demo.git &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;unsloth-finetune-demo &lt;span class=&#34;c1&#34;&gt;# Install with uv (recommended)&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;sync &lt;span class=&#34;c1&#34;&gt;# Run fine-tuning (quick test)&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;finetune&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--max-samples&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;configuration-deep-dive&#34;&gt;Configuration Deep Dive&lt;/h3&gt; &lt;p&gt;The key configuration lives in &lt;code&gt;config.py&lt;/code&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Model &amp;amp; Dataset&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MODEL_NAME&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 4B params, 128K context&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DATASET_NAME&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;glaiveai/glaive-function-calling-v2&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 113K examples&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# LoRA Configuration&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LORA_R&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Rank - higher = smarter but more VRAM&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LORA_ALPHA&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Scaling factor - usually 2x LORA_R&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MAX_SEQ_LENGTH&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4096&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Target all linear layers for best quality&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LORA_TARGET_MODULES&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;q_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;k_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;v_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;o_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;gate_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;up_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;down_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;blockquote&gt; &lt;p&gt;[!NOTE] &lt;strong&gt;The Alpha/Rank Ratio&lt;/strong&gt; Industry best practice in 2025: set &lt;strong&gt;alpha = 2 × rank&lt;/strong&gt; (e.g., rank=16, alpha=32). This provides stronger weight updates without destabilizing training.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;core-training-code&#34;&gt;Core Training Code&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;unsloth&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastLanguageModel&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;trl&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SFTTrainer&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;transformers&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TrainingArguments&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Load model with 4-bit quantization&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenizer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastLanguageModel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_pretrained&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_seq_length&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4096&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_in_4bit&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Add LoRA adapters&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastLanguageModel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_peft_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lora_alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_modules&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;q_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;k_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;v_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;o_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;gate_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;up_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;down_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;use_gradient_checkpointing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;unsloth&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Magic sauce for memory&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Train with SFTTrainer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;trainer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SFTTrainer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train_dataset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_seq_length&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4096&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;packing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Crucial for efficiency!&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrainingArguments&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;per_device_train_batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gradient_accumulation_steps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2e-4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_train_epochs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bf16&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;trainer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;fine-tuning-with-axolotl&#34;&gt;Fine-Tuning with Axolotl&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;[!NOTE] &lt;strong&gt;Demo Coming Soon&lt;/strong&gt; I&#39;m currently working on a hands-on Axolotl demo—stay tuned! In the meantime, check out the &lt;a href=&#34;https://huggingface.co/blog/accelerate-nd-parallel&#34;&gt;Accelerate n-D Parallelism Guide&lt;/a&gt; from Hugging Face for multi-GPU training strategies.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;For production and multi-GPU setups, Axolotl&#39;s config-first approach excels:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# axolotl_config.yaml&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;base_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;meta-llama/Meta-Llama-3-8B&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;model_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;LlamaForCausalLM&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# QLoRA configuration&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;load_in_4bit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;adapter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;qlora&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;lora_r&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;32&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;lora_alpha&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;16&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;lora_dropout&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;0.05&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;lora_target_modules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;q_proj&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;k_proj&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;v_proj&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;o_proj&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;gate_proj&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;up_proj&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;down_proj&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Dataset&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p p-Indicator&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;data/training_data.jsonl&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;alpaca&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Training settings&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;sequence_len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;4096&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;sample_packing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# Critical for speed!&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;micro_batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;gradient_accumulation_steps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;4&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;0.0002&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;num_epochs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;3&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Hardware&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;bf16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;true&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;flash_attention&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;true&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Run training:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;accelerate&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;launch&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-m&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;axolotl.cli.train&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;axolotl_config.yaml &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-5-evaluation&#34;&gt;Stage 5: Evaluation&lt;/h2&gt; &lt;p&gt;Training is easy; knowing if it worked is hard. Evaluation should happen before deployment.&lt;/p&gt; &lt;h3 id=&#34;automated-benchmarks&#34;&gt;Automated Benchmarks&lt;/h3&gt; &lt;p&gt;Use &lt;code&gt;lm-evaluation-harness&lt;/code&gt; for standardized testing:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;lm_eval&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--model&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;hf&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--model_args&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;pretrained&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;./outputs/merged-model&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--tasks&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;hellaswag,arc_easy,mmlu&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--batch_size&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;llm-as-judge&#34;&gt;LLM-as-Judge&lt;/h3&gt; &lt;p&gt;For subjective quality, use a larger model to evaluate:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;n&#34;&gt;judge_prompt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;Rate this response from 1-5 on:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;- Relevance&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;- Accuracy&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;- Formatting&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;Response: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{model_output}&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;Expected: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{ground_truth}&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;domain-specific-eval&#34;&gt;Domain-Specific Eval&lt;/h3&gt; &lt;p&gt;Create a held-out test set of real examples from your use case. This is the most important evaluation—generic benchmarks won&#39;t tell you if your function-calling model actually works.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-6-deployment-output-formats&#34;&gt;Stage 6: Deployment &amp;amp; Output Formats&lt;/h2&gt; &lt;p&gt;After training, you have three export options:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Output Formats&#34; src=&#34;../../../../assets/2026-01-04-finetuning-guide/output_formats.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;1-lora-adapter-default&#34;&gt;1. LoRA Adapter (Default)&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;finetune&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# Saves ~100-500MB adapter&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Size:&lt;/strong&gt; ~100-500 MB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Development, testing, multiple adapters on one base model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexibility:&lt;/strong&gt; Swap adapters without re-downloading the base model&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;2-merged-model&#34;&gt;2. Merged Model&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;finetune&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--merge&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# Creates standalone ~8-16GB model&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Size:&lt;/strong&gt; ~8-16 GB (full 16-bit weights)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Sharing on HuggingFace, vLLM serving, simple deployment&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Trade-off:&lt;/strong&gt; Larger storage, but no separate base model needed&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;3-gguf-format&#34;&gt;3. GGUF Format&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;finetune&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--gguf&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;q4_k_m&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# Creates ~2-4GB quantized model&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Size:&lt;/strong&gt; ~2-4 GB (Q4_K_M quantization)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; CPU inference, Ollama, llama.cpp, edge deployment&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Options:&lt;/strong&gt; &lt;code&gt;q4_k_m&lt;/code&gt; (balanced), &lt;code&gt;q5_k_m&lt;/code&gt; (higher quality), &lt;code&gt;q8_0&lt;/code&gt; (near-lossless)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;stage-7-serving-monitoring&#34;&gt;Stage 7: Serving &amp;amp; Monitoring&lt;/h2&gt; &lt;h3 id=&#34;with-vllm-production&#34;&gt;With vLLM (Production)&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Requires merged model format&lt;/span&gt; vllm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;serve&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;./outputs/unsloth-nemotron-function-calling-merged&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--host&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;.0.0.0&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--port&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--max-model-len&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4096&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Query via OpenAI-compatible API:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base_url&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;http://localhost:8000/v1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;api_key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;dummy&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;unsloth-nemotron-function-calling-merged&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Book a flight to Tokyo&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;with-ollama-local&#34;&gt;With Ollama (Local)&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Create Modelfile&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;FROM ./outputs/unsloth-nemotron-function-calling-gguf/model-q4_k_m.gguf&amp;#39;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Modelfile &lt;span class=&#34;c1&#34;&gt;# Import to Ollama&lt;/span&gt; ollama&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;create&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;my-function-model&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-f&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Modelfile &lt;span class=&#34;c1&#34;&gt;# Run&lt;/span&gt; ollama&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;my-function-model &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;with-llamacpp-cpu&#34;&gt;With llama.cpp (CPU)&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./main&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-m&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;./outputs/model-q4_k_m.gguf&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-p&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;What&amp;#39;s the weather in Tokyo?&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--ctx-size&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4096&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Follow the 7-stage lifecycle&lt;/strong&gt; — Data Preparation is the highest leverage step&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Don&#39;t default to fine-tuning&lt;/strong&gt; — Try RAG and prompting first; use the decision framework&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data is the foundation&lt;/strong&gt; — Use modern pipelines (DataTrove, Distilabel) with PII scrubbing for enterprise&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start with QLoRA&lt;/strong&gt; — Fine-tune 70B models on consumer GPUs (4x A100 minimum for production)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use ORPO for alignment&lt;/strong&gt; — Single-stage training is faster and simpler than DPO or PPO&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consider DoRA&lt;/strong&gt; — When LoRA hits performance ceiling on complex reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sample packing&lt;/strong&gt; is the single biggest training speedup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Start with Unsloth&lt;/strong&gt; for prototyping, &lt;strong&gt;Axolotl&lt;/strong&gt; for production&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Export to GGUF&lt;/strong&gt; for local/edge deployment&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt; &lt;h3 id=&#34;papers-research&#34;&gt;Papers &amp;amp; Research&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.14314&#34;&gt;QLoRA: Efficient Finetuning of Quantized LLMs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.09353&#34;&gt;DoRA: Weight-Decomposed Low-Rank Adaptation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.18290&#34;&gt;DPO: Direct Preference Optimization&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07691&#34;&gt;ORPO: Odds Ratio Preference Optimization&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34;&gt;PPO: Proximal Policy Optimization Algorithms&lt;/a&gt; — OpenAI, 2017&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.18466&#34;&gt;HFT: Half Fine-Tuning for Large Language Models&lt;/a&gt; — Mitigating catastrophic forgetting&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;data-processing-tools&#34;&gt;Data Processing Tools&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/datatrove&#34;&gt;DataTrove&lt;/a&gt; — Hugging Face data processing at scale&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/argilla-io/distilabel&#34;&gt;Distilabel&lt;/a&gt; — Synthetic data generation (Argilla)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/adbar/trafilatura&#34;&gt;Trafilatura&lt;/a&gt; — Web text extraction and crawling&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://fasttext.cc/&#34;&gt;FastText&lt;/a&gt; — Facebook AI language identification (supports 217 languages)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/presidio&#34;&gt;Microsoft Presidio&lt;/a&gt; — PII detection and anonymization&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/LeapBeyond/scrubadub&#34;&gt;scrubadub&lt;/a&gt; — Python library for PII removal&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;schema-guided-reasoning-sgr&#34;&gt;Schema-Guided Reasoning (SGR)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/mlc-ai/xgrammar&#34;&gt;xgrammar&lt;/a&gt; — Constrained decoding with FSMs&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/dottxt-ai/outlines&#34;&gt;outlines&lt;/a&gt; — Structured generation for LLMs&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;training-frameworks&#34;&gt;Training Frameworks&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://docs.unsloth.ai/&#34;&gt;Unsloth&lt;/a&gt; — Speed &amp;amp; efficiency champion (2-5x faster)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/axolotl-ai-cloud/axolotl&#34;&gt;Axolotl&lt;/a&gt; — Config-driven multi-GPU training&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/trl&#34;&gt;TRL (Transformer Reinforcement Learning)&lt;/a&gt; — Hugging Face RL training&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtune&#34;&gt;Torchtune&lt;/a&gt; — PyTorch-native fine-tuning library&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;inference-deployment&#34;&gt;Inference &amp;amp; Deployment&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://docs.vllm.ai/&#34;&gt;vLLM&lt;/a&gt; — High-throughput LLM serving engine&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://ollama.ai/&#34;&gt;Ollama&lt;/a&gt; — Local LLM runner for Mac/Windows/Linux&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; — CPU/GPU inference with GGUF format&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evaluation-harness&lt;/a&gt; — EleutherAI standardized LLM benchmarking&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;guides-resources&#34;&gt;Guides &amp;amp; Resources&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/slavadubrov/unsloth-finetune-demo&#34;&gt;Demo Repository&lt;/a&gt; — Practical fine-tuning example&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://notebooklm.google.com/notebook/f6bfdb56-8949-4929-87e4-ab6dee31a4a8&#34;&gt;LLM Fine-Tuning. Theoretical Intuition and Practical Implementation&lt;/a&gt; — NotebookLM research notebook&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/accelerate-nd-parallel&#34;&gt;Accelerate n-D Parallelism Guide&lt;/a&gt; — Hugging Face multi-GPU training strategies&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2026/01/04/the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production/</link> <pubDate>Sun, 04 Jan 2026 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2026/01/04/the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production/</guid> </item> <item> <title>Schema-Guided Reasoning on vLLM — Turning LLMs into Reliable Business Logic Engines</title> <author>Viacheslav Dubrov</author> <category>agents</category> <category>ai-engineering</category> <category>llm</category> <category>sgr</category> <category>structured-output</category> <category>vllm</category> <category>xgrammar</category> <description>&lt;h1 id=&#34;schema-guided-reasoning-on-vllm-turning-llms-into-reliable-business-logic-engines&#34;&gt;Schema-Guided Reasoning on vLLM — Turning LLMs into Reliable Business Logic Engines&lt;/h1&gt; &lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Schema-Guided Reasoning (SGR)&lt;/strong&gt; is a technique that forces LLMs to reason through predefined steps by enforcing structured output schemas. Instead of hoping the model follows your formatting instructions, you &lt;strong&gt;guarantee&lt;/strong&gt; it with constrained decoding. Combined with vLLM&#39;s xgrammar backend, you get 100% valid JSON output with near-zero latency overhead.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: You build an LLM-powered agent. It works in demos. In production, it outputs malformed JSON, skips reasoning steps, and gives inconsistent responses. You add retry loops, validation layers, larger models. Costs explode.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The fix&lt;/strong&gt;: Define your reasoning topology as a Pydantic schema. Let xgrammar enforce it at the token generation level. The LLM physically cannot produce invalid output.&lt;/p&gt; &lt;!-- more --&gt; &lt;hr /&gt; &lt;h2 id=&#34;what-is-schema-guided-reasoning&#34;&gt;What is Schema-Guided Reasoning?&lt;/h2&gt; &lt;p&gt;Schema-Guided Reasoning (SGR) is a technique pioneered by &lt;a href=&#34;https://abdullin.com/schema-guided-reasoning/&#34;&gt;Rinat Abdullin&lt;/a&gt; that guides LLMs to produce structured, clear, and predictable outputs by enforcing reasoning through predefined steps.&lt;/p&gt; &lt;p&gt;Instead of allowing free-form text completion (which can be inconsistent or ambiguous), the schema acts as a strict guideline. By creating a specific schema (or structured template), you explicitly define:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What steps&lt;/strong&gt; the model must go through (preventing skipped reasoning)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;In which order&lt;/strong&gt; it must reason (ensuring logical flow)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Where it should focus&lt;/strong&gt; attention (improving depth and accuracy)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it as giving the model a &#34;cognitive checklist&#34; that it &lt;strong&gt;must&lt;/strong&gt; follow.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;SGR Overview&#34; src=&#34;../../../../assets/2025-12-25-sgr-vllm/sgr_overview.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;why-sgr-matters&#34;&gt;Why SGR Matters&lt;/h3&gt; &lt;p&gt;The core insight is simple but powerful:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Schema = Cognitive Scaffold&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;When you define a schema with fields like &lt;code&gt;churn_analysis&lt;/code&gt;, &lt;code&gt;margin_math&lt;/code&gt;, and then &lt;code&gt;max_discount_percent&lt;/code&gt;, the model is &lt;strong&gt;forced&lt;/strong&gt; to populate these fields in order. It cannot jump to the discount decision without first analyzing the data.&lt;/p&gt; &lt;p&gt;This translates to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reproducible reasoning&lt;/strong&gt; — consistent inference across repeated runs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auditable outputs&lt;/strong&gt; — every reasoning step is explicit and inspectable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debuggable &amp;amp; testable&lt;/strong&gt; — intermediate outputs can be evaluated against test datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Works with smaller models&lt;/strong&gt; — the schema &#34;holds the hand&#34; of weaker models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;5-10% accuracy boost&lt;/strong&gt; — &lt;a href=&#34;https://abdullin.com/schema-guided-reasoning/&#34;&gt;commonly observed&lt;/a&gt; in production deployments&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;sgr-vs-chain-of-thought-vs-prompt-engineering&#34;&gt;SGR vs Chain of Thought vs Prompt Engineering&lt;/h2&gt; &lt;p&gt;Let&#39;s be precise about what makes SGR different from the approaches you&#39;re probably already using.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;SGR Comparison&#34; src=&#34;../../../../assets/2025-12-25-sgr-vllm/sgr_comparison.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;the-comparison&#34;&gt;The Comparison&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Feature&lt;/th&gt; &lt;th&gt;Prompt Engineering&lt;/th&gt; &lt;th&gt;Chain of Thought&lt;/th&gt; &lt;th&gt;Schema-Guided Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Output Structure&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Variable text&lt;/td&gt; &lt;td&gt;Free-form prose&lt;/td&gt; &lt;td&gt;Rigid JSON/Pydantic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Control Mechanism&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Semantic persuasion (&#34;Please output JSON&#34;)&lt;/td&gt; &lt;td&gt;Heuristic prompting (&#34;Let&#39;s think step by step&#34;)&lt;/td&gt; &lt;td&gt;Constrained decoding (grammar-based)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Reasoning Flow&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Model determines&lt;/td&gt; &lt;td&gt;Model determines&lt;/td&gt; &lt;td&gt;Developer determines (schema topology)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Auditability&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Low (requires parsing)&lt;/td&gt; &lt;td&gt;Low (requires reading prose)&lt;/td&gt; &lt;td&gt;High (field-level inspection)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Integration&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Difficult (regex parsing)&lt;/td&gt; &lt;td&gt;Difficult (variable format)&lt;/td&gt; &lt;td&gt;Trivial (native object deserialization)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Error Rate&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;High (format variability)&lt;/td&gt; &lt;td&gt;Moderate (hallucination of format)&lt;/td&gt; &lt;td&gt;Near-zero (syntax enforced by engine)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Model Requirement&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Strong instruction following&lt;/td&gt; &lt;td&gt;Strong reasoning capability&lt;/td&gt; &lt;td&gt;Works with smaller models too&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;prompt-engineering-semantic-persuasion&#34;&gt;Prompt Engineering: Semantic Persuasion&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Please analyze the customer data and output your response as valid JSON with the following structure: {&amp;quot;discount&amp;quot;: &amp;lt;number&amp;gt;, &amp;quot;reason&amp;quot;: &amp;lt;string&amp;gt;} Be careful with the formatting! &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: You&#39;re &lt;em&gt;hoping&lt;/em&gt; the model&#39;s semantic understanding of &#34;output JSON&#34; outweighs its tendency to be conversational. A model update, temperature change, or different few-shot examples can break your parser.&lt;/p&gt; &lt;h3 id=&#34;chain-of-thought-better-reasoning-same-structure-problems&#34;&gt;Chain of Thought: Better Reasoning, Same Structure Problems&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Let&amp;#39;s think step by step: 1. First, I&amp;#39;ll analyze the customer&amp;#39;s churn risk... 2. Then I&amp;#39;ll calculate the margin... 3. Therefore, I recommend a 15% discount. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;CoT improves reasoning &lt;strong&gt;accuracy&lt;/strong&gt; but makes structure &lt;strong&gt;worse&lt;/strong&gt;. The output is unpredictable prose that&#39;s nearly impossible to parse reliably. You end up needing a second LLM call to extract structured data from the reasoning.&lt;/p&gt; &lt;h3 id=&#34;sgr-structured-chain-of-thought&#34;&gt;SGR: Structured Chain of Thought&lt;/h3&gt; &lt;p&gt;SGR doesn&#39;t abandon CoT&#39;s insight that intermediate reasoning improves accuracy. It &lt;strong&gt;formalizes&lt;/strong&gt; it:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;PricingLogic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Data Analysis (must complete before decision)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;churn_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Analyze churn_probability&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;financial_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Analyze cart_value and margin&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Math Enforcement (explicit calculation)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;margin_math&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Calculate: &amp;#39;Cart $X * Y% = $Z&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. Decision Constraint (bounded by prior analysis)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_discount_percent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Max allowed discount&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 4. Final Output&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offer_code&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;customer_message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The model &lt;strong&gt;cannot&lt;/strong&gt; output &lt;code&gt;max_discount_percent&lt;/code&gt; without first populating &lt;code&gt;churn_analysis&lt;/code&gt;, &lt;code&gt;financial_analysis&lt;/code&gt;, and &lt;code&gt;margin_math&lt;/code&gt;. The schema enforces the reasoning order.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;sgr-patterns&#34;&gt;SGR Patterns&lt;/h2&gt; &lt;p&gt;SGR enables three foundational patterns for controlling LLM reasoning. These can be combined for complex workflows.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;SGR Patterns&#34; src=&#34;../../../../assets/2025-12-25-sgr-vllm/sgr_patterns.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;1-cascade-sequential-reasoning-steps&#34;&gt;1. Cascade: Sequential Reasoning Steps&lt;/h3&gt; &lt;p&gt;Cascade ensures the model follows predefined reasoning steps in order. Each field must be completed before the next.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;typing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Annotated&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;annotated_types&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Ge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Le&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;CandidateEvaluation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Evaluate a job candidate with enforced reasoning order.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Step 1: Summarize (forces context awareness)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;brief_candidate_summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Step 2: Rate (bounded integer)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rate_skill_match&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Annotated&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Ge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Le&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Step 3: Decide (constrained choices)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;final_recommendation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;hire&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;reject&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;hold&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Candidate evaluation, document classification, compliance analysis, medical diagnosis&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: The model must complete &lt;code&gt;brief_candidate_summary&lt;/code&gt; before it can rate, and must rate before it can recommend. No shortcuts allowed.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;2-routing-semantic-switch-statement&#34;&gt;2. Routing: Semantic Switch Statement&lt;/h3&gt; &lt;p&gt;Routing forces the model to explicitly choose one path from multiple options. This is implemented using &lt;code&gt;Union&lt;/code&gt; types.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;typing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Union&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;FeatureLookup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Route to database lookup.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rationale&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;fetch_user_features&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;fetch_user_features&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;GeneralResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Standard response for non-pricing queries.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;respond&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;respond&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;RouterSchema&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;The model must pick exactly ONE branch.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Union&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;FeatureLookup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GeneralResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Intent classification, tool selection, support triage, multi-agent dispatch&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: The &lt;code&gt;Literal&lt;/code&gt; type with a discriminator field (like &lt;code&gt;tool_name&lt;/code&gt;) ensures the model commits to one branch and fills in the required fields for that specific path.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;3-cycle-repeated-reasoning-with-lists&#34;&gt;3. Cycle: Repeated Reasoning with Lists&lt;/h3&gt; &lt;p&gt;Cycle forces the model to produce multiple items, with constraints on minimum and maximum count.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;typing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Annotated&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;annotated_types&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinLen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MaxLen&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;RiskFactor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;explanation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;severity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;low&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;medium&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;high&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;RiskAssessment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Generate 2-4 risk factors.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;factors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Annotated&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RiskFactor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinLen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MaxLen&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Risk assessment, issue extraction, parallel tool calls, multi-step planning&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: The &lt;code&gt;MinLen&lt;/code&gt; and &lt;code&gt;MaxLen&lt;/code&gt; annotations force the model to generate at least 2 but no more than 4 items. Combined with Routing, this enables parallel tool dispatch.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;making-sgr-work-constrained-decoding&#34;&gt;Making SGR Work: Constrained Decoding&lt;/h2&gt; &lt;p&gt;The patterns above are powerful, but they&#39;re just Pydantic schemas — how do we actually &lt;strong&gt;enforce&lt;/strong&gt; them? The answer is &lt;strong&gt;Constrained Decoding&lt;/strong&gt; (also called Structured Output).&lt;/p&gt; &lt;p&gt;Constrained Decoding works by modifying the token generation process. Instead of allowing the model to freely sample from its vocabulary, the decoding engine applies a &lt;strong&gt;grammar mask&lt;/strong&gt; that blocks tokens that would violate the schema. This happens at the inference engine level, not in your application code.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[!TIP] SGR doesn&#39;t require &#34;reasoning models&#34; (like o1 or DeepSeek-R1). It works well with instruction-tuned models, and especially well with models distilled from reasoning models.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;supported-cloud-providers&#34;&gt;Supported Cloud Providers&lt;/h3&gt; &lt;p&gt;Most modern LLM providers now support Structured Outputs via constrained decoding:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Provider&lt;/th&gt; &lt;th&gt;Support&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/structured-outputs&#34;&gt;Structured Outputs&lt;/a&gt; (including Azure). &lt;a href=&#34;https://abdullin.com/schema-guided-reasoning/&#34;&gt;GPT-5 uses JSON Schema via llguidance&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Google/Gemini&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemini-api/docs/structured-output&#34;&gt;JSON Schema&lt;/a&gt; support since Nov 2025 (Pydantic and Zod)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/&#34;&gt;Custom Structured Output&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Grok&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://docs.x.ai/docs/guides/structured-outputs&#34;&gt;Structured Outputs&lt;/a&gt; for multiple models&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Fireworks AI&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://docs.fireworks.ai/structured-responses/structured-response-formatting&#34;&gt;JSON Schema&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Cerebras&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://inference-docs.cerebras.ai/capabilities/structured-outputs&#34;&gt;Structured Outputs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Depends on downstream provider, maps to JSON Schema&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;supported-inference-engines&#34;&gt;Supported Inference Engines&lt;/h3&gt; &lt;p&gt;For self-hosted models, most modern inference engines support constrained decoding:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Engine&lt;/th&gt; &lt;th&gt;Backend&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://github.com/mlc-ai/xgrammar&#34;&gt;xgrammar&lt;/a&gt; or &lt;a href=&#34;https://github.com/guidance-ai/llguidance&#34;&gt;guidance&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://github.com/dottxt-ai/outlines&#34;&gt;Outlines&lt;/a&gt;, &lt;a href=&#34;https://github.com/mlc-ai/xgrammar&#34;&gt;XGrammar&lt;/a&gt;, or &lt;a href=&#34;https://github.com/guidance-ai/llguidance&#34;&gt;llguidance&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-api/llm_guided_decoding.py&#34;&gt;GuidedDecoding&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href=&#34;https://ollama.com/blog/structured-outputs&#34;&gt;Structured Outputs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;why-this-article-focuses-on-vllm-xgrammar&#34;&gt;Why This Article Focuses on vLLM + xgrammar&lt;/h3&gt; &lt;p&gt;For this article, we&#39;ll dive deep into &lt;strong&gt;vLLM with the xgrammar backend&lt;/strong&gt; because:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Production-grade&lt;/strong&gt;: vLLM is the most widely deployed open-source LLM inference engine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero-overhead&lt;/strong&gt;: xgrammar is implemented in C++ with near-zero latency impact&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI-compatible API&lt;/strong&gt;: Easy migration from cloud to self-hosted&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full schema support&lt;/strong&gt;: Handles complex nested schemas, unions, and recursive structures&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let&#39;s look at how xgrammar actually enforces these schemas at the token level.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;how-xgrammar-enforces-schemas&#34;&gt;How xgrammar Enforces Schemas&lt;/h2&gt; &lt;p&gt;Now let&#39;s get precise about &lt;strong&gt;when&lt;/strong&gt; and &lt;strong&gt;how&lt;/strong&gt; xgrammar enforces your schema. Understanding this helps you debug and tune your SGR workflows.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;xgrammar Enforcement&#34; src=&#34;../../../../assets/2025-12-25-sgr-vllm/xgrammar_enforcement.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;where-does-masking-happen&#34;&gt;Where Does Masking Happen?&lt;/h3&gt; &lt;p&gt;Here&#39;s the key insight: &lt;strong&gt;xgrammar modifies the output logits AFTER the model&#39;s forward pass, BEFORE sampling&lt;/strong&gt;. It does not change the model itself — it filters what tokens can be selected.&lt;/p&gt; &lt;p&gt;The standard LLM inference loop looks like this:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;1. Input tokens → GPU Forward Pass → Logits (probability scores for all ~128K tokens) 2. Logits → Sampling (temperature, top-p, etc.) → Next Token 3. Repeat until done &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;xgrammar inserts itself between steps 1 and 2:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;1. Input tokens → GPU Forward Pass → Raw Logits 2. Raw Logits → xgrammar Logits Processor → Masked Logits 3. Masked Logits → Sampling → Next Token (guaranteed valid) 4. Repeat until done &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The critical point: &lt;strong&gt;the model computes its full probability distribution on the GPU first&lt;/strong&gt;. Then xgrammar, running on CPU, applies a bitmask to the logits before sampling. Invalid tokens get their logits set to &lt;code&gt;-∞&lt;/code&gt;, which makes their probability exactly 0 after softmax.&lt;/p&gt; &lt;h3 id=&#34;the-two-phase-process&#34;&gt;The Two-Phase Process&lt;/h3&gt; &lt;p&gt;xgrammar&#39;s efficiency comes from splitting the work into two phases:&lt;/p&gt; &lt;h4 id=&#34;phase-1-grammar-compilation-one-time-before-inference&#34;&gt;Phase 1: Grammar Compilation (one-time, before inference)&lt;/h4&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# This happens once per schema&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenizer_info&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xgr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TokenizerInfo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_huggingface&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;grammar_compiler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xgr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GrammarCompiler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenizer_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compiled_grammar&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;grammar_compiler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compile_json_schema&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;schema_json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;During compilation, xgrammar:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Converts your JSON Schema to a Context-Free Grammar (CFG)&lt;/li&gt; &lt;li&gt;Builds a Pushdown Automaton (PDA) — like a state machine with a stack for handling nested structures like &lt;code&gt;{&#34;a&#34;: {&#34;b&#34;: {&#34;c&#34;: ...}}}&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Pre-computes which tokens are valid at each grammar position (the &#34;adaptive token mask cache&#34;)&lt;/li&gt; &lt;li&gt;Categorizes tokens as &#34;context-independent&#34; (can be pre-checked) or &#34;context-dependent&#34; (must be checked at runtime based on stack state)&lt;/li&gt; &lt;/ol&gt; &lt;blockquote&gt; &lt;p&gt;[!NOTE] About 99% of tokens are context-independent and can be cached (&lt;a href=&#34;https://arxiv.org/abs/2411.15100&#34;&gt;XGrammar paper&lt;/a&gt;). This is why xgrammar is so fast — most validity checks are just cache lookups.&lt;/p&gt; &lt;/blockquote&gt; &lt;h4 id=&#34;phase-2-runtime-mask-generation-every-token&#34;&gt;Phase 2: Runtime Mask Generation (every token)&lt;/h4&gt; &lt;p&gt;At each generation step:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &lt;code&gt;GrammarMatcher&lt;/code&gt; tracks the current position in the grammar&lt;/li&gt; &lt;li&gt;It retrieves the pre-computed mask for context-independent tokens (cache lookup)&lt;/li&gt; &lt;li&gt;It runs the PDA to check the remaining context-dependent tokens&lt;/li&gt; &lt;li&gt;It combines these into a final bitmask and applies it to the logits&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;why-pushdown-automata-matter&#34;&gt;Why Pushdown Automata Matter&lt;/h3&gt; &lt;p&gt;You might wonder: why not just use regex? The answer is &lt;strong&gt;nesting&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;A regular expression (which is a Finite State Machine) cannot reliably match structures like:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;profile&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;settings&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;theme&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;dark&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The problem is matching the closing braces &lt;code&gt;}}}&lt;/code&gt; — you need to &#34;remember&#34; how many you opened. A Pushdown Automaton has a &lt;strong&gt;stack&lt;/strong&gt; that tracks this context, enabling it to handle arbitrary nesting depth.&lt;/p&gt; &lt;p&gt;This is why xgrammar can enforce complex schemas with Union types, nested objects, and recursive structures — capabilities that simpler regex-based approaches cannot match.&lt;/p&gt; &lt;h3 id=&#34;concrete-example-generating-a-float-field&#34;&gt;Concrete Example: Generating a Float Field&lt;/h3&gt; &lt;p&gt;When the model is generating &lt;code&gt;&#34;max_discount_percent&#34;:&lt;/code&gt;, xgrammar knows from the schema that a &lt;code&gt;float&lt;/code&gt; comes next. The mask:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Allows&lt;/strong&gt; (probability unchanged): &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt;, ..., &lt;code&gt;9&lt;/code&gt;, &lt;code&gt;.&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Blocks&lt;/strong&gt; (probability → 0): &lt;code&gt;&#34;&lt;/code&gt;, &lt;code&gt;{&lt;/code&gt;, &lt;code&gt;[&lt;/code&gt;, &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;, &lt;code&gt;null&lt;/code&gt;, and all 128K+ other tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model&#39;s forward pass might have assigned high probability to the word &lt;code&gt;&#34;fifteen&#34;&lt;/code&gt;. But after xgrammar&#39;s mask, that token has probability 0. The model &lt;strong&gt;must&lt;/strong&gt; output digits.&lt;/p&gt; &lt;h3 id=&#34;performance-why-near-zero-overhead&#34;&gt;Performance: Why &#34;Near-Zero Overhead&#34;?&lt;/h3&gt; &lt;p&gt;Three factors make xgrammar fast:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Parallel execution&lt;/strong&gt;: Mask computation (CPU) overlaps with the next forward pass (GPU). While the GPU computes logits for token N+1, the CPU computes the mask for token N.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Caching&lt;/strong&gt;: 99%+ of token validity is pre-computed during grammar compilation. Runtime checks are mostly cache lookups.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;C++ implementation&lt;/strong&gt;: The hot path is optimized C++, not Python. The mask is applied directly to logits in-place.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In benchmarks, xgrammar often shows &lt;strong&gt;negligible overhead&lt;/strong&gt; — and sometimes structured generation is &lt;em&gt;faster&lt;/em&gt; than unconstrained generation because the constrained vocabulary reduces sampling complexity.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;practical-implementation-with-vllm&#34;&gt;Practical Implementation with vLLM&lt;/h2&gt; &lt;p&gt;Let&#39;s look at a complete implementation using the &lt;a href=&#34;https://github.com/slavadubrov/sgr-discount-manager&#34;&gt;sgr-discount-manager&lt;/a&gt; project — a demo that shows SGR patterns for dynamic pricing.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Agent Workflow&#34; src=&#34;../../../../assets/2025-12-25-sgr-vllm/agent_workflow.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;project-structure&#34;&gt;Project Structure&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sgr/ ├── agent.py # Main orchestration ├── models/ │ └── schemas.py # Pydantic SGR schemas ├── prompts/ │ ├── routing.py # Phase 1 prompts │ └── pricing.py # Phase 3 prompts ├── store/ │ └── hybrid_store.py # Hot/Cold data retrieval └── utils/ └── llm_client.py # LLM client wrapper with xgrammar &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;step-1-define-your-schemas&#34;&gt;Step 1: Define Your Schemas&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# sgr/models/schemas.py&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;typing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Union&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# --- Phase 1: Routing (Union for branching) ---&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;FeatureLookup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Route to DB lookup if pricing context is needed.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rationale&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;fetch_user_features&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;fetch_user_features&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;GeneralResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Standard response for non-pricing queries.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Literal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;respond&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;respond&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;RouterSchema&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Union&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;FeatureLookup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GeneralResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# --- Phase 2: Pricing Logic (Cascade for sequential reasoning) ---&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;PricingLogic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Strict reasoning topology for dynamic pricing.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Fields are ordered to enforce the analysis→decision flow.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Data Analysis (Reflection)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;churn_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Analyze churn_probability (High &amp;gt; 0.7).&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;financial_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Analyze cart_value and profit_margin.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Hard Math Enforcement&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;margin_math&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Calculate absolute profit: &amp;#39;Cart $200 * 0.20 Margin = $40&amp;#39;.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. The Decision Constraint&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_discount_percent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Max allowed discount %. NEVER exceed margin.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 4. Final Output&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offer_code&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Generated code (e.g. SAVE20).&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;customer_message&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;The final polite offer text.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;step-2-create-the-llm-client-with-xgrammar&#34;&gt;Step 2: Create the LLM Client with xgrammar&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# sgr/utils/llm_client.py&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;typing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TypeVar&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;json&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TypeVar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;T&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bound&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;LLMClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Wrapper for vLLM with xgrammar-enforced structured generation.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base_url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;http://localhost:8000/v1&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base_url&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;base_url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;api_key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;EMPTY&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_get_available_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;_get_available_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Auto-detect the model running on vLLM server.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;models&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;models&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;models&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;models&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;pass&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Qwen/Qwen2.5-7B-Instruct&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;run_sgr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema_class&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Run inference with Schema-Guided Response constraints.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Uses vLLM&amp;#39;s guided_json with xgrammar backend to enforce&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; strict schema constraints at the token generation level.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema_class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_json_schema&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Enhance system message with schema for model guidance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enhanced_messages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;copy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enhanced_messages&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enhanced_messages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;system&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema_json&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dumps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;schema_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;indent&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enhanced_messages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;system&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;enhanced_messages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Respond with JSON matching this schema:&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;schema_json&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# The magic: vLLM&amp;#39;s guided_json with xgrammar backend&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;enhanced_messages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;temperature&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Low temp for deterministic reasoning&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extra_body&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;guided_json&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Pydantic schema as dict&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;guided_decoding_backend&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;xgrammar&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Hardware-enforced&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw_response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completion&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;choices&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;message&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema_class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_validate_json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;blockquote&gt; &lt;p&gt;[!NOTE] The &lt;code&gt;guided_json&lt;/code&gt; parameter accepts a JSON Schema dict. Combined with &lt;code&gt;guided_decoding_backend: &#34;xgrammar&#34;&lt;/code&gt;, this ensures the LLM can only generate tokens that form valid JSON matching your schema.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;step-3-orchestrate-the-agent&#34;&gt;Step 3: Orchestrate the Agent&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# sgr/agent.py&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;.models.schemas&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PricingLogic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RouterSchema&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;.prompts.routing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;build_routing_prompt&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;.prompts.pricing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;build_pricing_context_prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ASSISTANT_FETCH_MESSAGE&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;.store.hybrid_store&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HybridFeatureStore&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;.utils.llm_client&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LLMClient&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;pricing_agent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;user_query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Process a pricing query with three-phase SGR workflow.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;llm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LLMClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature_store&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HybridFeatureStore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Build conversation history&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;system&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;build_routing_prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user_query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# --- Phase 1: Routing (Uses RouterSchema) ---&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;🤖 Processing: &amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;user_query&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39; for &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;decision&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;llm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run_sgr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RouterSchema&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;📍 Routing decision: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;decision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;respond&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;decision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# --- Phase 2: Context Retrieval ---&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;decision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;action&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tool_name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;fetch_user_features&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;🔍 Fetching features for &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;...&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;context&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature_store&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_user_context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;user_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Error: User profile not found.&amp;quot;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot; [Data] LTV: $&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;user_ltv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | &amp;quot;&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Margin: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cart_profit_margin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;%&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Inject context into conversation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;assistant&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ASSISTANT_FETCH_MESSAGE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;build_pricing_context_prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;churn_prob&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;churn_probability&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cart_val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;current_cart_value&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;margin&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;cart_profit_margin&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user_ltv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;context&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;user_ltv&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;})&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# --- Phase 3: SGR Logic Execution (Uses PricingLogic) ---&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;🧠 Calculating Offer (Schema Enforced)...&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;llm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run_sgr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PricingLogic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Audit log — the SGR benefit: explicit reasoning traces&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot; [Audit] Math: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;offer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;margin_math&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot; [Audit] Max Allowed: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;offer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_discount_percent&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;%&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;customer_message&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;I&amp;#39;m sorry, I couldn&amp;#39;t process your request.&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pricing_agent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;I want a discount or I&amp;#39;m leaving!&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user_102&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;💬 Final Reply: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;step-4-run-vllm-with-xgrammar&#34;&gt;Step 4: Run vLLM with xgrammar&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Start vLLM server with xgrammar backend (default in recent versions)&lt;/span&gt; python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-m&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;vllm.entrypoints.openai.api_server&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--model&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Qwen/Qwen2.5-7B-Instruct&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--port&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Run the agent&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-m&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;sgr.agent &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;example-output&#34;&gt;Example Output&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;🤖 Processing: &amp;#39;I want a discount or I&amp;#39;m leaving!&amp;#39; for user_102 📍 Routing decision: fetch_user_features 🔍 Fetching features for user_102... [Data] LTV: $1,500 | Margin: 20% 🧠 Calculating Offer (Schema Enforced)... [Audit] Math: Cart $200 * 0.20 Margin = $40 [Audit] Max Allowed: 15.0% 💬 Final Reply: We value your loyalty! Here&amp;#39;s a special 15% discount with code SAVE15. This reflects our appreciation for your continued business with us. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The audit log shows exactly how the model reasoned: it calculated the margin ($40 on a $200 cart at 20% margin), and correctly bounded the discount to stay within the profit constraint.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h2&gt; &lt;h3 id=&#34;schema-design&#34;&gt;Schema Design&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Order fields by reasoning flow&lt;/strong&gt;: Put analysis fields before decision fields&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use descriptive Field descriptions&lt;/strong&gt;: They guide the model&#39;s attention&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Constrain with Literal and Annotated&lt;/strong&gt;: Use &lt;code&gt;Literal[&#34;a&#34;, &#34;b&#34;]&lt;/code&gt; for enums, &lt;code&gt;Annotated[int, Ge(1), Le(10)]&lt;/code&gt; for bounds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keep schemas focused&lt;/strong&gt;: One schema per reasoning phase, compose with multiple calls&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;vllm-configuration&#34;&gt;vLLM Configuration&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Use low temperature&lt;/strong&gt; (0.1-0.3) for deterministic reasoning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Let xgrammar handle structure&lt;/strong&gt;: Don&#39;t over-engineer prompts for formatting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monitor token usage&lt;/strong&gt;: SGR typically uses fewer tokens than CoT (no verbose prose)&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;production-considerations&#34;&gt;Production Considerations&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Schema versioning&lt;/strong&gt;: Track schema changes like API versions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallback handling&lt;/strong&gt;: Even with SGR, network/server errors need graceful handling&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audit logging&lt;/strong&gt;: Log raw SGR outputs for compliance and debugging&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test with edge cases&lt;/strong&gt;: Ensure schemas handle boundary conditions&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Schema-Guided Reasoning bridges the gap between the flexibility of LLMs and the reliability requirements of production systems. By defining your reasoning topology as a Pydantic schema and letting xgrammar enforce it, you get:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Guaranteed valid output&lt;/strong&gt; — no retry loops, no parsing failures&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explicit reasoning traces&lt;/strong&gt; — every step is auditable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smaller model viability&lt;/strong&gt; — the schema compensates for weaker instruction-following&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lower costs&lt;/strong&gt; — fewer tokens, no retries, smaller models work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The &lt;a href=&#34;https://github.com/slavadubrov/sgr-discount-manager&#34;&gt;sgr-discount-manager&lt;/a&gt; demo shows how these patterns work in practice. Clone it, run it, and adapt the schemas for your use case.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt; &lt;h3 id=&#34;sgr-framework&#34;&gt;SGR Framework&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://abdullin.com/schema-guided-reasoning/&#34;&gt;Schema-Guided Reasoning (SGR)&lt;/a&gt; — Rinat Abdullin&#39;s original framework&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://abdullin.com/schema-guided-reasoning/patterns&#34;&gt;SGR Patterns&lt;/a&gt; — Cascade, Routing, Cycle patterns&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;xgrammar&#34;&gt;xgrammar&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2411.15100&#34;&gt;XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models&lt;/a&gt; — Yixin Dong et al., arXiv:2411.15100 (technical paper with benchmarks)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/mlc-ai/xgrammar&#34;&gt;xgrammar GitHub&lt;/a&gt; — Fast, flexible structured generation library&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://xgrammar.mlc.ai/docs/&#34;&gt;xgrammar Documentation&lt;/a&gt; — Official docs with quick start guide&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://xgrammar.mlc.ai/docs/start/quick_start&#34;&gt;xgrammar Quick Start&lt;/a&gt; — Getting started with xgrammar&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar&#34;&gt;Achieving Efficient Structured Generation with XGrammar&lt;/a&gt; — MLC blog post on xgrammar internals&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;vllm&#34;&gt;vLLM&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://docs.vllm.ai/en/latest/features/structured_outputs.html&#34;&gt;vLLM Structured Outputs&lt;/a&gt; — Official documentation&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;demo-project&#34;&gt;Demo Project&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/slavadubrov/sgr-discount-manager&#34;&gt;sgr-discount-manager&lt;/a&gt; — Working demo with all code examples from this post&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/</link> <pubDate>Sun, 28 Dec 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/</guid> </item> <item> <title>Context Engineering in the Agentic‑AI Era — and How to Cook It</title> <author>Viacheslav Dubrov</author> <category>agents</category> <category>ai-engineering</category> <category>context-compression</category> <category>context-layer</category> <category>context-optimization</category> <category>guardrails</category> <category>memory</category> <category>rag</category> <category>retrieval</category> <description>&lt;h1 id=&#34;context-engineering-in-the-agenticai-era-and-how-to-cook-it&#34;&gt;Context Engineering in the Agentic‑AI Era — and How to Cook It&lt;/h1&gt; &lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;Context engineering&lt;/em&gt; (the &lt;strong&gt;context layer&lt;/strong&gt;) is the pipeline that selects, structures, and governs &lt;strong&gt;what the model sees at the moment of decision&lt;/strong&gt;: &lt;strong&gt;Instructions, Examples, Knowledge, Memory, Skills, Tools, Guardrails&lt;/strong&gt;. Agentic systems live or die by this layer. Below is a field‑tested blueprint and patterns.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: You build an agent. It works in demos, fails in production. Why? The model gets the wrong context at the wrong time—stale memory, irrelevant docs, no safety checks, ambiguous instructions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The fix&lt;/strong&gt;: Design the context layer deliberately. This guide shows you how.&lt;/p&gt; &lt;!-- more --&gt; &lt;hr /&gt; &lt;h2 id=&#34;why-now&#34;&gt;Why now&lt;/h2&gt; &lt;p&gt;Picture this: your customer support agent runs for three weeks. It handles 200 tickets. Then it suddenly starts hallucinating product details, mixing up customers, and calling the wrong APIs. The model didn&#39;t get worse—the context did.&lt;/p&gt; &lt;p&gt;Here&#39;s why context engineering became critical in 2025:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Agents moved from chat to action.&lt;/strong&gt; Multi‑step planning, tool use, and sub‑agents raised the bar for &lt;em&gt;repeatable context assembly&lt;/em&gt; vs. one‑off prompts. A single bad context decision can cascade through a 10‑step plan.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Memory and standards arrived.&lt;/strong&gt; Centralized user/org memory (and standards like MCP) make it feasible to load personal/org context &lt;em&gt;safely&lt;/em&gt;—if you design the layer properly. Without governance, you leak PII or overload the window.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Retrieval matured.&lt;/strong&gt; Hybrid search, reranking, and graph‑aware retrieval (e.g., GraphRAG) reduce hallucinations and token waste. But only if you route queries to the right retrieval strategy.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Value focus shifted.&lt;/strong&gt; Many &#34;agentic&#34; pilots stall not because of model quality but because of weak context design/governance. A deliberate context layer is the fix.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;key-concepts-for-beginners&#34;&gt;Key Concepts for Beginners&lt;/h2&gt; &lt;p&gt;Before we dive in, let&#39;s define a few terms that will appear frequently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Context Window&lt;/strong&gt;: The &#34;working memory&#34; of the model. It&#39;s the maximum amount of text (measured in tokens) the model can process at once. If you exceed it, the model crashes or forgets the beginning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tokens&lt;/strong&gt;: The basic units of text for an LLM. Roughly, 1,000 tokens ≈ 750 words.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention Budget&lt;/strong&gt;: Language models process tokens through attention mechanisms that create pairwise relationships between all tokens. For n tokens, this creates n² relationships. As context grows, this budget gets stretched thin—meaning information in the middle of context receives less attention than information at the beginning or end.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;: Numerical representations of text. We use them to search for &#34;meaning&#34; rather than just keywords (e.g., searching for &#34;dog&#34; might find &#34;puppy&#34;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JSON Schema&lt;/strong&gt;: A standard way to describe the structure of JSON data. It allows us to force the model to output specific fields (like &lt;code&gt;{&#34;answer&#34;: &#34;...&#34;, &#34;citations&#34;: [...]}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP (Model Context Protocol)&lt;/strong&gt;: An open standard that enables AI models to interact with external data and tools securely. Think of it as a &#34;USB port&#34; for AI apps to connect to your local files, databases, or Slack.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Context Window Composition&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/context_window_composition.svg&#34; /&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;what-is-the-context-layer&#34;&gt;What is the context layer?&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;A &lt;strong&gt;pipeline + policy&lt;/strong&gt; that (1) &lt;strong&gt;selects &amp;amp; structures&lt;/strong&gt; inputs per step, (2) &lt;strong&gt;applies controls&lt;/strong&gt; (format/safety/policy), and (3) &lt;strong&gt;feeds&lt;/strong&gt; the model/agent with &lt;strong&gt;just‑enough, just‑in‑time&lt;/strong&gt; context.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Think of it as the assembly line that prepares exactly what the model needs to make a good decision—nothing more, nothing less.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context is a finite resource.&lt;/strong&gt; Like humans with limited working memory, language models have an attention budget that depletes as context grows. Every new token consumes some of this budget. The engineering problem is finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.&lt;/p&gt; &lt;p&gt;There&#39;s no single canonical definition. Different teams ship different stacks. But a practical, shared decomposition is:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Context Layer Architecture&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/context_layer_components.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;1-instructions&#34;&gt;1) Instructions&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: A durable &lt;strong&gt;contract&lt;/strong&gt; for behavior: role, tone, constraints, output schema, evaluation goals. Modern models respect instruction &lt;strong&gt;hierarchies&lt;/strong&gt; (system &amp;gt; developer &amp;gt; user).&lt;/p&gt; &lt;h4 id=&#34;when-to-use-instructions&#34;&gt;When to use Instructions&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You need &lt;strong&gt;consistent output&lt;/strong&gt; (reports, SQL, API calls, JSON).&lt;/li&gt; &lt;li&gt;You must apply policy (e.g., redact PII, reject unsupported asks).&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;instruction-patterns&#34;&gt;Instruction Patterns&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Role &amp;amp; policy blocks&lt;/strong&gt;: keep &lt;em&gt;rules&lt;/em&gt; separate from the user task.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured outputs&lt;/strong&gt;: JSON Schema → deterministic downstream.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction hierarchy&lt;/strong&gt;: split &lt;em&gt;system&lt;/em&gt;, &lt;em&gt;developer&lt;/em&gt;, &lt;em&gt;user&lt;/em&gt; explicitly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Plain example (policy block)&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Instruction Hierarchy&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/instruction_hierarchy.svg&#34; /&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;SYSTEM RULES - Role: support assistant for ACME. - Always output valid JSON per AnswerSchema. - If a request needs account data, ask for the account ID. - Never include secrets or internal URLs. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h4 id=&#34;schemaguided-reasoning-sgr&#34;&gt;Schema‑Guided Reasoning (SGR)&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Drive the agent with JSON Schemas for the plan, tool arguments, intermediate results, and the final answer. The model emits/consumes JSON at each step; your code validates it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Reduces ambiguity, makes retries/repairs deterministic, and improves safety by enforcing types and required fields throughout the loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Define schemas for &lt;code&gt;Plan&lt;/code&gt;, &lt;code&gt;ToolArgs&lt;/code&gt;, &lt;code&gt;StepResult&lt;/code&gt;, and &lt;code&gt;FinalAnswer&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;At each agent step, the model outputs JSON matching one of these schemas.&lt;/li&gt; &lt;li&gt;Your code validates the JSON before proceeding.&lt;/li&gt; &lt;li&gt;If validation fails, attempt one automatic repair (e.g., add missing required fields with defaults).&lt;/li&gt; &lt;li&gt;If repair fails, refuse and log the error.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Concrete example&lt;/strong&gt;: Instead of the model saying &#34;I&#39;ll search for the customer&#39;s tickets&#34;, it outputs:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;action&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;call_tool&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;tool&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;search_tickets&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;args&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;customer_id&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;A-123&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;limit&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;expected_schema&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;TicketList&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Your code validates &lt;code&gt;args&lt;/code&gt; against the tool&#39;s schema &lt;em&gt;before&lt;/em&gt; calling the API.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;2-examples&#34;&gt;2) Examples&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: A few short input→output examples that show the exact format, tone, and steps the model should follow.&lt;/p&gt; &lt;h4 id=&#34;when-to-use-examples&#34;&gt;When to use Examples&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You need the model to match a &lt;strong&gt;specific template&lt;/strong&gt; (tables, JSON, SQL, API calls).&lt;/li&gt; &lt;li&gt;You want &lt;strong&gt;domain‑specific&lt;/strong&gt; phrasing/labels or consistent tone.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;example-patterns&#34;&gt;Example Patterns&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Canonical demos&lt;/strong&gt;: show the &lt;em&gt;exact&lt;/em&gt; target structure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bad vs. good&lt;/strong&gt;: contrast common mistakes with the desired result.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema‑first + examples&lt;/strong&gt;: pair your JSON Schema with 2–3 short demos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keep it short&lt;/strong&gt;: many small, focused demos beat one long example.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&#34;3-knowledge&#34;&gt;3) Knowledge&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Grounding via retrieval (vector + keyword), reranking, graphs, web, or enterprise sources.&lt;/p&gt; &lt;h4 id=&#34;when-to-use-knowledge&#34;&gt;When to use Knowledge&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You need &lt;strong&gt;fresh or private facts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;You want &lt;strong&gt;cited, defensible&lt;/strong&gt; answers.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;knowledge-patterns&#34;&gt;Knowledge Patterns&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid retrieval&lt;/strong&gt; (BM25 + dense) with &lt;strong&gt;reranker&lt;/strong&gt; to shrink tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph‑aware&lt;/strong&gt; retrieval (GraphRAG) for cross‑doc relations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive RAG&lt;/strong&gt;: route between &lt;em&gt;no retrieval&lt;/em&gt;, &lt;em&gt;single‑shot&lt;/em&gt;, and &lt;em&gt;iterative&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Adaptive Retrieval Router&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/retrieval_router.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Params that matter&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chunking&lt;/strong&gt;: split by semantic boundary (paragraphs, sections) &amp;gt; fixed size.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;top‑k&lt;/strong&gt;: start with 10–20 for hybrid, rerank to 3–5.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MMR (diversity) λ&lt;/strong&gt;: 0.7 as default.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Citations&lt;/strong&gt;: Always include source references and quotes.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&#34;4-memory&#34;&gt;4) Memory&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Durable context across turns/sessions: &lt;strong&gt;short‑term&lt;/strong&gt; (conversation state), &lt;strong&gt;long‑term&lt;/strong&gt; (user/app facts), &lt;strong&gt;episodic&lt;/strong&gt; (events), &lt;strong&gt;semantic&lt;/strong&gt; (facts/entities).&lt;/p&gt; &lt;h4 id=&#34;when-to-use-memory&#34;&gt;When to use Memory&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You want personalization and continuity.&lt;/li&gt; &lt;li&gt;Multiple agents coordinate over days/weeks.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;memory-patterns&#34;&gt;Memory Patterns&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Entity memories&lt;/strong&gt; (names, IDs, preferences) + expiry policies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoped retrieval&lt;/strong&gt; from long-term store (vector/kv/graph).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression integration&lt;/strong&gt;: When short-term memory grows large, apply &lt;a href=&#34;#context-compression-strategies&#34;&gt;Context Compression Strategies&lt;/a&gt; &lt;a href=&#34;#references&#34;&gt;[7]&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Memory Scoping&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/memory_scoping.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Expiry rules&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preferences: 365 days&lt;/li&gt; &lt;li&gt;Episodic events: 90 days&lt;/li&gt; &lt;li&gt;Short-term state: clear after session ends&lt;/li&gt; &lt;li&gt;Entities: no expiry, but require periodic validation&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&#34;5-skills&#34;&gt;5) Skills&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Composable domain expertise that agents discover and load dynamically. &lt;a href=&#34;https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills&#34;&gt;Agent Skills&lt;/a&gt; are a framework introduced by Anthropic for equipping agents with specialized capabilities by capturing and sharing procedural knowledge.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Building a skill for an agent is like putting together an onboarding guide for a new hire. — Anthropic Engineering Blog&lt;/p&gt; &lt;/blockquote&gt; &lt;h4 id=&#34;when-to-use-skills&#34;&gt;When to use Skills&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You need &lt;strong&gt;domain-specific expertise&lt;/strong&gt; (PDF manipulation, git operations, data analysis).&lt;/li&gt; &lt;li&gt;You want &lt;strong&gt;reusable procedures&lt;/strong&gt; across agents or organizations.&lt;/li&gt; &lt;li&gt;You need to &lt;strong&gt;specialize&lt;/strong&gt; an agent without hardcoding behaviors.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;what-is-a-skill&#34;&gt;What is a Skill?&lt;/h4&gt; &lt;p&gt;A Skill is an organized folder containing instructions, scripts, and resources that agents can discover and load dynamically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SKILL.md file&lt;/strong&gt;: Contains name, description (in YAML frontmatter), and the skill&#39;s instructions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Additional files&lt;/strong&gt;: Scripts, references, templates that the skill can reference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt;: Python scripts or other executables the agent can run as tools&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;skills-pattern-progressive-disclosure&#34;&gt;Skills Pattern: Progressive Disclosure&lt;/h4&gt; &lt;p&gt;The key design principle is &lt;strong&gt;progressive disclosure&lt;/strong&gt;—loading information only when needed. See &lt;a href=&#34;#the-progressive-disclosure-principle&#34;&gt;The Progressive Disclosure Principle&lt;/a&gt; for the general pattern.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Level 1 (Startup)&lt;/strong&gt;: Only skill names and descriptions are loaded into context&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Level 2 (Activation)&lt;/strong&gt;: When relevant, the full SKILL.md is loaded&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Level 3+ (Deep dive)&lt;/strong&gt;: Additional referenced files loaded only as needed&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This means skill content is effectively unbounded—agents don&#39;t need to load everything into context at once.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;┌─────────────────────────────────────────────────────────┐ │ Context Window │ ├─────────────────────────────────────────────────────────┤ │ System Prompt │ │ ├── Core instructions │ │ └── Skill metadata (name + description only) │ │ • pdf: &amp;quot;Manipulate PDF documents&amp;quot; │ │ • git: &amp;quot;Advanced git operations&amp;quot; │ │ • context-compression: &amp;quot;Manage long sessions&amp;quot; │ ├─────────────────────────────────────────────────────────┤ │ [User triggers task requiring PDF skill] │ │ │ │ → Agent reads pdf/SKILL.md into context │ │ → Agent reads pdf/forms.md (only if filling forms) │ │ → Agent executes pdf/extract_fields.py (without loading)│ └─────────────────────────────────────────────────────────┘ &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h4 id=&#34;skill-best-practices&#34;&gt;Skill Best Practices&lt;/h4&gt; &lt;p&gt;From Anthropic&#39;s guidelines:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Start with evaluation&lt;/strong&gt;: Identify gaps by running agents on representative tasks, build skills to address shortcomings&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structure for scale&lt;/strong&gt;: Split large SKILL.md into separate files, keeping paths separate for mutually exclusive contexts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Think from Claude&#39;s perspective&lt;/strong&gt;: Monitor skill usage, iterate on name/description to improve triggering accuracy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Iterate with Claude&lt;/strong&gt;: Ask Claude to capture successful approaches into reusable skill content&lt;/li&gt; &lt;/ol&gt; &lt;h4 id=&#34;security-considerations&#34;&gt;Security Considerations&lt;/h4&gt; &lt;p&gt;Skills can introduce vulnerabilities since they provide new capabilities through instructions and code:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Install skills only from trusted sources&lt;/li&gt; &lt;li&gt;Audit skills before use—review bundled files, code dependencies, and network connections&lt;/li&gt; &lt;li&gt;Pay attention to instructions that connect to external sources&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3 id=&#34;6-tools&#34;&gt;6) Tools&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Function calls to fetch data or take actions (APIs, DB, search, file ops, &#34;computer use&#34;).&lt;/p&gt; &lt;h4 id=&#34;when-to-use-tools&#34;&gt;When to use Tools&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You want &lt;strong&gt;deterministic&lt;/strong&gt; side‑effects and data fidelity.&lt;/li&gt; &lt;li&gt;You orchestrate &lt;strong&gt;plan → call → verify → continue&lt;/strong&gt; loops.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;tool-patterns&#34;&gt;Tool Patterns&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tool‑first planning&lt;/strong&gt; + &lt;strong&gt;post‑call validators&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured outputs&lt;/strong&gt; between steps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallbacks&lt;/strong&gt; when tools fail (retry → degrade → human‑in‑loop).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Tool Execution Loop&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/tool_execution_loop.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key concepts&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Idempotent&lt;/strong&gt;: safe to retry without side effects (GET=yes, POST/DELETE=no).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Postconditions&lt;/strong&gt;: checks after each call (non_empty_result, status==&#34;ok&#34;, valid_json).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallback chain&lt;/strong&gt;: retry → degrade gracefully → human-in-loop.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;A Note on MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MCP is becoming the standard for how agents connect to tools and data &lt;a href=&#34;#references&#34;&gt;[6]&lt;/a&gt;. Instead of custom API wrappers for every service, you run an &#34;MCP Server&#34; for each. Your agent automatically discovers available tools and resources.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;MCP Architecture&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/mcp_context_architecture.svg&#34; /&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;7-guardrails&#34;&gt;7) Guardrails&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Input/output validation, safety filters, jailbreak defense, schema enforcement, content policy.&lt;/p&gt; &lt;h4 id=&#34;when-to-use-guardrails&#34;&gt;When to use Guardrails&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;You need compliance/brand integrity.&lt;/li&gt; &lt;li&gt;You want &lt;strong&gt;typed, correct&lt;/strong&gt; outputs and safe behavior.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;guardrail-patterns&#34;&gt;Guardrail Patterns&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Programmable rails&lt;/strong&gt; (policy rules + actions).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema + semantic validators&lt;/strong&gt; (types, regex, evals).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Central policy + observability&lt;/strong&gt; (dashboards, red‑teaming).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Guardrails Flow&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/guardrails_flow.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repair vs refuse flow&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Schema violations&lt;/strong&gt;: Attempt automatic repair once. If repair fails, refuse with clear error.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Policy violations&lt;/strong&gt;: Refuse immediately (no repair attempt). Suggest safe alternative.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Common guardrail types&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Input guards&lt;/strong&gt;: PII detection, prompt injection defense, toxicity filters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output guards&lt;/strong&gt;: schema validation, content policy, factual consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool guards&lt;/strong&gt;: rate limiting, permission checks, cost thresholds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory guards&lt;/strong&gt;: PII redaction before storage, expiry enforcement&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h3 id=&#34;concrete-example-support-bot-answering-a-ticket&#34;&gt;Concrete example: support bot answering a ticket&lt;/h3&gt; &lt;p&gt;Let&#39;s see how all these components work together. When a customer asks &#34;Why is my API key not working?&#34;, the context layer assembles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Instructions&lt;/strong&gt;: role = helpful support assistant for ACME, cite sources, return JSON {answer, sources, next_steps}.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Examples&lt;/strong&gt;: 2 short Q→A pairs showing tone and JSON shape (one about API keys, one about billing).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge&lt;/strong&gt;: search the help center and product runbooks for &#34;API key troubleshooting&#34;; include relevant quotes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: customer name &#34;Sam&#34;, account_id &#34;A-123&#34;, plan &#34;Pro&#34;, last interaction was &#34;API key created 3 days ago&#34;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skills&lt;/strong&gt;: load &lt;code&gt;ticket-handling&lt;/code&gt; skill with troubleshooting procedures, escalation policies, and resolution templates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;code&gt;search_tickets(customer_id)&lt;/code&gt;, &lt;code&gt;check_api_key_status(key)&lt;/code&gt;, &lt;code&gt;create_issue(description)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: redact any API key values in output; if schema fails, repair once; if policy violated (e.g., requesting to delete production data), refuse politely.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model receives all of this structured context, generates an answer, and the guardrails validate it before sending to the customer.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;context-fundamentals-deep-dive&#34;&gt;Context Fundamentals Deep Dive&lt;/h2&gt; &lt;p&gt;Understanding the anatomy of context is prerequisite to effective context engineering. This section provides the foundational knowledge for everything that follows.&lt;/p&gt; &lt;h3 id=&#34;the-anatomy-of-context&#34;&gt;The Anatomy of Context&lt;/h3&gt; &lt;p&gt;Context comprises several distinct components, each with different characteristics and constraints:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System Prompts&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;System prompts establish the agent&#39;s core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation.&lt;/p&gt; &lt;p&gt;The key is finding the right &#34;altitude&#34;—specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics. Too low (hardcoded brittle logic) creates fragility; too high (vague guidance) fails to give concrete signals.&lt;/p&gt; &lt;p&gt;Organize prompts into distinct sections using XML tagging or Markdown headers:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;lt;BACKGROUND_INFORMATION&amp;gt; You are a Python expert helping a development team. Current project: Data processing pipeline in Python 3.9+ &amp;lt;/BACKGROUND_INFORMATION&amp;gt; &amp;lt;INSTRUCTIONS&amp;gt; &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Write clean, idiomatic Python code &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Include type hints for function signatures &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Add docstrings for public functions &amp;lt;/INSTRUCTIONS&amp;gt; &amp;lt;TOOL_GUIDANCE&amp;gt; Use bash for shell operations, python for code tasks. File operations should use pathlib for cross-platform compatibility. &amp;lt;/TOOL_GUIDANCE&amp;gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Tool Definitions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool descriptions collectively steer agent behavior—poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The consolidation principle&lt;/strong&gt;: If a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Retrieved Documents&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information. The just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and loads data dynamically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Message History&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Message history contains the conversation between user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage. It serves as scratchpad memory where agents track progress and maintain task state.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tool Outputs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tool outputs are the results of agent actions: file contents, search results, command execution output, API responses. Research shows observations (tool outputs) can reach &lt;strong&gt;over 80% of total context usage&lt;/strong&gt; in typical agent trajectories &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt;. This creates pressure for strategies like observation masking and compaction.&lt;/p&gt; &lt;h3 id=&#34;context-windows-and-attention-mechanics&#34;&gt;Context Windows and Attention Mechanics&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The Attention Budget Constraint&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Language models process tokens through attention mechanisms that create pairwise relationships between all tokens. For n tokens, this creates n² relationships that must be computed. As context length increases, the model&#39;s ability to capture relationships gets stretched thin.&lt;/p&gt; &lt;p&gt;Models develop attention patterns from training data where shorter sequences predominate. This means models have less experience with context-wide dependencies. The result is an &#34;attention budget&#34; that depletes as context grows.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Progressive Disclosure&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/progressive_disclosure.svg&#34; /&gt;&lt;/p&gt; &lt;h4 id=&#34;the-progressive-disclosure-principle&#34;&gt;The Progressive Disclosure Principle&lt;/h4&gt; &lt;p&gt;Progressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions—sufficient to know when a skill might be relevant. Full content loads only when activated for specific tasks.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;gh&#34;&gt;# Instead of loading all documentation at once:&lt;/span&gt; &lt;span class=&#34;gh&#34;&gt;# Step 1: Load summary&lt;/span&gt; docs/api_summary.md # Lightweight overview &lt;span class=&#34;gh&#34;&gt;# Step 2: Load specific section as needed&lt;/span&gt; docs/api/endpoints.md # Only when API calls needed docs/api/authentication.md # Only when auth context needed &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;context-quality-versus-quantity&#34;&gt;Context Quality Versus Quantity&lt;/h3&gt; &lt;p&gt;The assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens.&lt;/p&gt; &lt;p&gt;Several factors create pressure for context efficiency:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Processing cost grows disproportionately with context length—not just double for double the tokens, but exponentially more in time and computing resources.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Degradation&lt;/strong&gt;: Model performance degrades beyond certain context lengths even when the window technically supports more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: Long inputs remain expensive even with prefix caching.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The guiding principle&lt;/strong&gt;: Informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;context-degradation-patterns&#34;&gt;Context Degradation Patterns&lt;/h2&gt; &lt;p&gt;Language models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Context Degradation Patterns&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/degradation_patterns.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;the-lost-in-middle-phenomenon&#34;&gt;The Lost-in-Middle Phenomenon&lt;/h3&gt; &lt;p&gt;The most well-documented degradation pattern is &#34;lost-in-middle,&#34; where models demonstrate U-shaped attention curves &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt;. Information at the beginning and end of context receives reliable attention, while information in the middle suffers from &lt;strong&gt;10-40% lower recall accuracy&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it happens&lt;/strong&gt;: Models allocate massive attention to the first token (often the BOS token) to stabilize internal states, creating an &#34;attention sink&#34; that consumes attention budget &lt;a href=&#34;#references&#34;&gt;[2]&lt;/a&gt;. As context grows, middle tokens fail to garner sufficient attention weight.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical fix&lt;/strong&gt;: Place critical information at the beginning or end of context. Use summary structures that surface key information at attention-favored positions.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Attention U-Curve&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/attention_u_curve.svg&#34; /&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;gh&#34;&gt;# Organize context with critical info at edges&lt;/span&gt; [CURRENT TASK] # At start &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Goal: Generate quarterly report &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Deadline: End of week [DETAILED CONTEXT] # Middle (less attention) &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;50 pages of data &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Multiple analysis sections &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Supporting evidence [KEY FINDINGS] # At end &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Revenue up 15% &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Costs down 8% &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;Growth in Region A &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Prompt duplication technique&lt;/strong&gt;: For critical instructions that must not be missed, duplicate them at both the beginning AND end of the context. Since models attend strongly to both edges, placing the same instruction in both positions ensures it gets proper attention regardless of context length. This is particularly useful for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;System constraints that must always be followed&lt;/li&gt; &lt;li&gt;Output format requirements&lt;/li&gt; &lt;li&gt;Safety policies and content guidelines&lt;/li&gt; &lt;/ul&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;gh&#34;&gt;# Example: Duplicating critical instructions&lt;/span&gt; [SYSTEM - START] CRITICAL: Always respond in JSON format. Never include PII. [... long context with documents, history, tools ...] [SYSTEM - REMINDER] CRITICAL: Always respond in JSON format. Never include PII. &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;context-poisoning&#34;&gt;Context Poisoning&lt;/h3&gt; &lt;p&gt;Context poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How poisoning occurs&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tool outputs contain errors or unexpected formats&lt;/li&gt; &lt;li&gt;Retrieved documents have incorrect or outdated information&lt;/li&gt; &lt;li&gt;Model-generated summaries introduce hallucinations that persist&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Detection symptoms&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Degraded output quality on tasks that previously succeeded&lt;/li&gt; &lt;li&gt;Tool misalignment (agents call wrong tools or parameters)&lt;/li&gt; &lt;li&gt;Persistent hallucinations despite correction attempts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Recovery&lt;/strong&gt;: Truncate context to before the poisoning point, explicitly note the poisoning and request re-evaluation, or restart with clean context preserving only verified information.&lt;/p&gt; &lt;h3 id=&#34;context-distraction&#34;&gt;Context Distraction&lt;/h3&gt; &lt;p&gt;Context distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge.&lt;/p&gt; &lt;p&gt;Research shows that &lt;strong&gt;even a single irrelevant document&lt;/strong&gt; reduces performance on tasks involving relevant documents &lt;a href=&#34;#references&#34;&gt;[8]&lt;/a&gt;. The effect follows a step function—the presence of any distractor triggers degradation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: Models cannot &#34;skip&#34; irrelevant context. They must attend to everything provided, creating distraction even when irrelevant information is clearly not useful.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mitigation&lt;/strong&gt;: Apply relevance filtering before loading documents. Use namespacing to make irrelevant sections easy to ignore. Consider whether information needs to be in context or can be accessed through tool calls.&lt;/p&gt; &lt;h3 id=&#34;context-confusion&#34;&gt;Context Confusion&lt;/h3&gt; &lt;p&gt;Context confusion arises when irrelevant information influences responses in ways that degrade quality. If you put something in context, the model has to pay attention to it—it may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints from different contexts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Signs&lt;/strong&gt;: Responses address the wrong aspect of queries, tool calls seem appropriate for different tasks, outputs mix requirements from multiple sources.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Explicit task segmentation (different tasks get different context windows), clear transitions between task contexts, state management that isolates context.&lt;/p&gt; &lt;h3 id=&#34;context-clash&#34;&gt;Context Clash&lt;/h3&gt; &lt;p&gt;Context clash develops when accumulated information directly conflicts, creating contradictory guidance. This differs from poisoning where one piece is incorrect—in clash, multiple correct pieces contradict each other.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Common sources&lt;/strong&gt;: Multi-source retrieval with contradictory information, version conflicts (outdated and current information both in context), perspective conflicts (valid but incompatible viewpoints).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: Explicit conflict marking, priority rules establishing which source takes precedence, version filtering to exclude outdated information.&lt;/p&gt; &lt;h3 id=&#34;model-specific-degradation-thresholds&#34;&gt;Model-Specific Degradation Thresholds&lt;/h3&gt; &lt;p&gt;Research provides concrete data on when performance degradation begins. Note that &#34;effective context length&#34; (where models maintain optimal performance) is often significantly smaller than advertised maximum context windows &lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt;:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Max Context&lt;/th&gt; &lt;th&gt;Effective Context&lt;/th&gt; &lt;th&gt;Degradation Notes&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-4 Turbo&lt;/td&gt; &lt;td&gt;128K tokens&lt;/td&gt; &lt;td&gt;~32K tokens&lt;/td&gt; &lt;td&gt;Retrieval degrades after 32K, accuracy suffers beyond 64K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPT-4o&lt;/td&gt; &lt;td&gt;128K tokens&lt;/td&gt; &lt;td&gt;~8K tokens&lt;/td&gt; &lt;td&gt;Complex NIAH accuracy drops from 99% to 70% at 32K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Claude 3.5 Sonnet&lt;/td&gt; &lt;td&gt;200K tokens&lt;/td&gt; &lt;td&gt;~4K tokens&lt;/td&gt; &lt;td&gt;Complex NIAH accuracy drops from 88% to 30% at 32K&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemini 1.5 Pro&lt;/td&gt; &lt;td&gt;1M tokens&lt;/td&gt; &lt;td&gt;~128K tokens&lt;/td&gt; &lt;td&gt;99% NIAH recall at 1M, best long-context performance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemini 2.0 Flash&lt;/td&gt; &lt;td&gt;1M tokens&lt;/td&gt; &lt;td&gt;~32K tokens&lt;/td&gt; &lt;td&gt;Complex NIAH accuracy drops from 94% to 48% at 32K&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;em&gt;Sources: RULER benchmark &lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt;, NoLiMa benchmark &lt;a href=&#34;#references&#34;&gt;[9]&lt;/a&gt;, Google technical reports&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key finding&lt;/strong&gt; &lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt;: Only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding—complex reasoning tasks show much steeper degradation.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;context-compression-strategies&#34;&gt;Context Compression Strategies&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Terminology note&lt;/strong&gt;: Context compression is an umbrella category that includes several techniques: summarization (for conversation history and memory), observation masking (for tool outputs), and selective trimming &lt;a href=&#34;#references&#34;&gt;[5]&lt;/a&gt;. The memory summarization referenced in the &lt;a href=&#34;#4-memory&#34;&gt;Memory&lt;/a&gt; section is one application of these broader compression strategies.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;When agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. &lt;strong&gt;The correct optimization target is tokens per task&lt;/strong&gt; &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt;: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Compression Strategies&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/compression_strategies.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;when-compression-is-needed&#34;&gt;When Compression is Needed&lt;/h3&gt; &lt;p&gt;Activate compression when:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent sessions exceed context window limits&lt;/li&gt; &lt;li&gt;Agents &#34;forget&#34; what files they modified&lt;/li&gt; &lt;li&gt;Debugging long-running coding or debugging sessions&lt;/li&gt; &lt;li&gt;Performance degrades in extended conversations&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;three-production-ready-approaches&#34;&gt;Three Production-Ready Approaches&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;1. Anchored Iterative Summarization&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key insight&lt;/strong&gt;: Structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;gu&#34;&gt;## Session Intent&lt;/span&gt; Debug 401 Unauthorized error on /api/auth/login despite valid credentials. &lt;span class=&#34;gu&#34;&gt;## Root Cause&lt;/span&gt; Stale Redis connection in session store. JWT generated correctly but session could not be persisted. &lt;span class=&#34;gu&#34;&gt;## Files Modified&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;auth.controller.ts: No changes (read only) &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;config/redis.ts: Fixed connection pooling configuration &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;services/session.service.ts: Added retry logic for transient failures &lt;span class=&#34;k&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;tests/auth.test.ts: Updated mock setup &lt;span class=&#34;gu&#34;&gt;## Test Status&lt;/span&gt; 14 passing, 2 failing (mock setup issues) &lt;span class=&#34;gu&#34;&gt;## Next Steps&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;1.&lt;/span&gt; Fix remaining test failures (mock session service) &lt;span class=&#34;k&#34;&gt;2.&lt;/span&gt; Run full test suite &lt;span class=&#34;k&#34;&gt;3.&lt;/span&gt; Deploy to staging &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;2. Opaque Compression&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Use when maximum token savings required and re-fetching costs are low.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Regenerative Full Summary&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles.&lt;/p&gt; &lt;h3 id=&#34;compression-comparison&#34;&gt;Compression Comparison&lt;/h3&gt; &lt;p&gt;Research from Factory.ai &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt; compared compression strategies on real production agent sessions:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Method&lt;/th&gt; &lt;th&gt;Compression Ratio&lt;/th&gt; &lt;th&gt;Quality Score&lt;/th&gt; &lt;th&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Anchored Iterative&lt;/td&gt; &lt;td&gt;98.6%&lt;/td&gt; &lt;td&gt;3.70/5&lt;/td&gt; &lt;td&gt;Long sessions, file tracking&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Regenerative&lt;/td&gt; &lt;td&gt;98.7%&lt;/td&gt; &lt;td&gt;3.44/5&lt;/td&gt; &lt;td&gt;Clear phase boundaries&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Opaque&lt;/td&gt; &lt;td&gt;99.3%&lt;/td&gt; &lt;td&gt;3.35/5&lt;/td&gt; &lt;td&gt;Maximum savings, short sessions&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;strong&gt;Understanding the metrics:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Compression Ratio (98.6%)&lt;/strong&gt;: The percentage of tokens removed. A 98.6% ratio means if you had 100,000 tokens of conversation history, only 1,400 tokens remain after compression (98,600 were removed).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quality Score (3.70/5)&lt;/strong&gt;: Measured via probe-based evaluation—after compression, the agent is asked questions that require recalling specific details from the truncated history (e.g., &#34;What files did we modify?&#34;, &#34;What was the error message?&#34;). A score of 3.70/5 means the agent answered ~74% of probes correctly.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;0.7% additional tokens&lt;/strong&gt;: Comparing Anchored Iterative (98.6%) to Opaque (99.3%), the difference is 0.7%. For a 100K token session: Anchored keeps 1,400 tokens, Opaque keeps only 700 tokens. That extra 700 tokens (0.7% of original) buys 0.35 quality points (3.70 vs 3.35)—meaning significantly better recall of task-critical details.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;the-artifact-trail-problem&#34;&gt;The Artifact Trail Problem&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Artifact trail integrity is the weakest dimension&lt;/strong&gt; across all compression methods, scoring 2.2-2.5 out of 5.0. Coding agents need to know which files were created, modified, read—and compression often loses this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Recommendation&lt;/strong&gt;: Implement a separate artifact index or explicit file-state tracking in agent scaffolding, beyond general summarization.&lt;/p&gt; &lt;h3 id=&#34;compression-trigger-strategies&#34;&gt;Compression Trigger Strategies&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Strategy&lt;/th&gt; &lt;th&gt;Trigger Point&lt;/th&gt; &lt;th&gt;Trade-off&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Fixed threshold&lt;/td&gt; &lt;td&gt;70-80% utilization&lt;/td&gt; &lt;td&gt;Simple but may compress early&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Sliding window&lt;/td&gt; &lt;td&gt;Keep last N turns + summary&lt;/td&gt; &lt;td&gt;Predictable context size&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Importance-based&lt;/td&gt; &lt;td&gt;Compress low-relevance first&lt;/td&gt; &lt;td&gt;Complex but preserves signal&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Task-boundary&lt;/td&gt; &lt;td&gt;Compress at task completions&lt;/td&gt; &lt;td&gt;Clean summaries, unpredictable timing&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;probe-based-evaluation&#34;&gt;Probe-Based Evaluation&lt;/h3&gt; &lt;p&gt;Traditional metrics like ROUGE fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Probe-based evaluation&lt;/strong&gt; directly measures quality by asking questions after compression:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Probe Type&lt;/th&gt; &lt;th&gt;What It Tests&lt;/th&gt; &lt;th&gt;Example Question&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Recall&lt;/td&gt; &lt;td&gt;Factual retention&lt;/td&gt; &lt;td&gt;&#34;What was the original error message?&#34;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Artifact&lt;/td&gt; &lt;td&gt;File tracking&lt;/td&gt; &lt;td&gt;&#34;Which files have we modified?&#34;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Continuation&lt;/td&gt; &lt;td&gt;Task planning&lt;/td&gt; &lt;td&gt;&#34;What should we do next?&#34;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Decision&lt;/td&gt; &lt;td&gt;Reasoning chain&lt;/td&gt; &lt;td&gt;&#34;What did we decide about the Redis issue?&#34;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;If compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;context-optimization-techniques&#34;&gt;Context Optimization Techniques&lt;/h2&gt; &lt;p&gt;Context optimization extends effective capacity through strategic compression, masking, caching, and partitioning. &lt;strong&gt;These techniques build on the &lt;a href=&#34;#context-compression-strategies&#34;&gt;Context Compression Strategies&lt;/a&gt; covered earlier&lt;/strong&gt;, applying them systematically for production use. Effective optimization can &lt;strong&gt;double or triple effective context capacity&lt;/strong&gt; without requiring larger models.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Optimization Techniques&#34; src=&#34;../../../../assets/2025-10-05-context-engineering/optimization_techniques.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;compaction-strategies&#34;&gt;Compaction Strategies&lt;/h3&gt; &lt;p&gt;Compaction summarizes context contents when approaching limits, then reinitializes with the summary. This distills contents in a high-fidelity manner, enabling continued operation with minimal degradation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Priority for compression&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tool outputs → replace with summaries&lt;/li&gt; &lt;li&gt;Old turns → summarize early conversation&lt;/li&gt; &lt;li&gt;Retrieved docs → summarize if recent versions exist&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Never compress&lt;/strong&gt;: system prompt&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Effective summaries preserve different elements by type&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tool outputs&lt;/strong&gt;: Preserve key findings, metrics, conclusions. Remove verbose raw output.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversational turns&lt;/strong&gt;: Preserve decisions, commitments, context shifts. Remove filler.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieved documents&lt;/strong&gt;: Preserve key facts and claims. Remove supporting elaboration.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;observation-masking&#34;&gt;Observation Masking&lt;/h3&gt; &lt;p&gt;Tool outputs can comprise &lt;strong&gt;over 80% of token usage&lt;/strong&gt; &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt;. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Masking decision matrix&lt;/strong&gt;:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Category&lt;/th&gt; &lt;th&gt;Action&lt;/th&gt; &lt;th&gt;Reasoning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Current task observations&lt;/td&gt; &lt;td&gt;Never mask&lt;/td&gt; &lt;td&gt;Critical to current work&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Most recent turn&lt;/td&gt; &lt;td&gt;Never mask&lt;/td&gt; &lt;td&gt;Immediately relevant&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Active reasoning&lt;/td&gt; &lt;td&gt;Never mask&lt;/td&gt; &lt;td&gt;In-progress thought&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3+ turns ago&lt;/td&gt; &lt;td&gt;Consider masking&lt;/td&gt; &lt;td&gt;Purpose likely served&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Repeated outputs&lt;/td&gt; &lt;td&gt;Always mask&lt;/td&gt; &lt;td&gt;Redundant&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Boilerplate&lt;/td&gt; &lt;td&gt;Always mask&lt;/td&gt; &lt;td&gt;Low signal&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h3 id=&#34;kv-cache-optimization&#34;&gt;KV-Cache Optimization&lt;/h3&gt; &lt;p&gt;The KV-cache stores Key and Value tensors computed during inference. Caching across requests with identical prefixes avoids recomputation, dramatically reducing cost and latency.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Optimize for caching&lt;/strong&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Stable content first (cacheable)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;context&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system_prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool_definitions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Frequently reused elements&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;context&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reused_templates&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Unique elements last&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;context&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique_content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Design for cache stability&lt;/strong&gt;: Avoid dynamic content like timestamps in prompts, use consistent formatting, keep structure stable across sessions.&lt;/p&gt; &lt;h3 id=&#34;context-partitioning&#34;&gt;Context Partitioning&lt;/h3&gt; &lt;p&gt;The most aggressive optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Result aggregation&lt;/strong&gt;: Validate all partitions completed, merge compatible results, summarize if still too large.&lt;/p&gt; &lt;h3 id=&#34;optimization-decision-framework&#34;&gt;Optimization Decision Framework&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;When to optimize&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Context utilization exceeds 70%&lt;/li&gt; &lt;li&gt;Response quality degrades in extended conversations&lt;/li&gt; &lt;li&gt;Costs increase due to long contexts&lt;/li&gt; &lt;li&gt;Latency increases with conversation length&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What to apply&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tool outputs dominate → observation masking&lt;/li&gt; &lt;li&gt;Retrieved documents dominate → summarization or partitioning&lt;/li&gt; &lt;li&gt;Message history dominates → compaction with summarization&lt;/li&gt; &lt;li&gt;Multiple components → combine strategies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Target metrics&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Compaction: 50-70% token reduction with &amp;lt;5% quality degradation&lt;/li&gt; &lt;li&gt;Masking: 60-80% reduction in masked observations&lt;/li&gt; &lt;li&gt;Cache optimization: 70%+ hit rate for stable workloads&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;how-to-cook-it-stepbystep&#34;&gt;How to cook it (step‑by‑step)&lt;/h2&gt; &lt;p&gt;Here&#39;s a practical recipe to implement the context layer in your agentic system. Start simple, then add complexity only when needed.&lt;/p&gt; &lt;h3 id=&#34;step-1-write-the-contract&#34;&gt;Step 1: Write the contract&lt;/h3&gt; &lt;p&gt;Define what your agent must do and how it should behave.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write system-level policies: role, constraints, safety rules&lt;/li&gt; &lt;li&gt;Write developer guidelines: output format, tone, citation requirements&lt;/li&gt; &lt;li&gt;Define JSON Schemas for all outputs: &lt;code&gt;AnswerSchema&lt;/code&gt;, &lt;code&gt;PlanSchema&lt;/code&gt;, &lt;code&gt;StepResultSchema&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;step-2-pick-retrieval-strategy&#34;&gt;Step 2: Pick retrieval strategy&lt;/h3&gt; &lt;p&gt;Start with hybrid retrieval (BM25 + vector) + reranker.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Decision tree&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Query is general knowledge? → No retrieval (parametric)&lt;/li&gt; &lt;li&gt;Query needs fresh/private facts? → Single-shot RAG (hybrid + rerank)&lt;/li&gt; &lt;li&gt;Query is complex/multi-part? → Iterative RAG (break into subqueries)&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;step-3-design-memory&#34;&gt;Step 3: Design memory&lt;/h3&gt; &lt;p&gt;Split short-term (conversation state) from long-term (user facts, history).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Short-term: store conversation state, last few turns. Clear after session.&lt;/li&gt; &lt;li&gt;Long-term: store user entities, preferences, episodic events.&lt;/li&gt; &lt;li&gt;Set expiry rules: preferences 365d, episodic 90d, short-term session-only.&lt;/li&gt; &lt;li&gt;Add PII redaction before storing anything.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;step-4-specify-tools&#34;&gt;Step 4: Specify tools&lt;/h3&gt; &lt;p&gt;Define clear tool signatures with validation and fallback strategies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Actions&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For each tool: write clear docstring, input schema, output schema&lt;/li&gt; &lt;li&gt;Mark idempotency: is it safe to retry?&lt;/li&gt; &lt;li&gt;Define postconditions and fallback chains&lt;/li&gt; &lt;li&gt;Validate tool arguments against schema &lt;em&gt;before&lt;/em&gt; calling&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;step-5-install-guardrails&#34;&gt;Step 5: Install guardrails&lt;/h3&gt; &lt;p&gt;Add input and output validation, safety filters, and policy enforcement.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick checklist&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;[ ] Redact PII (emails, SSNs, credit cards) before processing&lt;/li&gt; &lt;li&gt;[ ] Validate all outputs against JSON Schema&lt;/li&gt; &lt;li&gt;[ ] Block prompt injection attempts&lt;/li&gt; &lt;li&gt;[ ] Rate limit tool calls&lt;/li&gt; &lt;li&gt;[ ] Log all policy violations for auditing&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;step-6-add-observability-evals&#34;&gt;Step 6: Add observability &amp;amp; evals&lt;/h3&gt; &lt;p&gt;Instrument your context layer so you can debug and improve it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Essential traces&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which context sources loaded?&lt;/li&gt; &lt;li&gt;Token counts: input, output, cost&lt;/li&gt; &lt;li&gt;Retrieval metrics: query, top-k results, sources cited&lt;/li&gt; &lt;li&gt;Tool calls: which tools, arguments, results, failures&lt;/li&gt; &lt;li&gt;Guardrail triggers: input blocks, output repairs, policy refusals&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Metrics to track&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Exactness (schema validity)&lt;/strong&gt;: Target 99%+&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Groundedness (citation rate)&lt;/strong&gt;: Target 90%+ for knowledge queries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: Target &amp;lt;2s p95&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Target &amp;lt;$0.05 per query&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;step-7-iterate&#34;&gt;Step 7: Iterate&lt;/h3&gt; &lt;p&gt;Start with basics. Add advanced patterns only when you hit clear limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to add&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reflections&lt;/strong&gt;: when error rate &amp;gt; 5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Planners&lt;/strong&gt;: when tasks require &amp;gt; 3 sequential steps&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sub-agents&lt;/strong&gt;: when you have distinct domains&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;antipatterns&#34;&gt;Anti‑patterns&lt;/h2&gt; &lt;p&gt;Common mistakes that kill agentic systems. Avoid these.&lt;/p&gt; &lt;h3 id=&#34;1-stuff-the-window&#34;&gt;1. Stuff-the-window&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Dump every possible document, memory, and example into context on every query.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: Context rot. Signal-to-noise ratio collapses. See &lt;a href=&#34;#context-degradation-patterns&#34;&gt;Context Degradation Patterns&lt;/a&gt; for details on distraction and confusion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Route adaptively. Use compression and masking. See &lt;a href=&#34;#context-optimization-techniques&#34;&gt;Context Optimization Techniques&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;2-unvalidated-tool-results&#34;&gt;2. Unvalidated tool results&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Agent calls a tool, gets back data, and immediately feeds it to the model without checking.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: Malformed data crashes downstream logic. Null results cause hallucinations. This is a primary vector for &lt;a href=&#34;#context-poisoning&#34;&gt;Context Poisoning&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Always validate tool results against schema and postconditions.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;3-one-shot-everything&#34;&gt;3. One-shot everything&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Cram system policy, developer guidelines, examples, user query, memory, and knowledge into a single monolithic prompt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: No separation of concerns. Context window fills with duplicate boilerplate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Separate durable instructions from step-specific context. Use &lt;a href=&#34;#kv-cache-optimization&#34;&gt;KV-Cache Optimization&lt;/a&gt; patterns.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;4-unbounded-memory&#34;&gt;4. Unbounded memory&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Store every user interaction forever. Load all of it on every query.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: Context fills with stale, irrelevant memories. Privacy risks. See &lt;a href=&#34;#the-lost-in-middle-phenomenon&#34;&gt;Lost-in-Middle&lt;/a&gt; for why middle content gets ignored anyway.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Set retention policies. Implement scoped retrieval. Redact PII.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;5-rag-everywhere&#34;&gt;5. RAG everywhere&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Retrieve documents for every single query, even &#34;What is 2+2?&#34; or &#34;Hello&#34;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: Wastes latency and cost. Retrieval can inject noise that causes &lt;a href=&#34;#context-distraction&#34;&gt;Context Distraction&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Implement adaptive RAG routing. Use classifiers or heuristics.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;6-ignoring-guardrail-triggers&#34;&gt;6. Ignoring guardrail triggers&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Log guardrail violations but never review them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: You miss real attacks. You miss UX issues. Schema repairs shouldn&#39;t be frequent—if they are, your instructions are unclear.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Review guardrail triggers weekly.&lt;/p&gt; &lt;hr /&gt; &lt;h3 id=&#34;7-no-evals&#34;&gt;7. No evals&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Ship context layer changes without testing them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it fails&lt;/strong&gt;: Silent regressions. No way to compare variants objectively.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: Define 5–10 eval scenarios before shipping. Run on every change. Use &lt;a href=&#34;#probe-based-evaluation&#34;&gt;Probe-Based Evaluation&lt;/a&gt; to catch compression quality issues.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;quick-wins-ship-these-today&#34;&gt;Quick wins: ship these today&lt;/h2&gt; &lt;p&gt;If you already have an agent in production and want immediate improvements, start here. Each takes &amp;lt; 1 day.&lt;/p&gt; &lt;h3 id=&#34;1-add-output-schema-validation&#34;&gt;1. Add output schema validation&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Catch the majority of errors before they reach users.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;jsonschema&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;validate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ValidationError&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;validate_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;validate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;instance&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANSWER_SCHEMA&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ValidationError&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;repaired&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;auto_repair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;validate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;instance&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;repaired&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ANSWER_SCHEMA&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;repaired&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;2-instrument-basic-tracing&#34;&gt;2. Instrument basic tracing&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Debug significantly faster when things break.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;n&#34;&gt;logger&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dumps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;request_id&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;request_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;query&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;context_loaded&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;instructions&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;memory&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;knowledge&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;150&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;latency_ms&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1120&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;result&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;success&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}))&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;3-split-system-vs-user-messages&#34;&gt;3. Split system vs user messages&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Reduce token waste significantly by enabling &lt;a href=&#34;#kv-cache-optimization&#34;&gt;KV-Cache Optimization&lt;/a&gt;.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;system&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SYSTEM_POLICY&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DEVELOPER_GUIDELINES&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Query: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Memory: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;memory&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Knowledge: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;knowledge&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;4-add-citation-requirements&#34;&gt;4. Add citation requirements&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Build trust, enable auditing, reduce hallucinations.&lt;/p&gt; &lt;h3 id=&#34;5-set-memory-expiry&#34;&gt;5. Set memory expiry&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Prevent context pollution and privacy risks.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;load_memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;customer_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;entries&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;db&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_memory&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;customer_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;now&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;entries&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;expires_at&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;now&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Context engineering is the discipline that separates demo agents from production agents. The model didn&#39;t get worse—the context did. By understanding:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fundamentals&lt;/strong&gt;: Context is finite, attention is limited, progressive disclosure is key&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Degradation&lt;/strong&gt;: Lost-in-middle, poisoning, distraction, confusion, clash&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compression&lt;/strong&gt;: Tokens-per-task over tokens-per-request, structured summaries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: Compaction, masking, caching, partitioning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...you can build agents that work reliably at scale.&lt;/p&gt; &lt;p&gt;Start with the quick wins. Add complexity only when you hit clear limits. And always measure—you can&#39;t improve what you don&#39;t trace.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;This article incorporates content from the Agent Skills for Context Engineering collection, a set of reusable knowledge modules for building better AI agents.&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp;amp; Liang, P. (2023).&lt;/strong&gt; &#34;Lost in the Middle: How Language Models Use Long Contexts.&#34; &lt;em&gt;arXiv preprint arXiv:2307.03172&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2307.03172&#34;&gt;https://arxiv.org/abs/2307.03172&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key finding: 10-40% lower recall accuracy for information in the middle of context vs. beginning/end.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Xiao, G., Tian, Y., Chen, B., Han, S., &amp;amp; Lewis, M. (2023).&lt;/strong&gt; &#34;Efficient Streaming Language Models with Attention Sinks.&#34; &lt;em&gt;ICLR 2024&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2309.17453&#34;&gt;https://arxiv.org/abs/2309.17453&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Introduces the &#34;attention sink&#34; phenomenon where LLMs allocate disproportionate attention to initial tokens.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hsieh, C. Y., et al. (2024).&lt;/strong&gt; &#34;RULER: What&#39;s the Real Context Size of Your Long-Context Language Models?&#34; &lt;em&gt;COLM 2024&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2404.06654&#34;&gt;https://arxiv.org/abs/2404.06654&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key finding: Only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Factory.ai Research. (2025).&lt;/strong&gt; &#34;Evaluating Context Compression for AI Agents.&#34; &lt;a href=&#34;https://www.factory.ai/blog/evaluating-context-compression&#34;&gt;https://www.factory.ai/blog/evaluating-context-compression&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Source for compression strategy comparisons, tokens-per-task optimization, and probe-based evaluation methodology.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Li, Y., et al. (2023).&lt;/strong&gt; &#34;Compressing Context to Enhance Inference Efficiency of Large Language Models.&#34; &lt;em&gt;EMNLP 2023&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Research on selective context pruning using self-information metrics.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Anthropic. (2024).&lt;/strong&gt; &#34;Model Context Protocol (MCP) Specification.&#34; &lt;a href=&#34;https://modelcontextprotocol.io/&#34;&gt;https://modelcontextprotocol.io/&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Official specification for the MCP standard for AI-tool integration.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain/LangGraph. (2024).&lt;/strong&gt; &#34;How to add memory to the prebuilt ReAct agent.&#34; &lt;a href=&#34;https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/&#34;&gt;https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/&lt;/a&gt; — Demonstrates that summarization is one technique for managing memory within context limits.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., &amp;amp; Berant, J. (2024).&lt;/strong&gt; &#34;Making Retrieval-Augmented Language Models Robust to Irrelevant Context.&#34; &lt;em&gt;ICLR 2024&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2310.01558&#34;&gt;https://arxiv.org/abs/2310.01558&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key finding: Even a single irrelevant document can significantly reduce RAG performance, creating a &#34;distracting effect.&#34;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Maekawa, S., et al. (2025).&lt;/strong&gt; &#34;NoLiMa: Long-Context Evaluation Beyond Literal Matching.&#34; &lt;em&gt;ICML 2025&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2502.05167&#34;&gt;https://arxiv.org/abs/2502.05167&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key finding: GPT-4o effective context ~8K tokens, Claude 3.5 Sonnet ~4K tokens when latent reasoning is required (vs. literal matching). At 32K tokens, GPT-4o drops from 99.3% to 69.7% accuracy.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;additional-resources&#34;&gt;Additional Resources&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Anthropic Claude Documentation&lt;/strong&gt;: Best practices for long-context usage&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Cookbook&lt;/strong&gt;: Strategies for managing context windows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangChain Documentation&lt;/strong&gt;: Memory and retrieval patterns&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LlamaIndex Documentation&lt;/strong&gt;: RAG and chunking strategies&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/</link> <pubDate>Sat, 27 Dec 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/</guid> </item> <item> <title>Choosing the Right Open-Source LLM Variant &amp; File Format</title> <author>Viacheslav Dubrov</author> <category>guide</category> <category>llm</category> <description>&lt;h1 id=&#34;choosing-the-right-open-source-llm-variant-file-format&#34;&gt;Choosing the Right Open-Source LLM Variant &amp;amp; File Format&lt;/h1&gt; &lt;hr /&gt; &lt;h2 id=&#34;why-do-open-source-llms-have-so-many-confusing-names&#34;&gt;Why do open-source LLMs have so many confusing names?&lt;/h2&gt; &lt;p&gt;You&#39;ve probably seen model names like &lt;code&gt;Llama-3.1-8B-Instruct.Q4_K_M.gguf&lt;/code&gt; or &lt;code&gt;Mistral-7B-v0.3-A3B.awq&lt;/code&gt; and wondered what all those suffixes mean. It looks like a secret code, but the short answer is: &lt;strong&gt;they tell you two critical things.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open-source LLMs vary along &lt;strong&gt;two independent dimensions&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model variant&lt;/strong&gt; – the suffix in the name (&lt;code&gt;-Instruct&lt;/code&gt;, &lt;code&gt;-Distill&lt;/code&gt;, &lt;code&gt;-A3B&lt;/code&gt;, etc.) describes &lt;em&gt;how&lt;/em&gt; the model was trained and &lt;em&gt;what&lt;/em&gt; it&#39;s optimized for.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File format&lt;/strong&gt; – the extension (&lt;code&gt;.gguf&lt;/code&gt;, &lt;code&gt;.gptq&lt;/code&gt;, &lt;code&gt;.awq&lt;/code&gt;, etc.) describes &lt;em&gt;how&lt;/em&gt; the weights are stored and &lt;em&gt;where&lt;/em&gt; they run best (CPU, GPU, mobile, etc.).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Think of it like this: the &lt;strong&gt;model variant is the recipe&lt;/strong&gt;, and the &lt;strong&gt;file format is the container&lt;/strong&gt;. You can put the same soup (recipe) into a thermos, a bowl, or a takeout box (container) depending on where you plan to eat it.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;LLM Variant vs Format&#34; src=&#34;../../../../assets/2025-05-11-llm-variant-guide/llm_variant_vs_format.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Understanding both dimensions helps you avoid downloading 20 GB of the wrong model at midnight and then spending hours debugging CUDA errors.&lt;/p&gt; &lt;!-- more --&gt; &lt;hr /&gt; &lt;h2 id=&#34;model-variants-explained-the-recipe&#34;&gt;Model variants explained (the recipe)&lt;/h2&gt; &lt;p&gt;This is about the &lt;em&gt;brain&lt;/em&gt; of the model. How was it taught?&lt;/p&gt; &lt;h3 id=&#34;base-models&#34;&gt;Base models&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; The raw, pre-trained model straight from the training run. Think of it as the unfiltered brain that learned language patterns from massive text datasets (the entire internet) but hasn&#39;t been taught to follow instructions. It just predicts the next word.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You&#39;re planning to &lt;strong&gt;fine-tune&lt;/strong&gt; it for your specific domain.&lt;/li&gt; &lt;li&gt;You&#39;re doing research and need the &#34;pure&#34; foundation.&lt;/li&gt; &lt;li&gt;You want maximum creative freedom (no safety guardrails).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt; Won&#39;t reliably follow instructions. If you ask &#34;What is the capital of France?&#34;, it might reply &#34;and what is the capital of Germany?&#34; because it thinks it&#39;s completing a list of questions.&lt;/p&gt; &lt;h3 id=&#34;instruct-chat-models&#34;&gt;Instruct / Chat models&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; A base model that went through additional training (Supervised Fine-Tuning + RLHF) to understand and follow human instructions. This is what most people actually want when they say &#34;I want an LLM.&#34;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Building chatbots, AI agents, or RAG applications.&lt;/li&gt; &lt;li&gt;Function calling and tool use.&lt;/li&gt; &lt;li&gt;Day-to-day coding assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;95% of production use cases.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt; Slightly larger and slower than base models due to the extra training layers. May be less &#34;creative&#34; due to alignment training that makes it more predictable and helpful.&lt;/p&gt; &lt;h3 id=&#34;reasoning-cot-models-new&#34;&gt;Reasoning / CoT Models (New!)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; A new breed of models (like DeepSeek-R1 or o1-derivatives) trained with &#34;Chain of Thought&#34; (CoT) reinforcement learning. They &#34;think&#34; before they speak, generating internal reasoning tokens to solve complex logic, math, or coding problems before outputting the final answer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Complex coding tasks and debugging.&lt;/li&gt; &lt;li&gt;Math problems and logic puzzles.&lt;/li&gt; &lt;li&gt;When you need the model to double-check its work and avoid hallucinations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt; &lt;strong&gt;Slower inference&lt;/strong&gt;. They generate many &#34;thought&#34; tokens that you might not see but still have to wait for. They can also be overly verbose for simple &#34;hello world&#34; tasks.&lt;/p&gt; &lt;h3 id=&#34;distilled-models&#34;&gt;Distilled models&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; A smaller &#34;student&#34; model trained to mimic the behavior of a larger &#34;teacher&#34; model. Think of it as compressed knowledge—you get 70-80% of the performance at 30-50% of the size.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mobile or edge devices with limited resources.&lt;/li&gt; &lt;li&gt;Cost-sensitive SaaS where every millisecond counts.&lt;/li&gt; &lt;li&gt;High-throughput scenarios where you need to serve many requests.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt; Some loss in complex reasoning ability, but excellent efficiency. The token-per-watt ratio is hard to beat.&lt;/p&gt; &lt;h3 id=&#34;moe-mixture-of-experts-a3b-a22b-etc&#34;&gt;MoE (Mixture-of-Experts): A3B, A22B, etc.&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; A clever architecture where the model has many &#34;expert&#34; sub-networks, but only activates a subset for each token. &#34;A3B&#34; means &#34;3 billion parameters active&#34; out of a much larger total (often 30B+).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want &#34;big model&#34; smarts but only have 12-24 GB VRAM.&lt;/li&gt; &lt;li&gt;You need the reasoning power of a 30B model but with 7B inference costs.&lt;/li&gt; &lt;li&gt;You&#39;re running locally and want the best performance-per-memory ratio.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt; Takes more disk space (you&#39;re storing all the experts). Not every inference framework supports MoE routing yet—check compatibility first.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Choosing Model Variant&#34; src=&#34;../../../../assets/2025-05-11-llm-variant-guide/choose_model_variant.svg&#34; /&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Rule of thumb:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start with an &lt;strong&gt;Instruct&lt;/strong&gt; model—it&#39;s what most people need.&lt;/li&gt; &lt;li&gt;Hit memory or latency limits? Try a &lt;strong&gt;Distilled&lt;/strong&gt; or &lt;strong&gt;MoE&lt;/strong&gt; variant.&lt;/li&gt; &lt;li&gt;Need to solve a complex riddle? Try a &lt;strong&gt;Reasoning&lt;/strong&gt; model.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&#34;file-formats-explained-the-container&#34;&gt;File formats explained (the container)&lt;/h2&gt; &lt;p&gt;Now that you know &lt;em&gt;what&lt;/em&gt; kind of model you want, you need to pick &lt;em&gt;how&lt;/em&gt; it&#39;s packaged. File formats determine where your model runs best and how much memory it needs.&lt;/p&gt; &lt;h3 id=&#34;quantization-101-why-do-we-shrink-models&#34;&gt;Quantization 101: Why do we shrink models?&lt;/h3&gt; &lt;p&gt;Before we talk formats, let&#39;s talk &lt;strong&gt;Quantization&lt;/strong&gt;. Standard models use 16-bit numbers (FP16) for every weight. That&#39;s precise but huge. Quantization reduces these to 8-bit, 4-bit, or even 2-bit numbers.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FP16&lt;/strong&gt;: 2 bytes per parameter. (13B model ≈ 26 GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;4-bit&lt;/strong&gt;: 0.5 bytes per parameter. (13B model ≈ 6.5 GB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You lose a tiny bit of &#34;intelligence&#34; but gain massive speed and memory savings.&lt;/p&gt; &lt;h3 id=&#34;gguf-gguf&#34;&gt;GGUF (&lt;code&gt;.gguf&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; The successor to GGML and now the &lt;strong&gt;de-facto standard for local inference&lt;/strong&gt;. A single file that contains the model weights, metadata, and even the prompt template.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Apple Silicon (M1/M2/M3)&lt;/strong&gt;: It works natively with Metal acceleration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU Inference&lt;/strong&gt;: If you don&#39;t have a dedicated GPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Setup&lt;/strong&gt;: Works with &lt;code&gt;llama.cpp&lt;/code&gt;, Ollama, and LM Studio.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it&#39;s great:&lt;/strong&gt; One file, works everywhere. Supports multiple quantization levels.&lt;/p&gt; &lt;h4 id=&#34;decoding-gguf-names-q3_k_m-q5_k_m-etc&#34;&gt;Decoding GGUF Names (Q3_K_M, Q5_K_M, etc.)&lt;/h4&gt; &lt;p&gt;You&#39;ll often see a long list of files like &lt;code&gt;Q3_K_M&lt;/code&gt;, &lt;code&gt;Q4_K_S&lt;/code&gt;, &lt;code&gt;Q5_K_M&lt;/code&gt;. These aren&#39;t random; they are specific &#34;K-quant&#34; formats.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to read &lt;code&gt;Q3_K_M&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Q3&lt;/strong&gt;: Average &lt;strong&gt;3-bit&lt;/strong&gt; quantization for the weights.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;K&lt;/strong&gt;: Uses the &lt;strong&gt;K-quant&lt;/strong&gt; scheme (a newer, smarter quantization method that uses non-uniform precision).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;M&lt;/strong&gt;: &lt;strong&gt;Medium&lt;/strong&gt; block size. This refers to the internal layout (&lt;code&gt;S&lt;/code&gt; = Small, &lt;code&gt;L&lt;/code&gt; = Large).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Which one should you pick?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Q3_K_M (The &#34;Budget&#34; Choice)&lt;/strong&gt;: Use when you are tight on memory. It has a noticeable quality drop but allows you to run larger models on weaker hardware.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q4_K_M (The &#34;Standard&#34;)&lt;/strong&gt;: The sweet spot. Best balance of speed, size, and perplexity. Indistinguishable from uncompressed for most tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Q5_K_M (The &#34;Premium&#34;)&lt;/strong&gt;: Use if you have VRAM to spare. Quality is very close to FP16 / Q8 while still being quite compact.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Pro Tip:&lt;/strong&gt; It is often better to run a &lt;strong&gt;larger model at lower quantization&lt;/strong&gt; (e.g., Llama-70B at Q3) than a &lt;strong&gt;smaller model at high quantization&lt;/strong&gt; (e.g., Llama-8B at Q8). Intelligence scales with parameter count more than precision.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;gptq-safetensors-configjson&#34;&gt;GPTQ (&lt;code&gt;.safetensors&lt;/code&gt; + &lt;code&gt;config.json&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; Post-training quantization optimized specifically for &lt;strong&gt;Nvidia GPUs&lt;/strong&gt;. It uses second-order information to minimize accuracy loss when compressing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Production Servers&lt;/strong&gt;: Running on Linux with Nvidia GPUs (CUDA).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Throughput&lt;/strong&gt;: Very fast inference at 4-bit.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ExLlamaV2&lt;/strong&gt;: Can be run with the ExLlamaV2 loader for extreme speed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Watch out for:&lt;/strong&gt; Requires a GPU. Won&#39;t run efficiently on CPU or Mac.&lt;/p&gt; &lt;h3 id=&#34;awq-safetensors&#34;&gt;AWQ (&lt;code&gt;.safetensors&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; Activation-Aware Weight Quantization. It analyzes which weights matter most during inference and preserves their precision better than naive quantization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: Often matches FP16 accuracy more closely than GPTQ at 4-bit.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: Supported natively by the vLLM serving engine.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it&#39;s great:&lt;/strong&gt; It&#39;s &#34;smarter&#34; quantization. If you care about squeezing every drop of quality out of a 4-bit model, AWQ is often the winner.&lt;/p&gt; &lt;h3 id=&#34;pytorch-safetensors-fp16bf16&#34;&gt;PyTorch / Safetensors (FP16/BF16)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;What it is:&lt;/strong&gt; Full-precision weights with no quantization. The original format most models are released in.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cloud inference&lt;/strong&gt; with powerful GPUs (A100, H100).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; and continued training.&lt;/li&gt; &lt;li&gt;When accuracy is paramount and memory isn&#39;t a constraint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Trade-offs:&lt;/strong&gt; Largest memory and disk footprint. A 70B model in FP16 needs ~140 GB VRAM!&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Choosing File Format&#34; src=&#34;../../../../assets/2025-05-11-llm-variant-guide/choose_file_format.svg&#34; /&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; When in doubt, start with &lt;strong&gt;GGUF Q4_K_M&lt;/strong&gt;. It&#39;s the Swiss Army knife of LLM formats—runs on 8GB VRAM GPUs, modern CPUs, and everything in between. You can always optimize later.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2 id=&#34;how-to-actually-run-these-serving-engines&#34;&gt;How to actually run these? (Serving Engines)&lt;/h2&gt; &lt;p&gt;You have the file. Now what? You need an engine to run it.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: The easiest CLI tool.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Uses&lt;/em&gt;: GGUF.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Good for&lt;/em&gt;: Mac, Linux, Windows, Local development.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Command&lt;/em&gt;: &lt;code&gt;ollama run llama3&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;: A beautiful GUI application.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Uses&lt;/em&gt;: GGUF.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Good for&lt;/em&gt;: Beginners, testing models visually, Mac/Windows.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: The production standard.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Uses&lt;/em&gt;: AWQ, GPTQ, Safetensors (FP16).&lt;/li&gt; &lt;li&gt;&lt;em&gt;Good for&lt;/em&gt;: High-performance servers, deploying APIs, Linux/Docker.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Note&lt;/em&gt;: Doesn&#39;t support GGUF well (yet).&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Llama.cpp&lt;/strong&gt;: The engine behind Ollama.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Uses&lt;/em&gt;: GGUF.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Good for&lt;/em&gt;: Low-level integration, running on Raspberry Pis, Android, etc.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;putting-it-all-together-a-decision-framework&#34;&gt;Putting it all together: a decision framework&lt;/h2&gt; &lt;p&gt;Here&#39;s a practical flowchart to help you choose. Start with your constraints (hardware and use case), then pick the appropriate combination.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;LLM Decision Tree&#34; src=&#34;../../../../assets/2025-05-11-llm-variant-guide/llm_decision_tree.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;quick-recommendations-by-scenario&#34;&gt;Quick recommendations by scenario&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Scenario 1: Building a chatbot on a MacBook Pro (16GB RAM)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Instruct (Llama-3-8B or Mistral-7B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Format:&lt;/strong&gt; GGUF Q4_K_M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why:&lt;/strong&gt; Runs smoothly on CPU/Metal, fits in memory, one-file simplicity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Scenario 2: RAG system on a server with RTX 4090 (24GB VRAM)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Instruct or MoE (Mixtral 8x7B or Qwen-14B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Format:&lt;/strong&gt; EXL2 (via ExLlamaV2) or AWQ 4-bit&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why:&lt;/strong&gt; Maximizes the 24GB VRAM. EXL2 is blazing fast on Nvidia cards.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Scenario 3: Fine-tuning for domain-specific use on cloud GPU&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Base&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Format:&lt;/strong&gt; FP16 Safetensors&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why:&lt;/strong&gt; You need full precision for training. Start with the unaligned base model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Scenario 4: High-throughput API with cost constraints&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Distilled (DeepSeek-Distill or similar)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Format:&lt;/strong&gt; AWQ 4-bit running on vLLM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why:&lt;/strong&gt; vLLM + AWQ offers incredible throughput (tokens/sec) per dollar.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;common-pitfalls-and-misconceptions&#34;&gt;Common pitfalls and misconceptions&lt;/h2&gt; &lt;h3 id=&#34;all-4-bit-models-are-the-same-quality&#34;&gt;&#34;All 4-bit models are the same quality&#34;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Not true.&lt;/strong&gt; A QAT 4-bit model (trained in 4-bit) often beats an 8-bit post-training quantized model. The &lt;em&gt;method&lt;/em&gt; matters. AWQ typically preserves more accuracy than naive GPTQ.&lt;/p&gt; &lt;h3 id=&#34;moe-models-work-with-any-inference-engine&#34;&gt;&#34;MoE models work with any inference engine&#34;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Not yet.&lt;/strong&gt; &lt;code&gt;llama.cpp&lt;/code&gt; handles MoE routing well. Support in other engines varies. Always check compatibility before downloading a 50GB MoE model.&lt;/p&gt; &lt;h3 id=&#34;distilled-models-are-just-smaller-versions&#34;&gt;&#34;Distilled models are just smaller versions&#34;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Nope.&lt;/strong&gt; A distilled 7B model can outperform a vanilla 13B model because it learned from a much larger teacher (often 70B+). It&#39;s compressed &lt;em&gt;knowledge&lt;/em&gt;, not just compressed &lt;em&gt;parameters&lt;/em&gt;.&lt;/p&gt; &lt;h3 id=&#34;i-should-quantize-my-qat-model-further-to-save-space&#34;&gt;&#34;I should quantize my QAT model further to save space&#34;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Don&#39;t.&lt;/strong&gt; QAT models were already trained in low-bit precision. Quantizing them again usually degrades quality significantly. Use them as-is.&lt;/p&gt; &lt;h3 id=&#34;bigger-is-always-better&#34;&gt;&#34;Bigger is always better&#34;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Context matters.&lt;/strong&gt; A well-tuned 8B Instruct model often outperforms a poorly-aligned 70B base model for specific tasks. Match the model variant to your use case—size isn&#39;t everything.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;tldr-just-tell-me-what-to-download&#34;&gt;TL;DR - Just tell me what to download&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;If you just want something that works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download a &lt;strong&gt;&lt;code&gt;&amp;lt;model-name&amp;gt;-Instruct.Q4_K_M.gguf&lt;/code&gt;&lt;/strong&gt; file from Hugging Face.&lt;/li&gt; &lt;li&gt;Run it with &lt;strong&gt;Ollama&lt;/strong&gt; or &lt;strong&gt;LM Studio&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;If it&#39;s too slow → try a smaller model or Distilled variant.&lt;/li&gt; &lt;li&gt;If you&#39;re out of memory → try a Q3_K_M quantization.&lt;/li&gt; &lt;li&gt;If quality isn&#39;t good enough → move up to Q5_K_M or switch to a larger model.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Start simple, optimize only when needed. The defaults are good enough for 90% of use cases.&lt;/p&gt; &lt;/blockquote&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/</link> <pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/</guid> </item> <item> <title>Quick-guide on Running LLMs Locally on macOS</title> <author>Viacheslav Dubrov</author> <category>guide</category> <category>llm</category> <category>macos</category> <category>tools</category> <description>&lt;h1 id=&#34;quick-guide-on-running-llms-locally-on-macos&#34;&gt;Quick-guide on Running LLMs Locally on macOS&lt;/h1&gt; &lt;p&gt;Running Large Language Models (LLMs) locally on your Mac is a game-changer. It means &lt;strong&gt;faster responses&lt;/strong&gt;, &lt;strong&gt;complete privacy&lt;/strong&gt;, and &lt;strong&gt;zero API bills&lt;/strong&gt;. But with so many tools popping up every week, which one should you choose?&lt;/p&gt; &lt;p&gt;This guide breaks down the top options—from dead-simple menu bar apps to full-control command-line tools. We&#39;ll cover what makes each special, their trade-offs, and how to get started.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;why-run-locally&#34;&gt;Why Run Locally?&lt;/h2&gt; &lt;p&gt;Before we dive into the tools, let&#39;s look at the benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;🔒 Privacy&lt;/strong&gt;: Your data and prompts never leave your machine. Perfect for sensitive work.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;⚡ Speed&lt;/strong&gt;: No network latency. On Apple Silicon, responses can be faster than cloud APIs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;💰 Cost&lt;/strong&gt;: One-time download. No monthly subscriptions or token fees.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;✈️ Offline&lt;/strong&gt;: Work from a plane, a cabin, or a coffee shop with spotty Wi-Fi.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;key-concepts-for-beginners&#34;&gt;Key Concepts for Beginners&lt;/h3&gt; &lt;p&gt;If you&#39;re new to local LLMs, here are three terms you&#39;ll see often:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: The act of &#34;running&#34; the model to generate text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization (GGUF)&lt;/strong&gt;: A technique to shrink model sizes with minimal quality loss. You&#39;ll see filenames like &lt;code&gt;llama-3-8b-Q4_K_M.gguf&lt;/code&gt;. The &lt;code&gt;Q4&lt;/code&gt; means &#34;4-bit quantization&#34;—it uses less RAM than the full 16-bit model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apple Silicon (Metal)&lt;/strong&gt;: Apple&#39;s M1/M2/M3 chips have &#34;Unified Memory,&#34; allowing the CPU and GPU to share RAM. This makes Macs uniquely powerful for running huge models that would require expensive dedicated GPUs on a PC.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: A Mac with Apple Silicon (M1, M2, M3, or M4) is highly recommended. Intel Macs work but will be significantly slower.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;:&lt;ul&gt; &lt;li&gt;&lt;strong&gt;8GB&lt;/strong&gt;: Can run small models (Mistral 7B, Llama 3 8B) comfortably.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;16GB+&lt;/strong&gt;: Recommended for larger models and multitasking.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disk Space&lt;/strong&gt;: Models take up space! Plan for ~10-20GB for a good starter library.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Local LLM Architecture&#34; src=&#34;../../../../assets/2025-05-10-guide-llm-macos-tools/local_llm_flow.svg&#34; /&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;1-ollama-the-just-works-option&#34;&gt;1. Ollama - The &#34;Just Works&#34; Option&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href=&#34;https://ollama.com/download/mac&#34;&gt;ollama.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Think of Ollama as the &#34;Docker for LLMs.&#34; It wraps the complex engine (&lt;code&gt;llama.cpp&lt;/code&gt;) in a sleek, native package. You install it, run one command, and you&#39;re chatting. It handles all the messy details like model downloading and hardware acceleration automatically.&lt;/p&gt; &lt;h3 id=&#34;best-for&#34;&gt;Best For&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Beginners&lt;/strong&gt; who want to get started in 5 minutes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developers&lt;/strong&gt; who want a simple CLI tool.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;example-workflow&#34;&gt;Example Workflow&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Download and run Llama 3 (it auto-downloads if needed)&lt;/span&gt; ollama&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;llama3 &lt;span class=&#34;c1&#34;&gt;# 2. Use it in your code via the local API&lt;/span&gt; curl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;http://localhost:11434/api/generate&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-d&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;{&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;model&amp;quot;: &amp;quot;llama3&amp;quot;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;prompt&amp;quot;: &amp;quot;Explain quantum computing to a 5-year-old&amp;quot;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;stream&amp;quot;: false&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;}&amp;#39;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;pros-cons&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;✅ Pros&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;❌ Cons&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Easiest setup (Drag-and-drop &lt;code&gt;.dmg&lt;/code&gt;)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Core application is closed-source&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Great CLI (&lt;code&gt;ollama list&lt;/code&gt;, &lt;code&gt;ollama pull&lt;/code&gt;)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Less granular control over generation parameters&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Huge library of pre-configured models&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;2-lm-studio-the-visual-explorer&#34;&gt;2. LM Studio - The Visual Explorer&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href=&#34;https://lmstudio.ai&#34;&gt;lmstudio.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LM Studio is for those who prefer a beautiful Graphical User Interface (GUI) over a terminal. It features a built-in &#34;App Store&#34; style browser for models, letting you search HuggingFace directly. It also supports Apple&#39;s native MLX format, which can be faster on some Macs.&lt;/p&gt; &lt;h3 id=&#34;best-for_1&#34;&gt;Best For&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual learners&lt;/strong&gt; who want to explore and test different models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Developers&lt;/strong&gt; needing an OpenAI-compatible local server.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;example-workflow_1&#34;&gt;Example Workflow&lt;/h3&gt; &lt;p&gt;LM Studio has a great Python SDK, but it also provides a local server that mimics OpenAI&#39;s API, meaning you can use standard libraries:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Using the official LM Studio SDK&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;lmstudio&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LMStudio&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LMStudio&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;complete&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;llama-3-8b&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Write a haiku about debugging.&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;pros-cons_1&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;✅ Pros&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;❌ Cons&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Beautiful, easy-to-use interface&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;GUI is closed-source&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Native support for both GGUF and MLX models&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Larger download (~750MB)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Built-in RAG (Chat with your PDFs)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;3-llamacpp-the-power-users-tool&#34;&gt;3. llama.cpp - The Power User&#39;s Tool&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;github.com/ggml-org/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the engine that powers almost everyone else. If you want &lt;strong&gt;maximum performance&lt;/strong&gt;, &lt;strong&gt;bleeding-edge features&lt;/strong&gt;, or to embed an LLM into your own C++ application, this is the source. It&#39;s bare-metal, lightweight, and incredibly powerful.&lt;/p&gt; &lt;h3 id=&#34;best-for_2&#34;&gt;Best For&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Engineers&lt;/strong&gt; and &lt;strong&gt;Power Users&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Running on older or constrained hardware.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;example-workflow_2&#34;&gt;Example Workflow&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Install via Homebrew&lt;/span&gt; brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;llama.cpp &lt;span class=&#34;c1&#34;&gt;# 2. Download a model manually (e.g., from HuggingFace)&lt;/span&gt; huggingface-cli&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;download&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;TheBloke/Llama-3-8B-Instruct-GGUF&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--local-dir&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;. &lt;span class=&#34;c1&#34;&gt;# 3. Run inference with full control&lt;/span&gt; llama-cli&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-m&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;llama-3-8b-instruct.Q4_K_M.gguf&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-p&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Write a python script to sort a list&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-n&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;512&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--temp&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;.7&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--ctx-size&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4096&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;pros-cons_2&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;✅ Pros&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;❌ Cons&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Ultimate control over every parameter&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Steep learning curve (CLI only)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;MIT Licensed (Open Source)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Manual model management&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Extremely lightweight (&amp;lt;30MB)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;4-gpt4all-privacy-first-rag&#34;&gt;4. GPT4All - Privacy-First &amp;amp; RAG&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href=&#34;https://gpt4all.io&#34;&gt;gpt4all.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT4All focuses heavily on &lt;strong&gt;privacy&lt;/strong&gt; and &lt;strong&gt;documents&lt;/strong&gt;. Its standout feature is &#34;LocalDocs,&#34; which lets you point the app at a folder of PDFs, notes, or code, and chat with them instantly. It runs completely offline with no telemetry.&lt;/p&gt; &lt;h3 id=&#34;best-for_3&#34;&gt;Best For&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy advocates&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Users who want to &lt;strong&gt;chat with their own documents&lt;/strong&gt; (RAG) easily.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;pros-cons_3&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;✅ Pros&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;❌ Cons&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&#34;LocalDocs&#34; RAG is excellent and easy&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;GUI-only (no headless mode)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Completely offline &amp;amp; private&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Heavier resource usage than Ollama&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Cross-platform (Mac, Windows, Linux)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;5-koboldcpp-for-storytellers&#34;&gt;5. KoboldCPP - For Storytellers&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href=&#34;https://github.com/LostRuins/koboldcpp&#34;&gt;github.com/LostRuins/koboldcpp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A fork of &lt;code&gt;llama.cpp&lt;/code&gt; tailored for &lt;strong&gt;creative writing&lt;/strong&gt; and &lt;strong&gt;Role-Playing Games (RPGs)&lt;/strong&gt;. It features a web interface designed for long-form text generation, with tools to manage &#34;World Info,&#34; character memory, and story consistency.&lt;/p&gt; &lt;h3 id=&#34;best-for_4&#34;&gt;Best For&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Writers&lt;/strong&gt;, &lt;strong&gt;Novelists&lt;/strong&gt;, and &lt;strong&gt;RPG players&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;example-workflow_3&#34;&gt;Example Workflow&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Download the single binary&lt;/span&gt; wget&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-mac.zip &lt;span class=&#34;c1&#34;&gt;# 2. Run it (launches a web server)&lt;/span&gt; ./koboldcpp&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--model&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;llama-3-8b.gguf&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--port&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5001&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--smartcontext &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;pros-cons_4&#34;&gt;Pros &amp;amp; Cons&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;✅ Pros&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;❌ Cons&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Best-in-class tools for creative writing&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Niche UI (not great for coding/chat)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Single file executable (no installation)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;AGPL license (restrictive for commercial use)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;honorable-mention-mlx-lm&#34;&gt;Honorable Mention: MLX-LM&lt;/h2&gt; &lt;p&gt;If you are a Python developer specifically targeting Apple Silicon, check out &lt;strong&gt;&lt;a href=&#34;https://github.com/ml-explore/mlx-examples/tree/main/llms&#34;&gt;MLX-LM&lt;/a&gt;&lt;/strong&gt; by Apple. It&#39;s a framework optimized specifically for the M-series chips. While less &#34;user-friendly&#34; than Ollama, it&#39;s often the fastest way to run models if you&#39;re comfortable with Python.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;summary-which-tool-is-right-for-you&#34;&gt;Summary: Which Tool is Right for You?&lt;/h2&gt; &lt;p&gt;Here is a quick decision tree to help you decide:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Decision Tree&#34; src=&#34;../../../../assets/2025-05-10-guide-llm-macos-tools/tool_decision_tree.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;quick-comparison-table&#34;&gt;Quick Comparison Table&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Tool&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Interface&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Difficulty&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Best Feature&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;CLI / Menu Bar&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;⭐ (Easy)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&#34;Just Works&#34; experience&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;GUI&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;⭐ (Easy)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Model discovery &amp;amp; UI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;GPT4All&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;GUI&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;⭐ (Easy)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Chat with local docs (RAG)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;KoboldCPP&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Web UI&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;⭐⭐ (Medium)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Creative writing tools&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;CLI&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;⭐⭐⭐ (Hard)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Raw performance &amp;amp; control&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt; &lt;p&gt;You can&#39;t really go wrong with any of these. They all run locally, they all respect your privacy, and they all leverage the incredible power of Apple Silicon.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Start with Ollama&lt;/strong&gt; if you just want to see what the fuss is about.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Try LM Studio&lt;/strong&gt; if you want to browse models visually.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dive into llama.cpp&lt;/strong&gt; if you want to understand how it all works under the hood.&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/</link> <pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/</guid> </item> <item> <title>Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</title> <author>Viacheslav Dubrov</author> <category>Deep Learning</category> <category>Distributed Training</category> <category>GPU</category> <category>LLM</category> <category>Parallelism</category> <description>&lt;h1 id=&#34;scaling-large-language-models-practical-multi-gpu-and-multi-node-strategies-for-2025&#34;&gt;Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025&lt;/h1&gt; &lt;p&gt;The race to build bigger, better language models continues at breakneck speed. Today&#39;s state-of-the-art models require massive computing resources that no single GPU can handle. Whether you&#39;re training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.&lt;/p&gt; &lt;p&gt;This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face&#39;s &lt;a href=&#34;https://huggingface.co/spaces/nanotron/ultrascale-playbook&#34;&gt;Ultra-Scale Playbook&lt;/a&gt;.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before diving in, you should be familiar with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Basic Deep Learning&lt;/strong&gt;: Backpropagation, gradients, and optimizers (AdamW).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transformer Architecture&lt;/strong&gt;: Attention mechanisms, Feed-Forward Networks (FFN).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PyTorch Basics&lt;/strong&gt;: &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;DataLoader&lt;/code&gt;, and the training loop.&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;why-scaling-matters&#34;&gt;Why Scaling Matters&lt;/h2&gt; &lt;p&gt;Modern LLMs have outgrown single GPUs. Here&#39;s why scaling is no longer optional:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model size&lt;/strong&gt;: A 70B parameter model needs ~140GB in FP16 format - that&#39;s nearly 2x what an A100 (80GB) can hold&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training time&lt;/strong&gt;: Even with 8 top-tier A100 GPUs, training a 13B model from scratch takes weeks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context length&lt;/strong&gt;: Long contexts (32k+ tokens) easily exceed single-GPU memory limits&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference speed&lt;/strong&gt;: For production workloads, distributing inference reduces latency and increases throughput&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The solution? Split the workload across multiple GPUs. Let&#39;s explore how.&lt;/p&gt; &lt;h2 id=&#34;1-parallelism-techniques-explained-simply&#34;&gt;1. Parallelism Techniques Explained Simply&lt;/h2&gt; &lt;h3 id=&#34;11-data-parallelism-dp&#34;&gt;1.1 Data Parallelism (DP)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; Multiple workers with identical instruction manuals (the model), each working on different examples.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Each GPU gets a complete copy of the model&lt;/li&gt; &lt;li&gt;Each GPU processes different batches of data&lt;/li&gt; &lt;li&gt;After computing gradients, all GPUs synchronize by averaging their gradients&lt;/li&gt; &lt;li&gt;Everyone updates their model copy with the averaged gradients&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your model fits comfortably on a single GPU&lt;/li&gt; &lt;li&gt;You want to process more data faster&lt;/li&gt; &lt;li&gt;You need the simplest distributed setup with minimal code changes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitation:&lt;/strong&gt; Memory inefficient - every GPU stores the full model, so you&#39;re not saving memory, just increasing throughput.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Data Parallelism&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/data_parallelism.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href=&#34;https://pytorch.org/docs/stable/notes/ddp.html&#34;&gt;PyTorch DDP&lt;/a&gt;, &lt;a href=&#34;https://horovod.ai/&#34;&gt;Horovod&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;12-fully-sharded-data-parallelism-fsdp&#34;&gt;1.2 Fully Sharded Data Parallelism (FSDP)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; Like Data Parallelism, but memory-efficient. Each worker keeps only part of the instruction manual and borrows pages from colleagues when needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Model parameters, gradients, and optimizer states are &lt;strong&gt;sharded&lt;/strong&gt; (split) across all GPUs&lt;/li&gt; &lt;li&gt;During forward pass: each GPU gathers the parameters it needs from other GPUs&lt;/li&gt; &lt;li&gt;After using them, it discards those borrowed parameters to save memory&lt;/li&gt; &lt;li&gt;During backward pass: same gathering happens for gradient computation&lt;/li&gt; &lt;li&gt;After backward pass: gradients are reduced and each GPU updates only its own parameter shard&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your model is too large for a single GPU (typically &amp;gt;10B parameters)&lt;/li&gt; &lt;li&gt;You want to train bigger models without changing your code much&lt;/li&gt; &lt;li&gt;You&#39;re working on a single machine with multiple GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Real-world impact:&lt;/strong&gt; FSDP lets you train models 4-8x larger than what fits on one GPU.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;FSDP&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/fsdp.svg&#34; /&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[!NOTE] &amp;gt; &lt;strong&gt;Understanding ZeRO Stages&lt;/strong&gt; FSDP is often described in terms of &#34;ZeRO stages&#34; (Zero Redundancy Optimizer):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stage 1&lt;/strong&gt;: Shard optimizer states only (4x memory savings).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 2&lt;/strong&gt;: Shard gradients + optimizer states (8x memory savings).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 3&lt;/strong&gt;: Shard parameters + gradients + optimizer states (Linear memory savings with N GPUs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PyTorch FSDP defaults to Stage 3 behavior.&lt;/p&gt; &lt;/blockquote&gt; &lt;h4 id=&#34;example-enabling-fsdp-in-pytorch&#34;&gt;Example: Enabling FSDP in PyTorch&lt;/h4&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;torch.distributed.fsdp&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FullyShardedDataParallel&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FSDP&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Wrap your model&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MyLLM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FSDP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Train as usual&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optimizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;step&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href=&#34;https://pytorch.org/docs/stable/fsdp.html&#34;&gt;PyTorch FSDP&lt;/a&gt;, &lt;a href=&#34;https://www.deepspeed.ai/tutorials/zero/&#34;&gt;DeepSpeed ZeRO-3&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;13-tensor-parallelism-tp&#34;&gt;1.3 Tensor Parallelism (TP)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; Split individual layers across GPUs - like dividing a massive spreadsheet calculation where each person computes a few columns.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Take a single layer&#39;s weight matrix and split it into chunks&lt;/li&gt; &lt;li&gt;Each GPU gets one chunk and computes its portion of the output&lt;/li&gt; &lt;li&gt;Results are combined (via all-reduce or concatenation) before passing to the next layer&lt;/li&gt; &lt;li&gt;This happens at &lt;strong&gt;every&lt;/strong&gt; layer in the model&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Individual layers are too large even with FSDP (e.g., huge attention or FFN layers)&lt;/li&gt; &lt;li&gt;You have fast GPU-to-GPU connections (NVLink/NVSwitch)&lt;/li&gt; &lt;li&gt;You&#39;re working within a single node (TP doesn&#39;t scale well across nodes due to communication overhead)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sweet spot:&lt;/strong&gt; TP degree of 2-8 within a single machine with NVLink.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Tensor Parallelism&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/tensor_parallelism.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM&#34;&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;ColossalAI&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;14-pipeline-parallelism-pp&#34;&gt;1.4 Pipeline Parallelism (PP)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; Split the model vertically by layers - like an assembly line where each station handles specific layers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Divide your model into stages (e.g., layers 1-10, 11-20, 21-30)&lt;/li&gt; &lt;li&gt;Assign each stage to a different GPU&lt;/li&gt; &lt;li&gt;Send micro-batches through the pipeline: GPU 1 processes batch 1, sends output to GPU 2, then starts on batch 2&lt;/li&gt; &lt;li&gt;Multiple micro-batches flow through simultaneously to keep all GPUs busy&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Very deep models that don&#39;t fit on available GPUs even with FSDP&lt;/li&gt; &lt;li&gt;Multi-node training where inter-node bandwidth is limited&lt;/li&gt; &lt;li&gt;Combined with TP and FSDP for massive models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; Pipeline &#34;bubbles&#34; (idle time) at the start and end of each batch. Use multiple micro-batches to minimize this.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Pipeline Parallelism&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/pipeline_parallelism.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href=&#34;https://www.deepspeed.ai/tutorials/pipeline/&#34;&gt;DeepSpeed PP&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;Megatron-LM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1811.06965&#34;&gt;GPipe&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;15-context-parallelism-cp&#34;&gt;1.5 Context Parallelism (CP)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; For handling extremely long sequences - different people read different paragraphs of a book, then share key information.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Split a long sequence (e.g., 64K tokens) across multiple GPUs (e.g., 4 GPUs × 16K tokens each)&lt;/li&gt; &lt;li&gt;Each GPU runs self-attention on its local chunk&lt;/li&gt; &lt;li&gt;GPUs exchange keys and values to compute cross-attention (how tokens in one chunk relate to tokens in other chunks)&lt;/li&gt; &lt;li&gt;Results are merged to produce the final output&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Processing very long contexts (64K, 128K, or even 1M+ tokens)&lt;/li&gt; &lt;li&gt;Document analysis, long-form code generation, or book-length reasoning&lt;/li&gt; &lt;li&gt;When context length is the bottleneck, not model size&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Real-world impact:&lt;/strong&gt; Context Parallelism enables 100K+ token processing on consumer hardware that would otherwise max out at 8K tokens.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Context Parallelism&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/context_parallelism.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href=&#34;https://github.com/huggingface/picotron&#34;&gt;Picotron&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/nanotron&#34;&gt;Nanotron&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;16-expert-parallelism-mixture-of-experts-moe&#34;&gt;1.6 Expert Parallelism (Mixture of Experts - MoE)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The idea:&lt;/strong&gt; Specialized consultants - instead of activating the entire model for every input, each token gets routed only to the &#34;experts&#34; it needs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Replace dense feed-forward layers with multiple &#34;expert&#34; networks (e.g., 8 or 64 experts)&lt;/li&gt; &lt;li&gt;A gating network decides which experts (usually top-2) should process each token&lt;/li&gt; &lt;li&gt;Only those selected experts activate for that token&lt;/li&gt; &lt;li&gt;Different experts can live on different GPUs&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When to use it:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You want a model with 100B+ total parameters but only want to activate 13B per token&lt;/li&gt; &lt;li&gt;You need better parameter efficiency than dense models&lt;/li&gt; &lt;li&gt;You&#39;re okay with more complex training dynamics&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Real-world examples:&lt;/strong&gt; Mixtral-8x7B (56B total params, 13B active), Grok, DeepSeek-V2.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trade-off:&lt;/strong&gt; More parameters with less compute per token, but training can be trickier due to load balancing between experts.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Mixture of Experts&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/moe.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href=&#34;https://github.com/huggingface/picotron&#34;&gt;Picotron&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/nanotron&#34;&gt;Nanotron&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;quick-comparison-which-parallelism-should-you-use&#34;&gt;Quick Comparison: Which Parallelism Should You Use?&lt;/h3&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Technique&lt;/th&gt; &lt;th&gt;What It Splits&lt;/th&gt; &lt;th&gt;Best For&lt;/th&gt; &lt;th&gt;Memory Savings&lt;/th&gt; &lt;th&gt;Communication Cost&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Data Parallelism (DP)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Data batches&lt;/td&gt; &lt;td&gt;Models that fit on 1 GPU&lt;/td&gt; &lt;td&gt;None (copies model)&lt;/td&gt; &lt;td&gt;Low (only gradients)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;FSDP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Model + optimizer + gradients&lt;/td&gt; &lt;td&gt;Models too big for 1 GPU&lt;/td&gt; &lt;td&gt;High (4-8x)&lt;/td&gt; &lt;td&gt;Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Tensor Parallelism (TP)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Individual layers&lt;/td&gt; &lt;td&gt;Huge layers, fast GPUs&lt;/td&gt; &lt;td&gt;Medium&lt;/td&gt; &lt;td&gt;High (per layer)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Pipeline Parallelism (PP)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Layer groups (stages)&lt;/td&gt; &lt;td&gt;Very deep models&lt;/td&gt; &lt;td&gt;Medium&lt;/td&gt; &lt;td&gt;Low (between stages)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Context Parallelism (CP)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Sequence length&lt;/td&gt; &lt;td&gt;Long contexts (64K+ tokens)&lt;/td&gt; &lt;td&gt;High (for activations)&lt;/td&gt; &lt;td&gt;Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Expert Parallelism (MoE)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Experts in MoE layers&lt;/td&gt; &lt;td&gt;Massive sparse models&lt;/td&gt; &lt;td&gt;None (more params, less FLOPs)&lt;/td&gt; &lt;td&gt;Medium&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;strong&gt;Rule of thumb:&lt;/strong&gt; Start with FSDP. Add TP if individual layers are too big. Add PP if you need multiple nodes. Add CP if context length is your bottleneck.&lt;/p&gt; &lt;h2 id=&#34;2-practical-training-strategies&#34;&gt;2. Practical Training Strategies&lt;/h2&gt; &lt;p&gt;Now that you understand the techniques, here&#39;s what to actually do based on your hardware setup.&lt;/p&gt; &lt;h3 id=&#34;21-single-machine-2-8-gpus&#34;&gt;2.1 Single Machine (2-8 GPUs)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Recommended approach:&lt;/strong&gt; FSDP, optionally + TP&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What to do:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start with pure FSDP using PyTorch FSDP or DeepSpeed ZeRO-2/ZeRO-3&lt;/li&gt; &lt;li&gt;If your model has huge attention or FFN layers that still don&#39;t fit, add TP=2&lt;/li&gt; &lt;li&gt;Use Hugging Face &lt;code&gt;accelerate&lt;/code&gt; or PyTorch &lt;code&gt;torchrun&lt;/code&gt; for easy setup&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Hardware-specific tips:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Consumer GPUs (RTX 4090, etc.) with PCIe: Stick to TP=1 or TP=2 max&lt;/li&gt; &lt;li&gt;Server GPUs (A100, H100) with NVLink: You can efficiently use TP=2 to TP=4&lt;/li&gt; &lt;li&gt;8 GPUs in one box: FSDP alone often works great for models up to 70B&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;22-small-cluster-2-16-nodes-128-gpus&#34;&gt;2.2 Small Cluster (2-16 nodes, ≤128 GPUs)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Recommended approach:&lt;/strong&gt; 2D or 3D parallelism (TP + FSDP, optionally + PP)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What to do:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use TP within each node (e.g., TP=4 or TP=8 per node with NVLink)&lt;/li&gt; &lt;li&gt;Use FSDP across nodes for data parallelism&lt;/li&gt; &lt;li&gt;If your model is extremely deep, add PP to split it vertically across nodes&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Why this works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast intra-node connections (NVLink) handle TP&#39;s high communication needs&lt;/li&gt; &lt;li&gt;Slower inter-node connections (InfiniBand) only need to sync FSDP shards&lt;/li&gt; &lt;li&gt;Minimizes cross-node bandwidth requirements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Pro tip:&lt;/strong&gt; When using Pipeline Parallelism, set your number of micro-batches to at least 4× your pipeline degree to keep GPUs busy and minimize &#34;bubbles.&#34;&lt;/p&gt; &lt;h3 id=&#34;23-large-cluster-hundreds-or-thousands-of-gpus&#34;&gt;2.3 Large Cluster (Hundreds or Thousands of GPUs)&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Recommended approach:&lt;/strong&gt; 4D parallelism (DP × TP × PP × CP)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What to do:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Combine all four parallelism strategies to handle the largest models&lt;/li&gt; &lt;li&gt;Carefully map parallelism strategies to your hardware topology&lt;/li&gt; &lt;li&gt;Use tools like Megatron-LM or Nanotron that support 4D parallelism out of the box&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;When you need this:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training models with 70B+ parameters and 32K+ context windows&lt;/li&gt; &lt;li&gt;Pretraining from scratch (not fine-tuning)&lt;/li&gt; &lt;li&gt;Production-scale model training at big labs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance expectations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;With good InfiniBand networking: ~70-80% scaling efficiency&lt;/li&gt; &lt;li&gt;With excellent setup and tuning: ~85% scaling efficiency possible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Real-world example:&lt;/strong&gt; Training a 70B model with 32K context on 512 GPUs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TP=8 (within each 8-GPU node)&lt;/li&gt; &lt;li&gt;PP=4 (pipeline across 4 nodes)&lt;/li&gt; &lt;li&gt;CP=4 (split context across 4 chunks)&lt;/li&gt; &lt;li&gt;DP=4 (data parallelism for throughput)&lt;/li&gt; &lt;li&gt;Total: 8 × 4 × 4 × 4 = 512 GPUs&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;3-practical-tools-worth-learning&#34;&gt;3. Practical Tools Worth Learning&lt;/h2&gt; &lt;p&gt;Here&#39;s a quick guide to the most useful tools and when to reach for them:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Tool&lt;/th&gt; &lt;th&gt;When to Use It&lt;/th&gt; &lt;th&gt;Learning Curve&lt;/th&gt; &lt;th&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Hugging Face Accelerate&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Any distributed training with minimal code changes&lt;/td&gt; &lt;td&gt;★☆☆☆☆&lt;/td&gt; &lt;td&gt;Beginners, quick prototypes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;PyTorch FSDP&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Medium-large models (1-30B) on single node&lt;/td&gt; &lt;td&gt;★★☆☆☆&lt;/td&gt; &lt;td&gt;Most common use case&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSpeed ZeRO&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Multi-node training with good documentation&lt;/td&gt; &lt;td&gt;★★★☆☆&lt;/td&gt; &lt;td&gt;Production training&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Megatron-LM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Very large models (70B+), 3D/4D parallelism&lt;/td&gt; &lt;td&gt;★★★★☆&lt;/td&gt; &lt;td&gt;Advanced/production at scale&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Nanotron&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Learning/research on modern parallelism strategies&lt;/td&gt; &lt;td&gt;★★★☆☆&lt;/td&gt; &lt;td&gt;Education, experimentation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Fast inference with PagedAttention and KV caching&lt;/td&gt; &lt;td&gt;★★☆☆☆&lt;/td&gt; &lt;td&gt;Serving models in production&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Maximum inference speed on NVIDIA GPUs&lt;/td&gt; &lt;td&gt;★★★★☆&lt;/td&gt; &lt;td&gt;Production inference optimization&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h4 id=&#34;example-accelerate-config-for-fsdp&#34;&gt;Example: Accelerate Config for FSDP&lt;/h4&gt; &lt;p&gt;To get started with FSDP using Hugging Face Accelerate, you can run &lt;code&gt;accelerate config&lt;/code&gt; or create a &lt;code&gt;config.yaml&lt;/code&gt; like this:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;nt&#34;&gt;compute_environment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;LOCAL_MACHINE&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;distributed_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;FSDP&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;fsdp_config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;fsdp_auto_wrap_policy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;TRANSFORMER_BASED_WRAP&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;fsdp_backward_prefetch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;BACKWARD_PRE&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;fsdp_state_dict_type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;SHARDED_STATE_DICT&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;machine_rank&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;main_process_ip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;null&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;main_process_port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;null&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;main_training_function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;main&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;mixed_precision&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;bf16&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;num_machines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;num_processes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;8&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;use_cpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;false&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;My recommendation for getting started:&lt;/strong&gt; Start with Hugging Face Accelerate for learning, then graduate to PyTorch FSDP or DeepSpeed when you need more control.&lt;/p&gt; &lt;h2 id=&#34;4-making-the-right-choice-a-decision-framework&#34;&gt;4. Making the Right Choice: A Decision Framework&lt;/h2&gt; &lt;p&gt;Still not sure what to use? Follow this decision tree:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 1: Does your model fit on a single GPU?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Yes&lt;/strong&gt; → Use standard training (no parallelism needed)&lt;/li&gt; &lt;li&gt;❌ &lt;strong&gt;No&lt;/strong&gt; → Continue to Step 2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 2: Do you have multiple GPUs in one machine?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Yes&lt;/strong&gt; → Start with FSDP&lt;/li&gt; &lt;li&gt;❌ &lt;strong&gt;No&lt;/strong&gt; → You&#39;ll need a cluster or smaller model (skip to Step 4)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 3: Is FSDP alone enough?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Yes&lt;/strong&gt; → You&#39;re done! Use pure FSDP&lt;/li&gt; &lt;li&gt;❌ &lt;strong&gt;No, individual layers are too big&lt;/strong&gt; → Add TP=2 or TP=4&lt;/li&gt; &lt;li&gt;❌ &lt;strong&gt;No, context is too long&lt;/strong&gt; → Add CP&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 4: Training across multiple nodes?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start with: TP within nodes + FSDP across nodes&lt;/li&gt; &lt;li&gt;If model is very deep: Add PP to split layers across nodes&lt;/li&gt; &lt;li&gt;If you have 100+ GPUs and long contexts: Consider 4D parallelism (TP + PP + DP + CP)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual decision tree:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Scaling Decision Tree&#34; src=&#34;../../../../assets/2025-05-04-scaling-llms/scaling_decision_tree.svg&#34; /&gt;&lt;/p&gt; &lt;h2 id=&#34;5-the-ultra-scale-cheatsheet&#34;&gt;5. The Ultra-Scale Cheatsheet&lt;/h2&gt; &lt;p&gt;For a comprehensive visual summary, check out this guide from Hugging Face&#39;s team:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Ultra-Scale LLM Cheatsheet&#34; src=&#34;https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ultra-cheatsheet.svg&#34; /&gt;&lt;/p&gt; &lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Scaling LLMs is both an art and a science. The key takeaways:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Start simple:&lt;/strong&gt; Most people should begin with FSDP. It handles the majority of use cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add complexity only when needed:&lt;/strong&gt; Don&#39;t jump straight to 4D parallelism unless you&#39;re training at massive scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Match strategy to hardware:&lt;/strong&gt; TP works best within nodes, FSDP across nodes, PP for extreme depth.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools matter:&lt;/strong&gt; Use Accelerate to learn, FSDP or DeepSpeed for production.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The techniques here follow logical patterns based on hardware constraints and model architecture. With the right approach, you can scale from a single GPU to thousands, training models that would have been impossible just a few years ago.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Further resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/nanotron/ultrascale-playbook&#34;&gt;Hugging Face Ultra-Scale Playbook&lt;/a&gt; - Interactive guide with more details&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&#34;&gt;PyTorch FSDP Tutorial&lt;/a&gt; - Official getting started guide&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/tutorials/&#34;&gt;DeepSpeed Tutorials&lt;/a&gt; - Comprehensive DeepSpeed documentation&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/</link> <pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/</guid> </item> <item> <title>Quick Guide: Managing Python on macOS with uv</title> <author>Viacheslav Dubrov</author> <category>guide</category> <category>python</category> <category>tooling</category> <description>&lt;h2 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h2&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Install uv&lt;/span&gt; brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uv &lt;span class=&#34;c1&#34;&gt;# For new projects (modern workflow)&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;init&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# create project structure&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;add&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pandas&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;numpy&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# add dependencies&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;train.py&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# run your script&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# For existing projects (legacy workflow)&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;venv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# create virtual environment&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pip&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-r&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;requirements.txt&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# install dependencies&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;train.py&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# run your script&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Run tools without installing them&lt;/span&gt; uvx&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;ruff&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;check&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;.&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# run linter&lt;/span&gt; uvx&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;black&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;.&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# run formatter&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;why-uv&#34;&gt;Why uv?&lt;/h2&gt; &lt;p&gt;If you&#39;ve been using Python for a while, you&#39;re likely familiar with the &#34;tool fatigue&#34; of managing &lt;code&gt;pip&lt;/code&gt;, &lt;code&gt;virtualenv&lt;/code&gt;, &lt;code&gt;pip-tools&lt;/code&gt;, &lt;code&gt;pyenv&lt;/code&gt;, and &lt;code&gt;poetry&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;uv&lt;/code&gt; replaces all of them.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Written in Rust, it is designed to be a drop-in replacement that is &lt;strong&gt;10-100x faster&lt;/strong&gt; than existing tools. It unifies your workflow into a single, cohesive experience.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;The uv Ecosystem&#34; src=&#34;../../../../assets/2025-04-17-uv-on-macos/uv-ecosystem.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;It handles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Package management&lt;/strong&gt; (replacing &lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;pip-tools&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python installation&lt;/strong&gt; (replacing &lt;code&gt;pyenv&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Virtual environments&lt;/strong&gt; (replacing &lt;code&gt;virtualenv&lt;/code&gt; and &lt;code&gt;venv&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool execution&lt;/strong&gt; (replacing &lt;code&gt;pipx&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Project management&lt;/strong&gt; (replacing &lt;code&gt;poetry&lt;/code&gt; or &lt;code&gt;pdm&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;installing-uv&#34;&gt;Installing uv&lt;/h2&gt; &lt;p&gt;The easiest way to install &lt;code&gt;uv&lt;/code&gt; on macOS is via Homebrew:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uv &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;code&gt;uv&lt;/code&gt; automatically detects your Mac&#39;s architecture (Apple Silicon or Intel), so no extra configuration is needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Keep it updated:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;upgrade&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uv &lt;span class=&#34;c1&#34;&gt;# OR&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;self&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;update &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;core-concepts&#34;&gt;Core Concepts&lt;/h2&gt; &lt;p&gt;&lt;code&gt;uv&lt;/code&gt; simplifies Python development by handling three distinct use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Projects&lt;/strong&gt;: Building an application or library with dependencies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scripts&lt;/strong&gt;: Running a single-file Python script with inline dependencies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Running command-line utilities (like &lt;code&gt;ruff&lt;/code&gt; or &lt;code&gt;httpie&lt;/code&gt;) globally.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;1-modern-project-management&#34;&gt;1. Modern Project Management&lt;/h3&gt; &lt;p&gt;For new projects, &lt;code&gt;uv&lt;/code&gt; uses the standard &lt;code&gt;pyproject.toml&lt;/code&gt; for configuration and a cross-platform &lt;code&gt;uv.lock&lt;/code&gt; for reproducible builds.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Modern uv Project Structure&#34; src=&#34;../../../../assets/2025-04-17-uv-on-macos/uv-project-structure.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Start a new project:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;init&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;my-project &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;my-project &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This creates a clean project structure with a &lt;code&gt;pyproject.toml&lt;/code&gt;, &lt;code&gt;.gitignore&lt;/code&gt;, and a &lt;code&gt;hello.py&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Add dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Add runtime dependencies&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;add&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pandas&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;requests &lt;span class=&#34;c1&#34;&gt;# Add development dependencies&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;add&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pytest&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;ruff&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--dev &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Run your code:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;hello.py &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;code&gt;uv&lt;/code&gt; automatically manages the virtual environment in &lt;code&gt;.venv&lt;/code&gt;. You never need to manually activate it!&lt;/p&gt; &lt;h3 id=&#34;2-managing-python-versions&#34;&gt;2. Managing Python Versions&lt;/h3&gt; &lt;p&gt;Forget &lt;code&gt;pyenv&lt;/code&gt;. &lt;code&gt;uv&lt;/code&gt; can install and manage Python versions for you, keeping them isolated in &lt;code&gt;~/.cache/uv&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Install a specific version:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;.12 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Pin a version for your project:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pin&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;.11 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This creates a &lt;code&gt;.python-version&lt;/code&gt; file. When you run &lt;code&gt;uv run&lt;/code&gt;, it will automatically use the pinned version, downloading it if necessary. This ensures your entire team and CI pipeline use the &lt;em&gt;exact same Python version&lt;/em&gt;.&lt;/p&gt; &lt;h3 id=&#34;3-running-tools-with-uvx&#34;&gt;3. Running Tools with &lt;code&gt;uvx&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;Use &lt;code&gt;uvx&lt;/code&gt; (an alias for &lt;code&gt;uv tool run&lt;/code&gt;) to execute Python command-line tools without polluting your global environment or project dependencies.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Run a linter&lt;/span&gt; uvx&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;ruff&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;check&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;. &lt;span class=&#34;c1&#34;&gt;# Run a formatter&lt;/span&gt; uvx&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;black&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;. &lt;span class=&#34;c1&#34;&gt;# Start a temporary Jupyter server&lt;/span&gt; uvx&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--from&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;jupyterlab&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;jupyter&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;lab &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Each tool runs in its own isolated, temporary environment. It&#39;s fast, clean, and safe.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;legacy-projects-requirementstxt&#34;&gt;Legacy Projects (requirements.txt)&lt;/h2&gt; &lt;p&gt;If you have an existing project using &lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;uv&lt;/code&gt; works as a drop-in replacement for &lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;venv&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Create a virtual environment&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;venv &lt;span class=&#34;c1&#34;&gt;# Install dependencies (lightning fast!)&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pip&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-r&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;requirements.txt &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;app.py &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;performance-notes&#34;&gt;Performance Notes&lt;/h2&gt; &lt;p&gt;Why is &lt;code&gt;uv&lt;/code&gt; so fast?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Rust&lt;/strong&gt;: It&#39;s built with performance in mind, without the overhead of Python startup times.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Global Cache&lt;/strong&gt;: It caches built wheels globally. If you&#39;ve installed &lt;code&gt;numpy&lt;/code&gt; in one project, installing it in another is instant (using copy-on-write links on macOS).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallelism&lt;/strong&gt;: It downloads and installs packages in parallel, maximizing your bandwidth.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Task&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Old Way&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;The uv Way&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Install Python&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;pyenv install 3.12&lt;/code&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;uv python install 3.12&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;New Project&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;mkdir proj &amp;amp;&amp;amp; cd proj &amp;amp;&amp;amp; python -m venv .venv&lt;/code&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;uv init proj&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Install Package&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;pip install pandas &amp;amp;&amp;amp; pip freeze &amp;gt; requirements.txt&lt;/code&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;uv add pandas&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Run Script&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;source .venv/bin/activate &amp;amp;&amp;amp; python script.py&lt;/code&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;uv run script.py&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Run Tool&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;pipx run black&lt;/code&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;uvx black&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Switching to &lt;code&gt;uv&lt;/code&gt; on macOS is one of the highest-ROI changes you can make to your Python workflow today. It&#39;s faster, simpler, and standard-compliant.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/</link> <pubDate>Sun, 23 Nov 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/</guid> </item> <item> <title>Quick-Guide on setting up a MacBook for AI Engineering</title> <author>Viacheslav Dubrov</author> <category>guide</category> <category>macos</category> <category>tooling</category> <description>&lt;h1 id=&#34;quick-guide-on-setting-up-a-macbook-for-ai-engineering&#34;&gt;Quick-Guide on setting up a MacBook for AI Engineering&lt;/h1&gt; &lt;p&gt;Setting up a new MacBook for AI development doesn&#39;t have to be overwhelming. Here&#39;s my streamlined 10-step process to transform a fresh macOS installation into a fully functional AI engineering workstation.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;1-install-xcode-command-line-tools&#34;&gt;1. Install Xcode Command Line Tools&lt;/h2&gt; &lt;p&gt;Start by installing the Xcode Command Line Tools. These are essential building blocks for any software development on macOS, including AI and data science work.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;xcode-select&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--install &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This command opens a dialog that walks you through the installation process.&lt;/p&gt; &lt;h2 id=&#34;2-install-homebrew&#34;&gt;2. Install Homebrew&lt;/h2&gt; &lt;p&gt;Next, install &lt;a href=&#34;https://brew.sh&#34;&gt;Homebrew&lt;/a&gt;, the go-to package manager for macOS. It makes installing and managing software incredibly simple. Run this command:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/bin/bash&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-c&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;curl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-fsSL&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The installer will guide you through the process and may ask for your password.&lt;/p&gt; &lt;h2 id=&#34;3-install-essential-development-tools&#34;&gt;3. Install essential development tools&lt;/h2&gt; &lt;p&gt;Now let&#39;s install the core tools you&#39;ll need for AI engineering:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;openssl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;readline&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;sqlite3&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;xz&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;zlib&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pyenv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;htop&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;gitmoji&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pandoc&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;ncdu&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;tmux &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Here&#39;s what each tool does:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Python environment:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/pyenv/pyenv&#34;&gt;pyenv&lt;/a&gt; — manage multiple Python versions seamlessly&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/astral-sh/uv&#34;&gt;uv&lt;/a&gt; — fast Python package manager and environment handler&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;System libraries:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://www.openssl.org&#34;&gt;openssl&lt;/a&gt; — SSL/TLS cryptography support&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://tiswww.case.edu/php/chet/readline/rltop.html&#34;&gt;readline&lt;/a&gt; — command-line text editing&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://sqlite.org&#34;&gt;sqlite3&lt;/a&gt; — lightweight embedded database&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://tukaani.org/xz&#34;&gt;xz&lt;/a&gt; — advanced data compression&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://zlib.net&#34;&gt;zlib&lt;/a&gt; — compression library&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Productivity tools:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://htop.dev&#34;&gt;htop&lt;/a&gt; — visual system monitor and process viewer&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/tmux/tmux&#34;&gt;tmux&lt;/a&gt; — manage multiple terminal sessions&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://dev.yorhel.nl/ncdu&#34;&gt;ncdu&lt;/a&gt; — analyze disk usage interactively&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://gitmoji.dev&#34;&gt;gitmoji&lt;/a&gt; — add emojis to commit messages&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://pandoc.org&#34;&gt;pandoc&lt;/a&gt; — convert documents between formats&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For more detailed information about using &lt;code&gt;uv&lt;/code&gt; for Python development, check out my &lt;a href=&#34;../../17/quick-guide-managing-python-on-macos-with-uv/&#34;&gt;Quick-Guide on managing Python on macOS with uv&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2 id=&#34;4-choose-your-terminal&#34;&gt;4. Choose your terminal&lt;/h2&gt; &lt;p&gt;The default macOS Terminal works fine, but I&#39;ve found better alternatives. I recently switched from &lt;a href=&#34;https://www.iterm2.com&#34;&gt;iTerm2&lt;/a&gt; to &lt;a href=&#34;https://www.warp.dev/&#34;&gt;Warp&lt;/a&gt;. Warp is a modern, Rust-based terminal with built-in AI features that make your workflow smoother.&lt;/p&gt; &lt;p&gt;You can download Warp from their &lt;a href=&#34;https://www.warp.dev/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &lt;h3 id=&#34;optional-iterm2-configuration&#34;&gt;Optional: iTerm2 configuration&lt;/h3&gt; &lt;p&gt;If you prefer the battle-tested iTerm2, here&#39;s my recommended setup:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Enable natural text editing:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open Preferences → Profiles → Keys → Key Mappings&lt;/li&gt; &lt;li&gt;Click the Presets… dropdown&lt;/li&gt; &lt;li&gt;Select &#34;Natural Text Editing&#34;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Choose a color theme:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Browse themes at &lt;a href=&#34;https://github.com/mbadolato/iTerm2-Color-Schemes&#34;&gt;iTerm2-Color-Schemes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Open Preferences → Profiles → Colors → Color Presets…&lt;/li&gt; &lt;li&gt;Click Import and select your downloaded theme&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&#34;5-set-up-zsh-with-oh-my-zsh&#34;&gt;5. Set up Zsh with Oh My Zsh&lt;/h2&gt; &lt;p&gt;Modern macOS comes with Zsh as the default shell, but we&#39;ll enhance it with &lt;a href=&#34;https://github.com/robbyrussell/oh-my-zsh&#34;&gt;Oh My Zsh&lt;/a&gt;, a framework that makes Zsh more powerful and easier to customize:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;zsh sh&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-c&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;curl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-fsSL&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The Oh My Zsh installer will back up your existing Zsh configuration and set up the new one.&lt;/p&gt; &lt;h2 id=&#34;6-add-zsh-plugins-for-superpowers&#34;&gt;6. Add Zsh plugins for superpowers&lt;/h2&gt; &lt;p&gt;Plugins make your terminal smarter and more productive. Edit your &lt;code&gt;~/.zshrc&lt;/code&gt; file to add these plugins:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;nv&#34;&gt;plugins&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=(&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;aws&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;bgnotify&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;docker&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;docker-compose &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;emoji&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;forklift&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;gcloud&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;git&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;history&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;iterm2 &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;keychain&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;kubectl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;macos&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pre-commit &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pyenv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pylint&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;screen&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;themes &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;tmux&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;virtualenv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;vscode &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;zsh-autosuggestions&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;zsh-syntax-highlighting &lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;You can find detailed descriptions of all plugins in the &lt;a href=&#34;https://github.com/ohmyzsh/ohmyzsh/wiki/Plugins&#34;&gt;Oh My Zsh plugins wiki&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Extra installation required:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The last two plugins need separate installation (but it&#39;s quick!):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/zsh-users/zsh-autosuggestions&#34;&gt;zsh-autosuggestions&lt;/a&gt; — suggests commands as you type based on your history&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/zsh-users/zsh-syntax-highlighting&#34;&gt;zsh-syntax-highlighting&lt;/a&gt; — highlights commands in real-time to catch errors&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Follow the installation instructions on their respective GitHub pages.&lt;/p&gt; &lt;h2 id=&#34;7-make-your-terminal-beautiful-with-powerlevel10k&#34;&gt;7. Make your terminal beautiful with Powerlevel10k&lt;/h2&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/romkatv/powerlevel10k&#34;&gt;Powerlevel10k&lt;/a&gt; is a gorgeous Zsh theme that displays useful information like your current directory, Git status, Python environment, and more. The best part? It comes with an interactive configuration wizard that walks you through customizing it to your preferences.&lt;/p&gt; &lt;p&gt;Follow the &lt;a href=&#34;https://github.com/romkatv/powerlevel10k&#34;&gt;installation instructions&lt;/a&gt; on their GitHub page.&lt;/p&gt; &lt;h3 id=&#34;font-setup-for-other-editors&#34;&gt;Font setup for other editors&lt;/h3&gt; &lt;p&gt;If you use VSCode or other editors with integrated terminals, you&#39;ll want to use compatible fonts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open your editor&#39;s settings&lt;/li&gt; &lt;li&gt;Search for &lt;code&gt;terminal.integrated.fontFamily&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Set it to &lt;code&gt;MesloLGS NF&lt;/code&gt; (this font is installed with Powerlevel10k)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For detailed font setup instructions, check the &lt;a href=&#34;https://github.com/romkatv/powerlevel10k/blob/master/font.md&#34;&gt;Powerlevel10k font guide&lt;/a&gt;.&lt;/p&gt; &lt;h2 id=&#34;8-pick-your-code-editor-and-ai-assistant&#34;&gt;8. Pick your code editor and AI assistant&lt;/h2&gt; &lt;p&gt;For AI engineering, you&#39;ll want both a powerful IDE and AI coding assistants. Here&#39;s my setup:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;IDE:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://cursor.sh&#34;&gt;Cursor&lt;/a&gt; — a VSCode fork with native AI pair programming features&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com&#34;&gt;VSCode&lt;/a&gt; — the industry standard with an enormous extension ecosystem&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AI Assistants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://openai.com/codex/&#34;&gt;OpenAI Codex&lt;/a&gt; — OpenAI&#39;s code generation model for intelligent code completion&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://claude.ai&#34;&gt;Claude&lt;/a&gt; — Anthropic&#39;s AI assistant for complex coding tasks and architecture discussions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My preference:&lt;/strong&gt; I use Cursor as my IDE alongside Codex (or Claude Code) running in parallel.&lt;/p&gt; &lt;h2 id=&#34;9-additional-developer-tools&#34;&gt;9. Additional developer tools&lt;/h2&gt; &lt;p&gt;Round out your setup with these applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/apps/desktop&#34;&gt;GitHub Desktop&lt;/a&gt; — visual Git client for managing repositories&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://www.docker.com&#34;&gt;Docker&lt;/a&gt; — containerization platform (or check out &lt;a href=&#34;https://spacelift.io/blog/docker-alternatives&#34;&gt;alternatives&lt;/a&gt; like Podman)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://ollama.com&#34;&gt;Ollama&lt;/a&gt; or &lt;a href=&#34;https://lmstudio.ai&#34;&gt;LM Studio&lt;/a&gt; — run large language models locally on your Mac&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;10-youre-ready-to-build&#34;&gt;10. You&#39;re ready to build&lt;/h2&gt; &lt;p&gt;That&#39;s it! You now have a complete AI engineering setup that mirrors what I use daily. This configuration removes the friction between having an idea and building with AI models. From here, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Start new Python projects with &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Run local LLMs for development and testing&lt;/li&gt; &lt;li&gt;Manage your code with Git and GitHub&lt;/li&gt; &lt;li&gt;Work efficiently in a beautiful, customized terminal&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/</link> <pubDate>Tue, 04 Nov 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/</guid> </item> <item> <title>LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes</title> <author>Viacheslav Dubrov</author> <category>Deployment</category> <category>Inference</category> <category>Kubernetes</category> <category>LLM</category> <category>LoRA</category> <description>&lt;h1 id=&#34;lorax-playbook-orchestrating-thousands-of-lora-adapters-on-kubernetes&#34;&gt;LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes&lt;/h1&gt; &lt;p&gt;Serving dozens of fine-tuned large language models used to mean provisioning one GPU per model. &lt;strong&gt;LoRAX (LoRA eXchange)&lt;/strong&gt; flips that math on its head: keep a single base model in memory and hot-swap lightweight LoRA adapters per request.&lt;/p&gt; &lt;p&gt;This guide shows you how LoRAX achieves &lt;strong&gt;near-constant cost per token&lt;/strong&gt; regardless of how many fine-tunes you&#39;re serving. We&#39;ll cover:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;What LoRA is&lt;/strong&gt; and why it&#39;s a game-changer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRAX vs. vLLM&lt;/strong&gt;: When to use which.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kubernetes Deployment&lt;/strong&gt;: A production-ready Helm guide.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Usage&lt;/strong&gt;: REST, Python, and OpenAI-compatible examples.&lt;/li&gt; &lt;/ul&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;background-what-is-lora&#34;&gt;Background: What is LoRA?&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Low-Rank Adaptation (LoRA)&lt;/strong&gt; is a fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture.&lt;/p&gt; &lt;p&gt;In simple terms: instead of retraining the entire model (which is slow and produces massive files), LoRA trains a tiny set of &#34;diffs&#34; that represent the new knowledge.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full Fine-tuning&lt;/strong&gt;: Produces a 20GB+ file for a 7B model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA Fine-tuning&lt;/strong&gt;: Produces a ~100MB adapter file.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This massive size reduction is what makes dynamic serving possible. You can store thousands of adapters on disk and load them into GPU memory in milliseconds.&lt;/p&gt; &lt;h2 id=&#34;the-problem-lorax-solves&#34;&gt;The problem LoRAX solves&lt;/h2&gt; &lt;p&gt;Traditional multi-model serving is expensive. Each fine-tuned model needs its own GPU memory allocation, which means serving 50 customer-specific models requires 50 separate deployments—or at least 50x the memory. The costs scale linearly with every new variant you add.&lt;/p&gt; &lt;p&gt;LoRAX is an Apache 2.0 project from &lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;Predibase&lt;/a&gt; that extends the &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;Hugging Face Text Generation Inference server&lt;/a&gt; with three critical features: dynamic adapter loading, tiered weight caching, and multi-adapter batching. These let you serve hundreds of tenant-specific LoRA adapters on a single Ampere-class GPU without sacrificing throughput or latency.&lt;/p&gt; &lt;p&gt;Here&#39;s the key insight: LoRA fine-tuning produces small delta weights (adapters) rather than full model copies. LoRAX exploits this by loading just the base model into GPU memory and injecting adapter weights on demand. Unused adapters consume zero VRAM.&lt;/p&gt; &lt;h2 id=&#34;how-it-works-four-core-innovations&#34;&gt;How it works: four core innovations&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;1. Dynamic adapter loading&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adapter weights are injected just-in-time for each request. The base model stays resident in GPU memory while adapters load on the fly without blocking other requests. This means you can catalog thousands of adapters but only pay memory costs for the ones actively serving traffic.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Tiered weight caching&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LoRAX stages adapters across three layers: GPU VRAM for hot adapters, CPU RAM for warm ones, and disk for cold storage. This hierarchy prevents out-of-memory crashes while keeping swap times fast enough that users don&#39;t notice the difference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Continuous multi-adapter batching&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here&#39;s where the magic happens. LoRAX extends continuous batching strategies to work across different adapters in parallel. Requests targeting different fine-tunes can share the same forward pass, keeping the GPU fully utilized. Benchmarks from Predibase show that processing 1M tokens spread across 32 different adapters takes about the same time as 1M tokens on a single model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Battle-tested foundation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LoRAX builds on Hugging Face&#39;s Text Generation Inference (TGI) server, inheriting production-grade optimizations: FlashAttention 2, paged attention, SGMV kernels for multi-adapter inference, and streaming responses. You get the stability of TGI plus the flexibility of dynamic adapter switching.&lt;/p&gt; &lt;h3 id=&#34;the-economics-near-constant-cost-scaling&#34;&gt;The economics: near-constant cost scaling&lt;/h3&gt; &lt;p&gt;The chart below demonstrates the cost advantage. While traditional dedicated deployments (dark gray) scale linearly—double the models means double the cost—LoRAX (orange) keeps per-token costs nearly flat regardless of how many adapters you serve. Even hosted API fine-tunes from providers like OpenAI (light gray) can&#39;t match this efficiency for multi-model scenarios.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;LoRAX cost per million tokens vs number of models&#34; src=&#34;https://slavadubrov.github.io/blog/assets/2025-10-22-lorax-serving-guide/lorax-performance.png&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Cost per million tokens as the number of fine-tuned models increases. LoRAX maintains near-constant costs through efficient multi-adapter batching, while dedicated deployments scale linearly. Source: &lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;LoRAX GitHub&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;h3 id=&#34;request-flow-diagram&#34;&gt;Request flow diagram&lt;/h3&gt; &lt;p&gt;&lt;img alt=&#34;LoRAX Request Flow&#34; src=&#34;../../../../assets/2025-10-22-lorax-serving-guide/lorax-request-flow.svg&#34; /&gt;&lt;/p&gt; &lt;h2 id=&#34;when-to-use-lorax&#34;&gt;When to use LoRAX&lt;/h2&gt; &lt;p&gt;LoRAX makes economic and operational sense in specific scenarios. Here&#39;s when it shines:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-tenant SaaS applications&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You&#39;re building a platform where each of your 500 customers gets a customized chatbot fine-tuned on their data. Traditional serving would require 500 model deployments. LoRAX serves all 500 from a single GPU by loading the relevant adapter when a customer request arrives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Domain-specific expert routers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Your company maintains specialized LLMs for law, medicine, finance, and engineering. Instead of four separate 13B model deployments, LoRAX runs one base LLaMA 2 13B instance and routes to the appropriate adapter based on the incoming request domain.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rapid experimentation and A/B testing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Testing 10 different fine-tuning approaches in production? With LoRAX you deploy once and switch between variants by changing the &lt;code&gt;adapter_id&lt;/code&gt; parameter. No infrastructure changes, no service restarts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Resource-constrained or edge deployments&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On-prem installations or edge devices often have limited GPU resources. A single NVIDIA A10G can host a quantized 7B base model plus dozens of task-specific adapters, eliminating the need for one GPU per model.&lt;/p&gt; &lt;h2 id=&#34;architecture-memory-hierarchy-and-request-scheduling&#34;&gt;Architecture: memory hierarchy and request scheduling&lt;/h2&gt; &lt;p&gt;The core of LoRAX is its three-tier memory hierarchy. Understanding this helps you predict performance and plan capacity.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;LoRAX Memory Hierarchy&#34; src=&#34;../../../../assets/2025-10-22-lorax-serving-guide/lorax-memory-hierarchy.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;LoRAX treats each adapter as a lightweight &#34;view&#34; on the shared base model. The scheduler coalesces requests so that serving 32 different adapters can be as fast as serving one—even across a million tokens of throughput. Adapters typically weigh 10-200MB each, compared to multi-gigabyte full models.&lt;/p&gt; &lt;h2 id=&#34;deploy-lorax-on-kubernetes&#34;&gt;Deploy LoRAX on Kubernetes&lt;/h2&gt; &lt;p&gt;LoRAX ships with production-ready Helm charts and Docker images, making Kubernetes deployment straightforward.&lt;/p&gt; &lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt; &lt;p&gt;Before you start, ensure you have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Kubernetes cluster with NVIDIA GPUs (Ampere generation or newer: A10, A100, H100)&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Runtime&lt;/a&gt; configured on GPU nodes&lt;/li&gt; &lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt; and &lt;code&gt;helm&lt;/code&gt; installed locally&lt;/li&gt; &lt;li&gt;Persistent storage for adapter caches—mount a PersistentVolume to &lt;code&gt;/data&lt;/code&gt; in the pod&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;quick-start-with-the-official-helm-chart&#34;&gt;Quick start with the official Helm chart&lt;/h3&gt; &lt;p&gt;&lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; is the package manager for Kubernetes—it simplifies deploying applications by bundling all the necessary Kubernetes resources (Deployments, Services, ConfigMaps, etc.) into a single &#34;chart.&#34; Instead of writing and managing dozens of YAML files manually, you can deploy complex applications with a single command.&lt;/p&gt; &lt;p&gt;Predibase retired their public Helm repository in late 2024, so the supported workflow is to clone the LoRAX repository and install the chart from disk. Run these commands from your workstation:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Clone the LoRAX repository and switch into it&lt;/span&gt; git&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;clone&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;https://github.com/predibase/lorax.git &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;lorax &lt;span class=&#34;c1&#34;&gt;# Make sure kubectl can talk to your cluster&lt;/span&gt; kubectl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;config&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;current-context kubectl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;get&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;nodes &lt;span class=&#34;c1&#34;&gt;# Build chart dependencies (generates charts/lorax/charts/*.tgz)&lt;/span&gt; helm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;dependency&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;update&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;charts/lorax &lt;span class=&#34;c1&#34;&gt;# Optional: render manifests locally to verify everything is templating&lt;/span&gt; helm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;template&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;mistral-7b-release&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;charts/lorax&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;/tmp/lorax-rendered.yaml &lt;span class=&#34;c1&#34;&gt;# Deploy with default settings (Mistral-7B-Instruct)&lt;/span&gt; helm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;upgrade&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;mistral-7b-release&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;charts/lorax &lt;span class=&#34;c1&#34;&gt;# Watch the pod come up&lt;/span&gt; kubectl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;get&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pods&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-w &lt;span class=&#34;c1&#34;&gt;# Check logs to see model loading progress&lt;/span&gt; kubectl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;logs&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-f&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;deploy/mistral-7b-release-lorax &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The chart creates a Deployment (one replica by default) and a ClusterIP Service listening on port 80. The first startup downloads the base model from Hugging Face and loads it into GPU memory—this can take a few minutes depending on your network and GPU. Subsequent restarts reuse the cached weights from the persistent volume.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; If &lt;code&gt;helm upgrade --install&lt;/code&gt; returns &lt;code&gt;Kubernetes cluster unreachable&lt;/code&gt;, your current kubeconfig context points at a cluster that is offline. Start your local cluster (e.g., Docker Desktop, kind, minikube) or switch to a reachable context with &lt;code&gt;kubectl config use-context&lt;/code&gt;. Running &lt;code&gt;kubectl get nodes&lt;/code&gt; before deploying helps confirm the API server is available.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;customize-the-base-model-and-scaling&#34;&gt;Customize the base model and scaling&lt;/h3&gt; &lt;p&gt;You can swap in a different base model or adjust resources by creating a custom values file. Here&#39;s an example &lt;code&gt;llama2-values.yaml&lt;/code&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Use LLaMA 2 7B Chat instead of Mistral&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;modelId&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;meta-llama/Llama-2-7b-chat-hf&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Enable 4-bit quantization to save VRAM&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;modelArgs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;quantization&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;quot;bitsandbytes&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Scale to 2 replicas for high availability&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;replicaCount&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Request exactly 1 GPU per pod&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;resources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;limits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;nvidia.com/gpu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l l-Scalar l-Scalar-Plain&#34;&gt;1&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Deploy with your custom configuration:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;helm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;upgrade&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-f&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;llama2-values.yaml&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;llama2-chat-release&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;charts/lorax &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Run those commands from the cloned &lt;code&gt;lorax/&lt;/code&gt; repository so Helm can locate the chart directory.&lt;/p&gt; &lt;p&gt;LoRAX supports popular open-source models out of the box: LLaMA 2, CodeLlama, Mistral, Mixtral, Qwen, and others. Check the &lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;model compatibility list&lt;/a&gt; for the latest additions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Exposing the service&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The default Service type is ClusterIP, which only allows access within the cluster. For external traffic, either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a LoadBalancer Service (on cloud providers)&lt;/li&gt; &lt;li&gt;Set up an Ingress with TLS termination&lt;/li&gt; &lt;li&gt;Place an API gateway in front for authentication and rate limiting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cleanup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you&#39;re done testing, free up the GPU resources:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;helm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uninstall&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;mistral-7b-release &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This removes the Deployment, Service, and all pods. Cached model weights remain in the PersistentVolume unless you delete that separately.&lt;/p&gt; &lt;h2 id=&#34;working-with-the-lorax-apis&#34;&gt;Working with the LoRAX APIs&lt;/h2&gt; &lt;p&gt;Once deployed, LoRAX exposes three ways to interact with it: a REST API compatible with Hugging Face TGI, a Python client library, and an OpenAI-compatible endpoint. All three methods support dynamic adapter switching.&lt;/p&gt; &lt;h3 id=&#34;rest-api&#34;&gt;REST API&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;/generate&lt;/code&gt; endpoint accepts JSON payloads with your prompt and optional parameters. Using the base model without any adapter:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Basic request to the base model (no adapter)&lt;/span&gt; curl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-X&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;POST&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;http://localhost:8080/generate&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-H&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Content-Type: application/json&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-d&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;{&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;inputs&amp;quot;: &amp;quot;Write a short poem about the sea.&amp;quot;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;parameters&amp;quot;: {&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;max_new_tokens&amp;quot;: 64,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;temperature&amp;quot;: 0.7&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; }&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; }&amp;#39;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The response includes the generated text and metadata like token counts and timing information.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Loading a specific adapter&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Add an &lt;code&gt;adapter_id&lt;/code&gt; parameter to target a fine-tuned model. Here&#39;s an example using a math-specialized adapter:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-X&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;POST&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;http://localhost:8080/generate&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-H&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Content-Type: application/json&amp;quot;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-d&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;{&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;inputs&amp;quot;: &amp;quot;Natalia sold 48 clips in April, and then half as many in May. How many clips did she sell in total?&amp;quot;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;parameters&amp;quot;: {&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;max_new_tokens&amp;quot;: 64,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; &amp;quot;adapter_id&amp;quot;: &amp;quot;vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k&amp;quot;&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; }&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt; }&amp;#39;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;On the first call with a new &lt;code&gt;adapter_id&lt;/code&gt;, LoRAX downloads the adapter from Hugging Face Hub and caches it under &lt;code&gt;/data&lt;/code&gt;. Subsequent requests use the cached version. You can also load adapters from local paths by specifying &lt;code&gt;&#34;adapter_source&#34;: &#34;local&#34;&lt;/code&gt; alongside a file path.&lt;/p&gt; &lt;h3 id=&#34;python-client&#34;&gt;Python client&lt;/h3&gt; &lt;p&gt;For programmatic access, install the &lt;code&gt;lorax-client&lt;/code&gt; package:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;lorax-client &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The client wraps the REST API with a clean interface:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;lorax&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Client&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Connect to your LoRAX instance (default port 8080)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Client&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;http://localhost:8080&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Explain the significance of the moon landing in 1969.&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Generate using the base model (no adapter loaded)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base_response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_new_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Base model:&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;base_response&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generated_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Generate using a fine-tuned adapter&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# The adapter_id can be a Hugging Face repo ID or a local path&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adapter_response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_new_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;80&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adapter_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;alignment-handbook/zephyr-7b-dpo-lora&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;With adapter:&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adapter_response&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;generated_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;The client supports streaming responses, adjusting decoding parameters (temperature, top-p, repetition penalty), and accessing token-level details. Check the &lt;a href=&#34;https://github.com/predibase/lorax&#34;&gt;client reference&lt;/a&gt; for advanced usage patterns.&lt;/p&gt; &lt;h3 id=&#34;openai-compatible-endpoint&#34;&gt;OpenAI-compatible endpoint&lt;/h3&gt; &lt;p&gt;LoRAX implements the OpenAI Chat Completions API under the &lt;code&gt;/v1&lt;/code&gt; path. This lets you drop LoRAX into tools that expect OpenAI&#39;s API format—LangChain, Semantic Kernel, or custom applications.&lt;/p&gt; &lt;p&gt;Use the &lt;code&gt;model&lt;/code&gt; field to specify which adapter to load:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Point the OpenAI client at LoRAX&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;openai&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;api_key&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;EMPTY&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# LoRAX doesn&amp;#39;t require an API key by default&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;openai&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;api_base&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;http://localhost:8080/v1&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# The model parameter becomes the adapter_id&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# This allows seamless integration with tools like LangChain&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;openai&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ChatCompletion&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;alignment-handbook/zephyr-7b-dpo-lora&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;system&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;You are a friendly chatbot who speaks like a pirate.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;role&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;How many parrots can a person own?&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;choices&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;message&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;content&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This compatibility unlocks two powerful use cases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Drop-in replacement:&lt;/strong&gt; Migrate existing applications from OpenAI&#39;s hosted models to your own infrastructure by changing one configuration line&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool integration:&lt;/strong&gt; Use LoRAX with any framework that supports OpenAI&#39;s API without custom adapters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Note that the first request to a new adapter may have higher latency while LoRAX downloads and loads it. Plan for this in user-facing applications by preloading popular adapters or showing loading states.&lt;/p&gt; &lt;h2 id=&#34;trade-offs-to-consider&#34;&gt;Trade-offs to consider&lt;/h2&gt; &lt;h3 id=&#34;what-lorax-does-well&#34;&gt;What LoRAX does well&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Dramatic cost reduction for multi-model scenarios&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Serve hundreds or thousands of fine-tuned models on a single GPU. Traditional approaches would require separate deployments for each model, multiplying infrastructure costs linearly. LoRAX keeps costs nearly constant as you add adapters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zero memory waste&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adapters are loaded just-in-time when requests arrive. Unused models consume no VRAM. This means you can maintain a catalog of 1,000+ specialized models but only pay for the handful actively serving traffic at any moment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Production-grade performance&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Continuous multi-adapter batching keeps latency and throughput comparable to single-model serving. Predibase benchmarks show that serving 32 different adapters simultaneously adds minimal overhead compared to serving one model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Proven foundation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built on Hugging Face TGI, LoRAX inherits battle-tested optimizations: FlashAttention 2, paged attention, streaming token generation, and SGMV kernels for efficient multi-adapter inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deployment maturity&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ships with Docker images, Helm charts, Prometheus metrics, and OpenTelemetry tracing. The Apache 2.0 license means you can use it commercially without restrictions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Broad model support&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Works with popular open-source architectures: LLaMA 2, CodeLlama, Mistral, Mixtral, Qwen, and more. Supports quantization (4-bit via bitsandbytes, GPTQ, AWQ) to reduce memory footprint.&lt;/p&gt; &lt;h3 id=&#34;limitations-and-constraints&#34;&gt;Limitations and constraints&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Tied to LoRA-based fine-tuning&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;All your adapters must come from LoRA-style fine-tuning of the same base model. Full fine-tunes that produce standalone models won&#39;t work without conversion. If you have completely different model architectures, you&#39;ll need separate LoRAX deployments for each base.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cold start latency&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The first request after startup loads the base model into GPU memory (can take 30-90 seconds for larger models). First-time adapter requests also incur a download delay if pulling from Hugging Face. Plan for this with health checks and preloading strategies.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cache thrashing under bursty load&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If traffic suddenly hits dozens of different adapters, LoRAX may shuffle weights between GPU, CPU RAM, and disk. While adapter swaps are fast (~10ms from RAM), a very large working set can cause temporary slowdowns. Monitor GPU memory and adapter cache hit rates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fast-moving project&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;LoRAX forked from TGI in late 2023 and evolves rapidly. Expect frequent updates and occasional breaking changes as the maintainers track upstream TGI improvements and add new features. Pin versions carefully in production.&lt;/p&gt; &lt;h2 id=&#34;alternatives-lorax-vs-vllm&#34;&gt;Alternatives: LoRAX vs. vLLM&lt;/h2&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; is another popular high-throughput serving engine that recently added multi-LoRA support. How do they compare?&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Feature&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;LoRAX&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;vLLM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Primary Focus&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Massive Scale&lt;/strong&gt;: Serving hundreds/thousands of adapters.&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;High Throughput&lt;/strong&gt;: Maximum tokens/sec for fewer active adapters.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Dynamic Swapping&lt;/strong&gt;: Aggressively offloads to CPU/disk.&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Batching&lt;/strong&gt;: Optimized for concurrent execution of active adapters.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Best For&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Long-tail SaaS&lt;/strong&gt;: 1000s of tenants, sporadic usage.&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;High-traffic tiers&lt;/strong&gt;: 5-10 heavily used adapters.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Base&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Hugging Face TGI&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Custom Paged Attention Engine&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;strong&gt;Choose LoRAX if:&lt;/strong&gt; You have a &#34;long tail&#34; of adapters (e.g., one per user) where most are idle at any given time. LoRAX&#39;s tiered caching excels here.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Choose vLLM if:&lt;/strong&gt; You have a small set of highly active adapters and raw throughput is your top priority.&lt;/p&gt; &lt;h2 id=&#34;getting-started-a-practical-roadmap&#34;&gt;Getting started: a practical roadmap&lt;/h2&gt; &lt;p&gt;If LoRAX fits your use case, here&#39;s how to move from prototype to production:&lt;/p&gt; &lt;h3 id=&#34;1-start-small&#34;&gt;1. Start small&lt;/h3&gt; &lt;p&gt;Deploy LoRAX with the base model you&#39;re already using and 3-5 representative adapters. Verify that adapter loading works and measure baseline latency for your workload.&lt;/p&gt; &lt;h3 id=&#34;2-measure-and-profile&#34;&gt;2. Measure and profile&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Track adapter cache hit rates and GPU memory usage under realistic traffic patterns&lt;/li&gt; &lt;li&gt;Identify your &#34;hot&#34; adapters (top 20% by request volume) and consider preloading them at startup&lt;/li&gt; &lt;li&gt;Measure P50, P95, and P99 latency for both cached and cold adapter loads&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;3-optimize-for-your-workload&#34;&gt;3. Optimize for your workload&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;If you have a few very popular adapters, increase GPU memory allocation to keep more adapters hot&lt;/li&gt; &lt;li&gt;If you have long-tail usage across hundreds of adapters, tune the tiered cache settings to balance RAM and disk&lt;/li&gt; &lt;li&gt;Use quantization (4-bit bitsandbytes or GPTQ) if VRAM is tight&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;4-scale-horizontally&#34;&gt;4. Scale horizontally&lt;/h3&gt; &lt;p&gt;Once you understand single-instance behavior, add replicas for high availability. Place a load balancer in front that routes based on &lt;code&gt;adapter_id&lt;/code&gt; to improve cache locality—requests for the same adapter hitting the same replica means better cache utilization.&lt;/p&gt; &lt;h3 id=&#34;5-monitor-continuously&#34;&gt;5. Monitor continuously&lt;/h3&gt; &lt;p&gt;Set up dashboards for GPU utilization, adapter cache metrics, and request latency broken down by adapter. Watch for cache thrashing during traffic spikes and adjust your scaling strategy accordingly.&lt;/p&gt; &lt;p&gt;With LoRAX, orchestrating specialized LLM experiences becomes a matter of routing adapter IDs—not provisioning endless GPUs. The economics shift from linear scaling to near-constant costs, making multi-model serving viable even for small teams.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/</link> <pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/</guid> </item> <item> <title>Domain-driven design for AI agents: a beginner-friendly guide</title> <author>Viacheslav Dubrov</author> <category>agents</category> <category>ai-engineering</category> <category>architecture</category> <category>domain-driven-design</category> <description>&lt;h1 id=&#34;domain-driven-design-for-ai-agents-a-beginner-friendly-guide&#34;&gt;Domain-driven design for AI agents: a beginner-friendly guide&lt;/h1&gt; &lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Domain-driven design (DDD) gives AI agent teams a shared language, clear boundaries, and code that mirrors the real world. Use it to tame prompt spaghetti, enforce business rules, and evolve systems without breaking everything.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;img alt=&#34;DDD Layered Architecture&#34; src=&#34;../../../../assets/2025-10-20-domain-driven-design-ai-agents/ddd_layered_architecture.svg&#34; /&gt;&lt;/p&gt; &lt;!-- more --&gt; &lt;hr /&gt; &lt;h2 id=&#34;why-domain-driven-design-matters-for-ai-agents&#34;&gt;Why domain-driven design matters for AI agents&lt;/h2&gt; &lt;p&gt;AI agent projects fail for a surprising reason: not because the code is bad, but because developers and domain experts can&#39;t understand each other. You&#39;ve seen it—business teams ask for a &#34;policy check&#34; and get back a &lt;code&gt;process_data()&lt;/code&gt; method. Nobody knows what it does, so requirements drift and systems calcify.&lt;/p&gt; &lt;p&gt;Domain-driven design (DDD) fixes this by putting the &lt;strong&gt;business domain&lt;/strong&gt; at the center. Not the database schema. Not the prompt template. The actual real-world process you&#39;re trying to model. This alignment delivers three immediate wins:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Shared language.&lt;/strong&gt; Everyone—product, ops, engineering—uses the same words. When compliance says &#34;refund request&#34;, that&#39;s what appears in your code, prompts, and documentation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Focused scope.&lt;/strong&gt; You build what matters: the core workflows, compliance rules, and critical metrics. Not mountains of glue code that break when requirements shift.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptability.&lt;/strong&gt; When policies change (and they will), you update one well-defined slice instead of hunting through a monolithic tangle.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This matters most in complex domains where rules evolve constantly—think finance, healthcare, operations, or any regulated industry. DDD gives you a fighting chance to keep up.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;strategic-building-blocks&#34;&gt;Strategic building blocks&lt;/h2&gt; &lt;p&gt;DDD isn&#39;t one big idea—it&#39;s a toolkit of patterns that work together. It&#39;s often split into two parts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Strategic Design&lt;/strong&gt;: The &#34;big picture&#34; stuff. Defining boundaries, teams, and how systems talk. This is crucial for multi-agent systems.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tactical Design&lt;/strong&gt;: The code-level patterns (Entities, Aggregates). This keeps your agent&#39;s internal logic clean.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here are the core concepts you&#39;ll use every day.&lt;/p&gt; &lt;h3 id=&#34;ubiquitous-language&#34;&gt;Ubiquitous language&lt;/h3&gt; &lt;p&gt;This is the shared vocabulary that shows up everywhere: in meetings, documentation, prompts, and method names. No translation layers between &#34;business speak&#34; and &#34;code speak.&#34;&lt;/p&gt; &lt;p&gt;If compliance says &#34;policy check&#34;, your method is &lt;code&gt;run_policy_check()&lt;/code&gt;, not &lt;code&gt;process_data()&lt;/code&gt;. If doctors say &#34;admit patient&#34;, you write &lt;code&gt;admit_patient()&lt;/code&gt;, not &lt;code&gt;add_user()&lt;/code&gt;.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;PatientRegistry&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;admit_patient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;patient_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Admit a patient to the registry - term used by medical staff.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This eliminates translation gaps and makes code self-documenting. When requirements change, the language change is obvious and localized.&lt;/p&gt; &lt;h3 id=&#34;bounded-contexts&#34;&gt;Bounded contexts&lt;/h3&gt; &lt;p&gt;Large systems need explicit boundaries. Why? Because the same word means different things in different parts of the business.&lt;/p&gt; &lt;p&gt;Take &#34;product&#34; in e-commerce. In the &lt;strong&gt;Inventory&lt;/strong&gt; context, a product is a catalog item with SKUs and stock counts. In the &lt;strong&gt;Billing&lt;/strong&gt; context, it&#39;s a line item with pricing rules and tax calculations. In &lt;strong&gt;Order Management&lt;/strong&gt;, it&#39;s a quantity and delivery promise.&lt;/p&gt; &lt;p&gt;Bounded contexts let each subdomain have its own definition without conflict. Translation layers or interfaces connect them when they need to talk.&lt;/p&gt; &lt;p&gt;Bounded contexts let each subdomain have its own definition without conflict. Translation layers or interfaces connect them when they need to talk.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Bounded Contexts&#34; src=&#34;../../../../assets/2025-10-20-domain-driven-design-ai-agents/ddd_bounded_contexts.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;This keeps each model lean and prevents the &#34;one size fits all&#34; model that becomes unwieldy as complexity grows.&lt;/p&gt; &lt;h3 id=&#34;entities-and-value-objects&#34;&gt;Entities and value objects&lt;/h3&gt; &lt;p&gt;These are the basic building blocks of your domain model. Understanding the difference is key.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Entities&lt;/strong&gt; have identity that persists over time. A &lt;code&gt;Task&lt;/code&gt; with ID &lt;code&gt;123&lt;/code&gt; is the same task even if you change its description, status, or due date. Two entities are equal if they have the same ID, regardless of their attributes.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;SupportTicket&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ticket_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# This is the identity&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;customer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;issue&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;status&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;OPEN&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;status&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;OPEN&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ValueError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Ticket already closed&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;status&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;CLOSED&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Value objects&lt;/strong&gt; have no identity—they&#39;re defined entirely by their attributes. Two &lt;code&gt;TimeSlot&lt;/code&gt; objects with the same start and end times are interchangeable. Value objects are immutable; instead of changing them, you create new ones.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TimeSlot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# e.g., &amp;quot;2025-10-18 09:00&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# e.g., &amp;quot;2025-10-18 10:00&amp;quot;&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@property&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;duration&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Compute duration from start to end&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Use entities for things that have lifecycles (&lt;code&gt;Order&lt;/code&gt;, &lt;code&gt;User&lt;/code&gt;, &lt;code&gt;AgentSession&lt;/code&gt;). Use value objects for descriptions and measurements (&lt;code&gt;EmailAddress&lt;/code&gt;, &lt;code&gt;Priority&lt;/code&gt;, &lt;code&gt;Location&lt;/code&gt;).&lt;/p&gt; &lt;h3 id=&#34;aggregates&#34;&gt;Aggregates&lt;/h3&gt; &lt;p&gt;Aggregates are clusters of related entities and value objects that get treated as one unit. Think of them as consistency boundaries—within an aggregate, business rules must always hold true.&lt;/p&gt; &lt;p&gt;Every aggregate has one &lt;strong&gt;aggregate root&lt;/strong&gt;—an entity that controls access to everything inside. Want to modify something in the aggregate? Go through the root. This enforces invariants and prevents invalid states.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# This is the aggregate root&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_factory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Business rule enforced here: no duplicate task IDs&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;any&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ValueError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Task ID already exists&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;External code never touches the &lt;code&gt;tasks&lt;/code&gt; list directly—it always calls &lt;code&gt;add_task()&lt;/code&gt;. This guarantees the &#34;no duplicate IDs&#34; rule can never be violated. When you save to a database, you typically save the entire aggregate at once.&lt;/p&gt; &lt;h3 id=&#34;repositories&#34;&gt;Repositories&lt;/h3&gt; &lt;p&gt;Repositories abstract away persistence. To your domain code, it feels like working with an in-memory collection—no SQL queries, no database sessions, just clean methods like &lt;code&gt;save()&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This separation has real benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Domain logic stays clean.&lt;/strong&gt; It doesn&#39;t care if data lives in Postgres, MongoDB, or a JSON file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing is trivial.&lt;/strong&gt; Swap in an in-memory repository for tests without touching domain code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage can evolve.&lt;/strong&gt; Switch from SQLite to Redis without rewriting business rules.&lt;/li&gt; &lt;/ul&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;abc&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ABC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;abstractmethod&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_factory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;PlanRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ABC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Domain layer defines the interface.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@abstractmethod&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;save&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@abstractmethod&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plan_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Plan&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;InMemoryPlanRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PlanRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Infrastructure layer provides the implementation.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;save&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Plan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plan&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plan&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plan_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Plan&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plan_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Your domain code only knows about &lt;code&gt;PlanRepository&lt;/code&gt; (the interface). The infrastructure layer plugs in the actual implementation.&lt;/p&gt; &lt;h3 id=&#34;domain-events&#34;&gt;Domain events&lt;/h3&gt; &lt;p&gt;Domain events capture important things that happen in your system. They&#39;re named in past tense—&lt;code&gt;OrderPlaced&lt;/code&gt;, &lt;code&gt;TaskCompleted&lt;/code&gt;, &lt;code&gt;PaymentFailed&lt;/code&gt;—because they represent facts.&lt;/p&gt; &lt;p&gt;Events make implicit side effects explicit. Instead of one module directly calling another when something happens, the domain raises an event. Other parts of the system subscribe and react independently.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskCompleted&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed_at&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;When a task finishes, you emit &lt;code&gt;TaskCompleted&lt;/code&gt;. A notification service might listen for this event and send an email. A reporting service might log it for analytics. The important part: the task aggregate doesn&#39;t need to know about emails or analytics. It just announces what happened.&lt;/p&gt; &lt;p&gt;This decouples workflows and makes cross-context communication clean. It&#39;s especially powerful in multi-agent systems where agents react to each other&#39;s events.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;translating-ddd-to-agent-architectures&#34;&gt;Translating DDD to agent architectures&lt;/h2&gt; &lt;p&gt;AI agents deal with complexity—multi-step workflows, unreliable LLM outputs, evolving requirements. DDD&#39;s patterns map surprisingly well to these challenges. Here&#39;s how the concepts translate:&lt;/p&gt; &lt;h3 id=&#34;bounded-contexts-become-agents-or-skills&#34;&gt;Bounded contexts become agents or skills&lt;/h3&gt; &lt;p&gt;Each agent (or major capability) is a bounded context. A research orchestrator might coordinate three specialized agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Trends Agent&lt;/strong&gt; — gathers market data using its own vocabulary and tools&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compliance Agent&lt;/strong&gt; — runs policy checks with regulatory terminology&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost Agent&lt;/strong&gt; — estimates expenses with finance-specific rules&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each has its own model, terminology, and invariants. They communicate through well-defined interfaces or events.&lt;/p&gt; &lt;p&gt;Each has its own model, terminology, and invariants. They communicate through well-defined interfaces or events.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Agent Orchestration&#34; src=&#34;../../../../assets/2025-10-20-domain-driven-design-ai-agents/agent_orchestration.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Even in a single-agent system, you might define internal contexts—a Planning module and an Execution module, each with its own domain model.&lt;/p&gt; &lt;h3 id=&#34;prompts-honor-the-ubiquitous-language&#34;&gt;Prompts honor the ubiquitous language&lt;/h3&gt; &lt;p&gt;Use domain terms in system prompts, tool descriptions, and function signatures. If compliance experts say &#34;policy check&#34;, that exact phrase appears in your prompts and code. This keeps humans and agents synchronized and makes the system easier to review and debug.&lt;/p&gt; &lt;h3 id=&#34;state-becomes-explicit-entities&#34;&gt;State becomes explicit entities&lt;/h3&gt; &lt;p&gt;LLMs are often stateless, but complex agents maintain state—conversation sessions, goals, intermediate results, tool outputs. Model these as entities or value objects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;ConversationSession&lt;/code&gt; entity with ID and message history&lt;/li&gt; &lt;li&gt;&lt;code&gt;Task&lt;/code&gt; entity representing units of work&lt;/li&gt; &lt;li&gt;&lt;code&gt;ToolOutput&lt;/code&gt; value object for immutable results&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Explicit modeling enables validation, business rules, and reuse. You can enforce rules like &#34;a task can&#39;t be completed until dependencies finish&#34; directly in the entity methods.&lt;/p&gt; &lt;h3 id=&#34;aggregates-express-agent-plans&#34;&gt;Aggregates express agent plans&lt;/h3&gt; &lt;p&gt;A &lt;code&gt;Plan&lt;/code&gt; aggregate root can govern task lists, enforce limits, and maintain priorities. When an LLM proposes adding 50 tasks, the aggregate enforces a maximum of 10. When it suggests duplicate work, the aggregate rejects it. This keeps AI proposals within business constraints.&lt;/p&gt; &lt;h3 id=&#34;domain-events-drive-orchestration&#34;&gt;Domain events drive orchestration&lt;/h3&gt; &lt;p&gt;Agents raise events—&lt;code&gt;ResearchCompleted&lt;/code&gt;, &lt;code&gt;ThresholdExceeded&lt;/code&gt;, &lt;code&gt;PolicyViolationDetected&lt;/code&gt;. Other agents or services listen and react without tight coupling. This event-driven approach is the future of scalable agent systems: it lets multiple agents collaborate in real-time without being hard-wired together.&lt;/p&gt; &lt;h3 id=&#34;business-rules-wrap-ai-actions&#34;&gt;Business rules wrap AI actions&lt;/h3&gt; &lt;p&gt;LLM outputs flow through domain services or entity methods. If an LLM suggests a refund amount beyond policy limits, your &lt;code&gt;RefundRequest&lt;/code&gt; value object validates and rejects it. The AI can improvise, but business rules have the final say. This keeps agents safe and aligned with policy.&lt;/p&gt; &lt;h3 id=&#34;the-anti-corruption-layer-acl&#34;&gt;The Anti-Corruption Layer (ACL)&lt;/h3&gt; &lt;p&gt;When working with LLMs, you are dealing with a probabilistic, creative, and occasionally chaotic entity. Your domain model, however, must be deterministic and safe. You cannot let the raw output of an LLM leak directly into your domain logic.&lt;/p&gt; &lt;p&gt;Enter the &lt;strong&gt;Anti-Corruption Layer (ACL)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;LLM Domain Interaction&#34; src=&#34;../../../../assets/2025-10-20-domain-driven-design-ai-agents/llm_domain_interaction.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;The ACL acts as a gatekeeper. It translates the &#34;wild&#34; output of the LLM into the &#34;strict&#34; language of your domain.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Ingest&lt;/strong&gt;: Receive raw text or JSON from the LLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Validate&lt;/strong&gt;: Use Pydantic models to check structure and types.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sanitize&lt;/strong&gt;: Ensure values fall within acceptable ranges (e.g., no negative prices).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Translate&lt;/strong&gt;: Convert DTOs (Data Transfer Objects) into Domain Entities.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If validation fails, the ACL rejects the data—often sending an error message back to the LLM so it can correct itself. This loop ensures that &lt;strong&gt;only valid data ever touches your core business logic&lt;/strong&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;example-a-task-assistant-modeled-with-ddd&#34;&gt;Example: a task assistant modeled with DDD&lt;/h2&gt; &lt;p&gt;Let&#39;s build a personal task assistant that handles requests like &#34;Remind me to buy milk tomorrow&#34; or &#34;What&#39;s on my to-do list?&#34; We&#39;ll apply DDD principles step by step.&lt;/p&gt; &lt;h3 id=&#34;1-map-the-contexts&#34;&gt;1. Map the contexts&lt;/h3&gt; &lt;p&gt;Start by breaking the problem into subdomains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task Management&lt;/strong&gt; — handling to-do items and reminders (core domain)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scheduling&lt;/strong&gt; — calendar events and meetings&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Notifications&lt;/strong&gt; — sending alerts and emails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We&#39;ll focus on Task Management first. The others can evolve as separate bounded contexts or companion agents.&lt;/p&gt; &lt;p&gt;We&#39;ll focus on Task Management first. The others can evolve as separate bounded contexts or companion agents.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Task Assistant Context Map&#34; src=&#34;../../../../assets/2025-10-20-domain-driven-design-ai-agents/task_assistant_context_map.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;2-speak-the-same-language&#34;&gt;2. Speak the same language&lt;/h3&gt; &lt;p&gt;Establish the vocabulary with domain experts (or just common sense for personal tasks): &#34;task&#34;, &#34;deadline&#34;, &#34;reminder&#34;, &#34;priority&#34;. Use these exact terms everywhere—prompt templates, method names, UI labels. No translation layers.&lt;/p&gt; &lt;h3 id=&#34;3-capture-entities-value-objects-and-events&#34;&gt;3. Capture entities, value objects, and events&lt;/h3&gt; &lt;p&gt;Now model the core concepts:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Entity:&lt;/strong&gt; &lt;code&gt;Task&lt;/code&gt; with identity (&lt;code&gt;id&lt;/code&gt;) and mutable state (&lt;code&gt;completed&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value object:&lt;/strong&gt; &lt;code&gt;Priority&lt;/code&gt; enum (immutable, defined by its value)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain event:&lt;/strong&gt; &lt;code&gt;TaskCompletedEvent&lt;/code&gt; to signal when work finishes&lt;/li&gt; &lt;/ul&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timezone&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;enum&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Enum&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Priority&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Enum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Value object: priority is defined by its value alone.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LOW&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NORMAL&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HIGH&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskCompletedEvent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Domain event: announces a task was completed.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Entity: identity persists even as attributes change.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;created_at&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_factory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timezone&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;utc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;priority&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Priority&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Priority&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NORMAL&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;mark_completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskCompletedEvent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Business rule: can&amp;#39;t complete an already-completed task.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ValueError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Task is already completed.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskCompletedEvent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timezone&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;utc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Notice how business rules live in the entity methods, not scattered across prompt templates.&lt;/p&gt; &lt;h3 id=&#34;4-shape-the-aggregate&#34;&gt;4. Shape the aggregate&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;TaskList&lt;/code&gt; is our aggregate root. It holds multiple &lt;code&gt;Task&lt;/code&gt; entities and enforces consistency rules across them. All modifications go through the root&#39;s methods.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Aggregate root: enforces invariants across all tasks.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_factory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Business rule: no duplicate tasks on the same day.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;any&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;existing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;existing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;existing&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ValueError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;A similar task on that date already exists.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_pending&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Query helper: find tasks that aren&amp;#39;t done yet.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_rebuild&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Resolve forward references for Pydantic.&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;External code never manipulates &lt;code&gt;tasks&lt;/code&gt; directly—it always goes through &lt;code&gt;add_task()&lt;/code&gt; or other root methods. This guarantees the &#34;no duplicates&#34; rule holds.&lt;/p&gt; &lt;h3 id=&#34;5-wrap-persistence-in-a-repository&#34;&gt;5. Wrap persistence in a repository&lt;/h3&gt; &lt;p&gt;The repository abstracts storage. Domain code doesn&#39;t know if tasks live in memory, a database, or a JSON file.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_factory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_rebuild&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Resolve forward references for Pydantic.&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Abstracts task storage - in-memory implementation for simplicity.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Retrieve a user&amp;#39;s task list, or create a new empty one.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;save_task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Persist changes to the task list.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;In production, you&#39;d swap this for a database implementation—say, using &lt;a href=&#34;https://www.sqlalchemy.org/&#34;&gt;SQLAlchemy&lt;/a&gt; or Postgres—without touching the domain logic.&lt;/p&gt; &lt;h3 id=&#34;6-run-the-flow&#34;&gt;6. Run the flow&lt;/h3&gt; &lt;p&gt;Here&#39;s how everything fits together when a user makes a request:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timedelta&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;uuid&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;uuid4&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;pydantic&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;completed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;bool&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BaseModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_factory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;any&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;existing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;existing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;existing&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;):&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;ValueError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;A similar task on that date already exists.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;TaskRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;save_task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskList&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_rebuild&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Resolve forward references for Pydantic.&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# User says: &amp;quot;Remind me to buy milk tomorrow&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# (In reality, an LLM would parse this into structured data)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user_input&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Remind me to buy milk tomorrow&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intent&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;add_task&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Initialize repository&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;repo&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TaskRepository&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intent&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;add_task&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Load the user&amp;#39;s task list&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;repo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;owner&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;User123&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Create a new task entity&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uuid4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;buy milk&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;today&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;timedelta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;days&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. Domain layer enforces business rules&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add_task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;repo&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;save_task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Task &amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39; added for &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;due_date&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Sorry, I couldn&amp;#39;t add that task: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exc&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Notice the separation of concerns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM layer&lt;/strong&gt; parses natural language into structured data (intent + parameters)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain layer&lt;/strong&gt; enforces business rules through entity methods&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repository layer&lt;/strong&gt; handles persistence without leaking into domain logic&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The LLM can be creative with parsing, but the domain ensures consistency. If the LLM tries to add a duplicate task, the aggregate root rejects it—no special-casing needed in prompts.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;tooling-to-bring-the-model-to-life&#34;&gt;Tooling to bring the model to life&lt;/h2&gt; &lt;p&gt;DDD doesn&#39;t require special frameworks, but certain tools make implementation smoother—especially for AI agents.&lt;/p&gt; &lt;h3 id=&#34;fastapi&#34;&gt;FastAPI&lt;/h3&gt; &lt;p&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; pairs beautifully with DDD. Use routers to separate bounded contexts (&lt;code&gt;/tasks&lt;/code&gt;, &lt;code&gt;/schedule&lt;/code&gt;), Pydantic models for request/response validation, and dependency injection to wire up repositories.&lt;/p&gt; &lt;p&gt;Structure your project in layers:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;project/ ├── domain/ # Pure business logic (entities, aggregates, value objects) ├── application/ # Use cases and command handlers ├── infrastructure/ # Repositories, databases, external APIs └── interface/ # FastAPI routers and HTTP contracts &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This layering (sometimes called &#34;onion architecture&#34;) keeps changes from rippling through your codebase. Swap the database? Touch only &lt;code&gt;infrastructure/&lt;/code&gt;. Change the UI? Touch only &lt;code&gt;interface/&lt;/code&gt;.&lt;/p&gt; &lt;h3 id=&#34;pydantic-and-pydantic-ai&#34;&gt;Pydantic and Pydantic AI&lt;/h3&gt; &lt;p&gt;&lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;Pydantic&lt;/a&gt; enforces invariants and validates data at runtime. Use it for entities, value objects, and especially for validating LLM outputs.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://ai.pydantic.dev/&#34;&gt;Pydantic AI&lt;/a&gt; takes this further: it ensures LLM responses conform to your domain schemas. Define an &lt;code&gt;AddTaskCommand&lt;/code&gt; with required fields, and Pydantic AI validates that the LLM&#39;s JSON output matches before you act on it. This brings structure to the chaotic world of AI outputs.&lt;/p&gt; &lt;p&gt;Another excellent tool is &lt;strong&gt;&lt;a href=&#34;https://github.com/jxnl/instructor&#34;&gt;Instructor&lt;/a&gt;&lt;/strong&gt;, which patches OpenAI (and other) clients to return Pydantic models directly. It&#39;s a lightweight way to implement your Anti-Corruption Layer.&lt;/p&gt; &lt;h3 id=&#34;ddd-helper-libraries&#34;&gt;DDD helper libraries&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://pypi.org/project/dddesign/&#34;&gt;DDDesign&lt;/a&gt;&lt;/strong&gt; — provides base classes for entities, repositories, and value objects built on Pydantic&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/proteanhq/protean&#34;&gt;Protean&lt;/a&gt;&lt;/strong&gt; — a full framework for DDD, CQRS, and event sourcing if you want something that comes with a lot of ready-made features out of the box&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most Python developers skip these and use vanilla classes with Pydantic, but they&#39;re worth exploring for large projects.&lt;/p&gt; &lt;h3 id=&#34;event-driven-tooling&#34;&gt;Event-driven tooling&lt;/h3&gt; &lt;p&gt;For domain events, consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/jek/blinker&#34;&gt;blinker&lt;/a&gt;&lt;/strong&gt; — lightweight in-process event dispatcher&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/redis/redis-py&#34;&gt;redis-py Pub/Sub&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href=&#34;https://www.rabbitmq.com/&#34;&gt;RabbitMQ&lt;/a&gt;&lt;/strong&gt; — for distributed events across services or agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;asyncio event patterns&lt;/strong&gt; — if you&#39;re already async&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Events are crucial for multi-agent orchestration. An agent emits &lt;code&gt;ResearchCompleted&lt;/code&gt;, others react—no tight coupling.&lt;/p&gt; &lt;h3 id=&#34;agent-frameworks&#34;&gt;Agent frameworks&lt;/h3&gt; &lt;p&gt;&lt;a href=&#34;https://python.langchain.com/&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://langchain-ai.github.io/langgraph/&#34;&gt;LangGraph&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;&gt;Haystack&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/semantic-kernel&#34;&gt;Semantic Kernel&lt;/a&gt;, &lt;a href=&#34;https://github.com/run-llama/llama_index&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;AutoGen&lt;/a&gt;, &lt;a href=&#34;https://ai.google.dev/gemini-api/docs/agent-developer-kit&#34;&gt;Google ADK&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/smolagents&#34;&gt;smolagents&lt;/a&gt;, and &lt;a href=&#34;https://github.com/joaomdmoura/crewAI&#34;&gt;CrewAI&lt;/a&gt; provide structure for modern agent workflows. Use them within your domain layer, but wrap them in your own interfaces. That way, swapping frameworks doesn&#39;t break your business logic.&lt;/p&gt; &lt;h3 id=&#34;testing&#34;&gt;Testing&lt;/h3&gt; &lt;p&gt;One of DDD&#39;s biggest wins: your domain layer tests without the whole stack running.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://docs.pytest.org/en/stable/&#34;&gt;PyTest&lt;/a&gt;&lt;/strong&gt; for unit tests on entities and aggregates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fake repositories&lt;/strong&gt; (in-memory) for integration tests&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM stubs&lt;/strong&gt; that return predetermined outputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Your domain code should never require a live LLM to test. The LLM is an implementation detail—your tests validate business rules.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;getting-started-checklist&#34;&gt;Getting started checklist&lt;/h2&gt; &lt;p&gt;Ready to apply DDD to your next agent project? Here&#39;s your roadmap:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Interview domain experts.&lt;/strong&gt; Draft the ubiquitous language—the vocabulary everyone will use. Document it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Map bounded contexts.&lt;/strong&gt; Draw the subdomains and mark where they need to talk to each other. Start with one core context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model entities and value objects.&lt;/strong&gt; What things have identity? What things are just values? Bake invariants into their methods.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Define aggregate roots.&lt;/strong&gt; Bundle related entities under one root that enforces consistency rules.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create repository interfaces.&lt;/strong&gt; Don&#39;t implement storage yet—just define &lt;code&gt;save()&lt;/code&gt; and &lt;code&gt;get()&lt;/code&gt; methods. Keep the domain clean.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Emit domain events.&lt;/strong&gt; For meaningful changes (order placed, task completed), raise events. Wire listeners later as needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wrap LLM outputs in schemas.&lt;/strong&gt; Use Pydantic models to enforce contracts. Don&#39;t let free-form text leak into your domain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add orchestration.&lt;/strong&gt; Build application services that coordinate agents via structured commands or events.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The golden rule: &lt;strong&gt;start with the domain, not the tech stack.&lt;/strong&gt; Understand the business problem first. Model it explicitly. Then let the AI tooling serve that model—not the other way around.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/</link> <pubDate>Mon, 20 Oct 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/</guid> </item> <item> <title>Building a Custom FeatureStoreLite MCP Server Using uv</title> <author>Viacheslav Dubrov</author> <category>guide</category> <category>mcp</category> <category>python</category> <category>uv</category> <description>&lt;h1 id=&#34;building-a-custom-featurestorelite-mcp-server-using-uv&#34;&gt;Building a Custom FeatureStoreLite MCP Server Using uv&lt;/h1&gt; &lt;p&gt;&lt;em&gt;A step-by-step guide that shows how to create your own lightweight feature store MCP server from scratch using FastMCP, run it through &lt;strong&gt;uv&lt;/strong&gt;, and integrate it with Claude Desktop.&lt;/em&gt;&lt;/p&gt; &lt;!-- more --&gt; &lt;hr /&gt; &lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt; &lt;p&gt;The &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; is an open standard that enables AI assistants (like Claude) to connect to external data and tools. Instead of building custom integrations for every tool, MCP provides a universal language for AI models to interact with your world.&lt;/p&gt; &lt;p&gt;In this tutorial, we will build a &lt;strong&gt;FeatureStoreLite&lt;/strong&gt; MCP server. This server will act as a bridge between an LLM and a feature store (a database of precomputed ML features), allowing the LLM to query and retrieve feature vectors for users, products, or documents.&lt;/p&gt; &lt;h3 id=&#34;why-build-this&#34;&gt;Why build this?&lt;/h3&gt; &lt;p&gt;Imagine you are an ML engineer debugging a pipeline. Instead of writing SQL queries or Python scripts to check feature values, you can simply ask Claude: &lt;em&gt;&#34;What is the feature vector for user_123?&#34;&lt;/em&gt; or &lt;em&gt;&#34;Show me the metadata for product_abc&#34;&lt;/em&gt;.&lt;/p&gt; &lt;h3 id=&#34;why-use-uv&#34;&gt;Why use &lt;code&gt;uv&lt;/code&gt;?&lt;/h3&gt; &lt;p&gt;We will use &lt;strong&gt;&lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;&lt;/strong&gt;, an extremely fast Python package installer and project manager. It simplifies dependency management and makes running our server reproducible and fast.&lt;/p&gt; &lt;h3 id=&#34;architecture-overview&#34;&gt;Architecture Overview&lt;/h3&gt; &lt;p&gt;Here is how the components interact:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;MCP Architecture&#34; src=&#34;../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/mcp_architecture.svg&#34; /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;User&lt;/strong&gt;: Asks a question in natural language.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Desktop&lt;/strong&gt;: The MCP Client that interprets the question and decides which tool to call.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Our Python application running &lt;code&gt;FastMCP&lt;/code&gt; that exposes tools (&lt;code&gt;get_feature&lt;/code&gt;, &lt;code&gt;store_feature&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQLite&lt;/strong&gt;: The backing storage for our feature vectors.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;2-setup-and-installation&#34;&gt;2. Setup and Installation&lt;/h2&gt; &lt;h3 id=&#34;21-install-uv&#34;&gt;2.1. Install &lt;code&gt;uv&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;If you haven&#39;t installed &lt;code&gt;uv&lt;/code&gt; yet, get it now. It&#39;s a game-changer for Python development.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# macOS/Linux&lt;/span&gt; curl&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-LsSf&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;https://astral.sh/uv/install.sh&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;|&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;sh &lt;span class=&#34;c1&#34;&gt;# Or via Homebrew&lt;/span&gt; brew&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;install&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;uv &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;22-initialize-the-project&#34;&gt;2.2. Initialize the Project&lt;/h3&gt; &lt;p&gt;Create a new directory and initialize a Python project. &lt;code&gt;uv init&lt;/code&gt; creates a &lt;code&gt;pyproject.toml&lt;/code&gt; for you.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# Create project directory&lt;/span&gt; mkdir&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;mcp-featurestore &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;mcp-featurestore &lt;span class=&#34;c1&#34;&gt;# Initialize Python project&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;init &lt;span class=&#34;c1&#34;&gt;# Add the MCP SDK with CLI tools&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;add&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;mcp[cli]&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;3-building-the-server&#34;&gt;3. Building the Server&lt;/h2&gt; &lt;p&gt;We will split our application into two files:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;database.py&lt;/code&gt;: Handles SQLite operations.&lt;/li&gt; &lt;li&gt;&lt;code&gt;featurestore_server.py&lt;/code&gt;: The MCP server definition.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;31-the-database-layer-databasepy&#34;&gt;3.1. The Database Layer (&lt;code&gt;database.py&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;This module manages the SQLite connection and provides helper functions. We&#39;ll also seed it with some dummy data so we have something to query.&lt;/p&gt; &lt;p&gt;Create &lt;code&gt;database.py&lt;/code&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# database.py&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;json&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;sqlite3&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_db_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Get the database path - always in the script&amp;#39;s directory&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;script_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dirname&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;abspath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;vm&#34;&gt;__file__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;script_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;features.db&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;init_db&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Initialize the feature store database with table and sample data&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sqlite3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;connect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_db_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; CREATE TABLE IF NOT EXISTS features (&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; key TEXT PRIMARY KEY,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; vector TEXT NOT NULL,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; metadata TEXT,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; )&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Sample data for experimentation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;example_features&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user_123&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;[0.1, 0.2, -0.5, 0.8, 0.3, -0.1, 0.9, -0.4]&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dumps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;user&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;123&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;segment&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;premium&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;product_abc&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;[0.7, -0.3, 0.4, 0.1, -0.8, 0.6, 0.2, -0.5]&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dumps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;product&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;abc&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;category&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;electronics&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Insert if not exists&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;metadata&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;example_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;INSERT INTO features (key, vector, metadata) VALUES (?, ?, ?)&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sqlite3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IntegrityError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;pass&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Already exists&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;commit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_db_connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sqlite3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;Get a database connection&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sqlite3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;connect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_db_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;init_db&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;✅ Database initialized successfully!&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Initialize the database:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;database.py &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;32-the-mcp-server-featurestore_serverpy&#34;&gt;3.2. The MCP Server (&lt;code&gt;featurestore_server.py&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;Now for the exciting part. We use &lt;code&gt;FastMCP&lt;/code&gt; to define our server. It uses decorators to turn standard Python functions into MCP Tools and Resources.&lt;/p&gt; &lt;p&gt;Create &lt;code&gt;featurestore_server.py&lt;/code&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# featurestore_server.py&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;json&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;mcp.server.fastmcp&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastMCP&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;database&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_db_connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;init_db&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Initialize the MCP Server&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mcp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FastMCP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;FeatureStoreLite&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Ensure DB is ready when server starts&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;init_db&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@mcp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;resource&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;schema://main&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_schema&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Resource: Provide the database schema.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Resources are passive data that LLMs can read like files.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_db_connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;SELECT sql FROM sqlite_master WHERE type=&amp;#39;table&amp;#39;&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fetchall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sql&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sql&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;schema&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sql&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;No tables found.&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;finally&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@mcp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;store_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Tool: Store a feature vector.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Tools are executable functions that LLMs can call to perform actions.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_db_connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Validate that vector is valid JSON&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;INSERT OR REPLACE INTO features (key, vector, metadata) VALUES (?, ?, ?)&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;commit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Successfully stored feature &amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;JSONDecodeError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;Error: Vector must be a valid JSON array string (e.g., &amp;#39;[0.1, 0.2]&amp;#39;)&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Error: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;finally&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@mcp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;get_feature&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Tool: Retrieve a feature vector by key.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_db_connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;SELECT vector, metadata FROM features WHERE key = ?&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fetchone&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dumps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;key&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;vector&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]),&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;metadata&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;indent&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Feature &amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#39; not found.&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;finally&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;nd&#34;&gt;@mcp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;list_features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;sd&#34;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; Tool: List all available feature keys.&lt;/span&gt; &lt;span class=&#34;sd&#34;&gt; &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_db_connection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rows&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;execute&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;SELECT key FROM features&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fetchall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dumps&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rows&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;finally&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;conn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;close&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;vm&#34;&gt;__name__&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mcp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;4-testing-with-mcp-inspector&#34;&gt;4. Testing with MCP Inspector&lt;/h2&gt; &lt;p&gt;Before connecting to Claude, use the &lt;strong&gt;MCP Inspector&lt;/strong&gt; to verify your server works. This web interface lets you test tools and view resources.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;mcp&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;dev&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;featurestore_server.py &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This command starts the server and opens the Inspector in your browser (usually &lt;code&gt;http://localhost:5173&lt;/code&gt; or similar).&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Inspector&#34; src=&#34;../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/inspector.png&#34; style=&#34;width:600px;max-width:100%;height:auto;&#34; /&gt;&lt;/p&gt; &lt;p&gt;Try calling &lt;code&gt;get_feature&lt;/code&gt; with &lt;code&gt;key=&#34;user_123&#34;&lt;/code&gt; in the Inspector to confirm it returns the JSON data.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;5-connecting-to-claude-desktop&#34;&gt;5. Connecting to Claude Desktop&lt;/h2&gt; &lt;p&gt;Now let&#39;s connect our server to Claude Desktop so we can talk to it.&lt;/p&gt; &lt;h3 id=&#34;51-configure-claude&#34;&gt;5.1. Configure Claude&lt;/h3&gt; &lt;p&gt;Edit your Claude Desktop configuration file:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;%APPDATA%/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Add your server to the &lt;code&gt;mcpServers&lt;/code&gt; object:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;mcpServers&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;featurestore&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;command&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;uv&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;quot;args&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;run&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;--with&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;mcp[cli]&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;mcp&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;run&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;/ABSOLUTE/PATH/TO/mcp-featurestore/featurestore_server.py&amp;quot;&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;⚠️ Important&lt;/strong&gt;: You must use the &lt;strong&gt;absolute path&lt;/strong&gt; to your &lt;code&gt;featurestore_server.py&lt;/code&gt; file.&lt;/p&gt; &lt;/blockquote&gt; &lt;h3 id=&#34;52-how-the-interaction-works&#34;&gt;5.2. How the Interaction Works&lt;/h3&gt; &lt;p&gt;When you ask Claude a question, the following workflow occurs:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;MCP Workflow&#34; src=&#34;../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/mcp_workflow.svg&#34; /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Claude sees the available tools (&lt;code&gt;get_feature&lt;/code&gt;, &lt;code&gt;list_features&lt;/code&gt;, etc.).&lt;/li&gt; &lt;li&gt;It determines that your question requires data from the feature store.&lt;/li&gt; &lt;li&gt;It constructs a tool call and sends it to your server.&lt;/li&gt; &lt;li&gt;Your server executes the Python function and returns the result.&lt;/li&gt; &lt;li&gt;Claude uses that result to answer your question.&lt;/li&gt; &lt;/ol&gt; &lt;h3 id=&#34;53-example-queries&#34;&gt;5.3. Example Queries&lt;/h3&gt; &lt;p&gt;Restart Claude Desktop and try these prompts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;&#34;List all available features.&#34; &lt;img alt=&#34;Question 2&#34; src=&#34;../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/question-2.png&#34; style=&#34;width:600px;max-width:100%;height:auto;&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&#34;Get the feature vector for user_123.&#34; &lt;img alt=&#34;Question 3&#34; src=&#34;../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/question-3.png&#34; style=&#34;width:600px;max-width:100%;height:auto;&#34; /&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&#34;Store a new feature for &#39;new_item&#39; with vector [0.5, 0.5] and metadata {&#39;type&#39;: &#39;test&#39;}.&#34;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;6-troubleshooting&#34;&gt;6. Troubleshooting&lt;/h2&gt; &lt;p&gt;If things aren&#39;t working, check these common issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&#34;Server connection failed&#34;&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check the logs: &lt;code&gt;tail -f ~/Library/Logs/Claude/mcp.log&lt;/code&gt; (macOS).&lt;/li&gt; &lt;li&gt;Ensure you used the &lt;strong&gt;absolute path&lt;/strong&gt; in the config file.&lt;/li&gt; &lt;li&gt;Verify &lt;code&gt;uv&lt;/code&gt; is in your system PATH or use the full path to the &lt;code&gt;uv&lt;/code&gt; binary.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&#34;Tool execution error&#34;&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use the &lt;strong&gt;Inspector&lt;/strong&gt; (&lt;code&gt;uv run mcp dev ...&lt;/code&gt;) to debug the specific tool.&lt;/li&gt; &lt;li&gt;Check if your &lt;code&gt;database.py&lt;/code&gt; is creating the &lt;code&gt;features.db&lt;/code&gt; file in the correct location.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;7-conclusion&#34;&gt;7. Conclusion&lt;/h2&gt; &lt;p&gt;You&#39;ve just built a functional MCP server that extends Claude&#39;s capabilities! This pattern—using &lt;code&gt;FastMCP&lt;/code&gt; for the server and &lt;code&gt;uv&lt;/code&gt; for execution—is a powerful way to build robust AI tools quickly.&lt;/p&gt; &lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/slavadubrov/mcp-featurestore&#34;&gt;The repo of this tutorial example&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/news/model-context-protocol&#34;&gt;Introduction to MCP&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://github.com/modelcontextprotocol/python-sdk&#34;&gt;MCP Python SDK&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://claude.ai/download&#34;&gt;Claude Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;</description> <link>https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/</link> <pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/</guid> </item> <item> <title>Quick-guide on Local Stable-Diffusion Toolkits for macOS</title> <author>Viacheslav Dubrov</author> <category>genai</category> <category>guide</category> <category>macos</category> <category>tools</category> <description>&lt;h1 id=&#34;quick-guide-on-local-stable-diffusion-toolkits-for-macos&#34;&gt;Quick-guide on Local Stable-Diffusion Toolkits for macOS&lt;/h1&gt; &lt;p&gt;Running generative AI models locally is a game-changer. It means &lt;strong&gt;zero cloud costs&lt;/strong&gt;, &lt;strong&gt;no censorship&lt;/strong&gt;, &lt;strong&gt;total privacy&lt;/strong&gt;, and &lt;strong&gt;unlimited experimentation&lt;/strong&gt;. Whether you&#39;re generating character portraits, architectural concepts, or just having fun, your Mac is more than capable of handling the workload thanks to Apple Silicon.&lt;/p&gt; &lt;p&gt;But with so many tools available, where do you start?&lt;/p&gt; &lt;p&gt;Below is a practical guide to the best macOS-ready interfaces. Each tool wraps the same powerful Stable Diffusion models but offers a completely different experience—from &#34;Apple-like&#34; simplicity to &#34;developer-grade&#34; control.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;how-these-tools-work&#34;&gt;How these tools work&lt;/h2&gt; &lt;p&gt;At their core, all these applications do the same thing: they provide a user interface (UI) for the Stable Diffusion models. They handle the complex &#34;plumbing&#34;—loading heavy model weights, managing memory, and talking to your Mac&#39;s GPU.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;AI Tools Workflow&#34; src=&#34;../../../../assets/2025-05-10-guide-ai-image-tools/ai_tools_workflow.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Because they all share the same underlying architecture, you can usually share &lt;strong&gt;model files&lt;/strong&gt; (&lt;code&gt;.safetensors&lt;/code&gt;) between them. Download a model once, and try it in different apps to see which workflow suits you.&lt;/p&gt; &lt;h3 id=&#34;the-apple-silicon-advantage&#34;&gt;The Apple Silicon Advantage&lt;/h3&gt; &lt;p&gt;Why is the Mac so good for this? It comes down to &lt;strong&gt;Unified Memory&lt;/strong&gt;. Unlike a PC with a separate graphics card (where you might have only 8GB or 12GB of VRAM), your Mac&#39;s GPU has access to your &lt;em&gt;entire&lt;/em&gt; system RAM.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Apple Silicon Stack&#34; src=&#34;../../../../assets/2025-05-10-guide-ai-image-tools/apple_silicon_stack.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;This means a MacBook Pro with 32GB or 64GB of RAM can load massive models (like SDXL or Flux) that would bring a typical gaming PC to its knees.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;1-draw-things-the-powerhouse-app&#34;&gt;1. Draw Things: The Powerhouse App&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href=&#34;https://apps.apple.com/de/app/draw-things-offline-ai-art/id6444050820?l=en-GB&#34;&gt;App Store Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What it is:&lt;/strong&gt; A native iOS/macOS app that feels like a professional design tool. It&#39;s surprisingly powerful, supporting ControlNet, Inpainting, and LoRAs right out of the box.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Users who want a &lt;strong&gt;native app experience&lt;/strong&gt; (no terminal!) but don&#39;t want to sacrifice advanced features.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Performance:&lt;/strong&gt; Highly optimized for Apple Silicon.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Feature Rich:&lt;/strong&gt; Supports Inpainting, Outpainting, ControlNet, and scriptable workflows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Offline First:&lt;/strong&gt; Runs completely offline after downloading models.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;UI can be a bit dense on smaller screens (it&#39;s designed to scale from iPhone to Mac).&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;2-diffusionbee-the-one-click-wonder&#34;&gt;2. DiffusionBee: The &#34;One-Click&#34; Wonder&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href=&#34;https://diffusionbee.com/download&#34;&gt;diffusionbee.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What it is:&lt;/strong&gt; The simplest way to run Stable Diffusion on a Mac. It strips away all the jargon. You don&#39;t &#34;load a checkpoint&#34;; you just select a style.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Absolute beginners who just want to make cool images &lt;em&gt;now&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero Setup:&lt;/strong&gt; Download the DMG, drag to Applications, run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clean UI:&lt;/strong&gt; Very &#34;Apple-like&#34; design.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built-in Tools:&lt;/strong&gt; Includes simple upscaling and background removal.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Limited Control:&lt;/strong&gt; You can&#39;t easily tweak advanced sampler settings or complex node pipelines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slower Updates:&lt;/strong&gt; New features (like the latest ControlNet models) take longer to arrive.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;3-comfyui-the-node-based-lab&#34;&gt;3. ComfyUI: The Node-Based Lab&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href=&#34;https://www.comfy.org/download&#34;&gt;comfy.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What it is:&lt;/strong&gt; A visual programming environment. Instead of sliders, you connect &#34;nodes&#34; with wires to build your image generation pipeline.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Power users, technical artists, and anyone who wants to understand &lt;em&gt;exactly&lt;/em&gt; how the image is being made.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ultimate Control:&lt;/strong&gt; Build custom workflows for specific tasks (e.g., &#34;Generate image -&amp;gt; Upscale -&amp;gt; Face Restore&#34;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Often faster than other UIs because it executes only what&#39;s needed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ecosystem:&lt;/strong&gt; Thousands of custom nodes created by the community.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Steep Learning Curve:&lt;/strong&gt; It looks like a bowl of spaghetti until you learn to read it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Requires some comfort with Python/Terminal (though installers exist).&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;4-stable-diffusion-webui-automatic1111&#34;&gt;4. Stable Diffusion WebUI (AUTOMATIC1111)&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Install Guide:&lt;/strong&gt; &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon&#34;&gt;Installation on Apple Silicon&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What it is:&lt;/strong&gt; The &#34;Swiss Army Knife&#34; of Stable Diffusion. It runs in your browser and has a tab for everything.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Enthusiasts who want to use the latest community extensions immediately.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Extensions:&lt;/strong&gt; If a new AI technique is released today, A1111 will have an extension for it tomorrow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tutorials:&lt;/strong&gt; The vast majority of YouTube tutorials use this interface.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Clunky UI:&lt;/strong&gt; It&#39;s functional but chaotic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Heavy:&lt;/strong&gt; Can be slower and more memory-hungry than ComfyUI or Draw Things.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=&#34;5-fooocus-midjourney-on-your-mac&#34;&gt;5. Fooocus: Midjourney on Your Mac&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href=&#34;https://github.com/lllyasviel/Fooocus&#34;&gt;github.com/lllyasviel/Fooocus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What it is:&lt;/strong&gt; An interface designed to mimic the ease of Midjourney. It automates all the technical choices (samplers, steps, refiners) so you can focus on the prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best for:&lt;/strong&gt; High-quality results with minimal tweaking.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Smart Defaults:&lt;/strong&gt; It &#34;just works&#34; and produces beautiful images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimalist:&lt;/strong&gt; No overwhelming sliders.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Less Customization:&lt;/strong&gt; Harder to force it to do something specific if it fights you.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Often optimized for NVIDIA GPUs first, so Mac performance can vary.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2 id=&#34;comparison-table&#34;&gt;Comparison Table&lt;/h2&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Tool&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Install Difficulty&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Interface Style&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Draw Things&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;★☆☆☆☆ (App Store)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Native App (Pro)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;The Sweet Spot&lt;/strong&gt; (Power + Ease)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;DiffusionBee&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;★☆☆☆☆ (DMG)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Native App (Simple)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Beginners&lt;/strong&gt; &amp;amp; Casual Use&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;ComfyUI&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;★★★☆☆ (Python)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Node Graph&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Complex Workflows&lt;/strong&gt; &amp;amp; Automation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;A1111 WebUI&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;★★★★☆ (Terminal)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Browser Dashboard&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Extensions&lt;/strong&gt; &amp;amp; Community Support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Fooocus&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;★★★☆☆ (Python)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Minimalist&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Midjourney-style&lt;/strong&gt; Prompting&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;hr /&gt; &lt;h2 id=&#34;decision-flowchart&#34;&gt;Decision Flowchart&lt;/h2&gt; &lt;p&gt;Not sure which one to pick? Follow this path:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Decision Tree&#34; src=&#34;../../../../assets/2025-05-10-guide-ai-image-tools/ai_tool_decision_tree.svg&#34; /&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;essential-tips-for-mac-users&#34;&gt;Essential Tips for Mac Users&lt;/h2&gt; &lt;h3 id=&#34;1-system-requirements&#34;&gt;1. System Requirements&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RAM is King:&lt;/strong&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;8GB:&lt;/strong&gt; Doable for basic 512x512 images, but expect slowness and crashes with newer models (SDXL).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;16GB:&lt;/strong&gt; The comfortable minimum. You can run most things, including SDXL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32GB+:&lt;/strong&gt; The dream. You can keep multiple models loaded and multitask while generating.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; AI models are huge (2GB - 6GB each). Get an external SSD if your Mac is low on space.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;2-where-to-get-models&#34;&gt;2. Where to get Models&lt;/h3&gt; &lt;p&gt;The software is just the engine; you need fuel (models).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://civitai.com&#34;&gt;Civitai&lt;/a&gt;:&lt;/strong&gt; The largest community for models. Look for &#34;Checkpoints&#34; that are compatible with SD 1.5 or SDXL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co&#34;&gt;Hugging Face&lt;/a&gt;:&lt;/strong&gt; The &#34;GitHub of AI&#34;. More technical, but the official source for base models from Stability AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;File Types:&lt;/strong&gt; Always look for &lt;code&gt;.safetensors&lt;/code&gt; files. Avoid &lt;code&gt;.ckpt&lt;/code&gt; files if possible, as they can theoretically contain malicious code (though rare).&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;3-start-simple&#34;&gt;3. Start Simple&lt;/h3&gt; &lt;p&gt;Don&#39;t try to install ComfyUI on day one. Start with &lt;strong&gt;DiffusionBee&lt;/strong&gt; or &lt;strong&gt;Draw Things&lt;/strong&gt;. Get a feel for how prompting works. Once you hit a wall (&#34;I wish I could control the pose of this character...&#34;), &lt;em&gt;then&lt;/em&gt; look into ControlNet and more advanced tools.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/</link> <pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/</guid> </item> <item> <title>The Ultimate Guide to `pyproject.toml`</title> <author>Viacheslav Dubrov</author> <category>best-practices</category> <category>guide</category> <category>python</category> <description>&lt;h1 id=&#34;the-ultimate-guide-to-pyprojecttoml&#34;&gt;The Ultimate Guide to &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/h1&gt; &lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt; &lt;p&gt;Think of &lt;code&gt;pyproject.toml&lt;/code&gt; as the &lt;strong&gt;&lt;code&gt;package.json&lt;/code&gt; for Python&lt;/strong&gt;. It&#39;s a single configuration file that holds your project&#39;s metadata, dependencies, and tool settings. Whether you use &lt;code&gt;.venv&lt;/code&gt;, &lt;code&gt;pyenv&lt;/code&gt;, or &lt;code&gt;uv&lt;/code&gt;, this one file simplifies development and makes collaboration smoother.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;what-is-pyprojecttoml&#34;&gt;What is &lt;code&gt;pyproject.toml&lt;/code&gt;?&lt;/h2&gt; &lt;p&gt;&lt;code&gt;pyproject.toml&lt;/code&gt; is a standardized configuration file that lives at the root of your Python project. It uses the TOML format (think INI files but better) and is backed by official Python Enhancement Proposals (PEPs).&lt;/p&gt; &lt;p&gt;The file evolved in two key stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://peps.python.org/pep-0518/&#34;&gt;PEP 518&lt;/a&gt;&lt;/strong&gt; (2016) introduced the &lt;code&gt;[build-system]&lt;/code&gt; table so build tools could declare their requirements in a standard way.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://peps.python.org/pep-0621/&#34;&gt;PEP 621&lt;/a&gt;&lt;/strong&gt; (2020) added the &lt;code&gt;[project]&lt;/code&gt; table for core package metadata—name, version, dependencies, and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Today, most Python developer tools (Black, isort, pytest, Ruff, mypy) read their configuration from &lt;code&gt;[tool.*]&lt;/code&gt; sections in this file, making it the central hub for your entire project setup.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Structure&#34; src=&#34;../../../../assets/2025-05-08-guide-pyproject-toml/pyproject_structure.svg&#34; /&gt;&lt;/p&gt; &lt;h2 id=&#34;why-should-you-care&#34;&gt;Why should you care?&lt;/h2&gt; &lt;h3 id=&#34;1-one-file-to-rule-them-all&#34;&gt;1. One file to rule them all&lt;/h3&gt; &lt;p&gt;Before &lt;code&gt;pyproject.toml&lt;/code&gt;, you&#39;d juggle &lt;code&gt;setup.py&lt;/code&gt;, &lt;code&gt;setup.cfg&lt;/code&gt;, &lt;code&gt;requirements.txt&lt;/code&gt;, &lt;code&gt;MANIFEST.in&lt;/code&gt;, and various dotfiles (&lt;code&gt;.flake8&lt;/code&gt;, &lt;code&gt;.coveragerc&lt;/code&gt;). Now everything lives in one place.&lt;/p&gt; &lt;h3 id=&#34;2-backend-agnostic-builds&#34;&gt;2. Backend-agnostic builds&lt;/h3&gt; &lt;p&gt;When you run &lt;code&gt;pip install .&lt;/code&gt;, pip reads &lt;code&gt;pyproject.toml&lt;/code&gt; and automatically installs whatever build tools your project needs (setuptools, flit, hatchling, etc.). You are no longer tied to &lt;code&gt;setuptools&lt;/code&gt;.&lt;/p&gt; &lt;h3 id=&#34;3-universal-tool-configuration&#34;&gt;3. Universal tool configuration&lt;/h3&gt; &lt;p&gt;Linters, formatters, test runners, and type checkers all know to look here for their settings. Your IDE, CI pipeline, and teammates all read from the same source of truth.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Tool Ecosystem&#34; src=&#34;../../../../assets/2025-05-08-guide-pyproject-toml/tool_ecosystem.svg&#34; /&gt;&lt;/p&gt; &lt;h2 id=&#34;anatomy-of-a-pyprojecttoml&#34;&gt;Anatomy of a &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;Here&#39;s what a typical file looks like with the three main sections:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# 1. Build system - tells pip/build how to package your project&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;[build-system]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requires&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;hatchling&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;build-backend&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;hatchling.build&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Project metadata and dependencies&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;[project]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;awesome-app&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;0.1.0&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;Short demo of pyproject.toml&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;readme&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;README.md&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requires-python&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&amp;gt;=3.12&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dependencies&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;fastapi&amp;gt;=0.111&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;uvicorn[standard]&amp;gt;=0.30&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Expose CLI commands&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;[project.scripts]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;awesome-cli&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;awesome_app.cli:main&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Optional dependencies (e.g., for development)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;[project.optional-dependencies]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dev&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;pytest&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;ruff&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;mypy&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. Tool configuration&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;[tool.ruff]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line-length&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target-version&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;py312&amp;quot;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;[tool.pytest.ini_options]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;addopts&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;-ra -q&amp;quot;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;testpaths&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;tests&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;breaking-it-down&#34;&gt;Breaking it down&lt;/h3&gt; &lt;h4 id=&#34;build-system&#34;&gt;&lt;code&gt;[build-system]&lt;/code&gt;&lt;/h4&gt; &lt;p&gt;Required if you want to package/distribute your project. Tells pip and build tools (like &lt;code&gt;python -m build&lt;/code&gt;) which backend to use.&lt;/p&gt; &lt;h4 id=&#34;project&#34;&gt;&lt;code&gt;[project]&lt;/code&gt;&lt;/h4&gt; &lt;p&gt;Your package metadata. This is where dependencies live instead of &lt;code&gt;requirements.txt&lt;/code&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;dependencies&lt;/code&gt;&lt;/strong&gt;: The runtime requirements for your package.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;optional-dependencies&lt;/code&gt;&lt;/strong&gt;: Groups of extra dependencies (e.g., &lt;code&gt;dev&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;docs&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;scripts&lt;/code&gt;&lt;/strong&gt;: Creates executable commands. In the example above, installing the package creates an &lt;code&gt;awesome-cli&lt;/code&gt; command that runs the &lt;code&gt;main&lt;/code&gt; function in &lt;code&gt;awesome_app/cli.py&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=&#34;tool&#34;&gt;&lt;code&gt;[tool.*]&lt;/code&gt;&lt;/h4&gt; &lt;p&gt;Configuration for any tool that supports it. Each tool gets its own namespace (e.g., &lt;code&gt;[tool.pytest.ini_options]&lt;/code&gt;, &lt;code&gt;[tool.mypy]&lt;/code&gt;).&lt;/p&gt; &lt;h2 id=&#34;does-it-replace-requirementstxt&#34;&gt;Does it replace &lt;code&gt;requirements.txt&lt;/code&gt;?&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;In modern workflows, yes.&lt;/strong&gt; Tools like &lt;a href=&#34;https://python-poetry.org/&#34;&gt;Poetry&lt;/a&gt;, &lt;a href=&#34;https://pdm-project.org/&#34;&gt;PDM&lt;/a&gt;, &lt;a href=&#34;https://hatch.pypa.io/&#34;&gt;Hatch&lt;/a&gt;, and &lt;a href=&#34;https://github.com/astral-sh/uv&#34;&gt;uv&lt;/a&gt; store dependencies directly in the &lt;code&gt;[project]&lt;/code&gt; section and generate lockfiles for reproducibility.&lt;/p&gt; &lt;p&gt;You only need &lt;code&gt;requirements.txt&lt;/code&gt; if:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You&#39;re working with legacy deployment systems that expect it.&lt;/li&gt; &lt;li&gt;You have simple CI scripts that haven&#39;t been updated.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most modern tools can export a &lt;code&gt;requirements.txt&lt;/code&gt; from your &lt;code&gt;pyproject.toml&lt;/code&gt; when needed:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&amp;gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;requirements.txt &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h2 id=&#34;choosing-a-build-backend&#34;&gt;Choosing a Build Backend&lt;/h2&gt; &lt;p&gt;One of the confusing parts of &lt;code&gt;pyproject.toml&lt;/code&gt; is choosing a build backend. Here is a quick comparison:&lt;/p&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Backend&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Best For&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Pros&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Cons&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Hatchling&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Modern standard&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Fast, extensible, supports plugins&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Newer, less legacy support&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Flit&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Simple packages&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Extremely simple, zero config&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Not for complex builds (C extensions)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Setuptools&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Legacy / Complex&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Supports everything (C extensions, etc.)&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Slower, complex configuration&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Poetry&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Poetry users&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Integrated with Poetry ecosystem&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;Locked into Poetry workflow&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt; Use &lt;strong&gt;Hatchling&lt;/strong&gt; for new pure-Python projects. It&#39;s the default for &lt;code&gt;uv&lt;/code&gt; and is becoming the industry standard.&lt;/p&gt; &lt;h2 id=&#34;migrating-an-existing-project&#34;&gt;Migrating an existing project&lt;/h2&gt; &lt;p&gt;If you have a legacy Python project, here&#39;s how to modernize it:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Migration Flow&#34; src=&#34;../../../../assets/2025-05-08-guide-pyproject-toml/migration_flow.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step-by-step:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Add &lt;code&gt;[build-system]&lt;/code&gt;&lt;/strong&gt; - Start with setuptools if you&#39;re not sure: &lt;code&gt;requires = [&#34;setuptools&amp;gt;=61&#34;, &#34;wheel&#34;]&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Move to &lt;code&gt;[project]&lt;/code&gt;&lt;/strong&gt; - Transfer name, version, dependencies from &lt;code&gt;setup.py&lt;/code&gt; or &lt;code&gt;setup.cfg&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Convert dev dependencies&lt;/strong&gt; - Put them in &lt;code&gt;[project.optional-dependencies].dev&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Configure tools&lt;/strong&gt; - Add &lt;code&gt;[tool.*]&lt;/code&gt; sections for Black, pytest, mypy, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handle &lt;code&gt;requirements.txt&lt;/code&gt;&lt;/strong&gt; - Either drop it or generate it from lockfile for legacy systems.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After migration, you can delete &lt;code&gt;setup.py&lt;/code&gt;, &lt;code&gt;setup.cfg&lt;/code&gt;, and most config dotfiles.&lt;/p&gt; &lt;h2 id=&#34;advanced-features&#34;&gt;Advanced Features&lt;/h2&gt; &lt;h3 id=&#34;cli-entry-points&#34;&gt;CLI Entry Points&lt;/h3&gt; &lt;p&gt;Instead of the old &lt;code&gt;console_scripts&lt;/code&gt; in &lt;code&gt;setup.py&lt;/code&gt;, use &lt;code&gt;[project.scripts]&lt;/code&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;k&#34;&gt;[project.scripts]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;my-tool&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;my_package.main:run&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;When a user installs your package, they can type &lt;code&gt;my-tool&lt;/code&gt; in their terminal.&lt;/p&gt; &lt;h3 id=&#34;workspaces-monorepos&#34;&gt;Workspaces (Monorepos)&lt;/h3&gt; &lt;p&gt;Tools like &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;hatch&lt;/code&gt; support workspaces, allowing you to manage multiple packages in a single repo.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;k&#34;&gt;[tool.uv.workspace]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;members&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;packages/*&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;This allows you to develop multiple interdependent packages and install them all into a single virtual environment for testing.&lt;/p&gt; &lt;h2 id=&#34;typical-workflows-with-uv&#34;&gt;Typical Workflows with &lt;code&gt;uv&lt;/code&gt;&lt;/h2&gt; &lt;h3 id=&#34;starting-a-new-project&#34;&gt;Starting a new project&lt;/h3&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;init&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;my_app&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# creates folder with pyproject.toml and .venv&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;my_app uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;add&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;requests&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;fastapi&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# adds to [project.dependencies] and installs&lt;/span&gt; uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pytest&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# runs tests in the venv&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;running-scripts&#34;&gt;Running scripts&lt;/h3&gt; &lt;p&gt;You can define scripts in &lt;code&gt;pyproject.toml&lt;/code&gt; (if using a task runner like &lt;code&gt;poe&lt;/code&gt; or &lt;code&gt;hatch&lt;/code&gt;) or just use &lt;code&gt;uv run&lt;/code&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;uv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;run&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;python&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;main.py &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h2 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Don&#39;t pin exact versions in libraries&lt;/strong&gt;: Use ranges (e.g., &lt;code&gt;requests&amp;gt;=2.30&lt;/code&gt;) so your library doesn&#39;t conflict with others.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Do pin versions in applications&lt;/strong&gt;: Use a lockfile (&lt;code&gt;uv.lock&lt;/code&gt; or &lt;code&gt;poetry.lock&lt;/code&gt;) to ensure reproducible builds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Group dev dependencies&lt;/strong&gt;: Keep testing, linting, and docs dependencies in separate optional groups (e.g., &lt;code&gt;dev&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;docs&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keep it clean&lt;/strong&gt;: Don&#39;t dump every possible config option in there. Stick to project-wide defaults.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; &lt;code&gt;pyproject.toml&lt;/code&gt; brings Python&#39;s project setup into the modern era. Whether you&#39;re packaging a library, managing dependencies, or configuring tools, this one file is your command center. Start with &lt;code&gt;uv&lt;/code&gt; for the smoothest experience, or integrate it into your existing workflow gradually.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/</link> <pubDate>Thu, 08 May 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/</guid> </item> <item> <title>Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc 🚀</title> <author>Viacheslav Dubrov</author> <category>guide</category> <category>macos</category> <category>tooling</category> <category>zsh</category> <description>&lt;h1 id=&#34;mastering-zsh-startup-zprofile-vs-zshrc&#34;&gt;Mastering Zsh Startup: &lt;code&gt;~/.zprofile&lt;/code&gt; vs &lt;code&gt;~/.zshrc&lt;/code&gt; 🚀&lt;/h1&gt; &lt;p&gt;If you&#39;ve ever wondered why your terminal feels slow, or why your environment variables aren&#39;t loading where you expect them to, you&#39;re likely battling the Zsh startup order.&lt;/p&gt; &lt;p&gt;The distinction between &lt;code&gt;~/.zprofile&lt;/code&gt; and &lt;code&gt;~/.zshrc&lt;/code&gt; is one of the most common sources of confusion for developers moving to Zsh (especially on macOS).&lt;/p&gt; &lt;h2 id=&#34;tldr&#34;&gt;TL;DR ⚡&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt; is for &lt;strong&gt;Environment Setup&lt;/strong&gt;. It runs &lt;em&gt;once&lt;/em&gt; when you log in (or open a terminal tab on macOS). Put your &lt;code&gt;PATH&lt;/code&gt;, &lt;code&gt;EDITOR&lt;/code&gt;, and language version managers (like &lt;code&gt;fnm&lt;/code&gt;, &lt;code&gt;pyenv&lt;/code&gt;) here.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt; is for &lt;strong&gt;Interactive Configuration&lt;/strong&gt;. It runs &lt;em&gt;every time&lt;/em&gt; you start a new shell instance. Put your aliases, prompt themes, and key bindings here.&lt;/li&gt; &lt;/ul&gt; &lt;!-- more --&gt; &lt;p&gt;&lt;img alt=&#34;Zsh Config Structure&#34; src=&#34;../../../../assets/2025-05-07-zshrc-vs-zprofile/zsh_config_structure.svg&#34; /&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;the-shell-startup-flow&#34;&gt;The Shell Startup Flow 🐚&lt;/h2&gt; &lt;p&gt;To understand where to put things, you need to understand &lt;em&gt;when&lt;/em&gt; files are loaded. Zsh has a specific hierarchy of configuration files.&lt;/p&gt; &lt;h3 id=&#34;login-vs-interactive-shells&#34;&gt;Login vs. Interactive Shells&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Login Shell&lt;/strong&gt;: The first shell you enter after authentication. On macOS, &lt;strong&gt;every new terminal tab or window is a login shell by default&lt;/strong&gt;. This is a key difference from Linux, where opening a terminal usually starts a &lt;em&gt;non-login&lt;/em&gt; interactive shell.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive Shell&lt;/strong&gt;: Any shell where you can type commands.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is the actual flow of execution when you open a terminal on macOS:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Zsh Startup Flow&#34; src=&#34;../../../../assets/2025-05-07-zshrc-vs-zprofile/zsh_startup_flow.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;the-loading-order&#34;&gt;The Loading Order&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zshenv&lt;/code&gt;&lt;/strong&gt;: (Optional) Runs for &lt;strong&gt;every&lt;/strong&gt; shell script and command. &lt;strong&gt;Avoid putting output or heavy logic here&lt;/strong&gt;, as it can break scripts. Use it only for essential environment variables that &lt;em&gt;must&lt;/em&gt; exist everywhere (rarely needed for average users).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt;: Runs only for &lt;strong&gt;login shells&lt;/strong&gt;. This is your &#34;setup&#34; phase.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt;: Runs for &lt;strong&gt;interactive shells&lt;/strong&gt;. This is your &#34;customization&#34; phase.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;~/.zlogin&lt;/code&gt;&lt;/strong&gt;: (Optional) Runs at the very end of a login shell startup.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2 id=&#34;what-goes-where&#34;&gt;What Goes Where? 📁&lt;/h2&gt; &lt;h3 id=&#34;1-zprofile-the-environment-layer&#34;&gt;1. &lt;code&gt;~/.zprofile&lt;/code&gt;: The Environment Layer 🌍&lt;/h3&gt; &lt;p&gt;Think of this as the foundation of your house. It sets up the rules of physics (paths, variables) that everything else relies on.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What belongs here:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;PATH&lt;/code&gt; modifications&lt;/strong&gt;: Adding directories to your executable path.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;: &lt;code&gt;EDITOR&lt;/code&gt;, &lt;code&gt;LANG&lt;/code&gt;, &lt;code&gt;GOPATH&lt;/code&gt;, &lt;code&gt;JAVA_HOME&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Initialization&lt;/strong&gt;: Things that modify the environment, like &lt;code&gt;pyenv&lt;/code&gt;, &lt;code&gt;rbenv&lt;/code&gt;, &lt;code&gt;fnm&lt;/code&gt;, or &lt;code&gt;cargo&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; These only need to be calculated once. If you put them in &lt;code&gt;.zshrc&lt;/code&gt;, they will be re-calculated every time you open a sub-shell or run a script, which is wasteful and can lead to duplicate entries in your &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# ~/.zprofile&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Set up your PATH&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Ensure local bin is first so your tools override system ones&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;PATH&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOME&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/.local/bin:/opt/homebrew/bin:&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$PATH&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Set global variables&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;EDITOR&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;nvim&amp;quot;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;VISUAL&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;nvim&amp;quot;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;LANG&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;en_US.UTF-8&amp;quot;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. Initialize Version Managers (The Heavy Lifters)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Doing this here keeps your shell startup snappy!&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;fnm&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;env&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;--use-on-cd&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;pyenv&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;init&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h3 id=&#34;2-zshrc-the-interactive-layer&#34;&gt;2. &lt;code&gt;~/.zshrc&lt;/code&gt;: The Interactive Layer 🎮&lt;/h3&gt; &lt;p&gt;Think of this as the interior decoration. It makes the house comfortable to live in.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What belongs here:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Aliases&lt;/strong&gt;: &lt;code&gt;alias g=&#39;git&#39;&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Starship, Powerlevel10k, or Pure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Completions&lt;/strong&gt;: &lt;code&gt;compinit&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key Bindings&lt;/strong&gt;: &lt;code&gt;bindkey&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Shell Options&lt;/strong&gt;: &lt;code&gt;setopt autocd&lt;/code&gt;, &lt;code&gt;setopt histignorealldups&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt; These settings only matter when a human is typing at the keyboard. A script running in the background doesn&#39;t need your fancy prompt or your git aliases.&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# ~/.zshrc&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Load your prompt (Visuals)&lt;/span&gt; autoload&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-Uz&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;promptinit&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;promptinit prompt&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;pure &lt;span class=&#34;c1&#34;&gt;# 2. Aliases (Shortcuts)&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;ll&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ls -lah&amp;#39;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;git&amp;#39;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;gs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;git status&amp;#39;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. Shell Options (Behavior)&lt;/span&gt; setopt&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;autocd&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# cd by just typing directory name&lt;/span&gt; setopt&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;histignorealldups&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# Don&amp;#39;t record duplicate history entries&lt;/span&gt; setopt&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;share_history&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# Share history between tabs&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 4. Completions (The Magic)&lt;/span&gt; autoload&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-Uz&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;compinit&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;compinit &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;why-not-just-put-everything-in-one-file&#34;&gt;Why Not Just Put Everything in One File? 🤔&lt;/h2&gt; &lt;p&gt;You might be asking: &lt;em&gt;&#34;Why can&#39;t I just put my aliases in &lt;code&gt;.zprofile&lt;/code&gt; and run them once? Why do I need to reload them?&#34;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;It comes down to &lt;strong&gt;Inheritance&lt;/strong&gt; vs. &lt;strong&gt;Re-definition&lt;/strong&gt;.&lt;/p&gt; &lt;h3 id=&#34;1-environment-variables-inherit&#34;&gt;1. Environment Variables Inherit 🧬&lt;/h3&gt; &lt;p&gt;When you set &lt;code&gt;export EDITOR=&#34;vim&#34;&lt;/code&gt; in a parent shell (like your login shell), every child process (sub-shells, scripts, programs) inherits that variable. You set it once, and it propagates everywhere. This is why &lt;code&gt;.zprofile&lt;/code&gt; is perfect for &lt;code&gt;export&lt;/code&gt;.&lt;/p&gt; &lt;h3 id=&#34;2-aliases-and-functions-do-not-inherit&#34;&gt;2. Aliases and Functions Do Not Inherit 🚫&lt;/h3&gt; &lt;p&gt;Aliases (&lt;code&gt;alias g=&#39;git&#39;&lt;/code&gt;) and shell functions are &lt;strong&gt;local&lt;/strong&gt; to the current shell instance. They are &lt;em&gt;not&lt;/em&gt; passed down to child shells.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you define an alias in &lt;code&gt;.zprofile&lt;/code&gt;, it exists in your top-level login shell.&lt;/li&gt; &lt;li&gt;If you then type &lt;code&gt;zsh&lt;/code&gt; to start a sub-shell, or run a script, that alias &lt;strong&gt;disappears&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;To make aliases available everywhere, you &lt;em&gt;must&lt;/em&gt; re-define them in every new interactive shell. That is exactly what &lt;code&gt;.zshrc&lt;/code&gt; does.&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;3-scripts-dont-need-human-features&#34;&gt;3. Scripts Don&#39;t Need &#34;Human&#34; Features 🤖&lt;/h3&gt; &lt;p&gt;When you run a shell script (e.g., &lt;code&gt;./deploy.sh&lt;/code&gt;), it starts a new, non-interactive shell.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It doesn&#39;t need your fancy prompt.&lt;/li&gt; &lt;li&gt;It doesn&#39;t need your &lt;code&gt;git&lt;/code&gt; aliases.&lt;/li&gt; &lt;li&gt;It definitely doesn&#39;t want to wait for &lt;code&gt;oh-my-zsh&lt;/code&gt; to load.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By keeping interactive config in &lt;code&gt;.zshrc&lt;/code&gt;, you ensure that your scripts run fast and clean, without being polluted by your personal customization.&lt;/p&gt; &lt;hr /&gt; &lt;h2 id=&#34;common-pitfalls-best-practices&#34;&gt;Common Pitfalls &amp;amp; Best Practices 🚫&lt;/h2&gt; &lt;h3 id=&#34;pitfall-1-putting-nvm-or-pyenv-in-zshrc&#34;&gt;🛑 Pitfall 1: Putting &lt;code&gt;nvm&lt;/code&gt; or &lt;code&gt;pyenv&lt;/code&gt; in &lt;code&gt;.zshrc&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The Symptom:&lt;/strong&gt; You open a new terminal tab, and it takes 2-3 seconds before you can type anything. &lt;strong&gt;The Cause:&lt;/strong&gt; Version managers often have heavy initialization scripts. If you put them in &lt;code&gt;.zshrc&lt;/code&gt;, they run every single time. &lt;strong&gt;The Fix:&lt;/strong&gt; Move them to &lt;code&gt;~/.zprofile&lt;/code&gt;.&lt;/p&gt; &lt;h3 id=&#34;pitfall-2-growing-path&#34;&gt;🛑 Pitfall 2: Growing &lt;code&gt;PATH&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;The Symptom:&lt;/strong&gt; Your &lt;code&gt;$PATH&lt;/code&gt; variable has the same directories listed 5 times. &lt;strong&gt;The Cause:&lt;/strong&gt; You have &lt;code&gt;export PATH=&#34;$HOME/bin:$PATH&#34;&lt;/code&gt; in your &lt;code&gt;.zshrc&lt;/code&gt;. Every time you reload the config (&lt;code&gt;source ~/.zshrc&lt;/code&gt;) or open a sub-shell, it appends the path again. &lt;strong&gt;The Fix:&lt;/strong&gt; Move &lt;code&gt;PATH&lt;/code&gt; definitions to &lt;code&gt;~/.zprofile&lt;/code&gt;.&lt;/p&gt; &lt;h3 id=&#34;pro-tip-the-reload-trick&#34;&gt;💡 Pro Tip: The &#34;Reload&#34; Trick&lt;/h3&gt; &lt;p&gt;If you make changes to &lt;code&gt;~/.zprofile&lt;/code&gt;, they won&#39;t apply to your current shell immediately because &lt;code&gt;.zprofile&lt;/code&gt; is only read at login. You have two options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Close the tab and open a new one (easiest).&lt;/li&gt; &lt;li&gt;Manually source it: &lt;code&gt;source ~/.zprofile&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For &lt;code&gt;.zshrc&lt;/code&gt; changes, you can always just run:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;~/.zshrc &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;hr /&gt; &lt;h2 id=&#34;a-robust-configuration-strategy&#34;&gt;A Robust Configuration Strategy 🛠️&lt;/h2&gt; &lt;p&gt;If you use multiple machines (e.g., macOS at work, Linux at home), you might want a setup that handles both gracefully.&lt;/p&gt; &lt;p&gt;Since Linux terminals often start as &lt;em&gt;non-login&lt;/em&gt; shells, they might skip &lt;code&gt;~/.zprofile&lt;/code&gt;. A common pattern to support both is to source &lt;code&gt;.zprofile&lt;/code&gt; from &lt;code&gt;.zshrc&lt;/code&gt; if it hasn&#39;t been loaded.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In your &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;c1&#34;&gt;# ~/.zshrc&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# If we are on Linux/Non-login shell, ensure environment is set&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-o&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;interactive&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;!&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-o&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;login&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]]&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;then&lt;/span&gt; &lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;-f&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;~/.zprofile&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;~/.zprofile &lt;span class=&#34;k&#34;&gt;fi&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# ... rest of your interactive config&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&#34;text-align: left;&#34;&gt;File&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Purpose&lt;/th&gt; &lt;th style=&#34;text-align: left;&#34;&gt;Examples&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;&lt;code&gt;~/.zshenv&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Critical Env Vars&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;ZDOTDIR&lt;/code&gt; (Advanced users only)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;&lt;code&gt;~/.zprofile&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Environment Setup&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;PATH&lt;/code&gt;, &lt;code&gt;EDITOR&lt;/code&gt;, &lt;code&gt;eval &#34;$(pyenv init -)&#34;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;strong&gt;Interactive Config&lt;/strong&gt;&lt;/td&gt; &lt;td style=&#34;text-align: left;&#34;&gt;&lt;code&gt;alias&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;bindkey&lt;/code&gt;, &lt;code&gt;compinit&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Keep your &lt;strong&gt;environment&lt;/strong&gt; in &lt;code&gt;.zprofile&lt;/code&gt; and your &lt;strong&gt;experience&lt;/strong&gt; in &lt;code&gt;.zshrc&lt;/code&gt;, and you&#39;ll have a fast, clean, and reliable terminal experience.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/</link> <pubDate>Wed, 07 May 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/</guid> </item> <item> <title>MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</title> <author>Viacheslav Dubrov</author> <category>infrastructure</category> <category>llmops</category> <category>mlops</category> <description>&lt;h1 id=&#34;mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond&#34;&gt;MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond&lt;/h1&gt; &lt;p&gt;The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models. From giant language models like GPT-4 to image diffusion models like Stable Diffusion, these powerful models have fundamentally changed how we build and operate ML systems.&lt;/p&gt; &lt;p&gt;In this post, I&#39;ll explore how ML infrastructure and MLOps practices have evolved to support foundation models. We&#39;ll contrast the &#34;classic&#34; era of MLOps with modern paradigms, examine what&#39;s changed, and look at the new patterns and workflows that have emerged. Think of it as upgrading from a standard toolbox to a fully automated factory—the principles are similar, but the scale and complexity are on a different level.&lt;/p&gt; &lt;!-- more --&gt; &lt;h2 id=&#34;1-the-mlops-landscape-a-few-years-ago-pre-foundation-model-era&#34;&gt;1. The MLOps Landscape a Few Years Ago (Pre-Foundation Model Era)&lt;/h2&gt; &lt;p&gt;A few years back, MLOps primarily meant applying DevOps principles to machine learning. The goal was simple: automate the model lifecycle from data preparation to deployment and monitoring.&lt;/p&gt; &lt;p&gt;Back then, ML systems were built around relatively smaller models, often trained from scratch on domain-specific data. Here&#39;s what the &#34;classic&#34; MLOps era looked like:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Classic MLOps Pipeline&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/classic_mlops_pipeline.svg&#34; /&gt;&lt;/p&gt; &lt;h3 id=&#34;11-end-to-end-pipelines&#34;&gt;1.1. End-to-End Pipelines&lt;/h3&gt; &lt;p&gt;Teams built end-to-end pipelines for data extraction, training, validation, and deployment. Apache Airflow orchestrated ETL and training workflows, while CI/CD systems ran automated tests and pushed models to production. The focus was on reproducibility and automation: package models in Docker containers, deploy them as REST microservices or batch jobs, and keep everything running smoothly.&lt;/p&gt; &lt;h3 id=&#34;12-experiment-tracking-and-model-versioning&#34;&gt;1.2. Experiment Tracking and Model Versioning&lt;/h3&gt; &lt;p&gt;Managing experiments and versions was critical. Platforms like MLflow and Weights &amp;amp; Biases (W&amp;amp;B) became popular for logging training runs, hyperparameters, and metrics. Data scientists could compare experiments and reliably reproduce results. Models were registered in model registries with version numbers, making rollbacks straightforward when a new model underperformed.&lt;/p&gt; &lt;h3 id=&#34;13-continuous-training-cicd&#34;&gt;1.3. Continuous Training &amp;amp; CI/CD&lt;/h3&gt; &lt;p&gt;Classic MLOps pipelines emphasized continuous integration of new data and models. A typical pipeline might retrain a model nightly or weekly as new data arrived, run a battery of tests, and if tests passed, deploy the new model automatically. Automation tools like Jenkins and GitLab CI/CD ensured that any change in data or code would trigger the pipeline reliably.&lt;/p&gt; &lt;h3 id=&#34;14-infrastructure-and-serving&#34;&gt;1.4. Infrastructure and Serving&lt;/h3&gt; &lt;p&gt;Serving a model in production meant a relatively small footprint—perhaps a few CPU cores or a single GPU for real-time inference. Kubernetes and Docker became the standard for deploying scalable inference services. Monitoring focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance metrics&lt;/strong&gt;: latency, throughput, memory usage&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model metrics&lt;/strong&gt;: prediction accuracy, concept drift detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System health&lt;/strong&gt;: uptime, error rates&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;15-feature-stores-and-data-management&#34;&gt;1.5. Feature Stores and Data Management&lt;/h3&gt; &lt;p&gt;For many ML applications (especially in finance or e-commerce), engineered features were as important as models. Feature stores provided a central place to manage features, ensuring consistency between training and serving. The emphasis was on structured data pipelines and feature engineering. Unstructured data like text and images required custom handling outside these stores.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;In summary&lt;/strong&gt;: Classic MLOps revolved around small-to-medium models and explicit feature engineering. The tooling was designed for managing many experiments and deployments—scaling out a large number of models for different tasks rather than scaling one enormous model. This paradigm worked well until models started growing dramatically in size and capability.&lt;/p&gt; &lt;h2 id=&#34;2-the-paradigm-shift-rise-of-large-scale-foundation-models&#34;&gt;2. The Paradigm Shift: Rise of Large-Scale Foundation Models&lt;/h2&gt; &lt;p&gt;Around 2018-2020, everything changed. Researchers began introducing foundation models—extremely large models pretrained on vast corpora, capable of being adapted to many tasks.&lt;/p&gt; &lt;p&gt;The progression was rapid:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;2018-2019&lt;/strong&gt;: BERT and GPT-2 showed the power of transfer learning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2020-2021&lt;/strong&gt;: GPT-3 and PaLM demonstrated what massive scale could achieve&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2021-2023&lt;/strong&gt;: Image models like DALL-E and Stable Diffusion brought generative AI to the mainstream&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2023-2024&lt;/strong&gt;: Foundation models became ubiquitous—available everywhere from Hugging Face to AWS Bedrock&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As one practitioner noted in early 2024: &#34;Foundational models are everywhere now—a stark change from just two years ago.&#34;&lt;/p&gt; &lt;p&gt;This shift created a fundamentally different paradigm. If classic models were like specialized kitchen gadgets (a toaster, a blender), foundation models are like a professional chef who can learn to cook anything with a little instruction.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Classic MLOps vs LLMOps&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/mlops_vs_llmops.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Here&#39;s how foundation models changed ML infrastructure:&lt;/p&gt; &lt;h3 id=&#34;21-pretrained-beats-from-scratch&#34;&gt;2.1. Pretrained Beats From Scratch&lt;/h3&gt; &lt;p&gt;Instead of training models from scratch, teams started with powerful pretrained models and fine-tuned them for specific tasks. This approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cuts training time&lt;/strong&gt; from weeks to hours or days&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reduces data requirements&lt;/strong&gt; from millions to thousands of examples&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enables smaller teams&lt;/strong&gt; to build sophisticated AI applications&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The largest models (with billions of parameters) are often used as-is via APIs or fine-tuned minimally. By 2024, the ML engineer&#39;s skillset shifted from &#34;how to build models&#34; to &#34;how to leverage and integrate foundation models&#34;—treating the model as a service rather than reinventing the wheel.&lt;/p&gt; &lt;h3 id=&#34;22-model-size-and-computational-demands&#34;&gt;2.2. Model Size and Computational Demands&lt;/h3&gt; &lt;p&gt;The sheer scale of these models introduced new challenges. A model with 175 billion parameters cannot be handled with the same infrastructure as one with 50 million parameters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key scaling challenges&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Requires powerful hardware (GPUs, TPUs) and distributed computing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model parallelism&lt;/strong&gt;: Sharding a single model across multiple GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data parallelism&lt;/strong&gt;: Synchronizing multiple GPU workers during training&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: Often requires multiple GPUs or specialized runtimes to keep latency acceptable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Libraries like DeepSpeed and ZeRO (Zero Redundancy Optimizer) were developed specifically to make training giant models feasible. The infrastructure requirements jumped by orders of magnitude.&lt;/p&gt; &lt;h3 id=&#34;23-emergence-of-llmops&#34;&gt;2.3. Emergence of LLMOps&lt;/h3&gt; &lt;p&gt;It became clear that operating these large models in production required extensions to classic MLOps. This led to &lt;strong&gt;LLMOps&lt;/strong&gt; (Large Language Model Operations)—essentially MLOps specialized for large models.&lt;/p&gt; &lt;p&gt;LLMOps builds on classic MLOps principles but addresses unique challenges:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Computational resources&lt;/strong&gt;: Managing expensive GPU clusters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt engineering&lt;/strong&gt;: Optimizing model behavior through input design&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety monitoring&lt;/strong&gt;: Detecting bias, harmful content, and data leakage&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance management&lt;/strong&gt;: Balancing latency, quality, and cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Issues that barely registered for smaller models—like producing biased text or leaking training data—became major considerations at LLM scale.&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Nested relationship of MLOps specialties - Machine Learning Ops (outermost), Generative AI Ops, LLM Ops, and Retrieval-Augmented Generation Ops (innermost)&#34; src=&#34;https://www.nvidia.com/content/nvidiaGDC/us/en_US/glossary/mlops/_jcr_content/root/responsivegrid/nv_container_1795650_1945302252/nv_image_2134560435_.coreimg.100.1290.jpeg/1741240029246/ai-ops-hierarchy.jpeg&#34; /&gt;&lt;/p&gt; &lt;p&gt;This diagram from NVIDIA illustrates how general MLOps (outer circle) has branched into specialized subfields like generative AI operations (for all generative models), LLMOps (for large language models), and even RAGOps for retrieval-augmented generation. The concentric circles indicate that these specializations build on the foundation of classic MLOps.&lt;/p&gt; &lt;h3 id=&#34;24-foundation-models-as-a-service&#34;&gt;2.4. Foundation Models as a Service&lt;/h3&gt; &lt;p&gt;Another major shift was the rise of &lt;strong&gt;models as a service&lt;/strong&gt;. Instead of deploying their own models, many applications now call external APIs:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;API Providers&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenAI, Cohere, AI21 Labs offer hosted LLMs&lt;/li&gt; &lt;li&gt;Google&#39;s Vertex AI provides Model Garden with pretrained models&lt;/li&gt; &lt;li&gt;AWS Bedrock hosts proprietary foundation models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model Hubs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face hosts thousands of pretrained models&lt;/li&gt; &lt;li&gt;Models can be downloaded or run in the cloud&lt;/li&gt; &lt;li&gt;Version control and community sharing became standard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This changed ML architecture fundamentally. Production pipelines might call external APIs for inference, introducing new considerations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: Network calls add overhead&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Pay-per-token pricing models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data privacy&lt;/strong&gt;: Sending data to third parties&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vendor lock-in&lt;/strong&gt;: Dependency on external services&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But it also saves the massive effort of managing model infrastructure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The paradigm shift&lt;/strong&gt;: From &#34;your data + your model code = trained model&#34; to &#34;your data + adaptation of a pretrained model = fine-tuned model (or just prompt it with your data).&#34;&lt;/p&gt; &lt;h2 id=&#34;3-new-requirements-and-capabilities-in-modern-ml-infrastructure&#34;&gt;3. New Requirements and Capabilities in Modern ML Infrastructure&lt;/h2&gt; &lt;p&gt;With foundation models at the center, today&#39;s ML infrastructure must support capabilities that were niche or non-existent just a few years ago. Here are the key new requirements:&lt;/p&gt; &lt;h3 id=&#34;31-distributed-training-and-model-parallelism&#34;&gt;3.1. Distributed Training and Model Parallelism&lt;/h3&gt; &lt;p&gt;Training a model with billions of parameters is beyond the capacity of a single machine. Modern ML infrastructure orchestrates distributed training across multiple nodes:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Distributed Training&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/distributed_training.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Two main approaches&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model parallelism&lt;/strong&gt;: Split the model&#39;s layers across multiple GPUs (each GPU handles part of the model)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data parallelism&lt;/strong&gt;: Replicate the model across GPUs and split the training data (synchronize gradients)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tools that enable this&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PyTorch Lightning, Horovod for general distributed training&lt;/li&gt; &lt;li&gt;NVIDIA&#39;s Megatron-LM for massive transformer models&lt;/li&gt; &lt;li&gt;Google&#39;s JAX/TPU ecosystem for TPU clusters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A few years ago, most teams trained models on a single server. Now, ML platforms must handle launching jobs on GPU clusters, managing faults, and aggregating gradients from dozens or hundreds of workers seamlessly.&lt;/p&gt; &lt;h3 id=&#34;32-efficient-fine-tuning-techniques&#34;&gt;3.2. Efficient Fine-Tuning Techniques&lt;/h3&gt; &lt;p&gt;Training from scratch is impractical for huge models, but even fine-tuning a multi-billion parameter model can be resource-intensive. This led to parameter-efficient fine-tuning methods:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Modern fine-tuning approaches&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LoRA (Low-Rank Adaptation)&lt;/strong&gt;: Updates only a small subset of parameters (adapters) instead of the entire network, dramatically reducing computational cost&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Tuning&lt;/strong&gt;: Optimizes only the prompt embeddings, keeping the model frozen&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adapter Modules&lt;/strong&gt;: Adds small trainable layers between frozen model layers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is a simple example of how you might configure LoRA using the &lt;code&gt;peft&lt;/code&gt; library:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;peft&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LoraConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_peft_model&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Configure LoRA&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;peft_config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LoraConfig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lora_alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_modules&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;q_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;v_proj&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lora_dropout&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.05&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task_type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;CAUSAL_LM&amp;quot;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Apply to base model&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# model = get_peft_model(base_model, peft_config)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# model.print_trainable_parameters()&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;ML infrastructure must now support complex workflows: load a base model from a hub, apply fine-tuned weight deltas, and deploy the combined model. Traditional training pipelines evolved significantly to accommodate this multi-step customization.&lt;/p&gt; &lt;h3 id=&#34;33-prompt-engineering-management&#34;&gt;3.3. Prompt Engineering &amp;amp; Management&lt;/h3&gt; &lt;p&gt;One surprising new artifact in modern ML pipelines is &lt;strong&gt;the prompt&lt;/strong&gt;. With LLMs, much of the model&#39;s behavior is controlled through the text prompt or input format you give it.&lt;/p&gt; &lt;p&gt;This created an entirely new discipline. Teams now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maintain &lt;strong&gt;prompt libraries&lt;/strong&gt; and templates&lt;/li&gt; &lt;li&gt;Use &lt;strong&gt;version control&lt;/strong&gt; for prompts (just like code)&lt;/li&gt; &lt;li&gt;Run &lt;strong&gt;A/B tests&lt;/strong&gt; to compare prompt variants&lt;/li&gt; &lt;li&gt;Store &lt;strong&gt;prompt versions&lt;/strong&gt; alongside model versions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is fundamentally different from classic ML, where inputs were just data features—not natural language instructions. Frameworks like LangChain now include prompt optimization as a first-class feature.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example prompt evolution&lt;/strong&gt;:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;v1: &amp;quot;Classify this text as positive or negative: {text}&amp;quot; v2: &amp;quot;You are a sentiment analyzer. Classify: {text}&amp;quot; v3: &amp;quot;Analyze sentiment. Return only &amp;#39;positive&amp;#39; or &amp;#39;negative&amp;#39;: {text}&amp;quot; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;Each version can produce different results, so tracking and testing prompts became as important as tracking model weights.&lt;/p&gt; &lt;h3 id=&#34;34-retrieval-augmented-generation-rag&#34;&gt;3.4. Retrieval-Augmented Generation (RAG)&lt;/h3&gt; &lt;p&gt;Foundation models have a fixed knowledge cutoff and limited context windows. To keep responses accurate and up-to-date, &lt;strong&gt;Retrieval-Augmented Generation (RAG)&lt;/strong&gt; has become a best practice.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How RAG works&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;RAG Workflow&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/rag_workflow.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;Instead of continuously retraining the model on new data (costly and slow), RAG fetches information at query time. The retrieved documents are appended to the prompt as additional context.&lt;/p&gt; &lt;p&gt;Here&#39;s a simplified view of how this looks in code using LangChain:&lt;/p&gt; &lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;langchain.vectorstores&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pinecone&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;langchain.llms&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nn&#34;&gt;langchain.chains&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RetrievalQA&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 1. Load the vector database&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# vector_db = Pinecone.from_existing_index(&amp;quot;my-index&amp;quot;, embeddings)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 2. Initialize the LLM&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# llm = OpenAI(temperature=0)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 3. Create the RAG chain&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# qa_chain = RetrievalQA.from_chain_type(&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# llm=llm,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# chain_type=&amp;quot;stuff&amp;quot;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# retriever=vector_db.as_retriever()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# )&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 4. Ask a question&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# response = qa_chain.run(&amp;quot;How does LLMOps differ from MLOps?&amp;quot;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;New infrastructure components&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vector databases&lt;/strong&gt; (Pinecone, Weaviate, FAISS, Milvus) for fast similarity search on embeddings&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding models&lt;/strong&gt; to convert documents into vectors&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Index management&lt;/strong&gt; to keep embeddings in sync with the latest data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In many ways, vector databases have replaced traditional feature stores. Unstructured data and semantic search took center stage over manual feature engineering.&lt;/p&gt; &lt;h3 id=&#34;35-data-streaming-and-real-time-data-feeds&#34;&gt;3.5. Data Streaming and Real-Time Data Feeds&lt;/h3&gt; &lt;p&gt;Modern applications—especially LLM-powered assistants—continuously ingest data: chat conversations, sensor data, event streams. This data needs to update the model&#39;s knowledge (via RAG) or trigger responses in real-time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The shift&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Classic MLOps&lt;/strong&gt;: Batch processing (daily/weekly training jobs)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modern LLMOps&lt;/strong&gt;: Real-time streaming data pipelines&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technologies driving this&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kafka&lt;/strong&gt; and event streaming platforms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time databases&lt;/strong&gt; (Redis, DynamoDB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online feature stores&lt;/strong&gt; with continuous updates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming embeddings&lt;/strong&gt; that update vector indexes in real-time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The boundary between data engineering and MLOps has blurred. Data pipelines now directly feed model inference rather than just training.&lt;/p&gt; &lt;h3 id=&#34;36-scalable-and-specialized-serving-infrastructure&#34;&gt;3.6. Scalable and Specialized Serving Infrastructure&lt;/h3&gt; &lt;p&gt;Serving a massive model is challenging. Modern ML infrastructure must support three key capabilities:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;High-Throughput, Low-Latency Serving&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Interactive applications (chatbots, image generators) demand fast responses. This requires:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU/TPU acceleration&lt;/strong&gt; for quick inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model quantization&lt;/strong&gt; to reduce precision and speed up serving&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU batching&lt;/strong&gt; to serve multiple requests in parallel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimized serving engines&lt;/strong&gt; like NVIDIA&#39;s TensorRT, Triton Inference Server, or DeepSpeed-Inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Serverless and Elastic Scaling&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new trend toward serverless ML has emerged. Platforms like Modal offer &#34;AWS Lambda but with GPU support&#34;—you provide code, they handle infrastructure and scaling.&lt;/p&gt; &lt;p&gt;Benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No always-running servers&lt;/li&gt; &lt;li&gt;Compute spins up on-demand&lt;/li&gt; &lt;li&gt;Scale to zero when idle (pay only per execution)&lt;/li&gt; &lt;li&gt;Automatic scaling under load&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tradeoffs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cold-start latency when spinning up&lt;/li&gt; &lt;li&gt;Managing statelessness&lt;/li&gt; &lt;li&gt;Less control over infrastructure&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This works well for irregular workloads where managing GPU clusters is overkill.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Distributed Model Serving&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For models too large for a single GPU, inference itself can be distributed. The model is sharded across multiple machines, each handling part of the forward pass.&lt;/p&gt; &lt;p&gt;Example: Serving a 175B parameter model on-premises requires multiple GPUs working together. Modern ML infrastructure must launch distributed inference replicas and route requests appropriately.&lt;/p&gt; &lt;h3 id=&#34;37-monitoring-observability-and-guardrails&#34;&gt;3.7. Monitoring, Observability, and Guardrails&lt;/h3&gt; &lt;p&gt;With great power comes great responsibility. Large models can generate incorrect or inappropriate outputs in ways small models never did. Modern ML systems need three layers of monitoring:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance and Reliability&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The basics still matter:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Latency, throughput, memory usage&lt;/li&gt; &lt;li&gt;GPU utilization and costs&lt;/li&gt; &lt;li&gt;Autoscaling policies (scale up under load, fall back to smaller models if needed)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Output Quality and Safety&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We now monitor the &lt;em&gt;content&lt;/em&gt; of outputs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Content filtering&lt;/strong&gt;: Detect hate speech, PII, harmful content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Moderation APIs&lt;/strong&gt;: Use OpenAI&#39;s moderation API or custom filters&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bias detection&lt;/strong&gt;: Continuously evaluate for biased responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: Intercept adversarial inputs and ensure outputs stay within bounds&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These &#34;guardrails&#34; have become essential in LLMOps—they&#39;re not optional.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feedback Loops&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Continuous improvement now includes human feedback:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Collect user interactions (likes, corrections, ratings)&lt;/li&gt; &lt;li&gt;Use feedback to fine-tune models or adjust prompts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RLHF (Reinforcement Learning from Human Feedback)&lt;/strong&gt;: Explicitly use human ratings to refine behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The infrastructure must support collecting and managing this feedback data securely.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;In summary&lt;/strong&gt;: Today&#39;s ML infrastructure manages entire ecosystems—base models, fine-tuning adapters, prompt templates, retrieval indexes, monitoring detectors, and more. The complexity is higher, but so is the capability.&lt;/p&gt; &lt;h2 id=&#34;4-evolving-system-architecture-and-design-patterns&#34;&gt;4. Evolving System Architecture and Design Patterns&lt;/h2&gt; &lt;p&gt;Given these new requirements, how are ML systems actually structured today? Here are the key design patterns that have emerged:&lt;/p&gt; &lt;h3 id=&#34;41-modular-pipelines-orchestration&#34;&gt;4.1. Modular Pipelines &amp;amp; Orchestration&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Classic tools&lt;/strong&gt; (Kubeflow Pipelines, Apache Airflow) are still used for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fine-tuning workflows&lt;/li&gt; &lt;li&gt;Batch scoring jobs&lt;/li&gt; &lt;li&gt;Periodic model retraining&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;New tools&lt;/strong&gt; have emerged for modern needs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metaflow, Flyte, ZenML&lt;/strong&gt;: Pythonic workflows that integrate seamlessly with ML libraries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight orchestration&lt;/strong&gt;: For low-latency inference, application code often replaces heavyweight workflow engines&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key difference: engineers no longer need to leave their development environment to manage the flow from data to deployment.&lt;/p&gt; &lt;h3 id=&#34;42-model-hubs-and-registries&#34;&gt;4.2. Model Hubs and Registries&lt;/h3&gt; &lt;p&gt;Model management evolved with centralized hubs:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;External hubs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face Hub&lt;/strong&gt;: Thousands of models, datasets, and scripts&lt;/li&gt; &lt;li&gt;One-stop shop for ML components&lt;/li&gt; &lt;li&gt;Plug-and-play architecture (fetch models at startup)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Internal registries&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MLflow Registry, SageMaker Model Registry for bespoke models&lt;/li&gt; &lt;li&gt;Combined with external foundation models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The shift&lt;/strong&gt;: Instead of building everything in-house, engineers now plan for how to fine-tune and adapt third-party models. This has accelerated development dramatically.&lt;/p&gt; &lt;h3 id=&#34;43-feature-stores-vs-vector-databases&#34;&gt;4.3. Feature Stores vs. Vector Databases&lt;/h3&gt; &lt;p&gt;The data layer has fundamentally changed:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Feature Store vs Vector DB&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/feature_store_vs_vector_db.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Traditional feature stores&lt;/strong&gt; handled structured data with manual feature engineering.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Modern vector databases&lt;/strong&gt; (Pinecone, Weaviate, Chroma, Milvus) handle:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High-dimensional embeddings&lt;/li&gt; &lt;li&gt;Fast similarity search&lt;/li&gt; &lt;li&gt;Semantic search and deduplication&lt;/li&gt; &lt;li&gt;RAG for LLMs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You&#39;ll often see both: a vector DB for unstructured semantic lookup and a data warehouse for structured analytics.&lt;/p&gt; &lt;h3 id=&#34;44-unified-platforms-end-to-end&#34;&gt;4.4. Unified Platforms (End-to-End)&lt;/h3&gt; &lt;p&gt;The complexity of modern ML has driven adoption of end-to-end platforms that abstract infrastructure details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cloud platforms&lt;/strong&gt; evolved to support foundation models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Google Vertex AI&lt;/strong&gt;: Auto-distributed training on TPU pods, Model Garden with LLMs, one-click deployment&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AWS SageMaker&lt;/strong&gt;: Distributed training, model parallelism, and Bedrock for hosted foundation models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Azure Machine Learning&lt;/strong&gt;: Integrated training, deployment, and monitoring&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These platforms provide managed services like &#34;fine-tune this 20B parameter model on your data&#34; or &#34;embed and index your text data for retrieval.&#34;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open-source and startups&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MosaicML&lt;/strong&gt; (now Databricks): Efficient training and deployment for large models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Argilla, Label Studio&lt;/strong&gt;: Data labeling and prompt dataset creation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ClearML, MLflow&lt;/strong&gt;: Experiment tracking tied to pipeline execution&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=&#34;45-inference-gateways-and-apis&#34;&gt;4.5. Inference Gateways and APIs&lt;/h3&gt; &lt;p&gt;The proliferation of model sizes led to &lt;strong&gt;inference gateways&lt;/strong&gt;—routers that intelligently direct requests:&lt;/p&gt; &lt;p&gt;&lt;img alt=&#34;Inference Gateway&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/inference_gateway.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Route based on latency requirements&lt;/li&gt; &lt;li&gt;Different models for different subscription tiers&lt;/li&gt; &lt;li&gt;A/B testing new models on a fraction of traffic&lt;/li&gt; &lt;li&gt;Fallback to smaller models under high load&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This decouples the client-facing API from model implementation, allowing seamless model swaps and testing.&lt;/p&gt; &lt;h3 id=&#34;46-agentic-systems&#34;&gt;4.6. Agentic Systems&lt;/h3&gt; &lt;p&gt;A cutting-edge pattern: &lt;strong&gt;AI agents&lt;/strong&gt; that dynamically choose sequences of actions to accomplish tasks.&lt;/p&gt; &lt;p&gt;Unlike static chains, agents can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Call external tools (calculators, search engines, databases)&lt;/li&gt; &lt;li&gt;Decide workflows at runtime based on context&lt;/li&gt; &lt;li&gt;Invoke different models for different subtasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Enabling frameworks&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LangChain&#39;s agent mode&lt;/li&gt; &lt;li&gt;OpenAI&#39;s function calling&lt;/li&gt; &lt;li&gt;AutoGPT and similar systems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This emerging pattern requires new operational practices (sometimes called &#34;AgentOps&#34;):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Robust monitoring to prevent unwanted actions&lt;/li&gt; &lt;li&gt;Detailed logging to trace decision paths&lt;/li&gt; &lt;li&gt;Safety guardrails to limit agent capabilities&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;img alt=&#34;Agentic Workflow&#34; src=&#34;../../../../assets/2025-05-06-from-mlops-to-llmops/agentic_workflow.svg&#34; /&gt;&lt;/p&gt; &lt;p&gt;While not yet widespread in production, agentic systems represent the frontier of LLMOps.&lt;/p&gt; &lt;h2 id=&#34;5-getting-started-with-llmops&#34;&gt;5. Getting Started with LLMOps&lt;/h2&gt; &lt;p&gt;If you are new to this field, the ecosystem can feel overwhelming. Here is a recommended path to get your hands dirty:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Play with APIs&lt;/strong&gt;: Start by using OpenAI or Anthropic APIs to understand prompt engineering.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Build a RAG App&lt;/strong&gt;: Use &lt;strong&gt;LangChain&lt;/strong&gt; or &lt;strong&gt;LlamaIndex&lt;/strong&gt; to build a simple &#34;Chat with your PDF&#34; app. This introduces you to vector databases and retrieval.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Try Fine-Tuning&lt;/strong&gt;: Use &lt;strong&gt;Hugging Face&lt;/strong&gt; to fine-tune a small model (like Llama-3-8B or Mistral-7B) on a custom dataset using Google Colab.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deploy&lt;/strong&gt;: Try deploying your fine-tuned model using &lt;strong&gt;vLLM&lt;/strong&gt; or &lt;strong&gt;Ollama&lt;/strong&gt; locally, then move to a cloud provider.&lt;/li&gt; &lt;/ol&gt; &lt;h2 id=&#34;6-conclusion-from-mlops-to-llmops-and-beyond&#34;&gt;6. Conclusion: From MLOps to LLMOps and Beyond&lt;/h2&gt; &lt;p&gt;In just a few years, we&#39;ve witnessed a transformation in how we approach machine learning in production.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What remains the same&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automation, reproducibility, collaboration&lt;/li&gt; &lt;li&gt;Focus on reliability and efficiency&lt;/li&gt; &lt;li&gt;DevOps principles applied to ML&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What changed dramatically&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scale: From millions to billions of parameters&lt;/li&gt; &lt;li&gt;Approach: From training from scratch to adapting foundation models&lt;/li&gt; &lt;li&gt;Infrastructure: From single servers to distributed GPU clusters&lt;/li&gt; &lt;li&gt;Data layer: From feature stores to vector databases&lt;/li&gt; &lt;li&gt;Monitoring: From performance metrics to content safety guardrails&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This gave rise to &lt;strong&gt;LLMOps&lt;/strong&gt;—a specialization of MLOps for managing the lifecycle of large models. It&#39;s not just hype. The differences are tangible in day-to-day workflows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How we fine-tune models (LoRA, adapters)&lt;/li&gt; &lt;li&gt;How we deploy them (distributed serving, serverless GPUs)&lt;/li&gt; &lt;li&gt;How we monitor them (content filtering, bias detection)&lt;/li&gt; &lt;li&gt;What infrastructure we need (vector databases, GPU clusters)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The evolution continues&lt;/strong&gt;. As models grow and AI systems become more autonomous, we&#39;re already seeing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AgentOps&lt;/strong&gt; for managing AI agents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAGOps&lt;/strong&gt; for retrieval-augmented systems&lt;/li&gt; &lt;li&gt;Even more specialized operational practices&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But the end goal remains: &lt;strong&gt;reliably deliver the benefits of machine learning to end-users and business applications, at scale and with trustworthiness.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Teams that successfully navigate this evolution harness foundation models to build products faster than ever—while maintaining the reliability and efficiency that good operations provide.&lt;/p&gt; &lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt; &lt;p&gt;The insights and examples in this post are supported by recent research and industry sources, including an MDPI review on transitioning from MLOps to LLMOps, NVIDIA&#39;s technical blogs on GenAIOps and LLMOps, and various practitioner articles and discussions capturing the state of ML in 2024. Platforms like Modal and Ray have published guides showing new deployment patterns (serverless GPUs, distributed serving) in action.&lt;/p&gt;</description> <link>https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/</link> <pubDate>Tue, 06 May 2025 00:00:00 +0000</pubDate> <source url="https://slavadubrov.github.io/feed_rss_updated.xml">Edge of Context: Tips and Tricks in Machine Learning</source><guid isPermaLink="true">https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/</guid> </item> </channel> </rss>