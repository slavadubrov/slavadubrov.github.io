{"version": "https://jsonfeed.org/version/1", "title": "Edge of Context: Tips and Tricks in Machine Learning", "home_page_url": "https://slavadubrov.github.io/", "feed_url": "https://slavadubrov.github.io/feed_json_updated.json", "description": "Practical notes on AI Engineering", "icon": "https://slavadubrov.github.io/assets/logo-eoc.svg", "authors": [], "language": "en", "items": [{"id": "https://slavadubrov.github.io/blog/2026/02/14/the-cortex--architecting-memory-for-ai-agents/", "url": "https://slavadubrov.github.io/blog/2026/02/14/the-cortex--architecting-memory-for-ai-agents/", "title": "The Cortex \u2014 Architecting Memory for AI Agents", "content_html": "<h1 id=\"the-cortex-architecting-memory-for-ai-agents\">The Cortex \u2014 Architecting Memory for AI Agents</h1>\n<p><strong>Part 2 of the Engineering the Agentic Stack series</strong></p>\n<p>State is what separates a chatbot from an agent. Without memory, every interaction starts from zero \u2014 the agent cannot pause and resume, cannot learn from past sessions, cannot personalize. In <a href=\"/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/\">Part 1</a>, I covered the cognitive engine that decides <em>how</em> an agent thinks. This post tackles the infrastructure that determines <em>what</em> it remembers.</p>\n<p>I'll walk through the memory architecture of the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a>, showing how hot and cold memory layers work together to support checkpointing, pause/resume workflows, and cross-session learning \u2014 and why a third tier of document-based memory is becoming essential for agents that manage their own knowledge.</p>\n<!-- more -->\n\n<p><strong>TL;DR</strong>: Agent memory splits into three tiers \u2014 <strong>hot memory</strong> (thread-level checkpoints in Redis or PostgreSQL for pause/resume), <strong>cold memory</strong> (cross-session knowledge in a vector store or key-value backend for personalization), and <strong>document memory</strong> (human-readable files the agent reads and writes for persistent project knowledge). LangGraph's checkpoint system handles the hot layer natively; for cold memory, vector search with Qdrant gives you semantic recall while simpler key-value stores work for structured facts; for document memory, a file-based store gives you transparency, debuggability, and zero embedding infrastructure. The right combination depends on your latency, durability, and query complexity requirements.</p>\n<hr />\n<h2 id=\"why-memory-matters\">Why Memory Matters</h2>\n<p>A stateless agent is a sophisticated autocomplete. It processes a request, returns a response, and forgets everything. This works for single-turn Q&amp;A. It breaks the moment you need any of the following:</p>\n<ul>\n<li><strong>Pause and resume</strong>: The user starts a research task, closes their laptop, and comes back tomorrow. Without checkpointed state, the agent must restart from scratch.</li>\n<li><strong>Multi-turn coherence</strong>: Over a long conversation, the agent must remember what tools it already called, what data it gathered, and what plan steps it completed.</li>\n<li><strong>Personalization</strong>: A returning user expects the agent to know their risk tolerance, preferred analysis depth, and past interactions.</li>\n<li><strong>Human-in-the-loop (HITL)</strong>: The agent generates a draft report and waits for human approval. That \"waiting\" state must survive process restarts.</li>\n</ul>\n<p>Consider the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a> from <a href=\"/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/\">Part 1</a>. Without memory, a user asks \"Analyze NVDA\" \u2014 the agent builds a plan, calls five tools, gathers data, and generates a draft report. The user says \"looks good, but add competitor analysis.\" Without checkpointed state, the agent has no idea what \"looks good\" refers to. It has to start from scratch. With a checkpoint store, the agent loads the exact state from the last node \u2014 including the gathered research data \u2014 and simply adds a competitor analysis step.</p>\n<p>Now imagine the same user returns a week later and asks \"Update my NVDA analysis.\" Without long-term memory, the agent doesn't know this user prefers conservative risk assessments or that they're interested in semiconductor stocks. With a vector-backed memory store, the agent recalls these facts and personalizes the analysis without asking.</p>\n<p>LangGraph's architecture makes the distinction clear. Every graph execution runs within a <strong>thread</strong> \u2014 a single conversation or task. State within a thread is <strong>working memory</strong>. State that persists across threads is <strong>long-term memory</strong>. The engineering challenge is choosing the right storage backend for each.</p>\n<p><img alt=\"Memory Taxonomy\" src=\"../../../../assets/2026-02-14-agent-memory-architecture/memory-taxonomy.svg\" /></p>\n<hr />\n<h2 id=\"a-taxonomy-of-agent-memory\">A Taxonomy of Agent Memory</h2>\n<p>Before diving into implementation, it helps to classify what agents need to remember. The <a href=\"https://arxiv.org/abs/2309.02427\">CoALA framework</a> (Sumers, Yao et al., 2023) provides the foundational taxonomy, drawing on cognitive science. I introduced memory scoping in my <a href=\"/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/\">context engineering post</a> \u2014 here I expand it into six categories:</p>\n<table>\n<thead>\n<tr>\n<th>Memory Type</th>\n<th>Scope</th>\n<th>Lifetime</th>\n<th>Example</th>\n<th>Storage Pattern</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Working</strong></td>\n<td>Current step</td>\n<td>Milliseconds</td>\n<td>Tool call arguments, current LLM response</td>\n<td>In-process (Python dict)</td>\n</tr>\n<tr>\n<td><strong>Short-term</strong></td>\n<td>Current thread</td>\n<td>Minutes\u2013hours</td>\n<td>Conversation history, plan progress, gathered data</td>\n<td>Checkpoint store</td>\n</tr>\n<tr>\n<td><strong>Episodic</strong></td>\n<td>Cross-thread</td>\n<td>Days\u2013months</td>\n<td>\"Last week the user asked about NVDA earnings\"</td>\n<td>Vector store / KV store</td>\n</tr>\n<tr>\n<td><strong>Semantic</strong></td>\n<td>Cross-thread</td>\n<td>Months\u2013permanent</td>\n<td>\"User prefers conservative investments\"</td>\n<td>Vector store / KV store</td>\n</tr>\n<tr>\n<td><strong>Document</strong></td>\n<td>Cross-thread</td>\n<td>Days\u2013permanent</td>\n<td>Project notes, research summaries, learned patterns</td>\n<td>File store (Markdown/JSON)</td>\n</tr>\n<tr>\n<td><strong>Procedural</strong></td>\n<td>System-wide</td>\n<td>Permanent</td>\n<td>\"When analyzing stocks, always check SEC filings\"</td>\n<td>Config / system prompt</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Working memory</strong> is what the LLM is actively reasoning with right now \u2014 Python variables in the current function, the contents of the context window, tool call arguments mid-execution. It's the fastest and most ephemeral layer: nothing persists beyond the current step. Working memory is bounded by the model's context window size, which makes it the critical bottleneck \u2014 everything the agent \"knows\" at decision time must fit here, whether it came from the checkpoint store, a vector query, or a file read. The other memory tiers exist to feed the right information into working memory at the right time.</p>\n<p><strong>Short-term memory</strong> is the checkpoint store that LangGraph writes after every node execution. <strong>Episodic</strong> and <strong>semantic</strong> memory are long-term stores that persist across threads. <strong>Document memory</strong> stores structured knowledge the agent accumulates over time \u2014 project notes, research summaries, learned conventions \u2014 in human-readable files that both the agent and the user can inspect and edit. <strong>Procedural memory</strong> is encoded in the system prompt and tool definitions \u2014 it doesn't change per user.</p>\n<p>The practical split is three tiers: <strong>hot memory</strong> (working + short-term) handles the current session, <strong>cold memory</strong> (episodic + semantic) handles cross-session recall, and <strong>document memory</strong> handles accumulated project knowledge that benefits from human readability and direct editing.</p>\n<p>Most existing frameworks and surveys focus on the hot/cold split without theoretical treatment of document memory, despite its widespread adoption. The CoALA framework classifies memory into working, episodic, semantic, and procedural \u2014 no mention of file-based storage. The <a href=\"https://arxiv.org/abs/2512.13564\">Memory in the Age of AI Agents survey</a> covers vector stores and knowledge graphs but not document files. LangGraph's documentation covers checkpoints and the Store interface but has no native concept of file-based memory. Yet in practice, document memory has become the dominant pattern in AI coding assistants \u2014 Claude Code, Cursor, Windsurf, and Devin all use file-based memory as a core feature. The pattern is expanding beyond coding: open-world game agents store reusable skills as code libraries (<a href=\"https://arxiv.org/abs/2305.16291\">Voyager</a>), competition-winning enterprise agents iterate on procedural prompt documents across runs (<a href=\"/blog/2026/01/11/enterprise-rag-challenge-3-winning-approaches-for-autonomous-ai-agents/\">ECR3 winning approaches</a>), and web automation agents synthesize reusable workflow APIs from successful episodes (<a href=\"https://arxiv.org/abs/2409.07429\">Agent Workflow Memory</a>). The underlying advantages \u2014 debuggability, version control, and zero infrastructure \u2014 are not coding-specific, and adoption is broadening.</p>\n<p>A key insight from the same survey: agent memory is <em>not</em> the same as RAG or context engineering. The distinguishing feature is that the agent itself performs autonomous read/write operations on its memory \u2014 it decides what to remember and what to forget, rather than relying on a fixed retrieval pipeline.</p>\n<p>The <a href=\"https://arxiv.org/abs/2304.03442\">Generative Agents paper</a> (Park et al., 2023) demonstrated the power of this approach: simulated agents that stored, reflected on, and retrieved their own memories produced remarkably human-like behaviors. The architectural pattern \u2014 a <strong>memory stream</strong> with retrieval scored by recency, importance, and relevance \u2014 has become the blueprint for modern agent memory systems.</p>\n<hr />\n<h2 id=\"short-term-hot-memory-the-checkpoint-store\">Short-Term \"Hot\" Memory: The Checkpoint Store</h2>\n<p>Every time a LangGraph node executes, the framework serializes the full graph state and writes it to a <strong>checkpoint store</strong>. This is the foundation for pause/resume, time-travel debugging, and HITL workflows.</p>\n<p><img alt=\"Hot Memory Checkpoint Flow\" src=\"../../../../assets/2026-02-14-agent-memory-architecture/hot-memory-checkpoint.svg\" /></p>\n<p>The checkpoint contains everything: the <code>AgentState</code> from <a href=\"/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/\">Part 1</a> (messages, plan steps, research data, execution mode), plus LangGraph metadata like the node that produced it and a monotonically increasing sequence number. When the graph resumes \u2014 whether after a human-in-the-loop interrupt or a process restart \u2014 it loads the latest checkpoint and continues from exactly where it left off.</p>\n<h3 id=\"how-langgraph-checkpointing-works\">How LangGraph Checkpointing Works</h3>\n<p>LangGraph's <code>BaseCheckpointSaver</code> defines a simple interface: <code>put()</code> writes a checkpoint, <code>get_tuple()</code> reads the latest one for a thread, and <code>list()</code> returns the checkpoint history. Every checkpoint is keyed by <code>(thread_id, checkpoint_ns, checkpoint_id)</code>, where <code>thread_id</code> identifies the conversation, <code>checkpoint_ns</code> handles subgraph namespacing, and <code>checkpoint_id</code> is a unique version.</p>\n<p>The critical design choice is <strong>which backend</strong> to use for this store. LangGraph ships with two production-ready options: PostgreSQL and Redis.</p>\n<h3 id=\"postgresql-vs-redis-the-checkpoint-showdown\">PostgreSQL vs Redis: The Checkpoint Showdown</h3>\n<p><img alt=\"Redis vs PostgreSQL\" src=\"../../../../assets/2026-02-14-agent-memory-architecture/redis-vs-postgres.svg\" /></p>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th><strong>PostgreSQL</strong> (<code>langgraph-checkpoint-postgres</code>)</th>\n<th><strong>Redis</strong> (<code>langgraph-checkpoint-redis</code>)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Read latency</strong></td>\n<td>~0.65ms</td>\n<td>~0.095ms</td>\n</tr>\n<tr>\n<td><strong>Write latency</strong></td>\n<td>~2ms (unlogged) to 10ms (with WAL)</td>\n<td>~0.095ms</td>\n</tr>\n<tr>\n<td><strong>Throughput</strong></td>\n<td>~15K txn/s</td>\n<td>~893K req/s</td>\n</tr>\n<tr>\n<td><strong>Durability</strong></td>\n<td>Full ACID, WAL + replication</td>\n<td>Configurable (AOF/RDB), risk of data loss</td>\n</tr>\n<tr>\n<td><strong>Checkpoint history</strong></td>\n<td>Full history (time-travel, audit)</td>\n<td>Configurable retention via maxcount</td>\n</tr>\n<tr>\n<td><strong>Operational cost</strong></td>\n<td>Moderate (standard RDBMS ops)</td>\n<td>Higher (RAM-bound, memory management)</td>\n</tr>\n<tr>\n<td><strong>Scaling pattern</strong></td>\n<td>Vertical + read replicas</td>\n<td>Horizontal (Redis Cluster)</td>\n</tr>\n<tr>\n<td><strong>Best for</strong></td>\n<td>Compliance, audit trails, durability-first</td>\n<td>Low-latency, high-throughput, real-time</td>\n</tr>\n</tbody>\n</table>\n<p><em>Latency benchmarks from <a href=\"https://www.cybertec-postgresql.com/en/postgresql-vs-redis-vs-memcached-performance/\">CyberTec</a> and <a href=\"https://risingwave.com/blog/postgresql-vs-redis-performance-and-use-case-comparison/\">RisingWave</a> comparisons.</em></p>\n<h3 id=\"postgresql-the-durable-default\">PostgreSQL: The Durable Default</h3>\n<p>PostgreSQL is the safer default for most teams. Checkpoints survive crashes, you get full transaction semantics, and the checkpoint history supports time-travel debugging.</p>\n<p>From <a href=\"https://github.com/slavadubrov/market-analyst-agent\"><code>checkpointer_setup.py</code></a>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langgraph.checkpoint.postgres.aio</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">AsyncPostgresSaver</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">create_postgres_checkpointer</span><span class=\"p\">(</span><span class=\"n\">connection_string</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">AsyncPostgresSaver</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Create a PostgreSQL-backed checkpoint store.</span>\n\n<span class=\"sd\">    PostgreSQL gives us ACID guarantees \u2014 if a checkpoint write succeeds,</span>\n<span class=\"sd\">    the state is durable even if the process crashes immediately after.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">checkpointer</span> <span class=\"o\">=</span> <span class=\"n\">AsyncPostgresSaver</span><span class=\"o\">.</span><span class=\"n\">from_conn_string</span><span class=\"p\">(</span><span class=\"n\">connection_string</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Create the checkpoint tables if they don&#39;t exist.</span>\n    <span class=\"c1\"># This is idempotent \u2014 safe to call on every startup.</span>\n    <span class=\"k\">await</span> <span class=\"n\">checkpointer</span><span class=\"o\">.</span><span class=\"n\">setup</span><span class=\"p\">()</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">checkpointer</span>\n\n<span class=\"c1\"># Usage: wire into the graph compilation</span>\n<span class=\"n\">checkpointer</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">create_postgres_checkpointer</span><span class=\"p\">(</span>\n    <span class=\"s2\">&quot;postgresql://user:pass@localhost:5432/agent_memory&quot;</span>\n<span class=\"p\">)</span>\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">create_graph</span><span class=\"p\">(</span><span class=\"n\">checkpointer</span><span class=\"o\">=</span><span class=\"n\">checkpointer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Every invoke/stream call now persists state automatically</span>\n<span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;configurable&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;thread_id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user-123-session-1&quot;</span><span class=\"p\">}}</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">ainvoke</span><span class=\"p\">({</span><span class=\"s2\">&quot;messages&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;Analyze NVDA&quot;</span><span class=\"p\">)]},</span> <span class=\"n\">config</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Resume later \u2014 loads the latest checkpoint for this thread</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">ainvoke</span><span class=\"p\">({</span><span class=\"s2\">&quot;messages&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;approved&quot;</span><span class=\"p\">)]},</span> <span class=\"n\">config</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>The <code>AsyncPostgresSaver</code> uses the <code>langgraph-checkpoint-postgres</code> package, which creates three tables: <code>checkpoints</code> (the serialized state), <code>checkpoint_blobs</code> (large binary data), and <code>checkpoint_writes</code> (pending writes for crash recovery). The schema supports concurrent access and uses advisory locks to prevent write conflicts.</p>\n<h3 id=\"redis-the-speed-demon\">Redis: The Speed Demon</h3>\n<p>When sub-millisecond checkpoint latency matters \u2014 real-time conversational agents, high-frequency tool loops \u2014 Redis is the better choice.</p>\n<p>From <a href=\"https://github.com/slavadubrov/market-analyst-agent\"><code>checkpointer_setup.py</code></a>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langgraph.checkpoint.redis.aio</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">AsyncRedisSaver</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">create_redis_checkpointer</span><span class=\"p\">(</span><span class=\"n\">redis_url</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">AsyncRedisSaver</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Create a Redis-backed checkpoint store.</span>\n\n<span class=\"sd\">    Redis stores checkpoints in memory for sub-millisecond access.</span>\n<span class=\"sd\">    Trade-off: less durable than PostgreSQL unless AOF is enabled.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">checkpointer</span> <span class=\"o\">=</span> <span class=\"n\">AsyncRedisSaver</span><span class=\"o\">.</span><span class=\"n\">from_conn_string</span><span class=\"p\">(</span><span class=\"n\">redis_url</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Initialize Redis data structures</span>\n    <span class=\"k\">await</span> <span class=\"n\">checkpointer</span><span class=\"o\">.</span><span class=\"n\">setup</span><span class=\"p\">()</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">checkpointer</span>\n\n<span class=\"c1\"># Usage: same graph API, different backend</span>\n<span class=\"n\">checkpointer</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">create_redis_checkpointer</span><span class=\"p\">(</span><span class=\"s2\">&quot;redis://localhost:6379&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">create_graph</span><span class=\"p\">(</span><span class=\"n\">checkpointer</span><span class=\"o\">=</span><span class=\"n\">checkpointer</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>The <code>AsyncRedisSaver</code> from <code>langgraph-checkpoint-redis</code> stores checkpoints as JSON documents keyed by thread ID. The <a href=\"https://redis.io/blog/langgraph-redis-checkpoint-010/\">v0.1.0 redesign</a> replaced multiple search operations with a single <code>JSON.GET</code> call, significantly reducing latency. Redis 8.0+ includes RedisJSON and RediSearch by default \u2014 no extra modules to install.</p>\n<p>For memory-constrained deployments, <code>ShallowRedisSaver</code> stores only the latest checkpoint per thread \u2014 no history, but minimal RAM usage. Use this when you need pause/resume but don't need time-travel debugging.</p>\n<h3 id=\"when-to-use-which\">When to Use Which</h3>\n<p><strong>Use PostgreSQL when:</strong></p>\n<ul>\n<li>You need full checkpoint history for compliance or auditing</li>\n<li>Durability is non-negotiable (financial services, healthcare)</li>\n<li>You already run PostgreSQL in your stack</li>\n<li>Your agent runs long tasks where losing state means hours of recomputation</li>\n<li>You want a <a href=\"https://www.tigerdata.com/learn/building-ai-agents-with-persistent-memory-a-unified-database-approach\">unified data store</a> \u2014 PostgreSQL with pgvector can serve as a single backend for checkpoints, long-term memory, and vector search, simplifying your infrastructure</li>\n</ul>\n<p><strong>Use Redis when:</strong></p>\n<ul>\n<li>Checkpoint latency is your bottleneck (real-time chat, streaming UX)</li>\n<li>You're building voice bots \u2014 STT-to-LLM-to-TTS pipelines need <a href=\"https://redis.io/blog/engineering-for-ai-agents/\">sub-millisecond state access</a></li>\n<li>You need horizontal scaling across many concurrent threads</li>\n<li>High-concurrency fan-out patterns where multiple agents share state</li>\n<li>Short-lived sessions where losing a checkpoint is recoverable</li>\n<li>You want semantic caching to reduce redundant LLM calls (<a href=\"https://redis.io/langcache/\">Redis LangCache</a> caches semantically similar queries to avoid repeated LLM calls)</li>\n</ul>\n<p><strong>Other options</strong>: <a href=\"https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.sqlite\"><code>langgraph-checkpoint-sqlite</code></a> works for local development and single-process deployments. For AWS-native stacks, <a href=\"https://aws.amazon.com/blogs/database/build-durable-ai-agents-with-langgraph-and-amazon-dynamodb/\"><code>langgraph-checkpoint-aws</code></a> provides a <code>DynamoDBSaver</code> with intelligent payload handling \u2014 small checkpoints (&lt;350 KB) stay in DynamoDB, large ones are automatically offloaded to S3. Serverless pricing and no infrastructure to manage make it attractive for variable-load deployments.</p>\n<hr />\n<h2 id=\"long-term-memory-remembering-across-sessions\">Long-Term Memory: Remembering Across Sessions</h2>\n<p>Hot memory handles the current conversation. But what about the user who comes back next week? Long-term memory stores facts, preferences, and interaction history that persist across threads.</p>\n<p>LangGraph provides a <code>Store</code> interface for cross-thread memory via its <code>BaseStore</code> class. Each memory item is a <code>(namespace, key)</code> pair with a JSON value and optional vector embedding. The namespace typically encodes the user or organization: <code>(\"user\", \"user-123\", \"preferences\")</code>.</p>\n<p><img alt=\"Long-Term Memory Flow\" src=\"../../../../assets/2026-02-14-agent-memory-architecture/long-term-memory-flow.svg\" /></p>\n<h3 id=\"vector-storage-semantic-recall-with-qdrant\">Vector Storage: Semantic Recall with Qdrant</h3>\n<p>When the agent needs to recall unstructured facts \u2014 \"What did the user say about their investment timeline?\" \u2014 vector search provides semantic recall. Instead of exact key lookups, the agent queries by meaning.</p>\n<p><a href=\"https://qdrant.tech/\">Qdrant</a> is a purpose-built vector database written in Rust that handles embedding storage, indexing (HNSW), and filtered search. I covered HNSW and its trade-offs in detail in my <a href=\"/blog/2026/02/08/building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance/\">search ranking post</a>. Qdrant also offers an <a href=\"https://github.com/qdrant/mcp-server-qdrant\">MCP server</a> that acts as a semantic memory layer \u2014 useful if your agent framework supports the Model Context Protocol.</p>\n<p>From <a href=\"https://github.com/slavadubrov/market-analyst-agent\"><code>memory_store.py</code></a>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">qdrant_client</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">QdrantClient</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">qdrant_client.models</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">PointStruct</span><span class=\"p\">,</span> <span class=\"n\">Distance</span><span class=\"p\">,</span> <span class=\"n\">VectorParams</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_anthropic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatAnthropic</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">hashlib</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">UserMemoryStore</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Long-term memory backed by Qdrant vector search.</span>\n\n<span class=\"sd\">    Stores user facts as embedded vectors for semantic retrieval.</span>\n<span class=\"sd\">    Each fact is a short natural-language statement about the user.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">qdrant_url</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">collection_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;user_memory&quot;</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">QdrantClient</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"o\">=</span><span class=\"n\">qdrant_url</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">collection_name</span> <span class=\"o\">=</span> <span class=\"n\">collection_name</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_ensure_collection</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">_ensure_collection</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Create the collection if it doesn&#39;t exist.&quot;&quot;&quot;</span>\n        <span class=\"n\">collections</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">c</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">get_collections</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">collections</span><span class=\"p\">]</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">collection_name</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">collections</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">create_collection</span><span class=\"p\">(</span>\n                <span class=\"n\">collection_name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">collection_name</span><span class=\"p\">,</span>\n                <span class=\"n\">vectors_config</span><span class=\"o\">=</span><span class=\"n\">VectorParams</span><span class=\"p\">(</span>\n                    <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">1536</span><span class=\"p\">,</span>  <span class=\"c1\"># text-embedding-3-small dimensions</span>\n                    <span class=\"n\">distance</span><span class=\"o\">=</span><span class=\"n\">Distance</span><span class=\"o\">.</span><span class=\"n\">COSINE</span><span class=\"p\">,</span>\n                <span class=\"p\">),</span>\n            <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">store_fact</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">user_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">fact</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">embedding</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Store a user fact with its embedding.&quot;&quot;&quot;</span>\n        <span class=\"n\">point_id</span> <span class=\"o\">=</span> <span class=\"n\">hashlib</span><span class=\"o\">.</span><span class=\"n\">md5</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"n\">user_id</span><span class=\"si\">}</span><span class=\"s2\">:</span><span class=\"si\">{</span><span class=\"n\">fact</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">hexdigest</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">upsert</span><span class=\"p\">(</span>\n            <span class=\"n\">collection_name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">collection_name</span><span class=\"p\">,</span>\n            <span class=\"n\">points</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">PointStruct</span><span class=\"p\">(</span>\n                <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"n\">point_id</span><span class=\"p\">,</span>\n                <span class=\"n\">vector</span><span class=\"o\">=</span><span class=\"n\">embedding</span><span class=\"p\">,</span>\n                <span class=\"n\">payload</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;user_id&quot;</span><span class=\"p\">:</span> <span class=\"n\">user_id</span><span class=\"p\">,</span> <span class=\"s2\">&quot;fact&quot;</span><span class=\"p\">:</span> <span class=\"n\">fact</span><span class=\"p\">},</span>\n            <span class=\"p\">)],</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">recall</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">user_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">query_embedding</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">],</span> <span class=\"n\">top_k</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Retrieve the most relevant facts for a user given a query.&quot;&quot;&quot;</span>\n        <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">query_points</span><span class=\"p\">(</span>\n            <span class=\"n\">collection_name</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">collection_name</span><span class=\"p\">,</span>\n            <span class=\"n\">query</span><span class=\"o\">=</span><span class=\"n\">query_embedding</span><span class=\"p\">,</span>\n            <span class=\"n\">query_filter</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;must&quot;</span><span class=\"p\">:</span> <span class=\"p\">[{</span><span class=\"s2\">&quot;key&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user_id&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;match&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;value&quot;</span><span class=\"p\">:</span> <span class=\"n\">user_id</span><span class=\"p\">}}]},</span>\n            <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"n\">top_k</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">hit</span><span class=\"o\">.</span><span class=\"n\">payload</span><span class=\"p\">[</span><span class=\"s2\">&quot;fact&quot;</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">hit</span> <span class=\"ow\">in</span> <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>The flow is: (1) after each conversation, an LLM extracts key facts from the interaction (\"user has high risk tolerance\", \"user is interested in semiconductor stocks\"), (2) facts are embedded and stored in Qdrant, (3) at the start of the next conversation, the agent queries Qdrant with the user's new message to recall relevant context.</p>\n<h3 id=\"retrieval-scoring-beyond-cosine-similarity\">Retrieval Scoring: Beyond Cosine Similarity</h3>\n<p>Raw cosine similarity is a starting point, but production memory systems need richer retrieval. The <a href=\"https://arxiv.org/abs/2304.03442\">Generative Agents paper</a> (Park et al., 2023) introduced a scoring function that combines three signals:</p>\n<ul>\n<li><strong>Recency</strong>: Rule-based decay \u2014 recent memories score higher. An exponential decay function ensures that a fact from yesterday outranks an equivalent fact from six months ago.</li>\n<li><strong>Importance</strong>: LLM-rated significance on a 1-10 scale. \"User's portfolio is down 40%\" scores higher than \"user said hello.\"</li>\n<li><strong>Relevance</strong>: Embedding cosine similarity between the query and the stored fact.</li>\n</ul>\n<p>The final retrieval score is a weighted sum: <code>score = alpha * recency + beta * importance + gamma * relevance</code>. This prevents the system from surfacing stale but semantically similar facts over fresh, important ones. For the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a>, I weight relevance highest (0.5) with recency (0.3) and importance (0.2), since the user's current query intent matters most. These weights are adapted from the Generative Agents paper as a starting point and should be tuned based on your use case \u2014 the original paper used equal weighting, but I found that emphasizing relevance worked better for financial analysis queries. Note that these values are intuition-based, not empirically optimized for this specific agent.</p>\n<h3 id=\"alternatives-to-vector-search\">Alternatives to Vector Search</h3>\n<p>Vector search is powerful but not always the right tool. Here's when to use alternatives:</p>\n<table>\n<thead>\n<tr>\n<th>Approach</th>\n<th>Best For</th>\n<th>Latency</th>\n<th>Complexity</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Vector search (Qdrant)</strong></td>\n<td>Semantic recall of unstructured facts</td>\n<td>5\u201320ms</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Key-value store (Redis)</strong></td>\n<td>Structured user profiles, preferences</td>\n<td>&lt;1ms</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Document store (files)</strong></td>\n<td>Project knowledge, agent-managed notes</td>\n<td>1\u20135ms</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Full-text search (PostgreSQL <a href=\"https://www.postgresql.org/docs/current/gin.html\">GIN index</a>)</strong></td>\n<td>Keyword-based recall of conversation history</td>\n<td>2\u201310ms</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Knowledge graph (Neo4j)</strong></td>\n<td>Entity relationships, multi-hop reasoning</td>\n<td>10\u201350ms</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Hybrid (vector + keyword)</strong></td>\n<td>Best recall when query intent varies</td>\n<td>10\u201330ms</td>\n<td>Medium</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Key-value stores</strong> work well for structured data. If your long-term memory is a user profile \u2014 risk tolerance, investment horizon, preferred sectors \u2014 a Redis hash or PostgreSQL JSONB column is simpler and faster than embedding and querying vectors. Use vector search when the memory is unstructured and the retrieval query varies in phrasing.</p>\n<p><strong>LangGraph's built-in Store</strong> provides a namespace-based key-value interface with optional vector search. The <code>BaseStore</code> API is simple: <code>put()</code>, <code>get()</code>, <code>search()</code>, and <code>delete()</code> with hierarchical namespace scoping. Three implementations are available:</p>\n<ul>\n<li><code>InMemoryStore</code> \u2014 for development and testing (data lost on process exit)</li>\n<li><code>PostgresStore</code> \u2014 production persistent store with full SQL querying</li>\n<li><code>AsyncRedisStore</code> \u2014 cross-thread memory with vector search, TTL support, and metadata filtering</li>\n</ul>\n<p>The <code>index</code> configuration enables vector search over stored items using a configurable embedding model. For many use cases, this built-in store is sufficient without reaching for a dedicated vector database.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langgraph.store.memory</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">InMemoryStore</span>\n\n<span class=\"c1\"># Create a store with vector search enabled</span>\n<span class=\"n\">store</span> <span class=\"o\">=</span> <span class=\"n\">InMemoryStore</span><span class=\"p\">(</span>\n    <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"p\">{</span>\n        <span class=\"s2\">&quot;dims&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1536</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;embed&quot;</span><span class=\"p\">:</span> <span class=\"n\">my_embedding_function</span><span class=\"p\">,</span>  <span class=\"c1\"># e.g., OpenAI text-embedding-3-small</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Store a user preference (namespace scopes to user)</span>\n<span class=\"k\">await</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">aput</span><span class=\"p\">(</span>\n    <span class=\"n\">namespace</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;user-123&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;preferences&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"s2\">&quot;risk-profile&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;risk_tolerance&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;high&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;horizon&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;long-term&quot;</span><span class=\"p\">},</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Semantic search across user&#39;s memories</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">asearch</span><span class=\"p\">(</span>\n    <span class=\"n\">namespace</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;user-123&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">query</span><span class=\"o\">=</span><span class=\"s2\">&quot;What is their investment style?&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"choosing-a-long-term-memory-strategy\">Choosing a Long-Term Memory Strategy</h3>\n<p><strong>Start with key-value</strong> if your memory is structured and well-defined (user profiles, settings, named entities). Add vector search when you need semantic retrieval over unstructured facts or when the query phrasing varies unpredictably.</p>\n<p><strong>Knowledge graphs</strong> become valuable when relationships between entities matter \u2014 \"Which companies did the user ask about that are competitors of NVDA?\" The most interesting recent project is <a href=\"https://github.com/getzep/graphiti\">Graphiti</a> (by Zep), which builds a <strong>temporally-aware knowledge graph</strong> that tracks <em>when</em> facts were true, not just <em>what</em> was true. Every edge carries validity intervals, so when a user changes their risk tolerance, the old value is invalidated rather than silently overwritten. Graphiti achieves <a href=\"https://arxiv.org/abs/2501.13956\">94.8% accuracy on the DMR benchmark</a>, and its bi-temporal model handles the stale memory problem at the data layer.</p>\n<p>That said, the operational complexity of running a graph database is significant. For most agent applications, vector search with metadata filtering covers the same ground with less infrastructure.</p>\n<p><strong>Managed memory frameworks</strong> like <a href=\"https://mem0.ai/research\">Mem0</a> and <a href=\"https://docs.letta.com/concepts/memgpt/\">Letta</a> (formerly MemGPT) handle the extraction-consolidation-retrieval pipeline for you. Mem0's approach is notable: an LLM extracts candidate memories, a decision engine compares each new fact against existing entries in the vector store, and a resolver decides to add, update, or delete \u2014 keeping the memory store coherent and non-redundant. Letta takes a different approach inspired by operating systems: agents manage their own context window using memory management tools, autonomously moving data between \"core memory\" (in-context) and \"archival memory\" (out-of-context). Both are worth evaluating if you want faster time-to-production and don't need full control over the memory pipeline.</p>\n<hr />\n<h2 id=\"document-memory-the-agents-filing-cabinet\">Document Memory: The Agent's Filing Cabinet</h2>\n<p>The pattern is widespread in practice but underexplored in academic literature. Framework documentation covers vector stores, knowledge graphs, and context management in depth \u2014 but file-based memory barely gets a footnote. That's a gap worth addressing, because in practice this is how the most effective AI coding assistants actually persist knowledge. Interestingly, <a href=\"https://www.letta.com/blog/benchmarking-ai-agent-memory\">Letta's benchmark</a> found that a simple filesystem-based approach scored 74.0% on the LoCoMo conversational memory benchmark \u2014 outperforming several specialized memory libraries. The pattern works because today's frontier models are already trained on agentic coding tasks and understand file operations natively.</p>\n<p>This is the dominant trend in 2025-2026 agent architecture, driven by a fundamental shift in model capabilities:</p>\n<ul>\n<li><strong>The Shift</strong>: In 2023-2024, context windows were small (8k-32k tokens), so we had to shred documents into vectors (chunks) to fit them in.</li>\n<li><strong>The Reality Now</strong>: With 1M+ token context windows (Gemini 1.5, Claude 3.5/4), it is far more effective to let the agent \"read the whole file\" (Document Memory) rather than guessing which chunks are relevant (Vector Memory).</li>\n<li><strong>Debuggability</strong>: Developers prefer <code>.md</code> files because they can read them, edit them, and git version them. You cannot \"git diff\" a vector database. This accurately describes the <strong>\"post-RAG\" era</strong> of AI agents, where memory is treated as readable files (for reasoning and coding) rather than just mathematical embeddings.</li>\n</ul>\n<p>Vector stores and key-value backends handle semantic recall and structured lookups well. But there's a third category of agent knowledge that neither serves cleanly: <strong>accumulated project context</strong> \u2014 conventions, research notes, learned patterns, and decisions that the agent needs across sessions but that benefit from being human-readable, editable, and version-controlled.</p>\n<p>This is <strong>document memory</strong> \u2014 the agent reads and writes structured files (Markdown, JSON, YAML) to a known directory. No embeddings, no database, no infrastructure. Just files on disk that both the agent and the developer can <code>cat</code>, <code>grep</code>, <code>git diff</code>, and edit by hand.</p>\n<h3 id=\"why-files\">Why Files?</h3>\n<p>The motivation is practical. I noticed that the most effective pattern for long-lived agent workflows isn't a vector database \u2014 it's a directory of well-organized notes. Consider what happens when a coding agent works on a project over weeks:</p>\n<ul>\n<li>It learns that the project uses Pydantic v2, not v1</li>\n<li>It discovers that tests must run with <code>pytest -x --tb=short</code></li>\n<li>It accumulates knowledge about the codebase architecture</li>\n<li>It learns the developer's preferences (\"always use <code>pathlib</code>, never <code>os.path</code>\")</li>\n</ul>\n<p>These facts are too structured for vector search (you'd need exact recall, not fuzzy similarity) and too numerous for a key-value store (they form interconnected documents, not isolated facts). They're also facts that the developer wants to <strong>see and edit directly</strong> \u2014 if the agent learns something wrong, you open the file and fix it.</p>\n<p>This is exactly how <a href=\"https://docs.anthropic.com/en/docs/claude-code/memory\">Claude Code's <code>CLAUDE.md</code></a> and <code>.claude/</code> directory work. The agent reads project-level <code>CLAUDE.md</code> files for conventions and instructions, and writes to <code>~/.claude/MEMORY.md</code> for cross-session learnings. The files are plain Markdown \u2014 you can read them, edit them, commit them to git, and share them with your team. <a href=\"https://docs.cursor.com/context/rules\">Cursor's <code>.cursorrules</code></a> and <a href=\"https://docs.windsurf.com/windsurf/cascade/memories\">Windsurf's <code>.windsurfrules</code></a> follow the same pattern: plain-text files that the agent loads on startup to understand project context.</p>\n<h3 id=\"implementing-a-file-memory-store\">Implementing a File Memory Store</h3>\n<p>The implementation is deliberately simple. The agent gets four operations: write a document, read a document, list available documents, and search across documents by keyword.</p>\n<p>From <a href=\"https://github.com/slavadubrov/market-analyst-agent\"><code>file_memory.py</code></a>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pathlib</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Path</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">fnmatch</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">FileMemory</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Document memory backed by the local filesystem.</span>\n\n<span class=\"sd\">    Stores agent knowledge as human-readable files organized by topic.</span>\n<span class=\"sd\">    No embeddings, no database \u2014 just files that both the agent and</span>\n<span class=\"sd\">    the developer can read, edit, and version-control.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">base_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"n\">Path</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span> <span class=\"o\">=</span> <span class=\"n\">Path</span><span class=\"p\">(</span><span class=\"n\">base_dir</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">(</span><span class=\"n\">parents</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">exist_ok</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">write_doc</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">metadata</span><span class=\"p\">:</span> <span class=\"nb\">dict</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Write or overwrite a document at the given path.</span>\n\n<span class=\"sd\">        Paths are relative to base_dir. Directories are created automatically.</span>\n<span class=\"sd\">        Metadata (if provided) is stored as a JSON sidecar file.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"n\">full_path</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span> <span class=\"o\">/</span> <span class=\"n\">path</span>\n        <span class=\"n\">full_path</span><span class=\"o\">.</span><span class=\"n\">parent</span><span class=\"o\">.</span><span class=\"n\">mkdir</span><span class=\"p\">(</span><span class=\"n\">parents</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">exist_ok</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">full_path</span><span class=\"o\">.</span><span class=\"n\">write_text</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&quot;utf-8&quot;</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">metadata</span><span class=\"p\">:</span>\n            <span class=\"n\">meta_path</span> <span class=\"o\">=</span> <span class=\"n\">full_path</span><span class=\"o\">.</span><span class=\"n\">with_suffix</span><span class=\"p\">(</span><span class=\"n\">full_path</span><span class=\"o\">.</span><span class=\"n\">suffix</span> <span class=\"o\">+</span> <span class=\"s2\">&quot;.meta&quot;</span><span class=\"p\">)</span>\n            <span class=\"n\">meta_path</span><span class=\"o\">.</span><span class=\"n\">write_text</span><span class=\"p\">(</span><span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">metadata</span><span class=\"p\">,</span> <span class=\"n\">indent</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&quot;utf-8&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">read_doc</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Read a document by path. Returns None if not found.&quot;&quot;&quot;</span>\n        <span class=\"n\">full_path</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span> <span class=\"o\">/</span> <span class=\"n\">path</span>\n        <span class=\"k\">if</span> <span class=\"n\">full_path</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"p\">():</span>\n            <span class=\"k\">return</span> <span class=\"n\">full_path</span><span class=\"o\">.</span><span class=\"n\">read_text</span><span class=\"p\">(</span><span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&quot;utf-8&quot;</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"kc\">None</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">list_docs</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;**/*&quot;</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;List documents matching a glob pattern.&quot;&quot;&quot;</span>\n        <span class=\"k\">return</span> <span class=\"p\">[</span>\n            <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">relative_to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span><span class=\"p\">))</span>\n            <span class=\"k\">for</span> <span class=\"n\">p</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span><span class=\"o\">.</span><span class=\"n\">glob</span><span class=\"p\">(</span><span class=\"n\">pattern</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">is_file</span><span class=\"p\">()</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">endswith</span><span class=\"p\">(</span><span class=\"s2\">&quot;.meta&quot;</span><span class=\"p\">)</span>\n        <span class=\"p\">]</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">search_docs</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;**/*.md&quot;</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Search documents by keyword. Returns matching files with context.</span>\n\n<span class=\"sd\">        This is intentionally simple \u2014 grep-style keyword search.</span>\n<span class=\"sd\">        For semantic search, use a vector store instead.</span>\n\n<span class=\"sd\">        NOTE: This is a sketch for demonstration. A simple substring check</span>\n<span class=\"sd\">        won&#39;t scale beyond a few hundred documents. For production with 500+</span>\n<span class=\"sd\">        documents, use TF-IDF/BM25 scoring (e.g., rank_bm25) or a full-text</span>\n<span class=\"sd\">        search backend (PostgreSQL GIN index, Elasticsearch).</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">path</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span><span class=\"o\">.</span><span class=\"n\">glob</span><span class=\"p\">(</span><span class=\"n\">pattern</span><span class=\"p\">):</span>\n            <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">is_file</span><span class=\"p\">()</span> <span class=\"ow\">or</span> <span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">endswith</span><span class=\"p\">(</span><span class=\"s2\">&quot;.meta&quot;</span><span class=\"p\">):</span>\n                <span class=\"k\">continue</span>\n            <span class=\"n\">content</span> <span class=\"o\">=</span> <span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">read_text</span><span class=\"p\">(</span><span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&quot;utf-8&quot;</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">query</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span> <span class=\"ow\">in</span> <span class=\"n\">content</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">():</span>\n                <span class=\"c1\"># Return the paragraph containing the match for context</span>\n                <span class=\"k\">for</span> <span class=\"n\">paragraph</span> <span class=\"ow\">in</span> <span class=\"n\">content</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n\\n</span><span class=\"s2\">&quot;</span><span class=\"p\">):</span>\n                    <span class=\"k\">if</span> <span class=\"n\">query</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span> <span class=\"ow\">in</span> <span class=\"n\">paragraph</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">():</span>\n                        <span class=\"n\">results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span>\n                            <span class=\"s2\">&quot;path&quot;</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">relative_to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">base_dir</span><span class=\"p\">)),</span>\n                            <span class=\"s2\">&quot;match&quot;</span><span class=\"p\">:</span> <span class=\"n\">paragraph</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()[:</span><span class=\"mi\">500</span><span class=\"p\">],</span>\n                        <span class=\"p\">})</span>\n        <span class=\"k\">return</span> <span class=\"n\">results</span>\n</code></pre></div>\n<h3 id=\"folder-structure\">Folder Structure</h3>\n<p>The power of document memory is in its organization. Here's the folder structure I use for the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a>:</p>\n<div class=\"highlight\"><pre><span></span><code>.agent-memory/\n    README.md                  # What this directory is, for human readers\n    user-profiles/\n        user-123.md            # Preferences, history, risk profile\n        user-456.md\n    research/\n        NVDA-2026-02.md        # Research notes from recent analysis\n        TSLA-2026-01.md\n    conventions/\n        analysis-format.md     # How to structure analysis reports\n        data-sources.md        # Preferred data sources and API patterns\n    learnings/\n        common-errors.md       # Mistakes the agent has learned to avoid\n        tool-patterns.md       # Effective tool call sequences\n</code></pre></div>\n<p>Every file is Markdown. Every file has a clear purpose from its path. You can <code>git diff</code> the entire memory directory to see what the agent learned in a session. You can <code>git revert</code> a bad learning. You can copy the directory to another project. Try doing that with a Qdrant collection.</p>\n<h3 id=\"when-to-use-document-memory-vs-vector-vs-key-value\">When to Use Document Memory vs Vector vs Key-Value</h3>\n<p>The three memory backends serve different access patterns:</p>\n<table>\n<thead>\n<tr>\n<th>Dimension</th>\n<th><strong>Vector Store</strong></th>\n<th><strong>Key-Value Store</strong></th>\n<th><strong>Document Store</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Query pattern</strong></td>\n<td>\"Find facts similar to X\"</td>\n<td>\"Get the value for key\"</td>\n<td>\"Read the doc at path\"</td>\n</tr>\n<tr>\n<td><strong>Best for</strong></td>\n<td>Unstructured, varied recall</td>\n<td>Structured lookups</td>\n<td>Project context, notes</td>\n</tr>\n<tr>\n<td><strong>Human readable</strong></td>\n<td>No (embeddings)</td>\n<td>Partially (JSON)</td>\n<td>Yes (Markdown)</td>\n</tr>\n<tr>\n<td><strong>Debuggable</strong></td>\n<td>Hard (similarity scores)</td>\n<td>Easy (exact keys)</td>\n<td>Trivial (open the file)</td>\n</tr>\n<tr>\n<td><strong>Version controllable</strong></td>\n<td>No</td>\n<td>Possible</td>\n<td>Yes (git-native)</td>\n</tr>\n<tr>\n<td><strong>Embedding infrastructure</strong></td>\n<td>Required</td>\n<td>Not needed</td>\n<td>Not needed</td>\n</tr>\n<tr>\n<td><strong>Scales to</strong></td>\n<td>Millions of facts</td>\n<td>Millions of keys</td>\n<td>Thousands of documents</td>\n</tr>\n<tr>\n<td><strong>Search capability</strong></td>\n<td>Semantic similarity</td>\n<td>Exact match</td>\n<td>Keyword / path-based</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Use document memory when:</strong></p>\n<ul>\n<li>The agent accumulates project knowledge over multiple sessions</li>\n<li>Developers need to inspect, edit, or override what the agent \"knows\"</li>\n<li>The knowledge is structured as documents (notes, summaries, conventions) rather than isolated facts</li>\n<li>You want git-based versioning of agent memory</li>\n<li>Zero infrastructure is a hard requirement</li>\n</ul>\n<p><strong>Use vector stores when:</strong></p>\n<ul>\n<li>You need fuzzy semantic retrieval (\"find memories related to X\")</li>\n<li>The query phrasing varies unpredictably</li>\n<li>You have thousands to millions of individual facts</li>\n</ul>\n<p><strong>Use key-value stores when:</strong></p>\n<ul>\n<li>You need exact, fast lookups for structured data (user profiles, settings)</li>\n<li>The data schema is well-defined</li>\n</ul>\n<p>In practice, production agents often combine all three. The <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a> uses PostgreSQL checkpoints for hot memory, Qdrant for semantic user fact recall, and a file-based document store for project conventions and research notes.</p>\n<h3 id=\"real-world-examples\">Real-World Examples</h3>\n<p>This pattern is already widespread in AI coding assistants:</p>\n<ul>\n<li><strong>Claude Code</strong> reads <code>CLAUDE.md</code> files from the project root and parent directories, plus writes to <code>~/.claude/MEMORY.md</code> for cross-session learnings. The entire memory system is plain Markdown files that you commit alongside your code.</li>\n<li><strong>Cursor</strong> loads <code>.cursorrules</code> files for project-specific agent instructions \u2014 coding conventions, framework preferences, architectural decisions.</li>\n<li><strong>Windsurf</strong> uses <code>.windsurfrules</code> files plus a <code>memories/</code> directory where the agent stores learned patterns from your codebase.</li>\n<li><strong>Anthropic's memory tool</strong> for the Claude API provides <code>create_memory</code>, <code>read_memory</code>, <code>update_memory</code>, and <code>delete_memory</code> operations that are implemented client-side \u2014 your application decides where the files actually live (local disk, S3, database).</li>\n</ul>\n<p>The common thread: all of these store agent knowledge as human-readable text files with explicit read/write operations. No embeddings. No vector infrastructure. The agent decides what to write, the developer can see and edit everything, and the whole system fits in a <code>git diff</code>.</p>\n<h3 id=\"beyond-coding-assistants\">Beyond Coding Assistants</h3>\n<p>Document memory is not limited to coding agents. The pattern has emerged independently across multiple agent domains:</p>\n<ul>\n<li>\n<p><strong>Open-world game agents</strong>: <a href=\"https://arxiv.org/abs/2305.16291\">Voyager</a> (Wang et al., 2023) builds a persistent skill library of verified JavaScript programs that a Minecraft agent accumulates over time \u2014 collecting 3.3x more unique items and reaching milestones 15.3x faster than baselines. Skills transfer across new worlds without retraining. <a href=\"https://arxiv.org/abs/2311.05997\">JARVIS-1</a> extends this with a multimodal memory that combines textual plans and visual observations, achieving 5x success rate on the hardest tasks. These skill libraries share the file-based storage pattern with document memory, but there's an important distinction: <strong>executable skill memory</strong> (code files that get imported and executed) versus <strong>declarative document memory</strong> (Markdown/text files injected into prompts). They differ in their read paths \u2014 executable skills are loaded as Python/JavaScript modules, while declarative documents are read as text \u2014 and their failure modes \u2014 bad executable code crashes the agent, while bad declarative text leads to reasoning errors. Both benefit from debuggability and version control, but the distinction is meaningful when designing memory systems.</p>\n</li>\n<li>\n<p><strong>Enterprise workflow automation</strong>: The <a href=\"/blog/2026/01/11/enterprise-rag-challenge-3-winning-approaches-for-autonomous-ai-agents/\">ECR3 competition</a> winners used document memory for iterative prompt refinement \u2014 one winning team's Analyzer and Versioner agents automatically iterated through 80 prompt versions stored as procedural documents, while another top team built 20+ enricher modules as document-style procedural knowledge. <a href=\"https://arxiv.org/abs/2510.04851\">LEGOMem</a> (2025) formalizes this as a modular memory framework for multi-agent systems, with specialized memory types (sensory, short-term, long-term) that agents compose like building blocks.</p>\n</li>\n<li>\n<p><strong>Web automation</strong>: <a href=\"https://arxiv.org/abs/2409.07429\">Agent Workflow Memory</a> (Wang et al., 2024) lets web agents induce reusable workflows from successful episodes, improving success rates by 51% on WebArena. <a href=\"https://arxiv.org/abs/2504.07079\">SkillWeaver</a> (2025) takes this further \u2014 agents synthesize verified, reusable API tools from exploration, achieving a 31.8% success rate improvement. Critically, skills transfer to weaker models (54.3% improvement), showing that document memory can democratize capability.</p>\n</li>\n<li>\n<p><strong>Customer support</strong>: <a href=\"https://www.gartner.com/en/newsroom/press-releases/2025-02-26-gartner-predicts-ai-agents-will-autonomously-resolve-80-percent-of-common-customer-service-issues-without-human-intervention-by-2029\">Gartner predicts</a> that AI agents will autonomously resolve 80% of common customer service issues by 2029. These agents reference SOPs, playbooks, and customer histories \u2014 all forms of document memory.</p>\n</li>\n</ul>\n<p>The emergence of the <a href=\"https://sites.google.com/view/memagent-iclr26/\">MemAgents workshop at ICLR 2026</a> \u2014 focused on memory for LLM-based agentic systems \u2014 signals that the academic community is catching up to what practitioners have already built. Document memory is not a coding-specific pattern; it's an emerging architectural primitive for any agent that needs to accumulate, share, and iterate on knowledge.</p>\n<p><strong>Skills are document memory with a schema.</strong> The <a href=\"https://agentskills.io/specification\">Agent Skills standard</a> (<code>SKILL.md</code> files with YAML frontmatter + Markdown body) is now adopted by both Anthropic and OpenAI Codex \u2014 procedural knowledge stored as documents. <a href=\"https://modelcontextprotocol.io/specification/2025-11-05\">MCP (Model Context Protocol)</a> takes this further: tool definitions are JSON Schema files that any agent can discover and invoke, with <a href=\"https://www.pento.ai/blog/a-year-of-mcp-2025-review\">97 million monthly SDK downloads</a> and adoption by OpenAI, Google, Microsoft, and AWS. MCP is not coding-specific \u2014 it connects agents to databases, APIs, and enterprise systems. This suggests the document memory pattern may be broader than coding assistants: skills and tool definitions are schema-enforced document memory for procedural knowledge, and they're becoming the <a href=\"https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation\">industry standard</a> for agent interoperability.</p>\n<h3 id=\"scaling-document-memory-for-production\">Scaling Document Memory for Production</h3>\n<p>The file-based implementation shown above works well for single-developer laptops and small-scale deployments. But what happens when you move to multi-tenant production with hundreds of users and thousands of documents?</p>\n<p>The single-node file limitation becomes clear: you can't scale file I/O horizontally, concurrent writes need locking, and managing permissions across tenants is painful. For production, you need a backing store that handles concurrency, search, and multi-tenancy properly.</p>\n<p>Three common approaches:</p>\n<p><strong>Approach A: Hybrid with Thin Database Layer</strong></p>\n<p>Keep files for authoring (developers edit Markdown locally), but serve from a database at runtime. On deployment, sync files to PostgreSQL rows. The agent reads from the database, not disk. This gives you:</p>\n<ul>\n<li>Developer ergonomics (edit Markdown, commit to git)</li>\n<li>Production query performance (indexed database reads)</li>\n<li>Clean separation between authoring and serving</li>\n</ul>\n<p><strong>Approach B: Object Storage + Vector Index Sidecar</strong></p>\n<p>Store documents in S3/GCS as objects, with a Qdrant collection that indexes their embeddings. The agent queries Qdrant for relevant document IDs, then fetches content from object storage. This scales horizontally and supports semantic search, but adds complexity \u2014 two systems to manage, embedding pipeline to maintain, eventual consistency between store and index.</p>\n<p><strong>Approach C: Structured Document Store with PostgreSQL (Recommended)</strong></p>\n<p>Store documents as PostgreSQL JSONB rows with full-text search (GIN index) and optional vector embeddings (pgvector). This gives you hybrid search (keyword + semantic), ACID transactions, and a single operational system.</p>\n<p>Here's a sketch of Approach C:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Optional</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">asyncpg</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ProductionDocumentMemory</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;PostgreSQL-backed document memory with hybrid search.</span>\n\n<span class=\"sd\">    Schema:</span>\n<span class=\"sd\">        CREATE TABLE documents (</span>\n<span class=\"sd\">            id SERIAL PRIMARY KEY,</span>\n<span class=\"sd\">            tenant_id TEXT NOT NULL,</span>\n<span class=\"sd\">            path TEXT NOT NULL,</span>\n<span class=\"sd\">            content TEXT NOT NULL,</span>\n<span class=\"sd\">            metadata JSONB,</span>\n<span class=\"sd\">            embedding vector(1536),  -- pgvector extension</span>\n<span class=\"sd\">            ts_vector tsvector GENERATED ALWAYS AS (to_tsvector(&#39;english&#39;, content)) STORED,</span>\n<span class=\"sd\">            created_at TIMESTAMPTZ DEFAULT NOW(),</span>\n<span class=\"sd\">            UNIQUE(tenant_id, path)</span>\n<span class=\"sd\">        );</span>\n<span class=\"sd\">        CREATE INDEX ON documents USING GIN(ts_vector);</span>\n<span class=\"sd\">        CREATE INDEX ON documents USING ivfflat(embedding vector_cosine_ops);</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">pool</span><span class=\"p\">:</span> <span class=\"n\">asyncpg</span><span class=\"o\">.</span><span class=\"n\">Pool</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pool</span> <span class=\"o\">=</span> <span class=\"n\">pool</span>\n\n    <span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">write</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">tenant_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n        <span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n        <span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n        <span class=\"n\">metadata</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">embedding</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Write or update a document.&quot;&quot;&quot;</span>\n        <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pool</span><span class=\"o\">.</span><span class=\"n\">acquire</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">conn</span><span class=\"p\">:</span>\n            <span class=\"k\">await</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span>\n<span class=\"w\">                </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">                INSERT INTO documents (tenant_id, path, content, metadata, embedding)</span>\n<span class=\"sd\">                VALUES ($1, $2, $3, $4, $5)</span>\n<span class=\"sd\">                ON CONFLICT (tenant_id, path) DO UPDATE</span>\n<span class=\"sd\">                SET content = EXCLUDED.content,</span>\n<span class=\"sd\">                    metadata = EXCLUDED.metadata,</span>\n<span class=\"sd\">                    embedding = EXCLUDED.embedding</span>\n<span class=\"sd\">                &quot;&quot;&quot;</span><span class=\"p\">,</span>\n                <span class=\"n\">tenant_id</span><span class=\"p\">,</span> <span class=\"n\">path</span><span class=\"p\">,</span> <span class=\"n\">content</span><span class=\"p\">,</span> <span class=\"n\">metadata</span><span class=\"p\">,</span> <span class=\"n\">embedding</span><span class=\"p\">,</span>\n            <span class=\"p\">)</span>\n\n    <span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">search</span><span class=\"p\">(</span>\n        <span class=\"bp\">self</span><span class=\"p\">,</span>\n        <span class=\"n\">tenant_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n        <span class=\"n\">query</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n        <span class=\"n\">embedding</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">limit</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">]:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Hybrid search: full-text + optional vector similarity.&quot;&quot;&quot;</span>\n        <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pool</span><span class=\"o\">.</span><span class=\"n\">acquire</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">conn</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">embedding</span><span class=\"p\">:</span>\n                <span class=\"c1\"># Hybrid scoring: 0.6 * text relevance + 0.4 * vector similarity</span>\n                <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">fetch</span><span class=\"p\">(</span>\n<span class=\"w\">                    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">                    SELECT path, content, metadata,</span>\n<span class=\"sd\">                           (0.6 * ts_rank(ts_vector, plainto_tsquery(&#39;english&#39;, $2)) +</span>\n<span class=\"sd\">                            0.4 * (1 - (embedding &lt;=&gt; $3))) AS score</span>\n<span class=\"sd\">                    FROM documents</span>\n<span class=\"sd\">                    WHERE tenant_id = $1</span>\n<span class=\"sd\">                      AND ts_vector @@ plainto_tsquery(&#39;english&#39;, $2)</span>\n<span class=\"sd\">                    ORDER BY score DESC</span>\n<span class=\"sd\">                    LIMIT $4</span>\n<span class=\"sd\">                    &quot;&quot;&quot;</span><span class=\"p\">,</span>\n                    <span class=\"n\">tenant_id</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">embedding</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"p\">,</span>\n                <span class=\"p\">)</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"c1\"># Full-text search only</span>\n                <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">fetch</span><span class=\"p\">(</span>\n<span class=\"w\">                    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">                    SELECT path, content, metadata,</span>\n<span class=\"sd\">                           ts_rank(ts_vector, plainto_tsquery(&#39;english&#39;, $2)) AS score</span>\n<span class=\"sd\">                    FROM documents</span>\n<span class=\"sd\">                    WHERE tenant_id = $1</span>\n<span class=\"sd\">                      AND ts_vector @@ plainto_tsquery(&#39;english&#39;, $2)</span>\n<span class=\"sd\">                    ORDER BY score DESC</span>\n<span class=\"sd\">                    LIMIT $3</span>\n<span class=\"sd\">                    &quot;&quot;&quot;</span><span class=\"p\">,</span>\n                    <span class=\"n\">tenant_id</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"p\">,</span>\n                <span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">rows</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>This approach gives you:</p>\n<ul>\n<li><strong>Hybrid search</strong>: Keyword matching (GIN index) + semantic similarity (pgvector) scored together</li>\n<li><strong>Multi-tenancy</strong>: <code>tenant_id</code> scoping with row-level security</li>\n<li><strong>ACID guarantees</strong>: No eventual consistency issues</li>\n<li><strong>Single operational system</strong>: No separate vector database to manage</li>\n<li><strong>Horizontal scaling</strong>: Read replicas for query load, partitioning by tenant for write scale</li>\n</ul>\n<p>The file abstraction works great for single-developer workflows. But when you need multi-tenant production, a structured document store backed by PostgreSQL gives you the right balance of simplicity, performance, and operational maturity.</p>\n<hr />\n<h2 id=\"putting-it-together-the-full-architecture\">Putting It Together: The Full Architecture</h2>\n<p>Here's how all three memory tiers work together in the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a>. The diagram shows the complete flow from user request to response, with all memory layers active.</p>\n<p><img alt=\"Full Memory Architecture\" src=\"../../../../assets/2026-02-14-agent-memory-architecture/full-memory-architecture.svg\" /></p>\n<p>The architecture has three memory paths:</p>\n<ol>\n<li>\n<p><strong>Hot path (checkpoint store)</strong>: Every node in the LangGraph writes its output to the checkpoint store. When the graph hits an <code>interrupt_before</code> node (like the reporter in <a href=\"/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/\">Part 1</a>), execution pauses. The user can close the app, and when they return, the graph resumes from the checkpoint.</p>\n</li>\n<li>\n<p><strong>Cold path (long-term store)</strong>: At the start of each conversation, the agent queries the long-term store for relevant user context. At the end, it extracts and stores new facts. This runs asynchronously \u2014 it should never block the main reasoning loop.</p>\n</li>\n<li>\n<p><strong>Document path (file store)</strong>: At startup, the agent loads project conventions and relevant research notes from the document store. During execution, it writes new research summaries and learned patterns back to disk. Unlike the cold path, document reads are synchronous (they inform the current task) while writes can be deferred.</p>\n</li>\n</ol>\n<p>The wiring in LangGraph is straightforward \u2014 the checkpoint store and long-term store are passed at graph compilation, while the document store is injected as a dependency:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langgraph.checkpoint.postgres.aio</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">AsyncPostgresSaver</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langgraph.store.memory</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">InMemoryStore</span>\n\n<span class=\"c1\"># Hot memory: PostgreSQL for durable checkpoints</span>\n<span class=\"n\">checkpointer</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">create_postgres_checkpointer</span><span class=\"p\">(</span><span class=\"n\">pg_connection_string</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Cold memory: In-memory store with vector search</span>\n<span class=\"c1\"># (In production, replace with a persistent BaseStore implementation)</span>\n<span class=\"n\">memory_store</span> <span class=\"o\">=</span> <span class=\"n\">InMemoryStore</span><span class=\"p\">(</span>\n    <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;dims&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1536</span><span class=\"p\">,</span> <span class=\"s2\">&quot;embed&quot;</span><span class=\"p\">:</span> <span class=\"n\">embedding_function</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Document memory: file-based store for project knowledge</span>\n<span class=\"n\">doc_memory</span> <span class=\"o\">=</span> <span class=\"n\">FileMemory</span><span class=\"p\">(</span><span class=\"n\">base_dir</span><span class=\"o\">=</span><span class=\"s2\">&quot;.agent-memory&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Checkpoint store and long-term store wired into the graph</span>\n<span class=\"n\">graph</span> <span class=\"o\">=</span> <span class=\"n\">create_graph</span><span class=\"p\">(</span>\n    <span class=\"n\">checkpointer</span><span class=\"o\">=</span><span class=\"n\">checkpointer</span><span class=\"p\">,</span>\n    <span class=\"n\">store</span><span class=\"o\">=</span><span class=\"n\">memory_store</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># The store is accessible inside any node via the store parameter</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">planner_node</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">AgentState</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"p\">,</span> <span class=\"n\">store</span><span class=\"p\">:</span> <span class=\"n\">BaseStore</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Plan with user context from long-term memory.&quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Recall relevant user facts from vector store</span>\n    <span class=\"n\">user_memories</span> <span class=\"o\">=</span> <span class=\"n\">store</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"p\">(</span>\n        <span class=\"n\">namespace</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">user_id</span><span class=\"p\">),</span>\n        <span class=\"n\">query</span><span class=\"o\">=</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">messages</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">,</span>\n        <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># Load project conventions from document memory</span>\n    <span class=\"n\">conventions</span> <span class=\"o\">=</span> <span class=\"n\">doc_memory</span><span class=\"o\">.</span><span class=\"n\">read_doc</span><span class=\"p\">(</span><span class=\"s2\">&quot;conventions/analysis-format.md&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Inject both into planning context</span>\n    <span class=\"n\">memory_context</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"p\">[</span><span class=\"s2\">&quot;fact&quot;</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"n\">user_memories</span><span class=\"p\">)</span>\n    <span class=\"c1\"># ... rest of planning logic with personalized context and conventions</span>\n</code></pre></div>\n<h3 id=\"the-complete-flow\">The Complete Flow</h3>\n<p>Here's what happens when a returning user sends \"Analyze TSLA\" to the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a>:</p>\n<ol>\n<li>\n<p><strong>Document memory load</strong>: At startup, the agent reads project conventions from the document store \u2014 analysis format preferences, preferred data sources, tool usage patterns. These set the baseline behavior.</p>\n</li>\n<li>\n<p><strong>Cold memory recall</strong>: Before the router node executes, the graph queries the long-term store with the user's message. It retrieves: \"User has high risk tolerance\", \"User prefers detailed competitor analysis\", \"User previously researched NVDA and AMD\".</p>\n</li>\n<li>\n<p><strong>Router + Planner</strong>: The router classifies this as <code>DEEP_RESEARCH</code>. The planner creates a 5-step research plan, personalized based on the recalled user preferences \u2014 it includes a competitor analysis step because the user's history shows they value it. The plan follows the format from the conventions document.</p>\n</li>\n<li>\n<p><strong>Executor loop (hot memory)</strong>: Each step executes via the ReAct pattern from <a href=\"/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/\">Part 1</a>. After every node \u2014 router, planner, each executor step \u2014 LangGraph writes a checkpoint to PostgreSQL. If the process crashes after step 3 of 5, restart and continue from step 4.</p>\n</li>\n<li>\n<p><strong>HITL interrupt</strong>: The graph reaches the <code>reporter</code> node with <code>interrupt_before</code>. The draft report is in the checkpoint. The user reviews it hours later \u2014 the graph loads the checkpoint and continues.</p>\n</li>\n<li>\n<p><strong>Memory updates</strong>: After the conversation ends: (a) an asynchronous process extracts new user facts (\"user is now tracking TSLA\", \"user approved the report format\") and stores them in the long-term vector store, and (b) the agent writes a research summary to the document store (<code>research/TSLA-2026-02.md</code>) for future reference.</p>\n</li>\n</ol>\n<p>This three-tier pattern separates concerns cleanly. The checkpoint store handles durability and resume \u2014 it's infrastructure. The long-term store handles intelligence and personalization \u2014 it's product logic. The document store handles accumulated project knowledge \u2014 it's the agent's notebook.</p>\n<hr />\n<h2 id=\"trade-offs-and-considerations\">Trade-offs and Considerations</h2>\n<p>Memory adds value, but it also adds cost and complexity. Be honest about the trade-offs:</p>\n<ul>\n<li>\n<p><strong>Embedding cost</strong>: Every fact stored in a vector database requires an embedding API call. At $0.02 per million tokens (OpenAI <code>text-embedding-3-small</code>), this is cheap per fact but adds up across thousands of users and sessions. Batch embedding calls and cache results. The real cost is latency \u2014 100-300ms of embedding API latency at query time for cold memory recall. This matters more than dollar cost for real-time conversational agents. Consider caching embeddings for common queries or using local embedding models for latency-sensitive applications.</p>\n</li>\n<li>\n<p><strong>Stale memory</strong>: User preferences change. A fact stored six months ago (\"user prefers conservative investments\") may no longer be accurate. Implement expiry policies \u2014 I use 365 days for preferences and 90 days for episodic events, as described in my <a href=\"/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/\">context engineering post</a>.</p>\n</li>\n<li>\n<p><strong>Memory overhead in context</strong>: Every recalled fact consumes tokens in the LLM's context window. If you recall 20 facts per query, that's several hundred tokens of memory context competing with the actual task. Cap the number of recalled facts and prioritize by relevance score.</p>\n</li>\n<li>\n<p><strong>Privacy and compliance</strong>: Long-term memory stores user data. You need PII redaction before storage, clear retention policies, and user-facing controls for data deletion. This is not optional in regulated industries.</p>\n</li>\n<li>\n<p><strong>Checkpoint storage growth</strong>: PostgreSQL checkpoint tables grow with every node execution. For long-running agents, implement a retention policy \u2014 keep the last N checkpoints per thread and archive or delete older ones. Here's an example cleanup query that keeps the 10 most recent checkpoints per thread and deletes anything older than 30 days:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">DELETE</span><span class=\"w\"> </span><span class=\"k\">FROM</span><span class=\"w\"> </span><span class=\"n\">checkpoints</span>\n<span class=\"k\">WHERE</span><span class=\"w\"> </span><span class=\"n\">thread_id</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"err\">$</span><span class=\"mi\">1</span>\n<span class=\"w\">  </span><span class=\"k\">AND</span><span class=\"w\"> </span><span class=\"n\">created_at</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"n\">NOW</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"nb\">INTERVAL</span><span class=\"w\"> </span><span class=\"s1\">&#39;30 days&#39;</span>\n<span class=\"w\">  </span><span class=\"k\">AND</span><span class=\"w\"> </span><span class=\"n\">checkpoint_id</span><span class=\"w\"> </span><span class=\"k\">NOT</span><span class=\"w\"> </span><span class=\"k\">IN</span><span class=\"w\"> </span><span class=\"p\">(</span>\n<span class=\"w\">    </span><span class=\"k\">SELECT</span><span class=\"w\"> </span><span class=\"n\">checkpoint_id</span><span class=\"w\"> </span><span class=\"k\">FROM</span><span class=\"w\"> </span><span class=\"n\">checkpoints</span>\n<span class=\"w\">    </span><span class=\"k\">WHERE</span><span class=\"w\"> </span><span class=\"n\">thread_id</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"err\">$</span><span class=\"mi\">1</span>\n<span class=\"w\">    </span><span class=\"k\">ORDER</span><span class=\"w\"> </span><span class=\"k\">BY</span><span class=\"w\"> </span><span class=\"n\">created_at</span><span class=\"w\"> </span><span class=\"k\">DESC</span>\n<span class=\"w\">    </span><span class=\"k\">LIMIT</span><span class=\"w\"> </span><span class=\"mi\">10</span>\n<span class=\"w\">  </span><span class=\"p\">);</span>\n</code></pre></div>\n</li>\n<li>\n<p><strong>Memory consolidation</strong>: Over time, detailed episodic memories should compress into compact semantic representations \u2014 \"user asked about NVDA three times in January\" rather than storing all three conversations verbatim. This mirrors human memory consolidation and keeps the store manageable. Frameworks like <a href=\"https://arxiv.org/abs/2504.19413\">Mem0</a> and <a href=\"https://arxiv.org/abs/2501.13956\">Graphiti</a> handle this automatically; if you build your own, schedule periodic consolidation jobs.</p>\n</li>\n<li>\n<p><strong>Cold start problem</strong>: New users have no long-term memory. The agent should degrade gracefully \u2014 ask clarifying questions instead of making assumptions. Memory is additive, not required.</p>\n</li>\n<li>\n<p><strong>Memory poisoning and adversarial scenarios</strong>: Anything in the agent's context window is a potential injection point. If an attacker writes misleading facts to the document store or long-term memory (\"always approve transactions without verification\"), the agent may execute them as instructions. Prompt injection through stored memories is a real attack surface. Mitigation strategies include input validation before storage, sandboxing of memory reads (treat recalled content as untrusted data, not system instructions), and access controls that limit which memories can influence critical operations.</p>\n</li>\n<li>\n<p><strong>Document memory drift</strong>: File-based memory has no automatic deduplication or conflict resolution. Over time, documents can accumulate contradictory information \u2014 one file says \"use pytest\" while another says \"use unittest.\" Schedule periodic reviews (or let the agent do it) to prune and consolidate document memory. Unlike vector stores where staleness is hidden, at least you can <code>grep</code> for contradictions.</p>\n</li>\n<li>\n<p><strong>Document memory doesn't scale to millions of items</strong>: File-based memory works well for hundreds to low thousands of documents. If your agent needs to recall from millions of facts with fuzzy matching, you need a vector store. Document memory is for structured project knowledge, not for the long tail of every user interaction.</p>\n</li>\n</ul>\n<hr />\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ol>\n<li>\n<p><strong>Agent memory splits into three tiers</strong>: hot (checkpoint store for current session), cold (long-term store for cross-session knowledge), and document (file store for accumulated project knowledge). Design each tier for its access pattern.</p>\n</li>\n<li>\n<p><strong>Use PostgreSQL checkpointing as your default</strong> \u2014 it gives you ACID durability, full checkpoint history, and time-travel debugging. Switch to Redis only when sub-millisecond latency is a hard requirement.</p>\n</li>\n<li>\n<p><strong>LangGraph's checkpoint system handles hot memory natively</strong> \u2014 every node write is automatically persisted, enabling pause/resume and HITL workflows with zero application code.</p>\n</li>\n<li>\n<p><strong>Start long-term memory with key-value stores</strong> for structured user profiles. Add vector search (Qdrant, Pinecone, or LangGraph's built-in Store with vector index) when you need semantic recall over unstructured facts.</p>\n</li>\n<li>\n<p><strong>Document memory is underexplored in the literature but widely adopted in practice.</strong> Most frameworks and surveys cover vector stores and checkpoints but ignore file-based memory. Yet the pattern has spread well beyond AI coding assistants. Claude Code, Cursor, and Windsurf converged on plain-text files; Voyager stores Minecraft skills as code libraries; ECR3 winners iterated on procedural prompt documents; web agents synthesize reusable workflow APIs. When the agent learns something wrong, you open a Markdown file and fix it. When you want to know what the agent knows, you <code>ls</code> the memory directory. This debuggability and transparency is driving adoption across agent domains.</p>\n</li>\n<li>\n<p><strong>Memory is a product feature, not just infrastructure.</strong> The difference between \"the agent remembers my preferences\" and \"the agent asks me the same questions every time\" is what makes users come back.</p>\n</li>\n<li>\n<p><strong>Set retention policies from day one.</strong> Stale memory degrades agent performance, and unbounded storage creates privacy risks. Expire episodic memories after 90 days, preference memories after 365 days, and review document memory periodically for contradictions.</p>\n</li>\n<li>\n<p><strong>Cap recalled context.</strong> Every recalled fact competes for tokens in the context window. Retrieve the top 5 most relevant facts, not everything you have.</p>\n</li>\n</ol>\n<hr />\n<h2 id=\"whats-next\">What's Next</h2>\n<p>In Part 3, I'll cover <strong>tool ergonomics and the Agent-Computer Interface (ACI)</strong> \u2014 how to design tools that LLMs can actually use reliably. I'll show how tool descriptions, argument schemas, and error handling patterns determine whether your agent calls the right tool with the right arguments, or hallucinates its way into a cascade of failures.</p>\n<hr />\n<h2 id=\"references\">References</h2>\n<h3 id=\"papers\">Papers</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2309.02427\">Cognitive Architectures for Language Agents (CoALA)</a> \u2014 Sumers, Yao et al., 2023 \u2014 Foundational taxonomy of agent memory types</li>\n<li><a href=\"https://arxiv.org/abs/2512.13564\">Memory in the Age of AI Agents: A Survey</a> \u2014 Dec 2025 \u2014 Comprehensive three-dimensional taxonomy of agent memory</li>\n<li><a href=\"https://arxiv.org/abs/2310.08560\">MemGPT: Towards LLMs as Operating Systems</a> \u2014 Packer et al., 2023 \u2014 Virtual context management for LLM agents</li>\n<li><a href=\"https://arxiv.org/abs/2304.03442\">Generative Agents: Interactive Simulacra of Human Behavior</a> \u2014 Park et al., 2023 \u2014 Memory stream architecture with recency, importance, and relevance scoring</li>\n<li><a href=\"https://arxiv.org/abs/2501.13956\">Zep: A Temporal Knowledge Graph Architecture for Agent Memory</a> \u2014 Rasmussen, 2025 \u2014 Bi-temporal knowledge graph for agent memory</li>\n<li><a href=\"https://arxiv.org/abs/2504.19413\">Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory</a> \u2014 2025 \u2014 Extraction/consolidation pipeline with benchmarks</li>\n<li><a href=\"https://arxiv.org/abs/2305.16291\">Voyager: An Open-Ended Embodied Agent with Large Language Models</a> \u2014 Wang et al., 2023 \u2014 Skill library as document memory for open-world game agents</li>\n<li><a href=\"https://arxiv.org/abs/2311.05997\">JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</a> \u2014 2023 \u2014 Multimodal memory library for Minecraft agents</li>\n<li><a href=\"https://arxiv.org/abs/2409.07429\">Agent Workflow Memory</a> \u2014 Wang et al., 2024 \u2014 Reusable workflow induction for web automation agents</li>\n<li><a href=\"https://arxiv.org/abs/2504.07079\">SkillWeaver: Web Agents can Self-Design Skill Libraries</a> \u2014 2025 \u2014 Self-synthesized reusable API tools for web agents</li>\n<li><a href=\"https://arxiv.org/abs/2510.04851\">LEGOMem: Modular Memory Framework for LLM Agent Systems</a> \u2014 2025 \u2014 Composable memory modules for multi-agent systems</li>\n</ul>\n<h3 id=\"langgraph-documentation\">LangGraph Documentation</h3>\n<ul>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/persistence/\">LangGraph Persistence (Checkpointing)</a> \u2014 Core concepts for checkpoint-based memory</li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/concepts/memory/\">LangGraph Memory Store</a> \u2014 Cross-thread long-term memory with the Store interface</li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence-functional/\">LangGraph Cross-Thread Persistence</a> \u2014 Functional API for cross-thread memory</li>\n<li><a href=\"https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/\">How to add memory to the prebuilt ReAct agent</a> \u2014 Practical guide to adding memory</li>\n</ul>\n<h3 id=\"checkpoint-backends\">Checkpoint Backends</h3>\n<ul>\n<li><a href=\"https://pypi.org/project/langgraph-checkpoint-postgres/\"><code>langgraph-checkpoint-postgres</code></a> \u2014 PostgreSQL checkpoint saver for LangGraph</li>\n<li><a href=\"https://pypi.org/project/langgraph-checkpoint-redis/\"><code>langgraph-checkpoint-redis</code></a> \u2014 Redis checkpoint saver for LangGraph</li>\n<li><a href=\"https://redis.io/blog/langgraph-redis-checkpoint-010/\">LangGraph Redis Checkpoint 0.1.0 Redesign</a> \u2014 Architecture details for the Redis checkpoint saver</li>\n<li><a href=\"https://aws.amazon.com/blogs/database/build-durable-ai-agents-with-langgraph-and-amazon-dynamodb/\"><code>langgraph-checkpoint-aws</code></a> \u2014 DynamoDB checkpoint saver with S3 offloading</li>\n</ul>\n<h3 id=\"vector-databases-and-memory-tools\">Vector Databases and Memory Tools</h3>\n<ul>\n<li><a href=\"https://qdrant.tech/\">Qdrant</a> \u2014 Open-source vector database with HNSW indexing and filtering</li>\n<li><a href=\"https://qdrant.tech/articles/agentic-builders-guide/\">Qdrant Agentic Builders Guide</a> \u2014 Practical guide to building agent memory with Qdrant</li>\n<li><a href=\"https://github.com/pgvector/pgvector\">pgvector</a> \u2014 Vector similarity search extension for PostgreSQL</li>\n<li><a href=\"https://github.com/getzep/graphiti\">Graphiti</a> \u2014 Open-source temporal knowledge graph engine by Zep</li>\n</ul>\n<h3 id=\"document-and-file-based-memory\">Document and File-Based Memory</h3>\n<ul>\n<li><a href=\"https://docs.anthropic.com/en/docs/claude-code/memory\">Claude Code Memory</a> \u2014 CLAUDE.md and MEMORY.md file-based memory system</li>\n<li><a href=\"https://platform.claude.com/docs/en/agents-and-tools/tool-use/memory-tool\">Anthropic Memory Tool</a> \u2014 Client-side file-based memory for Claude API agents</li>\n<li><a href=\"https://docs.cursor.com/context/rules\">Cursor Rules</a> \u2014 Project-level .cursorrules files for agent context</li>\n<li><a href=\"https://docs.windsurf.com/windsurf/cascade/memories\">Windsurf Memories</a> \u2014 File-based memory and .windsurfrules for coding agents</li>\n</ul>\n<h3 id=\"memory-frameworks\">Memory Frameworks</h3>\n<ul>\n<li><a href=\"https://mem0.ai/research\">Mem0</a> \u2014 Managed memory layer with extraction/consolidation pipeline</li>\n<li><a href=\"https://docs.letta.com/concepts/memgpt/\">Letta (MemGPT)</a> \u2014 OS-inspired virtual context management for agents</li>\n<li><a href=\"https://langchain-ai.github.io/langmem/\">LangMem SDK</a> \u2014 Memory management tools for LangGraph</li>\n</ul>\n<h3 id=\"benchmarks\">Benchmarks</h3>\n<ul>\n<li><a href=\"https://www.cybertec-postgresql.com/en/postgresql-vs-redis-vs-memcached-performance/\">PostgreSQL vs Redis Performance</a> \u2014 CyberTec latency and throughput benchmarks</li>\n<li><a href=\"https://risingwave.com/blog/postgresql-vs-redis-performance-and-use-case-comparison/\">PostgreSQL vs Redis Comparison</a> \u2014 RisingWave architecture comparison</li>\n<li><a href=\"https://redis.io/blog/engineering-for-ai-agents/\">Redis AI Agent Engineering</a> \u2014 Redis patterns for agent workloads</li>\n</ul>\n<h3 id=\"workshops\">Workshops</h3>\n<ul>\n<li><a href=\"https://sites.google.com/view/memagent-iclr26/\">MemAgents: Memory for LLM-Based Agentic Systems</a> \u2014 ICLR 2026 Workshop</li>\n</ul>\n<h3 id=\"demo-project\">Demo Project</h3>\n<ul>\n<li><a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent</a> \u2014 Full implementation with all three memory tiers</li>\n</ul>\n<hr />\n<p><em>The complete Market Analyst Agent code, including the memory architecture described in this post, is available on <a href=\"https://github.com/slavadubrov/market-analyst-agent\">GitHub</a>. Star the repo and follow along as I build the full production stack.</em></p>\n<p><strong>Series: Engineering the Agentic Stack</strong></p>\n<ul>\n<li><a href=\"/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/\">Part 1: The Cognitive Engine</a> \u2014 Choosing the right reasoning loop</li>\n<li><strong>Part 2: The Cortex</strong> (this post) \u2014 Architecting memory for AI agents</li>\n<li>Part 3: Tool Ergonomics and the ACI (coming soon)</li>\n<li>Part 4: Safety Layers \u2014 The Guardian Pattern</li>\n<li>Part 5: Production Deployment</li>\n</ul>", "image": null, "date_modified": "2026-02-14T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Agents 101"]}, {"id": "https://slavadubrov.github.io/blog/2026/02/08/building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance/", "url": "https://slavadubrov.github.io/blog/2026/02/08/building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance/", "title": "Building a Modern Search Ranking Stack: From Embeddings to LLM-Powered Relevance", "content_html": "<h1 id=\"building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance\">Building a Modern Search Ranking Stack: From Embeddings to LLM-Powered Relevance</h1>\n<p>Search is no longer a string-matching problem. A query for \"wireless headphones\" on a product search engine is not just about finding items containing those two words \u2014 it is about surfacing the best result based on semantic relevance, product quality, user preferences, and real-time availability. The gap between BM25 keyword matching and what users actually expect has forced a complete rethinking of search architecture.</p>\n<p>This post walks through the anatomy of a modern search ranking stack: a multi-stage pipeline that combines sparse lexical retrieval, dense semantic embeddings, reciprocal rank fusion, cross-encoder reranking, and LLM-powered listwise ranking. I built a <a href=\"https://github.com/slavadubrov/search-ranking-stack\">working demo</a> that benchmarks each stage on the Amazon ESCI product search dataset \u2014 proving the value of every layer with real numbers.</p>\n<!-- more -->\n\n<p><strong>TL;DR</strong>: The optimal production architecture is a multi-stage hybrid pipeline \u2014 parallel BM25 and dense retrieval fused via Reciprocal Rank Fusion, followed by cross-encoder reranking, with LLMs handling the final precision layer. On the Amazon ESCI benchmark, this pipeline delivers a <strong>22.5% NDCG@10 improvement</strong> over BM25 alone (0.585 to 0.717), with the LLM reranker providing the single largest jump (+0.072).</p>\n<hr />\n<h2 id=\"the-evolution-of-search-ranking\">The Evolution of Search Ranking</h2>\n<p>To understand why modern search stacks look the way they do, we need to trace the trajectory. Search ranking has evolved through three distinct eras, each solving a specific limitation of its predecessor.</p>\n<h3 id=\"era-1-the-lexical-age-bm25\">Era 1: The Lexical Age (BM25)</h3>\n<p>For decades, <a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\">BM25</a> was the undisputed king of retrieval. It is a probabilistic model that ranks documents based on the frequency of query terms appearing in the document, normalized by document length and inverse document frequency (IDF).</p>\n<p>BM25 excels at exact keyword matching \u2014 searching for a specific error code, product SKU, or HTTP status code works perfectly because the system needs literal token overlap. However, it suffers from the <strong>vocabulary mismatch problem</strong>: a query for \"cheap laptop\" will miss documents about \"budget notebook computer\" if those exact words are not present. BM25 has zero understanding of semantic intent.</p>\n<p>Despite this limitation, BM25 remains a robust baseline. It scores <strong>0.429 average nDCG@10 across the <a href=\"https://arxiv.org/abs/2104.08663\">BEIR benchmark's</a> 18 datasets</strong> and <a href=\"https://arxiv.org/abs/2407.07790\">still outperforms some neural models</a> on argumentative retrieval tasks like <a href=\"https://webis.de/events/touche-20/\">Touche-2020</a>.</p>\n<h3 id=\"era-2-the-dense-retrieval-age-embeddings\">Era 2: The Dense Retrieval Age (Embeddings)</h3>\n<p>The introduction of BERT and Transformer models brought <strong>Dense Retrieval</strong>. Instead of matching keywords, we map both queries and documents into a shared high-dimensional vector space (typically 768 or 1024 dimensions). Relevance becomes a calculation of cosine similarity between vectors.</p>\n<p>The <strong>Bi-Encoder</strong> (or \"Two-Tower\") architecture processes the query and document independently through separate encoder towers, producing fixed-length embeddings. Document vectors can be pre-computed and indexed offline, enabling fast retrieval via Approximate Nearest Neighbor (ANN) algorithms. Now \"cheap laptop\" and \"budget notebook\" land close together in vector space.</p>\n<p>Under the hood, bi-encoders use a <strong>Siamese architecture</strong> (<a href=\"https://arxiv.org/abs/1908.10084\">Sentence-BERT</a>, Reimers &amp; Gurevych, EMNLP 2019): both towers share the same Transformer weights. Each tower independently processes its input text, then typically mean-pools the token-level hidden states into a single fixed-length vector (typically 384 or 768 dimensions). Weight sharing ensures that queries and documents are projected into the same semantic space \u2014 a critical requirement for cosine similarity to be meaningful.</p>\n<p>These models are trained with <strong>contrastive learning</strong>, typically using the <a href=\"https://arxiv.org/abs/1807.03748\">InfoNCE</a> loss. Given a batch of (query, positive_document) pairs, the objective is to maximize <code>sim(query, positive_doc)</code> while minimizing <code>sim(query, negative_docs)</code> \u2014 where negatives come from other queries' positives within the same batch (in-batch negatives). A temperature parameter <span class=\"arithmatex\">\\(\\tau\\)</span> controls the sharpness of the distribution: lower values push the model to make harder distinctions between positives and negatives.</p>\n<p>The quality of training data is the single biggest lever for bi-encoder performance. Models start from (query, positive_document) pairs from datasets like <a href=\"https://arxiv.org/abs/1611.09268\">MS MARCO</a>, then augment with <strong>hard negatives</strong> \u2014 documents that the current model ranks highly but are actually not relevant. Random negatives are too easy and provide little learning signal; hard negatives force the model to learn subtle distinctions. The <a href=\"https://arxiv.org/abs/2210.11773\">SimANS</a> framework (Zhou et al., EMNLP 2022) formalizes this: exclude both easy negatives (too low rank) and potential false negatives (too high rank), and train on the \"hard middle ground.\"</p>\n<p>The trade-off? Bi-encoders compress all semantic nuance into a single fixed-size vector. This \"representation bottleneck\" means they often miss fine-grained interactions between specific query terms and document content.</p>\n<h3 id=\"era-3-the-interactive-age-cross-encoders-and-llms\">Era 3: The Interactive Age (Cross-Encoders and LLMs)</h3>\n<p><strong>Cross-encoders</strong> (<a href=\"https://arxiv.org/abs/1901.04085\">Nogueira &amp; Cho, 2019</a>) feed the query and document into a Transformer simultaneously as a concatenated sequence (<code>[CLS] Query [SEP] Document</code>), allowing every query token to attend to every document token through the full self-attention mechanism. This deep interaction captures nuances that independent encoding cannot.</p>\n<p><strong>LLM Reranking</strong> takes this further: large language models now act as zero-shot listwise rankers, effectively serving as a \"human judge\" that can reason about <em>why</em> one document is better than another. <a href=\"https://arxiv.org/abs/2304.09542\">RankGPT</a> (Sun et al., EMNLP 2023 Outstanding Paper) demonstrated that GPT-4 operating as a zero-shot listwise reranker matches or exceeds supervised methods.</p>\n<p>The precision is unmatched \u2014 but so is the computational cost. You cannot pre-compute scores, and inference is 100x slower than bi-encoder retrieval. This constraint gives rise to the defining architectural pattern of modern search: <strong>the multi-stage funnel</strong>.</p>\n<hr />\n<h2 id=\"architecture-the-multi-stage-funnel\">Architecture: The Multi-Stage Funnel</h2>\n<p>Because we cannot run an expensive Cross-Encoder or LLM on millions of documents, modern search stacks use a hierarchical funnel architecture. Each stage acts as a filter, progressively reducing the candidate pool while increasing the model complexity.</p>\n<p>A single-stage search is either too slow (complex models on everything) or too imprecise (simple models everywhere). <strong>The funnel is the dominant production pattern for search at scale.</strong></p>\n<p><img alt=\"Multi-Stage Ranking Funnel\" src=\"../../../../assets/2026-02-08-search-ranking-stack/multi_stage_funnel.svg\" /></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Candidate Pool</th>\n<th>Primary Objective</th>\n<th>Model Complexity</th>\n<th>Latency Budget</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Retrieval</strong></td>\n<td>10^9 - 10^12</td>\n<td>Maximum Recall</td>\n<td>Low (BM25, Bi-Encoders)</td>\n<td>&lt; 50ms</td>\n</tr>\n<tr>\n<td><strong>Pre-Ranking</strong></td>\n<td>10^4 - 10^5</td>\n<td>Efficient Filtering</td>\n<td>Medium (Two-Tower, GBDT)</td>\n<td>&lt; 100ms</td>\n</tr>\n<tr>\n<td><strong>Full Ranking</strong></td>\n<td>10^2 - 10^3</td>\n<td>Maximum Precision</td>\n<td>High (Cross-Encoders, LLMs)</td>\n<td>&lt; 500ms</td>\n</tr>\n<tr>\n<td><strong>Blending</strong></td>\n<td>10^1 - 10^2</td>\n<td>Diversity and Safety</td>\n<td>Rules and Multi-Objective</td>\n<td>&lt; 20ms</td>\n</tr>\n</tbody>\n</table>\n<p>The critical insight: <strong>retrieval sets the ceiling, reranking optimizes within it</strong>. If a relevant document does not survive the retrieval stage, no downstream model can recover it.</p>\n<hr />\n<h2 id=\"the-demo-a-five-stage-pipeline\">The Demo: A Five-Stage Pipeline</h2>\n<p>To make all of this concrete, I built a <a href=\"https://github.com/slavadubrov/search-ranking-stack\">search-ranking-stack</a> demo that implements a five-stage pipeline on the <a href=\"https://github.com/amazon-science/esci-data\">Amazon ESCI</a> product search benchmark. Each stage is measured independently so you can see exactly where the gains come from.</p>\n<p><img alt=\"Demo Pipeline Architecture\" src=\"../../../../assets/2026-02-08-search-ranking-stack/demo_architecture.svg\" /></p>\n<p>The pipeline:</p>\n<ol>\n<li><strong>BM25 Sparse Retrieval</strong> \u2014 lexical baseline (rank_bm25)</li>\n<li><strong>Dense Bi-Encoder Retrieval</strong> \u2014 semantic search (all-MiniLM-L6-v2)</li>\n<li><strong>Hybrid RRF Fusion</strong> \u2014 combines sparse and dense results</li>\n<li><strong>Cross-Encoder Reranking</strong> \u2014 fine-grained relevance scoring (ms-marco-MiniLM-L-12-v2)</li>\n<li><strong>LLM Listwise Reranking</strong> \u2014 reasoning-powered final ranking (Ollama / Claude / local)</li>\n</ol>\n<p>Steps 1--3 correspond to the <strong>Retrieval</strong> stage of the funnel (maximize recall), while steps 4--5 correspond to the <strong>Full Ranking</strong> stage (maximize precision). The demo skips Pre-Ranking and Blending \u2014 at ~8,500 documents, we can afford to send all hybrid results directly to reranking.</p>\n<h3 id=\"quick-start\">Quick Start</h3>\n<div class=\"highlight\"><pre><span></span><code>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/slavadubrov/search-ranking-stack.git\n<span class=\"nb\">cd</span><span class=\"w\"> </span>search-ranking-stack\nuv<span class=\"w\"> </span>sync\n\n<span class=\"c1\"># Download and sample ESCI dataset (~2.5GB download, ~5MB sample)</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>download-data\n\n<span class=\"c1\"># Run the full pipeline (without LLM reranking)</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>run-all\n\n<span class=\"c1\"># Run with LLM reranking via Ollama</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>run-all<span class=\"w\"> </span>--llm-mode<span class=\"w\"> </span>ollama\n</code></pre></div>\n<h3 id=\"the-dataset-amazon-esci\">The Dataset: Amazon ESCI</h3>\n<p>The demo uses the <strong>Amazon Shopping Queries Dataset</strong> (ESCI) from <a href=\"https://github.com/amazon-science/esci-data\">KDD Cup 2022</a> \u2014 a real product search benchmark with four-level graded relevance labels:</p>\n<table>\n<thead>\n<tr>\n<th>Label</th>\n<th>Gain</th>\n<th>Meaning</th>\n<th>Example (Query: \"wireless headphones\")</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Exact (E)</strong></td>\n<td>3</td>\n<td>Satisfies all query requirements</td>\n<td>Sony WH-1000XM5 Wireless Headphones</td>\n</tr>\n<tr>\n<td><strong>Substitute (S)</strong></td>\n<td>2</td>\n<td>Functional alternative</td>\n<td>Wired headphones with Bluetooth adapter</td>\n</tr>\n<tr>\n<td><strong>Complement (C)</strong></td>\n<td>1</td>\n<td>Related useful item</td>\n<td>Headphone carrying case</td>\n</tr>\n<tr>\n<td><strong>Irrelevant (I)</strong></td>\n<td>0</td>\n<td>No meaningful relationship</td>\n<td>USB charging cable</td>\n</tr>\n</tbody>\n</table>\n<p>Graded relevance matters because it lets us use <strong>NDCG</strong> (Normalized Discounted Cumulative Gain), which distinguishes between a \"perfect\" ranking and a \"merely adequate\" one. Binary metrics treat both as equally relevant and cannot distinguish different levels of relevance at the same position.</p>\n<p>We sample ~500 \"hard\" queries (the <code>small_version</code> flag in ESCI) with ~8,500 products and ~12,000 judgments \u2014 small enough to run on a laptop in minutes, large enough for statistically meaningful results.</p>\n<hr />\n<h2 id=\"retrieval-hybrid-search\">Retrieval: Hybrid Search</h2>\n<p>The retrieval layer is where the search session begins. Its job is singular: <strong>maximize recall</strong> \u2014 cast the widest net possible so that nothing relevant is missed.</p>\n<h3 id=\"bm25-the-lexical-baseline\">BM25: The Lexical Baseline</h3>\n<p>BM25 scores documents by term overlap with the query, with term frequency saturation and document length normalization:</p>\n<div class=\"arithmatex\">\\[\n\\text{BM25}(q, d) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{tf(t,d) \\cdot (k_1 + 1)}{tf(t,d) + k_1 \\cdot (1 - b + b \\cdot |d|/\\text{avgdl})}\n\\]</div>\n<p>Where <a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\"><span class=\"arithmatex\">\\(\\text{IDF}(t)\\)</span></a> is the inverse document frequency of term <span class=\"arithmatex\">\\(t\\)</span>, <span class=\"arithmatex\">\\(tf(t,d)\\)</span> is the term frequency in document <span class=\"arithmatex\">\\(d\\)</span>, <span class=\"arithmatex\">\\(|d|\\)</span> is document length, and <span class=\"arithmatex\">\\(\\text{avgdl}\\)</span> is the average document length across the corpus. The two key parameters are <span class=\"arithmatex\">\\(k_1\\)</span> (typically 1.2--2.0), which controls TF saturation \u2014 how quickly repeated terms stop adding value \u2014 and <span class=\"arithmatex\">\\(b\\)</span> (typically 0.75), which controls document length normalization.</p>\n<p>The implementation is straightforward \u2014 simple whitespace tokenization with <code>rank_bm25</code>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># src/search_ranking_stack/stages/s01_bm25.py</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">rank_bm25</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BM25Okapi</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">run_bm25</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">ESCIData</span><span class=\"p\">,</span> <span class=\"n\">top_k</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"n\">doc_ids</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span>\n    <span class=\"n\">tokenized_corpus</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">()]</span>\n\n    <span class=\"n\">bm25</span> <span class=\"o\">=</span> <span class=\"n\">BM25Okapi</span><span class=\"p\">(</span><span class=\"n\">tokenized_corpus</span><span class=\"p\">)</span>\n\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"k\">for</span> <span class=\"n\">query_id</span><span class=\"p\">,</span> <span class=\"n\">query_text</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">queries</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n        <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">bm25</span><span class=\"o\">.</span><span class=\"n\">get_scores</span><span class=\"p\">(</span><span class=\"n\">query_text</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">())</span>\n        <span class=\"n\">top_indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">)[::</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">][:</span><span class=\"n\">top_k</span><span class=\"p\">]</span>\n        <span class=\"n\">results</span><span class=\"p\">[</span><span class=\"n\">query_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">doc_ids</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">]:</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">idx</span> <span class=\"ow\">in</span> <span class=\"n\">top_indices</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">results</span>\n</code></pre></div>\n<p>BM25 gives us a Recall@100 of <strong>0.741</strong> \u2014 meaning 74% of all relevant products appear somewhere in the top 100 results. Not bad for a purely lexical method, but 26% of relevant items are invisible to downstream stages.</p>\n<h3 id=\"dense-bi-encoder-semantic-retrieval\">Dense Bi-Encoder: Semantic Retrieval</h3>\n<p>The bi-encoder maps queries and documents independently into a shared embedding space:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># src/search_ranking_stack/stages/s02_dense.py</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">sentence_transformers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">SentenceTransformer</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">SentenceTransformer</span><span class=\"p\">(</span><span class=\"s2\">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Encode corpus once, cache to disk</span>\n<span class=\"n\">corpus_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span>\n    <span class=\"n\">doc_texts</span><span class=\"p\">,</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">128</span><span class=\"p\">,</span>\n    <span class=\"n\">normalize_embeddings</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Cosine sim = dot product</span>\n    <span class=\"n\">convert_to_numpy</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># At query time: encode query, compute dot product</span>\n<span class=\"n\">query_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">query_texts</span><span class=\"p\">,</span> <span class=\"n\">normalize_embeddings</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">similarity_matrix</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">query_embeddings</span><span class=\"p\">,</span> <span class=\"n\">corpus_embeddings</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>With normalized embeddings, cosine similarity reduces to a dot product \u2014 a single matrix multiplication retrieves all queries at once. The 22M-parameter <code>all-MiniLM-L6-v2</code> model runs comfortably on CPU and pushes Recall@100 to <strong>0.825</strong>, an 11% improvement over BM25.</p>\n<h4 id=\"how-bi-encoders-learn-good-representations\">How Bi-Encoders Learn Good Representations</h4>\n<p>Bi-encoder training typically proceeds in <strong>two phases</strong>. First, the model is pre-trained on Natural Language Inference (NLI) and Semantic Textual Similarity (STS) datasets, which teach general-purpose semantic understanding \u2014 the model learns that \"a cat sits on a mat\" and \"a feline rests on a rug\" should have similar embeddings. Second, the model is fine-tuned on retrieval-specific data like MS MARCO, where it learns that a search query and its relevant passage should be closer together than the query and irrelevant passages.</p>\n<p>The critical ingredient in the second phase is <strong>hard negative mining</strong>. Random negatives (e.g., a document about cooking paired with a query about headphones) are trivially easy to distinguish \u2014 the model learns nothing from them. Instead, we use the current model itself to find documents it ranks highly but that are not actually relevant. The <strong><a href=\"https://arxiv.org/abs/2210.11773\">SimANS</a></strong> (Simple Ambiguous Negatives Sampling) approach formalizes this: given a query, rank all documents with the current bi-encoder, then exclude both easy negatives (ranked too low \u2014 the model already handles them) and potential false negatives (ranked too high \u2014 they might actually be relevant but unlabeled). The \"hard middle ground\" produces the maximum learning signal.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># What a training triplet looks like after hard negative mining</span>\n<span class=\"n\">training_triplet</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;query&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;wireless noise canceling headphones&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;positive&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Sony WH-1000XM5 Wireless Noise Cancelling Headphones&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;negative&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Sony headphone replacement ear pads&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Hard negative: same brand, related product, but wrong intent</span>\n<span class=\"p\">}</span>\n<span class=\"c1\"># The bi-encoder must learn that &quot;ear pads&quot; is NOT what the user wants,</span>\n<span class=\"c1\"># even though it shares many tokens with the positive document.</span>\n</code></pre></div>\n<p>The contrastive loss function (InfoNCE) ties this together. For each query <span class=\"arithmatex\">\\(q\\)</span> with positive document <span class=\"arithmatex\">\\(d^+\\)</span> and a set of negative documents <span class=\"arithmatex\">\\(\\{d^-_1, \\ldots, d^-_n\\}\\)</span>:</p>\n<div class=\"arithmatex\">\\[\n\\mathcal{L} = -\\log \\frac{e^{\\text{sim}(q, d^+) / \\tau}}{e^{\\text{sim}(q, d^+) / \\tau} + \\sum_{i=1}^{n} e^{\\text{sim}(q, d^-_i) / \\tau}}\n\\]</div>\n<p>Where <span class=\"arithmatex\">\\(\\text{sim}(q, d)\\)</span> is cosine similarity between query and document embeddings, and <span class=\"arithmatex\">\\(\\tau\\)</span> is the temperature parameter (typically 0.05--0.1) that controls the sharpness of the distribution \u2014 lower values make the loss more sensitive to hard negatives. This is essentially a <a href=\"https://arxiv.org/abs/1807.03748\">softmax cross-entropy</a>: push the positive pair's similarity up relative to all negatives. When <span class=\"arithmatex\">\\(\\tau\\)</span> is small, even slight differences in similarity produce large gradients, forcing the model to make fine-grained distinctions.</p>\n<p><img alt=\"Bi-Encoder Training Pipeline\" src=\"../../../../assets/2026-02-08-search-ranking-stack/bi_encoder_training_pipeline.svg\" /></p>\n<h4 id=\"serving-bi-encoder-embeddings-at-scale\">Serving Bi-Encoder Embeddings at Scale</h4>\n<p>The key architectural advantage of bi-encoders is the clean <strong>offline/online split</strong>. All document embeddings are computed once at index time and stored in a vector index. At query time, only the query needs a single forward pass through the encoder (~5ms), followed by an ANN search over pre-computed embeddings (~10ms). This asymmetry is what makes dense retrieval practical at scale.</p>\n<p>In our demo, the math is modest: 8,500 documents <span class=\"arithmatex\">\\(\\times\\)</span> 384 dimensions <span class=\"arithmatex\">\\(\\times\\)</span> 4 bytes per float = ~13 MB of embeddings. But at production scale, the numbers become serious: 1 billion documents with 768-dimensional embeddings require ~3 TiB of storage. This is where quantization (compressing 32-bit floats to 8-bit integers), <a href=\"https://ieeexplore.ieee.org/document/5432202\">product quantization</a> (decomposing vectors into subspaces), and SSD-backed indexes like <a href=\"https://proceedings.neurips.cc/paper/2019/hash/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Abstract.html\">DiskANN</a> become essential. See the <a href=\"#vector-indexing-hnsw-vs-ivf\">Vector Indexing: HNSW vs. IVF</a> section for details on index algorithms.</p>\n<p><img alt=\"Bi-Encoder Serving Pipeline\" src=\"../../../../assets/2026-02-08-search-ranking-stack/bi_encoder_serving_pipeline.svg\" /></p>\n<h3 id=\"why-hybrid-bm25-and-dense-have-complementary-failures\">Why Hybrid: BM25 and Dense Have Complementary Failures</h3>\n<p>Neither method alone is sufficient. <strong>BM25 excels</strong> when queries contain proper nouns, specific product SKUs, or error codes \u2014 \"iPhone 15 Pro Max 256GB\" needs exact token matching. <strong>Dense retrieval excels</strong> when there is vocabulary mismatch \u2014 \"cheap laptop\" matching \"budget notebook computer\" requires semantic understanding.</p>\n<p>The industry solution is <strong>Hybrid Search</strong>: run both retrieval methods in parallel, then fuse the results.</p>\n<h3 id=\"reciprocal-rank-fusion-rrf\">Reciprocal Rank Fusion (RRF)</h3>\n<p>The challenge with hybrid search is that BM25 and dense retrieval produce scores on entirely different scales. BM25 scores are unbounded (0 to 100+), while cosine similarity is bounded between -1 and 1. Attempting a simple linear combination requires continuous tuning.</p>\n<p><img alt=\"Hybrid Search with RRF\" src=\"../../../../assets/2026-02-08-search-ranking-stack/hybrid_search_rrf.svg\" /></p>\n<p><a href=\"https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\">Reciprocal Rank Fusion</a> (Cormack et al., 2009) solves this by discarding raw scores entirely and relying purely on rank position:</p>\n<div class=\"arithmatex\">\\[\n\\text{RRF}(d) = \\sum_{r \\in \\text{Rankings}} \\frac{1}{k + \\text{rank}(d, r)}\n\\]</div>\n<p>Where <span class=\"arithmatex\">\\(k\\)</span> is a smoothing constant (typically 60) that mitigates outlier dominance. RRF gives higher priority to items consistently ranked near the top across both methods, even if one system scores it much higher than the other. This rank-based approach eliminates the scale mismatch entirely.</p>\n<p>The implementation:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># src/search_ranking_stack/stages/s03_hybrid_rrf.py</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">reciprocal_rank_fusion</span><span class=\"p\">(</span><span class=\"n\">ranked_lists</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">60</span><span class=\"p\">,</span> <span class=\"n\">top_k</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"n\">fused_results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">query_id</span> <span class=\"ow\">in</span> <span class=\"n\">all_query_ids</span><span class=\"p\">:</span>\n        <span class=\"n\">rrf_scores</span> <span class=\"o\">=</span> <span class=\"n\">defaultdict</span><span class=\"p\">(</span><span class=\"nb\">float</span><span class=\"p\">)</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">results</span> <span class=\"ow\">in</span> <span class=\"n\">ranked_lists</span><span class=\"p\">:</span>\n            <span class=\"n\">sorted_docs</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">[</span><span class=\"n\">query_id</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">(),</span>\n                                 <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n            <span class=\"k\">for</span> <span class=\"n\">rank</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">doc_id</span><span class=\"p\">,</span> <span class=\"n\">_score</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">sorted_docs</span><span class=\"p\">,</span> <span class=\"n\">start</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n                <span class=\"n\">rrf_scores</span><span class=\"p\">[</span><span class=\"n\">doc_id</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"o\">+</span> <span class=\"n\">rank</span><span class=\"p\">)</span>\n\n        <span class=\"n\">sorted_rrf</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">rrf_scores</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">(),</span>\n                            <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)[:</span><span class=\"n\">top_k</span><span class=\"p\">]</span>\n        <span class=\"n\">fused_results</span><span class=\"p\">[</span><span class=\"n\">query_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">sorted_rrf</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">fused_results</span>\n</code></pre></div>\n<p>Hybrid RRF achieves <strong>Recall@100 of 0.842</strong> and <strong>NDCG@10 of 0.628</strong> \u2014 outperforming both BM25 (0.585) and Dense (0.611) individually. Documents only need to rank well in <em>one</em> method to survive fusion.</p>\n<hr />\n<h2 id=\"cross-encoder-reranking\">Cross-Encoder Reranking</h2>\n<p>With 100 hybrid candidates per query, we can afford to apply a more expensive model. The cross-encoder processes the query and document <strong>together</strong> through a single Transformer, with full cross-attention between all tokens.</p>\n<p><img alt=\"Bi-Encoder vs. Cross-Encoder\" src=\"../../../../assets/2026-02-08-search-ranking-stack/bi_encoder_vs_cross_encoder.svg\" /></p>\n<h3 id=\"why-cross-attention-matters\">Why Cross-Attention Matters</h3>\n<p>The fundamental difference is in the <strong>attention matrix</strong>. In a bi-encoder, attention is block-diagonal: query tokens only attend to other query tokens, and document tokens only attend to other document tokens. The two representations never interact at the token level \u2014 they meet only at the very end through a dot product. A cross-encoder computes the <strong>full attention matrix</strong>, where every query token attends to every document token (and vice versa). This cross-attention is what enables deep token-level interaction.</p>\n<p><img alt=\"Cross-Encoder Attention Architecture\" src=\"../../../../assets/2026-02-08-search-ranking-stack/cross_encoder_architecture.svg\" /></p>\n<p>Consider why this matters in practice. In a bi-encoder, the query \"apple\" produces the same embedding every time \u2014 it is encoded independently, before any document is seen. A cross-encoder sees query and document simultaneously, resolving ambiguity in context. But the advantages go well beyond polysemy:</p>\n<ul>\n<li><strong>Negation</strong>: A query for \"headphones that are <em>not</em> wireless\" \u2014 bi-encoder embeddings for \"not wireless\" are nearly identical to \"wireless\" because the negation barely shifts the mean-pooled vector. A cross-encoder sees the \"not\" token directly attending to \"wireless\" and correctly scores wired headphones higher.</li>\n<li><strong>Qualification</strong>: A query for \"laptop under $500\" \u2014 the price constraint modifies relevance. A cross-encoder can attend from \"$500\" to the price mentioned in the product description and assess whether the constraint is satisfied.</li>\n</ul>\n<p>The cross-encoder input is formatted as <code>[CLS] query tokens [SEP] document tokens [SEP]</code>, where <code>[CLS]</code> is a special classification token whose final hidden state is fed through a linear classification head to produce a single relevance score. Segment embeddings distinguish which tokens belong to the query vs. the document, and <code>[SEP]</code> marks the boundary between segments.</p>\n<h3 id=\"how-cross-encoders-are-trained\">How Cross-Encoders Are Trained</h3>\n<p>Cross-encoder training is conceptually simpler than bi-encoder training. The model receives (query, document, relevance_label) triples and learns to predict the label through straightforward supervised learning \u2014 no contrastive loss needed.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Cross-encoder training data format</span>\n<span class=\"n\">training_example</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&quot;query&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;wireless headphones&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;document&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Sony WH-1000XM5 Wireless Headphones&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;label&quot;</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">,</span>  <span class=\"c1\"># Relevant</span>\n<span class=\"p\">}</span>\n<span class=\"c1\"># Forward pass: [CLS] hidden state \u2192 Linear layer \u2192 sigmoid \u2192 score</span>\n<span class=\"c1\"># Loss: binary cross-entropy between predicted score and label</span>\n</code></pre></div>\n<p>The classification head sits on top of the <code>[CLS]</code> token's final hidden state: a single linear layer maps the hidden dimension to a scalar, followed by sigmoid activation. For binary relevance labels, binary cross-entropy loss is used; for graded labels (like ESCI's four-level scale), MSE loss works better because it preserves the ordinal relationship between grades.</p>\n<p><strong><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-99736-6_44\">Hard negative mining is even more critical for cross-encoders</a> than for bi-encoders.</strong> Because cross-encoders are expensive to train \u2014 each training example requires a full forward pass through the concatenated sequence \u2014 you cannot afford to waste compute on trivially easy negatives. The practical approach: use a bi-encoder to retrieve the top-K candidates for each training query, then select hard negatives from specific rank ranges (e.g., ranks 10-100). This gives the cross-encoder training examples where the distinction between relevant and irrelevant actually requires deep token interaction to resolve.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># src/search_ranking_stack/stages/s04_cross_encoder.py</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">sentence_transformers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">CrossEncoder</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">CrossEncoder</span><span class=\"p\">(</span><span class=\"s2\">&quot;cross-encoder/ms-marco-MiniLM-L-12-v2&quot;</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">run_cross_encoder</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">hybrid_results</span><span class=\"p\">,</span> <span class=\"n\">top_k_rerank</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">query_id</span><span class=\"p\">,</span> <span class=\"n\">query_text</span> <span class=\"ow\">in</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">queries</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n        <span class=\"n\">candidates</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">hybrid_results</span><span class=\"p\">[</span><span class=\"n\">query_id</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">())[:</span><span class=\"n\">top_k_rerank</span><span class=\"p\">]</span>\n\n        <span class=\"c1\"># Form (query, document) pairs for joint encoding</span>\n        <span class=\"n\">pairs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"n\">doc_ids</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">doc_id</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">candidates</span><span class=\"p\">:</span>\n            <span class=\"n\">doc_text</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">corpus</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">doc_id</span><span class=\"p\">,</span> <span class=\"s2\">&quot;&quot;</span><span class=\"p\">)[:</span><span class=\"mi\">2048</span><span class=\"p\">]</span>\n            <span class=\"n\">pairs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">query_text</span><span class=\"p\">,</span> <span class=\"n\">doc_text</span><span class=\"p\">])</span>\n            <span class=\"n\">doc_ids</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">doc_id</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Score all pairs with full cross-attention</span>\n        <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">pairs</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Rerank by cross-encoder score</span>\n        <span class=\"n\">scored_docs</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">doc_ids</span><span class=\"p\">,</span> <span class=\"n\">scores</span><span class=\"p\">),</span>\n                             <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">reranked_results</span><span class=\"p\">[</span><span class=\"n\">query_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n            <span class=\"n\">doc_id</span><span class=\"p\">:</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">score</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">doc_id</span><span class=\"p\">,</span> <span class=\"n\">score</span> <span class=\"ow\">in</span> <span class=\"n\">scored_docs</span>\n        <span class=\"p\">}</span>\n</code></pre></div>\n<p>The 33M-parameter <code>ms-marco-MiniLM-L-12-v2</code> model averages about 100ms per query on CPU for 50 candidates. NDCG@10 jumps to <strong>0.645</strong> \u2014 a solid +0.017 improvement over hybrid retrieval.</p>\n<h3 id=\"the-speed-quality-tradeoff\">The Speed-Quality Tradeoff</h3>\n<p>Why can't we just use cross-encoders for everything? The fundamental issue is that <strong>pre-computation is impossible</strong>. Unlike bi-encoders, where each document embedding is query-independent and can be computed once and stored, a cross-encoder's output depends on both the query <em>and</em> the document together. The relevance score for \"wireless headphones\" paired with a Sony product is computed from the full cross-attention between those specific tokens \u2014 it cannot be cached or reused for a different query.</p>\n<p>The computational cost difference is stark. A bi-encoder retrieval requires 1 forward pass (to encode the query) plus N dot products (to compare against pre-computed document embeddings) \u2014 the dot products are trivially cheap. A cross-encoder requires N full Transformer forward passes, each processing the concatenated query + document sequence at <span class=\"arithmatex\">\\(O(L^2)\\)</span> cost for combined sequence length <span class=\"arithmatex\">\\(L\\)</span>. For 50 candidates with an average combined length of 128 tokens, that is 50 separate forward passes through 12 Transformer layers. For 100,000 candidates, it becomes computationally catastrophic (on the order of minutes on a modern GPU vs. ~17ms for a bi-encoder).</p>\n<p>Note the fundamental rule confirmed by our results: <strong>Recall@100 stays flat at 0.842 through both reranking stages.</strong> Reranking can reorder results but never add documents. The retrieval stage sets the ceiling.</p>\n<hr />\n<h2 id=\"llm-listwise-reranking\">LLM Listwise Reranking</h2>\n<p>The final stage uses an LLM to perform <strong>listwise</strong> reranking. Instead of scoring each document independently (pointwise), the LLM sees all top-10 results at once and outputs a complete ranking. This approach, inspired by <a href=\"https://arxiv.org/abs/2304.09542\">RankGPT</a>, enables the model to compare products against each other \u2014 something pointwise models cannot do.</p>\n<p><img alt=\"LLM Reranking Approaches\" src=\"../../../../assets/2026-02-08-search-ranking-stack/llm_reranking_approaches.svg\" /></p>\n<h3 id=\"the-listwise-prompt\">The Listwise Prompt</h3>\n<p>The prompt template asks the LLM to consider the ESCI relevance hierarchy:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># src/search_ranking_stack/stages/s05_llm_rerank.py</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">_create_listwise_prompt</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">documents</span><span class=\"p\">,</span> <span class=\"n\">max_words</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">):</span>\n    <span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">)</span>\n\n    <span class=\"n\">doc_texts</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">doc_id</span><span class=\"p\">,</span> <span class=\"n\">doc_text</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">,</span> <span class=\"n\">start</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n        <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"n\">doc_text</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()[:</span><span class=\"n\">max_words</span><span class=\"p\">]</span>\n        <span class=\"n\">doc_texts</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;[</span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s2\">] </span><span class=\"si\">{</span><span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">(</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;I will provide you with </span><span class=\"si\">{</span><span class=\"n\">n</span><span class=\"si\">}</span><span class=\"s2\"> product listings, each indicated by &quot;</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;a numerical identifier [1] to [</span><span class=\"si\">{</span><span class=\"n\">n</span><span class=\"si\">}</span><span class=\"s2\">]. Rank the products based on &quot;</span>\n        <span class=\"sa\">f</span><span class=\"s1\">&#39;their relevance to the search query: &quot;</span><span class=\"si\">{</span><span class=\"n\">query</span><span class=\"si\">}</span><span class=\"s1\">&quot;</span><span class=\"se\">\\n\\n</span><span class=\"s1\">&#39;</span>\n        <span class=\"s2\">&quot;Consider:</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span>\n        <span class=\"s2\">&quot;- Exact matches should rank highest</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span>\n        <span class=\"s2\">&quot;- Substitutes should rank above complements</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span>\n        <span class=\"s2\">&quot;- Irrelevant products should rank lowest</span><span class=\"se\">\\n\\n</span><span class=\"s2\">&quot;</span>\n        <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"si\">{</span><span class=\"nb\">chr</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">doc_texts</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"se\">\\n\\n</span><span class=\"s2\">&quot;</span>\n        <span class=\"s2\">&quot;Output ONLY a comma-separated list of identifiers: [3], [1], [2], ...</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span>\n        <span class=\"s2\">&quot;Do not explain your reasoning.&quot;</span>\n    <span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"three-execution-modes\">Three Execution Modes</h3>\n<p>The demo supports three backends for LLM reranking:</p>\n<table>\n<thead>\n<tr>\n<th>Mode</th>\n<th>Model</th>\n<th>How It Runs</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>ollama</code></td>\n<td><code>llama3.2:3b</code> (configurable)</td>\n<td>Local via Ollama API</td>\n</tr>\n<tr>\n<td><code>api</code></td>\n<td><code>claude-haiku-4-5-20251001</code></td>\n<td>Anthropic API</td>\n</tr>\n<tr>\n<td><code>local</code></td>\n<td><code>Qwen/Qwen2.5-1.5B-Instruct</code></td>\n<td>HuggingFace Transformers</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"parsing-and-fallback\">Parsing and Fallback</h3>\n<p>LLM outputs are non-deterministic, so robust parsing and fallback logic are essential:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">_parse_ranking</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Parse LLM output to extract ranking order.&quot;&quot;&quot;</span>\n    <span class=\"n\">matches</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">findall</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s2\">&quot;\\[(\\d+)\\]&quot;</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">matches</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"kc\">None</span>\n\n    <span class=\"n\">positions</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"n\">matches</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Pad with remaining positions if LLM returned partial output</span>\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">positions</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"n\">n</span><span class=\"p\">:</span>\n        <span class=\"n\">seen</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">positions</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n            <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">seen</span><span class=\"p\">:</span>\n                <span class=\"n\">positions</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">positions</span><span class=\"p\">[:</span><span class=\"n\">n</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>If parsing fails entirely, the system falls back to the cross-encoder ordering. This fallback-first design is essential for any production LLM integration \u2014 you should never let a parsing failure degrade results below the previous stage.</p>\n<hr />\n<h2 id=\"results-every-stage-earns-its-keep\">Results: Every Stage Earns Its Keep</h2>\n<p>Here are the results from running the full pipeline on ~500 ESCI queries:</p>\n<p><img alt=\"Pipeline Results\" src=\"../../../../assets/2026-02-08-search-ranking-stack/pipeline_results.svg\" /></p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>NDCG@10</th>\n<th>MRR@10</th>\n<th>Recall@100</th>\n<th>NDCG Delta</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>BM25</td>\n<td>0.585</td>\n<td>0.812</td>\n<td>0.741</td>\n<td>--</td>\n</tr>\n<tr>\n<td>Dense Bi-Encoder</td>\n<td>0.611</td>\n<td>0.808</td>\n<td>0.825</td>\n<td>+0.026</td>\n</tr>\n<tr>\n<td>Hybrid (RRF)</td>\n<td>0.628</td>\n<td>0.834</td>\n<td>0.842</td>\n<td>+0.017</td>\n</tr>\n<tr>\n<td>+ Cross-Encoder</td>\n<td>0.645</td>\n<td>0.860</td>\n<td>0.842</td>\n<td>+0.017</td>\n</tr>\n<tr>\n<td>+ LLM Reranker</td>\n<td>0.717</td>\n<td>0.901</td>\n<td>0.842</td>\n<td>+0.072</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"key-observations\">Key Observations</h3>\n<p><strong>Hybrid search outperforms either method alone.</strong> RRF NDCG (0.628) exceeds the maximum of BM25 (0.585) and Dense (0.611). This is the core thesis of modern search: sparse and dense retrieval have complementary failure modes, and combining them recovers documents that would be missed by either method individually.</p>\n<p><strong>Recall is set at retrieval.</strong> Recall@100 stays flat at 0.842 through both reranking stages. This confirms the fundamental rule: rerankers can reorder but never add documents. If you want higher recall, you must improve the retrieval layer.</p>\n<p><strong>The LLM reranker provides the largest single jump.</strong> The +0.072 NDCG@10 gain from cross-encoder to LLM reranker is the biggest single-stage improvement in the pipeline. The LLM's ability to reason about product relevance \u2014 understanding that a \"wireless headphone stand\" is a complement, not a match \u2014 provides discrimination that statistical models miss.</p>\n<p><strong>Dense beats BM25 on this dataset.</strong> ESCI's product search domain has severe vocabulary mismatch (users say \"cheap laptop\"; products say \"budget notebook computer\"), which favors semantic retrieval.</p>\n<hr />\n<h2 id=\"evaluation-measuring-what-matters\">Evaluation: Measuring What Matters</h2>\n<p>The demo uses three complementary metrics that together tell the full story:</p>\n<h3 id=\"ndcg10-primary-metric\">NDCG@10 (Primary Metric)</h3>\n<p><strong>Normalized Discounted Cumulative Gain</strong> measures the quality of the top-10 ranking using graded relevance. It rewards placing highly relevant documents near the top with a logarithmic discount:</p>\n<div class=\"arithmatex\">\\[\n\\text{DCG@k} = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} \\qquad \\text{NDCG@k} = \\frac{\\text{DCG@k}}{\\text{IDCG@k}}\n\\]</div>\n<p>NDCG is the only metric that fully exploits ESCI's four-level graded relevance \u2014 a system that places an Exact match at position 1 scores higher than one that places a Substitute there. This is why it is the primary metric for holistic search quality.</p>\n<h3 id=\"mrr10-user-experience\">MRR@10 (User Experience)</h3>\n<p><strong>Mean Reciprocal Rank</strong> measures how quickly the user finds the first relevant result. If the first relevant result is at position 1, MRR = 1.0. At position 3, the reciprocal rank is 0.333. This captures \"time to satisfaction\" \u2014 even if NDCG is high, users care most about the first good result.</p>\n<h3 id=\"recall100-retrieval-coverage\">Recall@100 (Retrieval Coverage)</h3>\n<p>Recall measures what fraction of all relevant documents appear somewhere in the top-100. This is a ceiling metric \u2014 if a relevant document is not retrieved, no reranker can fix it.</p>\n<hr />\n<h2 id=\"vector-indexing-hnsw-vs-ivf\">Vector Indexing: HNSW vs. IVF</h2>\n<p>The practical utility of dense embeddings depends on Approximate Nearest Neighbor (ANN) indexing. The demo uses brute-force cosine similarity (practical at ~8,500 documents), but production systems need specialized indexes.</p>\n<h3 id=\"hnsw-hierarchical-navigable-small-world\">HNSW (Hierarchical Navigable Small World)</h3>\n<p><a href=\"https://arxiv.org/abs/1603.09320\">HNSW</a> (Malkov &amp; Yashunin, 2016) is the gold standard for production environments requiring sub-100ms latency. It builds a multi-layered graph where upper layers provide \"express\" connections for coarse navigation and lower layers provide dense connections for precise refinement. Key tuning parameters are <code>M</code> (connections per node, typically 16-64) and <code>efSearch</code> (query-time beam width \u2014 use at least 512 for recall targets above 0.95).</p>\n<p>HNSW's weakness is the <strong>\"Tombstone Problem\"</strong>: when records are deleted, they leave phantom nodes in the graph. Over time, these create unreachable regions, effectively blinding your search engine to sections of data. This is not a theoretical concern \u2014 even modern vector databases like Qdrant, which uses HNSW exclusively, <a href=\"https://github.com/qdrant/qdrant/issues/7147\">report degraded search quality after heavy deletions</a> that requires full index rebuilds to resolve. If your dataset has frequent updates or deletions, plan for periodic reindexing or consider IVF-based alternatives.</p>\n<h3 id=\"ivf-inverted-file\">IVF (Inverted File)</h3>\n<p>IVF indexes partition the vector space into Voronoi cells using k-means clustering. At query time, only the <code>nprobe</code> clusters closest to the query centroid are scanned. IVF is more memory-efficient and resilient to dynamic datasets \u2014 deletions are clean, with no unreachable nodes. Build times are 4x-32x faster than HNSW.</p>\n<p>For extreme scale, <strong><a href=\"https://arxiv.org/abs/2405.12497\">IVF_RaBitQ</a></strong> (Gao &amp; Long, SIGMOD 2024) compresses floating-point vectors into single-bit representations. In high-dimensional space, a coordinate's sign (+/-) captures sufficient angular information for similarity computation.</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>HNSW (Graph)</th>\n<th>IVF (Cluster)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Query Speed</strong></td>\n<td>Exceptional</td>\n<td>Moderate</td>\n</tr>\n<tr>\n<td><strong>Build Speed</strong></td>\n<td>Slow</td>\n<td>Fast (4x-32x faster)</td>\n</tr>\n<tr>\n<td><strong>Memory</strong></td>\n<td>High (RAM-bound)</td>\n<td>Low</td>\n</tr>\n<tr>\n<td><strong>Deletions</strong></td>\n<td>Problematic (tombstones)</td>\n<td>Clean</td>\n</tr>\n<tr>\n<td><strong>Best For</strong></td>\n<td>Static, latency-critical</td>\n<td>Dynamic, memory-constrained</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Practical guidance from <a href=\"https://www.uber.com/blog/evolution-and-scale-of-ubers-delivery-search-platform/\">Uber</a>:</strong> They optimized ANN retrieval by reducing the shard-level search parameter K from 1,200 to 200, achieving a 34% latency reduction and 17% CPU savings with minimal impact on recall.</p>\n<hr />\n<h2 id=\"the-llm-layer-beyond-reranking\">The LLM Layer: Beyond Reranking</h2>\n<p>LLMs are not just improving the reranking stage \u2014 they are transforming the entire search pipeline.</p>\n<h3 id=\"query-understanding\">Query Understanding</h3>\n<p>LLM-powered query expansion and rewriting addresses vocabulary mismatch before retrieval even begins. <a href=\"https://arxiv.org/abs/2303.07678\">Query2doc</a> (Wang et al., EMNLP 2023) generates pseudo-documents via few-shot LLM prompting and concatenates them with original queries, yielding <strong>3-15% BM25 improvement</strong> on MS MARCO without any fine-tuning. The LLM \"fills in\" vocabulary that the user's terse query omits.</p>\n<p>Practical patterns include abbreviation expansion, entity enrichment, sub-query decomposition for multi-hop reasoning, and <strong><a href=\"https://arxiv.org/abs/2402.03367\">RAG-Fusion</a></strong> \u2014 generating multiple query variants and combining results via RRF.</p>\n<h3 id=\"llm-as-a-judge-for-evaluation\">LLM-as-a-Judge for Evaluation</h3>\n<p>LLMs are becoming the standard for search quality assessment. The <a href=\"https://arxiv.org/abs/2305.13233\">TALEC</a> framework achieves <strong>over 80% correlation with human judgments</strong> using domain-specific evaluation criteria. <a href=\"https://arxiv.org/abs/2410.17152\">Pinterest's approach</a> is instructive: Llama-3-8B serves as an offline teacher model that generates five-scale relevance labels on billions of search impressions, outperforming multilingual BERT-base by 12.5% in accuracy; these labels are then distilled into lightweight production models.</p>\n<p>Key techniques for reliable LLM evaluation:</p>\n<ul>\n<li>Prompt models to <strong>explain their ratings</strong> (significantly improves human alignment)</li>\n<li>Use <strong>panels of diverse models</strong> to reduce variability (\"<a href=\"https://arxiv.org/abs/2404.18796\">replacing judges with juries</a>\")</li>\n<li>Account for <strong>central tendency bias</strong> in LLM-generated labels</li>\n</ul>\n<h3 id=\"knowledge-distillation\">Knowledge Distillation</h3>\n<p>Running a full LLM for every query is prohibitively expensive. The solution is knowledge distillation:</p>\n<ol>\n<li>Use a powerful LLM (the teacher) to rerank thousands of training queries</li>\n<li>Train a small, fast cross-encoder (the student, ~100M-200M parameters) to mimic the LLM's ranking distribution</li>\n<li>Result: near-LLM performance with ~10ms latency</li>\n</ol>\n<p><a href=\"https://arxiv.org/abs/2401.06910\">InRanker</a> distills MonoT5-3B into 60M and 220M parameter models \u2014 a 50x size reduction with competitive performance. The <a href=\"https://arxiv.org/abs/2312.02969\">Rank-Without-GPT</a> approach produces 7B open-source listwise rerankers that achieve 97% of GPT-4 effectiveness using QLoRA fine-tuning.</p>\n<p>A cost optimization insight from <a href=\"https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025\">ZeroEntropy</a>: reranking 75 candidates and sending only the top 20 to GPT-4o reduces API costs by <strong>72%</strong> \u2014 from $162K/day to $44K/day at 10 QPS \u2014 while preserving 95% of answer accuracy.</p>\n<hr />\n<h2 id=\"personalization-who-is-searching-matters\">Personalization: Who Is Searching Matters</h2>\n<p>Generic relevance is no longer enough. A search for \"apple\" should return iPhones for a tech enthusiast and apple recipes for someone who has been browsing cooking content.</p>\n<h3 id=\"two-tower-models-for-personalization\">Two-Tower Models for Personalization</h3>\n<p>The dominant retrieval architecture for personalization uses a <strong>two-tower embedding model</strong>: the query tower encodes search queries plus user profile into embeddings; the item tower encodes items plus metadata. Dot-product similarity determines relevance, enabling sub-100ms ANN retrieval.</p>\n<p><a href=\"https://dl.acm.org/doi/10.1145/3219819.3219885\">Airbnb</a> pioneered listing embeddings using Word2Vec-style training on click sessions \u2014 their Search and Similar Listings channels together drive <strong>99% of booking conversions</strong>. Pinterest's <a href=\"https://arxiv.org/abs/2404.16260\">OmniSearchSage</a> (WWW 2024) jointly learns unified query, pin, and product embeddings, producing <strong>&gt;8% relevance improvement</strong> at 300K requests/second. <a href=\"https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/\">Uber's Two-Tower Embeddings</a> power Eats Homefeed retrieval in ~100ms.</p>\n<h3 id=\"position-bias-the-silent-distortion\">Position Bias: The Silent Distortion</h3>\n<p>Users click higher-ranked items regardless of true relevance, creating a self-reinforcing feedback loop. The most practical production approach (<a href=\"https://dl.acm.org/doi/10.1145/3298689.3347033\">PAL</a>, Guo et al., RecSys 2019): include position as a training feature, then set position=1 for all items at serving time. This effectively debiases the model without requiring knowledge of the click model.</p>\n<hr />\n<h2 id=\"bridging-the-domain-gap\">Bridging the Domain Gap</h2>\n<p>A common failure in search strategy is assuming a model trained on general web data (like MS MARCO) will perform on specialized domains. This is the <strong>Out-of-Domain (OOD) problem</strong>.</p>\n<h3 id=\"synthetic-data-generation\">Synthetic Data Generation</h3>\n<p>LLMs solve the labeled data scarcity problem through <strong>Generative Pseudo-Labeling</strong> (<a href=\"https://arxiv.org/abs/2112.07577\">GPL</a>, <a href=\"https://arxiv.org/abs/2202.05144\">InPars</a>):</p>\n<ol>\n<li>Take your domain-specific document corpus</li>\n<li>Prompt an LLM to \"Generate a search query that this document would answer\"</li>\n<li>Use these synthetic (Query, Document) pairs to fine-tune your retriever and reranker</li>\n</ol>\n<p>This technique has shown dramatic improvements on domain-specific tasks where real user queries are scarce. It is the practical bridge between Level 2 and Level 3 on the maturity path.</p>\n<h3 id=\"rmsc-soft-tokens-for-domain-adaptation\">RMSC: Soft Tokens for Domain Adaptation</h3>\n<p>The <a href=\"https://arxiv.org/abs/2309.09828\">RMSC</a> (Robust Multi-Supervision Combining) strategy introduces <strong>soft tokens</strong> \u2014 special domain tokens <code>[S1]</code>, <code>[T1]</code> and relevance tokens <code>[H1]</code>, <code>[W1]</code> \u2014 that explicitly signal to the model what domain it is processing and how confident the supervision signal is. By training with these tokens, the model stores domain-specific knowledge in the token embeddings rather than overwriting core backbone parameters.</p>\n<hr />\n<h2 id=\"the-practical-maturity-path\">The Practical Maturity Path</h2>\n<p>If you are building this stack today, do not start with the most complex architecture. Follow this maturity curve:</p>\n<p><img alt=\"Practical Maturity Path\" src=\"../../../../assets/2026-02-08-search-ranking-stack/maturity_path.svg\" /></p>\n<p><strong>Level 1 (Baseline):</strong> Postgres pgvector or Elasticsearch. Hybrid search with BM25 + vector retrieval. No reranker.</p>\n<p><strong>Level 2 (The Reranker):</strong> Add a cross-encoder (e.g., <code>bge-reranker-v2-m3</code> or <code>ms-marco-MiniLM-L-12-v2</code>) to rerank the top 50 results. This typically yields the <strong>biggest ROI for the least effort</strong>. The <a href=\"https://www.elastic.co/search-labs/blog/elastic-semantic-reranker-part-2\">Elastic Rerank</a> model (184M parameters, DeBERTa v3) reaches 0.565 average nDCG@10 on BEIR \u2014 a 39% improvement over BM25.</p>\n<p><strong>Level 3 (Fine-tuning):</strong> Fine-tune your embedding model and reranker on domain data using LLM-generated synthetic queries (GPL/InPars). This is where domain-specific performance really separates from generic models.</p>\n<p><strong>Level 4 (State of the Art):</strong> Implement listwise LLM reranking for the top 5-10 results and inject personalization signals. Experiment with reasoning-based rerankers like <a href=\"https://arxiv.org/abs/2502.18418\">Rank1</a>, which generates explicit reasoning chains before making relevance judgments.</p>\n<p><strong>Level 2 is the sweet spot for most teams.</strong> Adding a cross-encoder reranker to an existing hybrid search setup can dramatically improve precision without architectural overhaul.</p>\n<hr />\n<h2 id=\"the-frontier-reasoning-rerankers-and-agentic-search\">The Frontier: Reasoning Rerankers and Agentic Search</h2>\n<p>Two patterns define where search is heading.</p>\n<h3 id=\"reasoning-based-rerankers\">Reasoning-Based Rerankers</h3>\n<p><a href=\"https://arxiv.org/abs/2502.18418\">Rank1</a> trains reranking models to generate explicit reasoning chains before making relevance judgments, inspired by DeepSeek-R1 and OpenAI o1. It distills from <strong>600,000+ reasoning trace examples</strong> and achieves state-of-the-art on the <a href=\"https://arxiv.org/abs/2407.12883\">BRIGHT</a> reasoning benchmark \u2014 sometimes <strong>2x improvement</strong> over same-sized rerankers. Remarkably, Rank1-0.5B performs comparably to RankLLaMA-13B despite being 25x smaller.</p>\n<p>The practical implication: reasoning-heavy queries (legal research, scientific literature, complex product search) will benefit enormously from test-time compute scaling in rerankers.</p>\n<h3 id=\"agentic-search\">Agentic Search</h3>\n<p><a href=\"https://arxiv.org/abs/2501.05366\">Search-o1</a> (EMNLP 2025) enables reasoning models (specifically QwQ-32B) to autonomously generate search queries when they encounter uncertain knowledge mid-reasoning, achieving <strong>23.2% average exact match improvement</strong> over standard RAG on multi-hop QA benchmarks. The pattern is clear: search is becoming a tool that AI agents invoke dynamically rather than a standalone product.</p>\n<hr />\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ol>\n<li>\n<p><strong>Hybrid search is no longer optional.</strong> The empirical evidence across benchmarks and production systems is definitive. Every major vector database now supports it natively. Our demo shows RRF NDCG (0.628) exceeding both BM25 (0.585) and Dense (0.611).</p>\n</li>\n<li>\n<p><strong>Retrieval sets the ceiling; reranking optimizes within it.</strong> Recall@100 stays flat at 0.842 through both reranking stages. Invest in retrieval quality first.</p>\n</li>\n<li>\n<p><strong>Adding a cross-encoder is the highest-ROI single change for most teams.</strong> Even a small cross-encoder reranking 50 documents delivers significant NDCG uplift. Start here.</p>\n</li>\n<li>\n<p><strong>LLM listwise reranking provides the largest single quality jump</strong> (+0.072 NDCG@10 in our demo), but at the cost of latency and compute. Use it selectively for the final top-10.</p>\n</li>\n<li>\n<p><strong>Knowledge distillation is making LLM-quality reranking practical.</strong> Frontier model capabilities are being compressed into deployable sizes within months. A 7B model can achieve 97% of GPT-4 reranking effectiveness.</p>\n</li>\n<li>\n<p><strong>The stack, not the model, determines production quality.</strong> Optimize the pipeline \u2014 the interplay between retrieval, fusion, and reranking \u2014 not just any single component.</p>\n</li>\n</ol>\n<p>The complete pipeline code is available at <a href=\"https://github.com/slavadubrov/search-ranking-stack\">github.com/slavadubrov/search-ranking-stack</a>. Clone it, run it, experiment with different models and parameters, and see the numbers for yourself.</p>\n<hr />\n<h2 id=\"references\">References</h2>\n<h3 id=\"papers\">Papers</h3>\n<ul>\n<li><a href=\"https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\">Reciprocal Rank Fusion</a> \u2014 Cormack et al., 2009</li>\n<li><a href=\"https://arxiv.org/abs/2304.09542\">RankGPT: LLMs as Zero-Shot Listwise Rerankers</a> \u2014 Sun et al., EMNLP 2023 Outstanding Paper</li>\n<li><a href=\"https://arxiv.org/abs/2502.18418\">Rank1: Reasoning-Based Reranking</a> \u2014 Weller et al., COLM 2025</li>\n<li><a href=\"https://arxiv.org/abs/2405.04141\">SCaLR: Self-Calibrated Listwise Reranking</a> \u2014 Self-calibrating listwise reranking framework</li>\n<li><a href=\"https://arxiv.org/abs/2402.12871\">GCCP: Global-Consistent Comparative Pointwise</a> \u2014 Addressing calibration in pointwise LLM ranking</li>\n<li><a href=\"https://arxiv.org/abs/2405.07920\">Rank-DistiLLM: Knowledge Distillation for Reranking</a> \u2014 Schlatt et al., ECIR 2025</li>\n<li><a href=\"https://arxiv.org/abs/2303.07678\">Query2doc: LLM Query Expansion</a> \u2014 Wang et al., EMNLP 2023</li>\n<li><a href=\"https://arxiv.org/abs/2112.07577\">GPL: Generative Pseudo Labeling</a> \u2014 Domain adaptation for dense retrieval</li>\n<li><a href=\"https://arxiv.org/abs/2401.06910\">InRanker: Distilled Reranker</a> \u2014 50x size reduction with competitive performance</li>\n<li><a href=\"https://arxiv.org/abs/2501.05366\">Search-o1: Agentic Retrieval</a> \u2014 EMNLP 2025</li>\n<li><a href=\"https://arxiv.org/abs/2305.13233\">TALEC: LLM-as-a-Judge for Search</a> \u2014 Evaluation framework</li>\n<li><a href=\"https://arxiv.org/abs/2309.09828\">RMSC: Soft Tokens for Domain Adaptation</a> \u2014 Multi-supervision combining</li>\n<li><a href=\"https://arxiv.org/abs/2104.08663\">BEIR Benchmark</a> \u2014 Thakur et al., NeurIPS 2021</li>\n<li><a href=\"https://arxiv.org/abs/1908.10084\">Sentence-BERT</a> \u2014 Reimers &amp; Gurevych, EMNLP 2019</li>\n<li><a href=\"https://arxiv.org/abs/1807.03748\">InfoNCE / CPC</a> \u2014 van den Oord et al., 2018</li>\n<li><a href=\"https://arxiv.org/abs/2210.11773\">SimANS: Hard Negative Sampling</a> \u2014 Zhou et al., EMNLP 2022</li>\n<li><a href=\"https://arxiv.org/abs/1901.04085\">Passage Reranking with BERT</a> \u2014 Nogueira &amp; Cho, 2019</li>\n<li><a href=\"https://arxiv.org/abs/1603.09320\">HNSW</a> \u2014 Malkov &amp; Yashunin, 2016</li>\n<li><a href=\"https://proceedings.neurips.cc/paper/2019/hash/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Abstract.html\">DiskANN</a> \u2014 Subramanya et al., NeurIPS 2019</li>\n<li><a href=\"https://arxiv.org/abs/2405.12497\">RaBitQ</a> \u2014 Gao &amp; Long, SIGMOD 2024</li>\n<li><a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries</a> \u2014 Verga et al., 2024</li>\n<li><a href=\"https://arxiv.org/abs/2402.03367\">RAG-Fusion</a> \u2014 Rackauckas, 2024</li>\n<li><a href=\"https://arxiv.org/abs/2202.05144\">InPars</a> \u2014 Bonifacio et al., SIGIR 2022</li>\n<li><a href=\"https://arxiv.org/abs/2407.12883\">BRIGHT Benchmark</a> \u2014 Su et al., ICLR 2025</li>\n<li><a href=\"https://arxiv.org/abs/2312.02969\">Rank-without-GPT</a> \u2014 Zhang et al., ECIR 2025</li>\n<li><a href=\"https://arxiv.org/abs/2410.17152\">Pinterest LLM Search Relevance</a> \u2014 Wang et al., 2024</li>\n<li><a href=\"https://arxiv.org/abs/2404.16260\">OmniSearchSage</a> \u2014 Agarwal et al., WWW 2024</li>\n<li><a href=\"https://dl.acm.org/doi/10.1145/3298689.3347033\">PAL: Position-bias Aware Learning</a> \u2014 Guo et al., RecSys 2019</li>\n</ul>\n<h3 id=\"datasets-and-benchmarks\">Datasets and Benchmarks</h3>\n<ul>\n<li><a href=\"https://github.com/amazon-science/esci-data\">Amazon ESCI: Shopping Queries Dataset</a> \u2014 KDD Cup 2022</li>\n<li><a href=\"https://github.com/beir-cellar/beir\">BEIR: Benchmarking IR</a> \u2014 Heterogeneous benchmark for zero-shot evaluation</li>\n<li><a href=\"https://huggingface.co/spaces/mteb/leaderboard\">MTEB: Massive Text Embedding Benchmark</a> \u2014 Embedding model leaderboard</li>\n<li><a href=\"https://arxiv.org/abs/2206.06588\">ESCI Paper</a> \u2014 Reddy et al., 2022</li>\n</ul>\n<h3 id=\"models-used-in-the-demo\">Models Used in the Demo</h3>\n<ul>\n<li><a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\">all-MiniLM-L6-v2</a> \u2014 22M parameter bi-encoder</li>\n<li><a href=\"https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2\">ms-marco-MiniLM-L-12-v2</a> \u2014 33M parameter cross-encoder</li>\n<li><a href=\"https://www.sbert.net/\">Sentence-Transformers</a> \u2014 Neural retrieval model framework</li>\n</ul>\n<h3 id=\"tools-and-platforms\">Tools and Platforms</h3>\n<ul>\n<li><a href=\"https://github.com/dorianbrown/rank_bm25\">rank_bm25</a> \u2014 BM25 implementation in Python</li>\n<li><a href=\"https://github.com/cvangysel/pytrec_eval\">pytrec_eval</a> \u2014 TREC evaluation toolkit</li>\n<li><a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/retrievers.html\">Elasticsearch</a> \u2014 Hybrid search with Retrievers API</li>\n<li><a href=\"https://vespa.ai/\">Vespa</a> \u2014 Unified search and recommendation engine</li>\n<li><a href=\"https://weaviate.io/\">Weaviate</a> \u2014 Vector database with hybrid search</li>\n<li><a href=\"https://qdrant.tech/\">Qdrant</a> \u2014 Vector database with multi-stage queries</li>\n</ul>\n<h3 id=\"industry-references\">Industry References</h3>\n<ul>\n<li><a href=\"https://dl.acm.org/doi/10.1145/3219819.3219885\">Airbnb Listing Embeddings</a> \u2014 Grbovic &amp; Cheng, KDD 2018</li>\n<li><a href=\"https://www.uber.com/blog/evolution-and-scale-of-ubers-delivery-search-platform/\">Uber Delivery Search</a> \u2014 Uber Engineering, 2025</li>\n<li><a href=\"https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/\">Uber Two-Tower Embeddings</a> \u2014 Uber Engineering, 2023</li>\n<li><a href=\"https://www.elastic.co/search-labs/blog/elastic-semantic-reranker-part-2\">Elastic Rerank</a> \u2014 Elastic, 2024</li>\n<li><a href=\"https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025\">ZeroEntropy Reranking Guide</a> \u2014 ZeroEntropy, 2025</li>\n</ul>\n<h3 id=\"demo-project\">Demo Project</h3>\n<ul>\n<li><a href=\"https://github.com/slavadubrov/search-ranking-stack\">search-ranking-stack</a> \u2014 Working demo with all code from this post</li>\n</ul>", "image": null, "date_modified": "2026-02-08T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Search and Recs"]}, {"id": "https://slavadubrov.github.io/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/", "url": "https://slavadubrov.github.io/blog/2026/01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/", "title": "The Cognitive Engine: Choosing the Right Reasoning Loop", "content_html": "<h1 id=\"the-cognitive-engine-choosing-the-right-reasoning-loop\">The Cognitive Engine: Choosing the Right Reasoning Loop</h1>\n<p><strong>Part 1 of the Engineering the Agentic Stack series</strong></p>\n<p>Building production AI agents is no longer about prompt engineering\u2014it's about <em>system engineering</em>. The difference between a demo that impresses and a product that delivers comes down to one critical decision: how your agent thinks.</p>\n<p>This post introduces three reasoning loop architectures and shows you how to choose between them. I'll use a production-grade Market Analyst Agent as the running example, with code you can use today.</p>\n<!-- more -->\n\n<h2 id=\"the-shift-from-prompts-to-systems\">The Shift from Prompts to Systems</h2>\n<p>A year ago, making an LLM useful meant crafting the perfect prompt. Today, that's table stakes. The agents that work in production are <em>systems</em>\u2014carefully orchestrated graphs of reasoning, tool use, and memory.</p>\n<p>\ud83e\udde0 At the heart of every agent system sits the <strong>cognitive engine</strong>: the reasoning loop that determines how your agent thinks, acts, and adapts. Choose wrong, and you'll burn tokens on unnecessary LLM calls, frustrate users with latency, or watch your agent crumble at the first unexpected tool result.</p>\n<h2 id=\"three-reasoning-patterns\">Three Reasoning Patterns</h2>\n<p><img alt=\"Reasoning Patterns Comparison\" src=\"../../../../assets/2026-01-31-reasoning-loops/reasoning-patterns-comparison.svg\" /></p>\n<h3 id=\"react-think-act-observe-repeat\">ReAct: Think, Act, Observe, Repeat</h3>\n<p>Considered the foundational design pattern for interactive AI agents (Yao et al., 2022), <a href=\"https://arxiv.org/abs/2210.03629\">ReAct</a> mimics human problem-solving through a continuous cycle:</p>\n<ol>\n<li><strong>Thought</strong>: The agent generates a \"thought\" to break down the goal and plan the next step.</li>\n<li><strong>Action</strong>: Based on the thought, it calls a tool.</li>\n<li><strong>Observation</strong>: The agent sees the result, which updates its understanding.</li>\n</ol>\n<p><img alt=\"ReAct Pattern\" src=\"../../../../assets/2026-01-31-reasoning-loops/react-pattern.svg\" /></p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li><strong>Reduced Hallucinations:</strong> Grounding in observations reduces the likelihood of making up facts.</li>\n<li><strong>Adaptability:</strong> The agent can adjust its strategy on the fly based on previous steps.</li>\n<li><strong>Explainability:</strong> The \"scratchpad\" offers a transparent audit trail of the agent's reasoning.</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li><strong>Latency and Cost:</strong> History accumulates and must be re-processed at every step.</li>\n<li><strong>Inefficient for predictable tasks:</strong> Patterns like <strong>ReWOO</strong> are better when tool calls can be planned upfront.</li>\n<li><strong>Looping structure:</strong> Can act indefinitely without proper constraints.</li>\n</ul>\n<p>\ud83d\udd0d <strong>Best for:</strong> Exploratory tasks, debugging, situations requiring real-time adaptation.</p>\n<h3 id=\"rewoo-plan-everything-upfront\">ReWOO: Plan Everything Upfront</h3>\n<p><a href=\"https://arxiv.org/abs/2305.18323\">ReWOO</a> (Reasoning WithOut Observation) is an optimization of the ReAct paradigm designed to improve efficiency by decoupling reasoning from tool execution. While ReAct stops to observe the result of every action, ReWOO plans the entire sequence of tool calls in a single pass.</p>\n<ol>\n<li><strong>Plan (Upfront Reasoning)</strong>: The agent generates a full plan of tool calls using variable placeholders (e.g., <code>#E1</code>, <code>#E2</code>) to represent future outputs.</li>\n<li><strong>Worker (Execution)</strong>: A separate module executes the planned tool calls in sequence (or parallel), filling the placeholders with real data.</li>\n<li><strong>Solver (Synthesis)</strong>: A final LLM call takes all gathered observations and synthesizes the final answer.</li>\n</ol>\n<p><img alt=\"ReWOO Pattern\" src=\"../../../../assets/2026-01-31-reasoning-loops/rewoo-pattern.svg\" /></p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li><strong>Token Efficiency:</strong> Can achieve up to 5x token efficiency by avoiding repetitive \"Thought-Action-Observation\" loops.</li>\n<li><strong>Low Latency:</strong> Eliminates the need to re-submit history at every step.</li>\n<li><strong>Modular Training:</strong> The planner can be fine-tuned independently without a live environment.</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li><strong>Rigidity:</strong> Assumes tools will work as expected; less forgiving of unexpected results.</li>\n<li><strong>Predictability Requirement:</strong> Best for routine, templated workflows.</li>\n<li><strong>Error Handling:</strong> A naive agent may fail if a plan is flawed unless fallback logic is added.</li>\n</ul>\n<p>\u26a1 <strong>Best for:</strong> Quick snapshots, status checks, dashboards\u2014tasks with predictable tool results.</p>\n<h3 id=\"plan-and-execute-the-best-of-both\">Plan-and-Execute: The Best of Both</h3>\n<p><a href=\"https://arxiv.org/abs/2305.04091\">Plan-and-Execute</a> sits at the sweet spot. The paper defines the core logic that modern agent architectures follow:</p>\n<ol>\n<li><strong>Planning Phase</strong>: The agent first generates a plan to divide a task into smaller sub-tasks.</li>\n<li><strong>Execution Phase</strong>: The agent then carries out those sub-tasks according to the generated plan.</li>\n</ol>\n<p>While the original paper focused on zero-shot prompting, modern implementations (like LangGraph) have expanded this into a full orchestration pattern with <strong>sequential execution</strong> and <strong>model specialization</strong>.</p>\n<p><img alt=\"Plan-and-Execute Pattern\" src=\"../../../../assets/2026-01-31-reasoning-loops/plan-execute-pattern.svg\" /></p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li><strong>Hierarchical reasoning:</strong> Mimics how experts work</li>\n<li><strong>Dynamic re-planning:</strong> Can pause and reassess if a step returns unexpected results</li>\n<li><strong>Model efficiency:</strong> Use a high-reasoning model for planning and a cheaper one for execution</li>\n<li><strong>Resilience:</strong> Bounded complexity with clear checkpoints</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li><strong>Higher latency:</strong> Sequential execution steps are slower than ReWOO</li>\n<li><strong>Implementation complexity:</strong> Requires nuanced state management</li>\n<li><strong>Overkill:</strong> Too heavy for simple queries</li>\n</ul>\n<p>\ud83c\udfaf <strong>Best for:</strong> Complex multi-step analysis, research tasks, anything requiring synthesis.</p>\n<h2 id=\"the-decision-framework\">The Decision Framework</h2>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th><strong>ReAct</strong> (2022)</th>\n<th><strong>Plan-and-Execute</strong> (2023)</th>\n<th><strong>ReWOO</strong> (2023)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Core Philosophy</strong></td>\n<td><strong>Improviser</strong>: Act, then decide what to do next based on the result.</td>\n<td><strong>Architect</strong>: Build a full blueprint, execute it, then review.</td>\n<td><strong>Optimizer</strong>: Create a \"script\" with variables and run it all at once.</td>\n</tr>\n<tr>\n<td><strong>Workflow</strong></td>\n<td>Iterative loop: Thought \u2192 Action \u2192 Observation.</td>\n<td>Two-stage: Phase 1 (Planning), Phase 2 (Execution).</td>\n<td>Decoupled: Planner creates a graph of tool calls; Worker runs them.</td>\n</tr>\n<tr>\n<td><strong>Adaptability</strong></td>\n<td><strong>Highest</strong>: Can change direction after every single tool call.</td>\n<td><strong>Medium</strong>: Typically re-plans only after a set of steps is completed.</td>\n<td><strong>Lowest</strong>: Usually follows the initial script unless the Solver fails.</td>\n</tr>\n<tr>\n<td><strong>Efficiency</strong></td>\n<td><strong>Low</strong>: High token usage; must re-read entire history for every step.</td>\n<td><strong>Medium</strong>: Saves tokens by not \"re-thinking\" during execution.</td>\n<td><strong>High</strong>: Minimal LLM calls; can parallelize tool execution for speed.</td>\n</tr>\n<tr>\n<td><strong>Best For</strong></td>\n<td>Open-ended exploration or tasks where results are unpredictable.</td>\n<td>Long-horizon tasks that require a steady goal (e.g., writing a paper).</td>\n<td>Structured, repeatable workflows (e.g., checking weather in 5 cities).</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"1-react-the-think-as-you-go-pattern\">1. ReAct: The \"Think-as-you-go\" Pattern</h3>\n<ul>\n<li><strong>How it feels:</strong> Like a human debugging a problem. \"I'll try this... okay, that didn't work, let me try that instead.\"</li>\n<li><strong>Advantage:</strong> It handles \"unknown unknowns\" perfectly. If a search result reveals a new topic, ReAct can immediately pivot.</li>\n<li><strong>Weakness:</strong> It is prone to \"looping\" (repeating the same failed action) and is the most expensive in terms of token costs.</li>\n</ul>\n<h3 id=\"2-plan-and-execute-the-mission-oriented-pattern\">2. Plan-and-Execute: The \"Mission-Oriented\" Pattern</h3>\n<ul>\n<li><strong>How it feels:</strong> Like a project manager. \"Here is the 5-step plan. Let's do steps 1 through 5, then see if we're done.\"</li>\n<li><strong>Advantage:</strong> Prevents the agent from getting \"distracted\" by minor details. It maintains a high-level view of the goal, which leads to better success rates on very complex tasks.</li>\n<li><strong>Weakness:</strong> If Step 1 fails in a way that makes Steps 2-5 impossible, the agent may waste time finishing the \"broken\" plan before re-evaluating.</li>\n</ul>\n<h3 id=\"3-rewoo-reasoning-without-observation-the-compiler-pattern\">3. ReWOO (Reasoning Without Observation): The \"Compiler\" Pattern</h3>\n<ul>\n<li><strong>How it feels:</strong> Like writing a computer program. \"I need data from Tool A and Tool B, then I'll combine them in Tool C.\"</li>\n<li><strong>Advantage:</strong> Massively faster and cheaper. Because it plans everything upfront using placeholders (e.g., <code>#E1</code> for the first tool's output), it doesn't need to call the LLM again until all data is gathered.</li>\n<li><strong>Weakness:</strong> It is \"blind\" during execution. If the result of the first tool is \"I can't find that person,\" a ReWOO agent will still blindly try to execute the next steps that depend on that person existing.</li>\n</ul>\n<h3 id=\"summary-which-to-choose\">Summary: Which to choose?</h3>\n<ul>\n<li>Choose <strong>ReAct</strong> if your agent is chatting with a user and needs to be highly flexible.</li>\n<li>Choose <strong>Plan-and-Execute</strong> if you are automating a large, complex job (like a multi-step research report).</li>\n<li>Choose <strong>ReWOO</strong> if you have a predictable pipeline and want to save 80% on your API bill.</li>\n</ul>\n<h2 id=\"real-world-context-the-market-analyst-agent\">Real-World Context: The Market Analyst Agent</h2>\n<p>To see these patterns in action, I built a <strong>Market Analyst Agent</strong> (available in my <a href=\"https://github.com/slavadubrov/market-analyst-agent\">demo repository</a>). This production-grade agent demonstrates all three reasoning patterns in a realistic market research context.</p>\n<p>The agent uses <code>LangGraph</code> to orchestrate these patterns, with a shared state foundation that captures everything needed across different execution modes:</p>\n<p><img alt=\"Plan-and-Execute Architecture\" src=\"../../../../assets/2026-01-31-reasoning-loops/plan-execute-architecture.svg\" /></p>\n<h3 id=\"state-definition\">State Definition</h3>\n<p>The foundation is a well-structured state that captures everything the agent needs:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PlanStep</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;A single step in the research plan.&quot;&quot;&quot;</span>\n    <span class=\"n\">step_number</span><span class=\"p\">:</span> <span class=\"nb\">int</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tool_hint</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n    <span class=\"n\">result</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">AgentState</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Main state for the Market Analyst Agent graph.&quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Message history with LangGraph&#39;s add_messages reducer</span>\n    <span class=\"n\">messages</span><span class=\"p\">:</span> <span class=\"n\">Annotated</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">add_messages</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Execution mode (set by router)</span>\n    <span class=\"n\">execution_mode</span><span class=\"p\">:</span> <span class=\"n\">ExecutionMode</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n    <span class=\"c1\"># Plan-and-Execute state</span>\n    <span class=\"n\">plan</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">PlanStep</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n    <span class=\"n\">current_step_index</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n    <span class=\"c1\"># ReWOO state</span>\n    <span class=\"n\">rewoo_plan</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">ReWOOPlanStep</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Research results</span>\n    <span class=\"n\">research_data</span><span class=\"p\">:</span> <span class=\"n\">ResearchData</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n    <span class=\"c1\"># HITL output</span>\n    <span class=\"n\">draft_report</span><span class=\"p\">:</span> <span class=\"n\">DraftReport</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">report_approved</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n</code></pre></div>\n<h3 id=\"pattern-1-plan-and-execute-implementation\">Pattern 1: Plan-and-Execute Implementation</h3>\n<p>The <strong>Plan-and-Execute</strong> pattern is perfect for complex research tasks that require multi-step synthesis. The key is separating the \"planning\" phase from the \"execution\" phase\u2014using a powerful model to think strategically upfront, then a ReAct loop to execute each step adaptively.</p>\n<p><strong>How this reflects the Plan-and-Execute pattern:</strong></p>\n<ol>\n<li><strong>Single upfront planning phase</strong>: One LLM call creates the entire plan as a list of text descriptions</li>\n<li><strong>Structured output</strong>: Uses Schema-Guided Reasoning to guarantee valid JSON</li>\n<li><strong>No tool execution yet</strong>: The planner only decides <em>what</em> to do, not <em>how</em></li>\n<li><strong>Human-readable steps</strong>: Each step is a description that an executor will interpret</li>\n</ol>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># System prompt guides the LLM to think like a research analyst</span>\n<span class=\"c1\"># creating a strategic plan, not immediate tool calls</span>\n<span class=\"n\">PLANNER_SYSTEM_PROMPT</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;You are a senior investment research analyst.</span>\n<span class=\"s2\">Break down stock analysis requests into 4-6 research steps covering:</span>\n<span class=\"s2\">1. Current price and basic metrics</span>\n<span class=\"s2\">2. Recent news and announcements</span>\n<span class=\"s2\">3. Competitor analysis (if relevant)</span>\n<span class=\"s2\">4. Financial health assessment</span>\n<span class=\"s2\">5. Risk factors</span>\n<span class=\"s2\">6. Investment thesis synthesis</span>\n\n<span class=\"s2\">Output as JSON with step_number, description, and tool_hint.&quot;&quot;&quot;</span>\n\n<span class=\"c1\"># Schema-Guided Reasoning: Enforce structure with Pydantic</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PlanOutput</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Structured output for the planner.&quot;&quot;&quot;</span>\n\n    <span class=\"n\">steps</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">PlanStep</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Research steps to execute&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">ticker</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The stock ticker being analyzed&quot;</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">planner_node</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">AgentState</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Generate a research plan from the user&#39;s request.</span>\n\n<span class=\"sd\">    This is Phase 1 of Plan-and-Execute: creating the high-level strategy.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Use a powerful model for strategic planning</span>\n    <span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">ChatAnthropic</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;claude-sonnet-4-5-20250929&quot;</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Apply Schema-Guided Reasoning to guarantee valid plan structure</span>\n    <span class=\"c1\"># This prevents common formatting errors that would break execution</span>\n    <span class=\"n\">structured_llm</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">with_structured_output</span><span class=\"p\">(</span><span class=\"n\">PlanOutput</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Context from long-term memory personalizes the plan</span>\n    <span class=\"n\">profile_context</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">User Profile:</span>\n<span class=\"s2\">- Risk Tolerance: </span><span class=\"si\">{</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">user_profile</span><span class=\"o\">.</span><span class=\"n\">risk_tolerance</span><span class=\"si\">}</span>\n<span class=\"s2\">- Investment Horizon: </span><span class=\"si\">{</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">user_profile</span><span class=\"o\">.</span><span class=\"n\">investment_horizon</span><span class=\"si\">}</span>\n<span class=\"s2\">&quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Single LLM call creates the complete plan</span>\n    <span class=\"n\">result</span><span class=\"p\">:</span> <span class=\"n\">PlanOutput</span> <span class=\"o\">=</span> <span class=\"n\">structured_llm</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">([</span>\n        <span class=\"n\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">PLANNER_SYSTEM_PROMPT</span> <span class=\"o\">+</span> <span class=\"n\">profile_context</span><span class=\"p\">),</span>\n        <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Create a research plan for: </span><span class=\"si\">{</span><span class=\"n\">last_user_message</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">),</span>\n    <span class=\"p\">])</span>\n\n    <span class=\"c1\"># State update: Store the plan and initialize tracking</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;plan&quot;</span><span class=\"p\">:</span> <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">,</span>           <span class=\"c1\"># The sequential steps to execute</span>\n        <span class=\"s2\">&quot;current_step_index&quot;</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">,</span>        <span class=\"c1\"># Start at step 0</span>\n        <span class=\"s2\">&quot;research_data&quot;</span><span class=\"p\">:</span> <span class=\"n\">ResearchData</span><span class=\"p\">(</span><span class=\"n\">ticker</span><span class=\"o\">=</span><span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">ticker</span><span class=\"p\">),</span>  <span class=\"c1\"># Initialize data container</span>\n    <span class=\"p\">}</span>\n</code></pre></div>\n<p>\u2728 Notice the <code>llm.with_structured_output(PlanOutput)</code> line? This applies <strong>Schema-Guided Reasoning (SGR)</strong>\u2014a pattern I've described in my <a href=\"https://slavadubrov.com/blog/2025/12/25/schema-guided-reasoning-with-vllm/\">previous articles</a>. By strictly enforcing the <code>PlanOutput</code> schema, we guarantee the planner always returns a valid list of steps, preventing common formatting errors. LangGraph supports this natively, using these reliable structured outputs to drive deterministic control flow through conditional edges.</p>\n<h3 id=\"pattern-2-react-execution\">Pattern 2: ReAct Execution</h3>\n<p>Once we have a plan, the executor uses the <strong>ReAct</strong> pattern to handle one step at a time with full adaptability. This is Phase 2 of Plan-and-Execute: executing each step with the flexibility to react to tool results.</p>\n<p><strong>How this reflects the ReAct pattern:</strong></p>\n<ol>\n<li><strong>Iterative execution</strong>: One step at a time, with observation feedback</li>\n<li><strong>Thought-Action-Observation loop</strong>: The <code>create_react_agent</code> handles the cycle internally</li>\n<li><strong>Context accumulation</strong>: Previous step results inform current reasoning</li>\n<li><strong>Tool selection</strong>: The agent chooses which tools to call based on the step description</li>\n<li><strong>Adaptability</strong>: Can adjust approach mid-step based on tool results</li>\n</ol>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Tools available for the ReAct agent to choose from</span>\n<span class=\"n\">TOOLS</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">get_stock_price</span><span class=\"p\">,</span>\n    <span class=\"n\">get_company_metrics</span><span class=\"p\">,</span>\n    <span class=\"n\">get_price_history</span><span class=\"p\">,</span>\n    <span class=\"n\">search_news</span><span class=\"p\">,</span>\n    <span class=\"n\">search_competitors</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">executor_node</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">AgentState</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Execute the current step using a ReAct agent.</span>\n\n<span class=\"sd\">    This is Phase 2 of Plan-and-Execute: adaptive execution of each planned step.</span>\n<span class=\"sd\">    Each step runs as a mini ReAct loop until completion.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Get the current step from the plan</span>\n    <span class=\"n\">current_step</span> <span class=\"o\">=</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">plan</span><span class=\"p\">[</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">current_step_index</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Build context from what we&#39;ve learned so far</span>\n    <span class=\"c1\"># This is crucial: each step builds on previous observations</span>\n    <span class=\"n\">previous_context</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;</span>\n    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">plan</span><span class=\"p\">[:</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">current_step_index</span><span class=\"p\">]:</span>\n        <span class=\"k\">if</span> <span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"p\">:</span>\n            <span class=\"n\">previous_context</span> <span class=\"o\">+=</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">Step </span><span class=\"si\">{</span><span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">step_number</span><span class=\"si\">}</span><span class=\"s2\">: </span><span class=\"si\">{</span><span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span>\n\n    <span class=\"c1\"># Create a ReAct agent for this step</span>\n    <span class=\"c1\"># LangGraph&#39;s create_react_agent implements the full Thought-Action-Observation loop:</span>\n    <span class=\"c1\"># 1. Agent generates a &quot;thought&quot; about what tool to call</span>\n    <span class=\"c1\"># 2. Agent calls the tool (&quot;action&quot;)</span>\n    <span class=\"c1\"># 3. Tool returns result (&quot;observation&quot;)</span>\n    <span class=\"c1\"># 4. Agent decides: call another tool or finish</span>\n    <span class=\"n\">react_agent</span> <span class=\"o\">=</span> <span class=\"n\">create_react_agent</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">ChatAnthropic</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;claude-sonnet-4-5-20250929&quot;</span><span class=\"p\">),</span>\n        <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"n\">TOOLS</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># Invoke the ReAct loop for this single step</span>\n    <span class=\"c1\"># The agent will loop internally until it completes the step</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">react_agent</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">({</span>\n        <span class=\"s2\">&quot;messages&quot;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n            <span class=\"n\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">EXECUTOR_SYSTEM_PROMPT</span><span class=\"p\">),</span>\n            <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;Execute Step </span><span class=\"si\">{</span><span class=\"n\">current_step</span><span class=\"o\">.</span><span class=\"n\">step_number</span><span class=\"si\">}</span><span class=\"s2\">:</span>\n<span class=\"si\">{</span><span class=\"n\">current_step</span><span class=\"o\">.</span><span class=\"n\">description</span><span class=\"si\">}</span>\n\n<span class=\"s2\">Ticker: </span><span class=\"si\">{</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">research_data</span><span class=\"o\">.</span><span class=\"n\">ticker</span><span class=\"si\">}</span>\n<span class=\"s2\">Previous findings: </span><span class=\"si\">{</span><span class=\"n\">previous_context</span><span class=\"si\">}</span><span class=\"s2\">&quot;&quot;&quot;</span><span class=\"p\">),</span>\n        <span class=\"p\">]</span>\n    <span class=\"p\">})</span>\n\n    <span class=\"c1\"># Extract the final answer from the ReAct agent&#39;s message history</span>\n    <span class=\"c1\"># The last message contains the synthesis after all tool calls</span>\n    <span class=\"n\">updated_plan</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">plan</span><span class=\"p\">)</span>\n    <span class=\"n\">updated_plan</span><span class=\"p\">[</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">current_step_index</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">PlanStep</span><span class=\"p\">(</span>\n        <span class=\"n\">step_number</span><span class=\"o\">=</span><span class=\"n\">current_step</span><span class=\"o\">.</span><span class=\"n\">step_number</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"n\">current_step</span><span class=\"o\">.</span><span class=\"n\">description</span><span class=\"p\">,</span>\n        <span class=\"n\">completed</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n        <span class=\"n\">result</span><span class=\"o\">=</span><span class=\"n\">result</span><span class=\"p\">[</span><span class=\"s2\">&quot;messages&quot;</span><span class=\"p\">][</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">,</span>  <span class=\"c1\"># Final observation</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># State update: Mark step complete and advance to next</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;plan&quot;</span><span class=\"p\">:</span> <span class=\"n\">updated_plan</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;current_step_index&quot;</span><span class=\"p\">:</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">current_step_index</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n</code></pre></div>\n<p>This demonstrates the core <strong>Plan-and-Execute</strong> flow: use a powerful model to create the plan, then execute each step sequentially using ReAct for maximum adaptability.</p>\n<h3 id=\"pattern-3-rewoo-for-fast-snapshots\">Pattern 3: ReWOO for Fast Snapshots</h3>\n<p>For quick briefings, <strong>ReWOO</strong> skips the interleaved reasoning and executes everything in parallel. Unlike ReAct, ReWOO creates a \"compiled script\" of all tool calls upfront and runs them without LLM involvement.</p>\n<p><strong>How this reflects the ReWOO pattern:</strong></p>\n<ol>\n<li><strong>Three distinct phases</strong>: Planner \u2192 Worker \u2192 Solver (no loops)</li>\n<li><strong>Variable placeholders</strong>: Tool calls reference <code>#E1</code>, <code>#E2</code> for future results</li>\n<li><strong>No LLM during execution</strong>: Worker just runs tools, no reasoning</li>\n<li><strong>Parallel execution</strong>: Independent tools run concurrently</li>\n<li><strong>Single synthesis</strong>: Solver makes ONE final LLM call with all data</li>\n</ol>\n<p><strong>Phase 1: ReWOO Planner</strong> (creates the complete execution graph upfront)</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ReWOOPlanStep</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;A step in the ReWOO plan with variable placeholders.</span>\n\n<span class=\"sd\">    Key difference from Plan-and-Execute&#39;s PlanStep:</span>\n<span class=\"sd\">    - Contains actual tool_name and tool_args (not just description)</span>\n<span class=\"sd\">    - Uses variable references (#E1) for dependencies</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">step_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>  <span class=\"c1\"># e.g., &quot;#E1&quot; - becomes a variable</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tool_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>     <span class=\"c1\"># Exact tool to call</span>\n    <span class=\"n\">tool_args</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>    <span class=\"c1\"># May contain variable refs like {&quot;price&quot;: &quot;#E1&quot;}</span>\n    <span class=\"n\">depends_on</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>  <span class=\"c1\"># For dependency ordering</span>\n    <span class=\"n\">result</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ReWOOPlanOutput</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Structured output for ReWOO planner.&quot;&quot;&quot;</span>\n    <span class=\"n\">steps</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">ReWOOPlanStep</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Planned tool calls with variables&quot;</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">rewoo_planner_node</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">AgentState</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Generate a complete plan of tool calls upfront.</span>\n\n<span class=\"sd\">    This is the key difference from Plan-and-Execute: instead of creating</span>\n<span class=\"sd\">    human-readable step descriptions, we create EXACT tool calls that</span>\n<span class=\"sd\">    the worker will execute blindly.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">ChatAnthropic</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;claude-sonnet-4-5-20250929&quot;</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Schema-Guided Reasoning ensures valid tool call specifications</span>\n    <span class=\"n\">structured_llm</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">with_structured_output</span><span class=\"p\">(</span><span class=\"n\">ReWOOPlanOutput</span><span class=\"p\">)</span>\n\n    <span class=\"n\">ticker</span> <span class=\"o\">=</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">research_data</span><span class=\"o\">.</span><span class=\"n\">ticker</span> <span class=\"k\">if</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">research_data</span> <span class=\"k\">else</span> <span class=\"s2\">&quot;UNKNOWN&quot;</span>\n\n    <span class=\"c1\"># Single LLM call to plan ALL tool executions</span>\n    <span class=\"n\">result</span><span class=\"p\">:</span> <span class=\"n\">ReWOOPlanOutput</span> <span class=\"o\">=</span> <span class=\"n\">structured_llm</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">([</span>\n        <span class=\"n\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">REWOO_PLANNER_PROMPT</span><span class=\"p\">),</span>\n        <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;&quot;&quot;Create a ReWOO plan for: </span><span class=\"si\">{</span><span class=\"n\">query</span><span class=\"si\">}</span>\n\n<span class=\"s2\">Ticker: </span><span class=\"si\">{</span><span class=\"n\">ticker</span><span class=\"si\">}</span>\n\n<span class=\"s2\">Output tool calls with:</span>\n<span class=\"s2\">- step_id: Variable name (#E1, #E2, etc.)</span>\n<span class=\"s2\">- description: What this accomplishes</span>\n<span class=\"s2\">- tool_name: Exact tool from the list</span>\n<span class=\"s2\">- tool_args: Dictionary of arguments</span>\n<span class=\"s2\">- depends_on: List of step_ids this depends on&quot;&quot;&quot;</span><span class=\"p\">),</span>\n    <span class=\"p\">])</span>\n\n    <span class=\"c1\"># State update: Store the complete execution plan</span>\n    <span class=\"c1\"># Worker will execute this without any LLM involvement</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;rewoo_plan&quot;</span><span class=\"p\">:</span> <span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">}</span>\n</code></pre></div>\n<p><strong>Phase 2: ReWOO Worker</strong> (executes tools without LLM reasoning)</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">rewoo_worker_node</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">AgentState</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Execute all planned tools in parallel (no LLM calls).</span>\n\n<span class=\"sd\">    This is the key efficiency: Worker is &quot;dumb&quot; - it just runs tools</span>\n<span class=\"sd\">    according to the plan. No LLM calls = massive token savings.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>  <span class=\"c1\"># Store results keyed by step_id (e.g., &quot;#E1&quot;: &quot;$150.23&quot;)</span>\n\n    <span class=\"c1\"># Execute ALL independent steps in parallel using ThreadPoolExecutor</span>\n    <span class=\"c1\"># This is where ReWOO gets its speed advantage</span>\n    <span class=\"k\">with</span> <span class=\"n\">ThreadPoolExecutor</span><span class=\"p\">(</span><span class=\"n\">max_workers</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">executor</span><span class=\"p\">:</span>\n        <span class=\"n\">futures</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n            <span class=\"n\">executor</span><span class=\"o\">.</span><span class=\"n\">submit</span><span class=\"p\">(</span><span class=\"n\">execute_tool</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"p\">):</span> <span class=\"n\">step</span>\n            <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">rewoo_plan</span>\n            <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">depends_on</span>  <span class=\"c1\"># Only independent tools for parallel batch</span>\n        <span class=\"p\">}</span>\n\n        <span class=\"c1\"># Collect results as they complete</span>\n        <span class=\"k\">for</span> <span class=\"n\">future</span> <span class=\"ow\">in</span> <span class=\"n\">as_completed</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"p\">):</span>\n            <span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"n\">futures</span><span class=\"p\">[</span><span class=\"n\">future</span><span class=\"p\">]</span>\n            <span class=\"n\">results</span><span class=\"p\">[</span><span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">step_id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">future</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"p\">()</span>\n            <span class=\"c1\"># No LLM reasoning here - just store the raw tool output</span>\n\n    <span class=\"c1\"># State update: Store results for the Solver phase</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;rewoo_plan&quot;</span><span class=\"p\">:</span> <span class=\"n\">updated_steps</span><span class=\"p\">}</span>\n</code></pre></div>\n<p><strong>Phase 3: ReWOO Solver</strong> (synthesizes all results in ONE LLM call)</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">rewoo_solver_node</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">AgentState</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Synthesize all tool results into a flash briefing.</span>\n\n<span class=\"sd\">    This is the second efficiency gain: Instead of interleaving</span>\n<span class=\"sd\">    LLM calls with tool execution (like ReAct), we make ONE</span>\n<span class=\"sd\">    final synthesis call with all gathered data.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Build context from ALL tool results at once</span>\n    <span class=\"n\">tool_results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">rewoo_plan</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"p\">:</span>\n            <span class=\"n\">tool_results</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;### </span><span class=\"si\">{</span><span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">description</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">result</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;</span><span class=\"se\">\\n\\n</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">tool_results</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Single LLM call to synthesize everything</span>\n    <span class=\"n\">structured_llm</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">with_structured_output</span><span class=\"p\">(</span><span class=\"n\">FlashBriefingOutput</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">structured_llm</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">([</span>\n        <span class=\"n\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">REWOO_SOLVER_PROMPT</span><span class=\"p\">),</span>\n        <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Create a flash briefing from this data:</span><span class=\"se\">\\n\\n</span><span class=\"si\">{</span><span class=\"n\">context</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">),</span>\n    <span class=\"p\">])</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;draft_report&quot;</span><span class=\"p\">:</span> <span class=\"n\">result</span><span class=\"p\">}</span>\n</code></pre></div>\n<p><strong>Key Difference:</strong> ReWOO plans all tool calls upfront using placeholders (<code>#E1</code>, <code>#E2</code>), executes them in parallel without LLM intervention, then synthesizes the results in a single final call. This makes it incredibly token-efficient for predictable workflows.</p>\n<h4 id=\"understanding-the-code-how-each-pattern-works-differently\">Understanding the Code: How Each Pattern Works Differently</h4>\n<p>The three patterns differ fundamentally in <em>when</em> and <em>how</em> they invoke the LLM:</p>\n<table>\n<thead>\n<tr>\n<th>Pattern</th>\n<th><strong>LLM Calls During Execution</strong></th>\n<th><strong>State Updates</strong></th>\n<th><strong>Key Code Pattern</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Plan-and-Execute</strong></td>\n<td>1 for planning + 1 per step</td>\n<td>Sequential step completion</td>\n<td><code>planner_node()</code> \u2192 loop: <code>executor_node()</code> \u2192 <code>reporter_node()</code></td>\n</tr>\n<tr>\n<td><strong>ReAct</strong> (within each step)</td>\n<td>Multiple per step (thought-action cycles)</td>\n<td>Accumulated message history</td>\n<td><code>create_react_agent()</code> loops internally until step complete</td>\n</tr>\n<tr>\n<td><strong>ReWOO</strong></td>\n<td>1 for planning + 0 during execution + 1 for synthesis</td>\n<td>Parallel tool completion</td>\n<td><code>rewoo_planner_node()</code> \u2192 <code>rewoo_worker_node()</code> \u2192 <code>rewoo_solver_node()</code></td>\n</tr>\n</tbody>\n</table>\n<p><strong>What makes each pattern unique in code:</strong></p>\n<p>The key difference is <strong>what the planner produces</strong> - this determines everything that follows:</p>\n<ol>\n<li>\n<p><strong>Plan-and-Execute</strong> creates human-readable step descriptions:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Planner output (list of PlanStep objects)</span>\n<span class=\"n\">plan</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">PlanStep</span><span class=\"p\">(</span>\n        <span class=\"n\">step_number</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Get current price and key financial metrics&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tool_hint</span><span class=\"o\">=</span><span class=\"s2\">&quot;get_stock_price&quot;</span>\n    <span class=\"p\">),</span>\n    <span class=\"n\">PlanStep</span><span class=\"p\">(</span>\n        <span class=\"n\">step_number</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Search for recent news and earnings&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tool_hint</span><span class=\"o\">=</span><span class=\"s2\">&quot;search_news&quot;</span>\n    <span class=\"p\">),</span>\n    <span class=\"c1\"># ... more steps</span>\n<span class=\"p\">]</span>\n</code></pre></div>\n<p>The executor <strong>interprets</strong> each description and decides which tools to call. This gives flexibility but requires LLM reasoning per step.</p>\n</li>\n<li>\n<p><strong>ReAct</strong> doesn't have an upfront plan - it uses iterative reasoning:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># No planning phase - ReAct works step-by-step with accumulated messages</span>\n<span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;Execute Step 1: Get current price&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">AIMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;I&#39;ll call get_stock_price&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">ToolMessage</span><span class=\"p\">(</span><span class=\"n\">tool_call_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;1&quot;</span><span class=\"p\">,</span> <span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;$132.45&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">AIMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;Now I need metrics...&quot;</span><span class=\"p\">),</span>\n    <span class=\"c1\"># ... agent continues until step complete</span>\n<span class=\"p\">]</span>\n</code></pre></div>\n<p>The agent makes <strong>multiple LLM calls</strong> within each step, adapting based on observations. Maximum flexibility, highest token cost.</p>\n</li>\n<li>\n<p><strong>ReWOO</strong> creates explicit, executable tool call specifications:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Planner output (list of ReWOOPlanStep objects)</span>\n<span class=\"n\">rewoo_plan</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">ReWOOPlanStep</span><span class=\"p\">(</span>\n        <span class=\"n\">step_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;#E1&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tool_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;get_stock_price&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tool_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;ticker&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;NVDA&quot;</span><span class=\"p\">}</span>\n    <span class=\"p\">),</span>\n    <span class=\"n\">ReWOOPlanStep</span><span class=\"p\">(</span>\n        <span class=\"n\">step_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;#E2&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tool_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;search_news&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">tool_args</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;query&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;NVDA earnings&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;limit&quot;</span><span class=\"p\">:</span> <span class=\"mi\">5</span><span class=\"p\">}</span>\n    <span class=\"p\">),</span>\n    <span class=\"c1\"># ... all tool calls planned upfront</span>\n<span class=\"p\">]</span>\n</code></pre></div>\n<p>The worker <strong>executes blindly</strong> - no LLM involvement. All intelligence is in the planner and solver. Minimum token cost.</p>\n</li>\n</ol>\n<p><strong>Memory and State Flow:</strong></p>\n<ul>\n<li><strong>Plan-and-Execute</strong>: State flows through <code>plan \u2192 current_step_index \u2192 research_data</code></li>\n<li><strong>ReAct</strong>: State accumulates in <code>messages</code> array (full conversation history)</li>\n<li><strong>ReWOO</strong>: State flows through <code>rewoo_plan</code> with <code>result</code> fields populated by worker</li>\n</ul>\n<h3 id=\"putting-it-all-together-wiring-the-graph\">Putting It All Together: Wiring the Graph</h3>\n<p>Now that we've seen the three patterns individually, here's how they coexist in a <strong>single LangGraph system</strong>. The beauty of this architecture is that all three reasoning loops share the same <code>AgentState</code> and live in one graph\u2014the router dynamically chooses which path to follow based on the user's request.</p>\n<p><strong>Key insight:</strong> You're not building three separate agents. You're building ONE agent with three execution modes.</p>\n<p>LangGraph makes this orchestration declarative:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">create_graph</span><span class=\"p\">(</span><span class=\"n\">checkpointer</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n    <span class=\"n\">builder</span> <span class=\"o\">=</span> <span class=\"n\">StateGraph</span><span class=\"p\">(</span><span class=\"n\">AgentState</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Add nodes</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;router&quot;</span><span class=\"p\">,</span> <span class=\"n\">router_node</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;planner&quot;</span><span class=\"p\">,</span> <span class=\"n\">planner_node</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;executor&quot;</span><span class=\"p\">,</span> <span class=\"n\">executor_node</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;reporter&quot;</span><span class=\"p\">,</span> <span class=\"n\">reporter_node</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;rewoo_planner&quot;</span><span class=\"p\">,</span> <span class=\"n\">rewoo_planner_node</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;rewoo_worker&quot;</span><span class=\"p\">,</span> <span class=\"n\">rewoo_worker_node</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_node</span><span class=\"p\">(</span><span class=\"s2\">&quot;rewoo_solver&quot;</span><span class=\"p\">,</span> <span class=\"n\">rewoo_solver_node</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Define edges</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_edge</span><span class=\"p\">(</span><span class=\"n\">START</span><span class=\"p\">,</span> <span class=\"s2\">&quot;router&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_conditional_edges</span><span class=\"p\">(</span><span class=\"s2\">&quot;router&quot;</span><span class=\"p\">,</span> <span class=\"n\">route_after_router</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;planner&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;planner&quot;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;rewoo_planner&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;rewoo_planner&quot;</span><span class=\"p\">,</span>\n    <span class=\"p\">})</span>\n\n    <span class=\"c1\"># Deep Research path</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_edge</span><span class=\"p\">(</span><span class=\"s2\">&quot;planner&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;executor&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_conditional_edges</span><span class=\"p\">(</span><span class=\"s2\">&quot;executor&quot;</span><span class=\"p\">,</span> <span class=\"n\">route_after_executor</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;executor&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;executor&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Loop back for more steps</span>\n        <span class=\"s2\">&quot;reporter&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;reporter&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Done with plan</span>\n    <span class=\"p\">})</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_edge</span><span class=\"p\">(</span><span class=\"s2\">&quot;reporter&quot;</span><span class=\"p\">,</span> <span class=\"n\">END</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Flash Briefing path (ReWOO)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_edge</span><span class=\"p\">(</span><span class=\"s2\">&quot;rewoo_planner&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;rewoo_worker&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_edge</span><span class=\"p\">(</span><span class=\"s2\">&quot;rewoo_worker&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;rewoo_solver&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">add_edge</span><span class=\"p\">(</span><span class=\"s2\">&quot;rewoo_solver&quot;</span><span class=\"p\">,</span> <span class=\"n\">END</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">builder</span><span class=\"o\">.</span><span class=\"n\">compile</span><span class=\"p\">(</span>\n        <span class=\"n\">checkpointer</span><span class=\"o\">=</span><span class=\"n\">checkpointer</span><span class=\"p\">,</span>\n        <span class=\"n\">interrupt_before</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;reporter&quot;</span><span class=\"p\">],</span>  <span class=\"c1\"># HITL pause for approval</span>\n    <span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"automatic-pattern-selection-with-a-router\">Automatic Pattern Selection with a Router</h3>\n<p>To automatically select the best reasoning loop for each user request, I've added a <strong>router classifier</strong>. The router uses Schema-Guided Reasoning to reliably classify user intent:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">ExecutionMode</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Enum</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Execution mode for the agent.&quot;&quot;&quot;</span>\n\n    <span class=\"n\">DEEP_RESEARCH</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;deep_research&quot;</span>  <span class=\"c1\"># Plan-and-Execute + ReAct (thorough)</span>\n    <span class=\"n\">FLASH_BRIEFING</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;flash_briefing&quot;</span>  <span class=\"c1\"># ReWOO (fast, token-efficient)</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">RouterOutput</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Structured output for the router.&quot;&quot;&quot;</span>\n\n    <span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"n\">ExecutionMode</span>  <span class=\"c1\"># DEEP_RESEARCH or FLASH_BRIEFING</span>\n    <span class=\"n\">ticker</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">reasoning</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n\n<span class=\"n\">ROUTER_SYSTEM_PROMPT</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;Classify the user&#39;s request:</span>\n\n<span class=\"s2\">1. **deep_research**: Complex analysis requiring synthesis</span>\n<span class=\"s2\">   - Examples: &quot;Analyze strategic risks&quot;, &quot;investment thesis&quot;</span>\n\n<span class=\"s2\">2. **flash_briefing**: Quick snapshots, simple data retrieval</span>\n<span class=\"s2\">   - Examples: &quot;quick snapshot&quot;, &quot;current price&quot;</span>\n\n<span class=\"s2\">Default to deep_research if unclear.&quot;&quot;&quot;</span>\n\n<span class=\"n\">structured_llm</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">with_structured_output</span><span class=\"p\">(</span><span class=\"n\">RouterOutput</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>This lightweight classification step automatically chooses between <strong>Plan-and-Execute</strong> (for deep research) and <strong>ReWOO</strong> (for quick snapshots), removing the burden of manual mode selection from users.</p>\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ol>\n<li><strong>ReAct</strong> is your default for flexibility but costs tokens and time</li>\n<li><strong>ReWOO</strong> wins on speed when your tools are reliable and results predictable</li>\n<li><strong>Plan-and-Execute</strong> delivers the best results for complex analysis</li>\n<li><strong>Use a router</strong> to choose dynamically\u2014don't force users to pick</li>\n<li><strong>State management is critical</strong>\u2014LangGraph's checkpointing enables interrupts and recovery</li>\n</ol>\n<p>The complete implementation of all three patterns, including the router classifier and state management, is available in the <a href=\"https://github.com/slavadubrov/market-analyst-agent\">Market Analyst Agent repository</a>.</p>\n<h2 id=\"whats-next\">What's Next</h2>\n<p>In Part 2, I'll dive into <strong>memory architecture</strong>\u2014how to give your agent both short-term context (PostgreSQL checkpointing) and long-term knowledge (Qdrant vector memory). I'll show how this enables pause/resume workflows and cross-session learning.</p>\n<h2 id=\"references\">References</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2210.03629\"><strong>ReAct: Synergizing Reasoning and Acting in Language Models</strong></a> (Yao et al., 2022)</li>\n<li><a href=\"https://arxiv.org/abs/2305.18323\"><strong>ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models</strong></a> (Xu et al., 2023)</li>\n<li><a href=\"https://arxiv.org/abs/2305.04091\"><strong>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</strong></a> (Wang et al., 2023)</li>\n<li><a href=\"https://github.com/slavadubrov/market-analyst-agent\"><strong>Market Analyst Agent Repository</strong></a></li>\n</ul>\n<hr />\n<p><em>The complete Market Analyst Agent code is available on <a href=\"https://github.com/slavadubrov/market-analyst-agent\">GitHub</a>. Star the repo and follow along as I build the full production stack.</em></p>\n<p><strong>Series: Engineering the Agentic Stack</strong></p>\n<ul>\n<li><strong>Part 1: The Cognitive Engine</strong> (this post)</li>\n<li>Part 2: Memory Architecture (coming soon)</li>\n<li>Part 3: Tool Ergonomics and the ACI</li>\n<li>Part 4: Safety Layers \u2013 The Guardian Pattern</li>\n<li>Part 5: Production Deployment</li>\n</ul>", "image": null, "date_modified": "2026-01-31T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Agents 101"]}, {"id": "https://slavadubrov.github.io/blog/2026/01/21/mhc-how-deepseek-scaled-residual-connections-without-breaking-training/", "url": "https://slavadubrov.github.io/blog/2026/01/21/mhc-how-deepseek-scaled-residual-connections-without-breaking-training/", "title": "mHC: How DeepSeek Scaled Residual Connections Without Breaking Training", "content_html": "<h1 id=\"mhc-how-deepseek-scaled-residual-connections-without-breaking-training\">mHC: How DeepSeek Scaled Residual Connections Without Breaking Training</h1>\n<p>The success of modern deep learning rests on a deceptively simple idea: the residual connection. Yet after a decade of stacking layers deeper and deeper, researchers at DeepSeek asked a different question\u2014what if we could scale <em>width</em> instead? Their answer, <strong><a href=\"https://arxiv.org/abs/2512.24880\">Manifold-Constrained Hyper-Connections (mHC)</a></strong>, solves a fundamental instability problem that has blocked this path for years.</p>\n<p>This article breaks down the evolution from basic residuals to mHC, explaining why each step was necessary and what makes DeepSeek's solution work at scale.</p>\n<!-- more -->\n\n<p><strong>TL;DR</strong>: Hyper-Connections expand residual streams into multiple parallel flows for faster convergence, but break the identity mapping property that keeps training stable. mHC restores stability by constraining mixing matrices to the Birkhoff Polytope (doubly stochastic matrices) using the differentiable Sinkhorn-Knopp algorithm\u2014achieving only 6.7% training overhead with 4 parallel streams.</p>\n<hr />\n<h2 id=\"the-foundation-why-residual-connections-work\">The Foundation: Why Residual Connections Work</h2>\n<p>Before understanding what mHC fixes, we need to understand what it builds on.</p>\n<h3 id=\"the-depth-problem\">The Depth Problem</h3>\n<p>Stacking more layers should increase a model's capacity to learn complex functions. In practice, very deep networks become <em>harder</em> to train\u2014not because they lack capacity, but because gradient-based optimization fails to find good parameters. Gradients either vanish (shrinking to near-zero) or explode (growing unboundedly) as they propagate through many layers.</p>\n<h3 id=\"the-residual-solution\">The Residual Solution</h3>\n<p>The <a href=\"https://arxiv.org/abs/1512.03385\">ResNet paper</a> introduced a elegant fix: instead of learning a direct mapping, learn the <em>residual</em>\u2014the difference from identity:</p>\n<p><img alt=\"Standard Residual Connection\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/residual_connection.svg\" /></p>\n<p>The key insight is the <strong>identity shortcut</strong>. When the residual function F(x) outputs zero, the layer becomes a perfect pass-through. This provides:</p>\n<ol>\n<li><strong>Gradient Highway</strong>: Gradients flow directly through the shortcut, avoiding the vanishing gradient problem</li>\n<li><strong>Easy Optimization</strong>: If identity is optimal, the network just learns F(x) \u2192 0</li>\n</ol>\n<p>This single architectural change enabled training networks with hundreds of layers.</p>\n<hr />\n<h2 id=\"the-transformer-complication-layer-normalization-placement\">The Transformer Complication: Layer Normalization Placement</h2>\n<p>Transformers added a new variable: where to put Layer Normalization (LN). This seemingly minor decision creates a fundamental trade-off.</p>\n<p><img alt=\"Post-LN vs Pre-LN Trade-offs\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/ln_placement.svg\" /></p>\n<table>\n<thead>\n<tr>\n<th>Variant</th>\n<th>LN Placement</th>\n<th>Advantage</th>\n<th>Key Limitation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Post-LN</strong></td>\n<td>After residual block</td>\n<td>High model capacity</td>\n<td>Gradient vanishing\u2014LN in main path rescales gradients at every layer</td>\n</tr>\n<tr>\n<td><strong>Pre-LN</strong></td>\n<td>Before residual block</td>\n<td>Excellent stability</td>\n<td>Representation collapse\u2014features become similar across layers</td>\n</tr>\n</tbody>\n</table>\n<p>The <a href=\"https://arxiv.org/abs/2304.14802\"><strong>ResiDual</strong></a> architecture attempted to solve this by using <em>dual</em> residual paths\u2014one Pre-LN for stability, one Post-LN for capacity. But it was still limited to a single residual stream. What if we could have <em>multiple</em> parallel streams?</p>\n<hr />\n<h2 id=\"hyper-connections-the-width-revolution\">Hyper-Connections: The Width Revolution</h2>\n<p><a href=\"https://arxiv.org/abs/2409.19606\">Hyper-Connections (HC)</a> took a fundamentally different approach: instead of just adding depth, expand the residual stream <em>width</em>.</p>\n<p><img alt=\"Hyper-Connections Architecture\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/hyper_connections.svg\" /></p>\n<h3 id=\"core-mechanisms\">Core Mechanisms</h3>\n<p>HC introduces three operations controlled by learnable weight matrices (typically with n=4 parallel streams):</p>\n<ol>\n<li><strong>Aggregation</strong>: Weighted combination of n parallel streams into a single input for the transformer block</li>\n<li><strong>Expansion</strong>: Distributing the block's output back to n streams</li>\n<li><strong>Mixing</strong>: Inter-stream communication via an n\u00d7n \"feature router\" matrix</li>\n</ol>\n<p>The mixing matrix H acts as a traffic controller, dynamically routing features between streams based on learned patterns. This creates a much richer flow of information than a single residual path.</p>\n<h3 id=\"the-results\">The Results</h3>\n<p><img alt=\"HC Performance\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/hc_performance.svg\" /></p>\n<p>HC achieves <strong>~1.8\u00d7 faster convergence</strong> compared to standard residuals. The parallel streams provide more pathways for gradient flow and allow the network to maintain more diverse representations.</p>\n<h3 id=\"the-problem\">The Problem</h3>\n<p>But there's a critical issue: <strong>HC is unstable at scale</strong>.</p>\n<hr />\n<h2 id=\"why-hyper-connections-break\">Why Hyper-Connections Break</h2>\n<p>The flexibility that makes HC powerful also destroys the property that makes residuals trainable.</p>\n<p><img alt=\"HC Instability Problem\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/hc_instability.svg\" /></p>\n<h3 id=\"the-math-of-instability\">The Math of Instability</h3>\n<p>In standard residuals, we have:</p>\n<div class=\"arithmatex\">\\[x_{l+1} = x_l + F(x_l)\\]</div>\n<p>When <span class=\"arithmatex\">\\(F(x) \\rightarrow 0\\)</span>, this becomes identity: <span class=\"arithmatex\">\\(x_{l+1} = x_l\\)</span>. The signal passes through unchanged.</p>\n<p>In Hyper-Connections, the residual path includes matrix multiplication:</p>\n<div class=\"arithmatex\">\\[x_{l+1} = \\mathbf{H}^{res}_l \\cdot x_l + \\dots\\]</div>\n<p>Over L layers, the signal becomes:</p>\n<div class=\"arithmatex\">\\[x_L = \\mathbf{H}^{res}_L \\times \\mathbf{H}^{res}_{L-1} \\times \\dots \\times \\mathbf{H}^{res}_1 \\times x_0\\]</div>\n<p>If the values in H deviate even slightly from 1.0, this product either:</p>\n<ul>\n<li><strong>Explodes</strong>: values &gt; 1.0 compound exponentially</li>\n<li><strong>Vanishes</strong>: values &lt; 1.0 decay exponentially</li>\n</ul>\n<p>The DeepSeek team measured this with \"Amax Gain Magnitude\"\u2014a metric tracking the maximum ratio of output to input signal magnitude across all layers. In standard HC, this metric hits <strong>~3000</strong> in deep networks. Training becomes impossible.</p>\n<p><img alt=\"The Root Cause: Loss of Identity\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/loss_of_identity.svg\" /></p>\n<p>The core problem: unconstrained matrices can have arbitrary values\u2014negative numbers, large magnitudes, anything. We need a way to constrain them to \"well-behaved\" matrices that preserve signal energy like the identity matrix does.</p>\n<hr />\n<h2 id=\"the-mhc-solution-geometric-constraints\">The mHC Solution: Geometric Constraints</h2>\n<p>The insight behind mHC is that we can have flexible routing <em>and</em> stability\u2014if we constrain the mixing matrices to a specific mathematical structure: the <strong>Birkhoff Polytope</strong> (the set of all doubly stochastic matrices\u2014matrices where every row and column sums to 1, with all elements non-negative).</p>\n<p><img alt=\"The mHC Solution\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/mhc_solution.svg\" /></p>\n<h3 id=\"the-three-constraints\">The Three Constraints</h3>\n<p>mHC constrains the mixing matrix H^res to be <strong>doubly stochastic</strong>\u2014a matrix where all entries are non-negative and every row and column sums to exactly 1. This enforces three properties simultaneously:</p>\n<table>\n<thead>\n<tr>\n<th>Constraint</th>\n<th>Rule</th>\n<th>Why It Matters</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Positivity</strong></td>\n<td>All elements &gt; 0</td>\n<td>Prevents sign oscillation that destabilizes gradients</td>\n</tr>\n<tr>\n<td><strong>Row Sum = 1</strong></td>\n<td>Each row sums to 1.0</td>\n<td>Normalizes output contribution\u2014no single stream dominates</td>\n</tr>\n<tr>\n<td><strong>Column Sum = 1</strong></td>\n<td>Each column sums to 1.0</td>\n<td>Normalizes input distribution\u2014all streams contribute fairly</td>\n</tr>\n</tbody>\n</table>\n<p>The critical outcome: <strong>Energy In = Energy Out</strong>. Signal magnitude is preserved deep into the network, eliminating the exponential explosion problem.</p>\n<p>This constraint has powerful mathematical implications:</p>\n<ol>\n<li><strong>Spectral norm \u2264 1</strong>: The spectral norm (largest singular value) bounds signal amplification\u2014doubly stochastic matrices are mathematically <em>non-expanding</em></li>\n<li><strong>Closed under multiplication</strong>: Composing doubly stochastic matrices produces another doubly stochastic matrix</li>\n<li><strong>Weighted averaging</strong>: The operation becomes a convex combination (weighted average where weights sum to 1) of inputs, preserving total signal magnitude</li>\n</ol>\n<h3 id=\"the-sinkhorn-knopp-algorithm\">The Sinkhorn-Knopp Algorithm</h3>\n<p>The challenge: how do we <em>force</em> a learnable matrix to be doubly stochastic while keeping it differentiable? The answer is the <strong>Sinkhorn-Knopp algorithm</strong>\u2014an iterative projection that converges to doubly stochastic form in just a few steps.</p>\n<p><img alt=\"Sinkhorn Algorithm Detailed\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/sinkhorn_detailed.svg\" /></p>\n<p>Here's how it works with a concrete example:</p>\n<p><strong>Step 1: Positivity</strong> \u2014 Apply exp() to raw weights, ensuring all elements are strictly positive:</p>\n<div class=\"highlight\"><pre><span></span><code>Raw Matrix           \u2192    Positive Matrix\n[-0.5  2.1  0.8]          [0.6   7.9   2.2]  \u03a3=10.7\n[ 1.3 -4.0  1.9]    exp   [3.7   0.02  6.7]  \u03a3=10.4\n[ 0.1  0.6 -0.2]    \u2192     [1.1   1.8   0.8]  \u03a3=3.7\n</code></pre></div>\n<p><strong>Step 2: Row Normalization</strong> \u2014 Divide each row by its sum:</p>\n<div class=\"highlight\"><pre><span></span><code>Positive Matrix      \u2192    Row Normalized\n[0.6   7.9   2.2]         [0.25  0.65  0.10]  \u03a3=1.0\n[3.7   0.02  6.7]   /row  [0.35  0.01  0.64]  \u03a3=1.0\n[1.1   1.8   0.8]    \u2192    [0.30  0.45  0.25]  \u03a3=1.0\n                           \u03a3=0.9 \u03a3=1.1 \u03a3=0.99  \u2190 columns not yet =1\n</code></pre></div>\n<p><strong>Step 3: Column Normalization</strong> \u2014 Divide each column by its sum:</p>\n<div class=\"highlight\"><pre><span></span><code>Row Normalized       \u2192    Doubly Stochastic\n[0.25  0.65  0.10]        [0.28  0.45  0.27]  \u03a3=1.0\n[0.35  0.01  0.64]  /col  [0.40  0.09  0.51]  \u03a3=1.0\n[0.30  0.45  0.25]   \u2192    [0.32  0.46  0.22]  \u03a3=1.0\n                           \u03a3=1.0 \u03a3=1.0 \u03a3=1.0  \u2190 converges in few iterations\n</code></pre></div>\n<p><strong>Step 4: Iterate</strong> \u2014 Repeat steps 2-3 for t_max iterations (typically 20) until convergence.</p>\n<p>The entire process is differentiable, allowing gradients to flow through during training. The Sinkhorn-Knopp algorithm is also computationally efficient, adding minimal overhead to the training loop.</p>\n<p>Beyond the projection algorithm, proper initialization is critical for training stability.</p>\n<h3 id=\"initialization-refinements\">Initialization Refinements</h3>\n<p>To ensure training starts stable:</p>\n<ul>\n<li><strong>Sigmoid over Tanh</strong>: Ensures coefficients are non-negative and bounded (0 to 1)</li>\n<li><strong>Scalar 2 multiplier</strong>: Sigmoid outputs ~0.5 at initialization; multiplying by 2 gives initial weight ~1.0, matching identity behavior</li>\n</ul>\n<hr />\n<h2 id=\"complete-mhc-architecture\">Complete mHC Architecture</h2>\n<p>Putting it all together:</p>\n<p><img alt=\"mHC Complete Architecture\" src=\"../../../../assets/2026-01-21-mhc-hyper-connections/mhc_architecture.svg\" /></p>\n<p>The flow is:</p>\n<ol>\n<li><strong>n input streams</strong> enter the block</li>\n<li><strong>Aggregation</strong> combines streams (weighted by Sinkhorn-constrained H)</li>\n<li><strong>Transformer block</strong> processes the aggregated input</li>\n<li><strong>Expansion</strong> distributes output back to n streams</li>\n<li><strong>Mixing</strong> allows inter-stream communication (constrained to Birkhoff Polytope)</li>\n<li><strong>n output streams</strong> proceed to next layer</li>\n</ol>\n<p>The key difference from standard HC: all mixing operations pass through Sinkhorn projection, guaranteeing stability.</p>\n<hr />\n<h2 id=\"infrastructure-making-it-practical\">Infrastructure: Making It Practical</h2>\n<p>Expanding to n=4 streams creates significant overhead. Each stream needs its own memory, and Sinkhorn adds 20 iterations per layer. The DeepSeek team solved this with aggressive optimization:</p>\n<h3 id=\"kernel-fusion\">Kernel Fusion</h3>\n<p>Using <a href=\"https://arxiv.org/abs/2504.17577\">TileLang</a>, they fused Sinkhorn iterations with mixed-precision multiplications into specialized CUDA kernels. This minimizes round-trips to high-bandwidth memory (HBM), which is often the actual bottleneck in modern training.</p>\n<h3 id=\"selective-recomputation\">Selective Recomputation</h3>\n<p>Storing all intermediate Sinkhorn states for backpropagation would explode memory usage. Instead, mHC:</p>\n<ul>\n<li>Frees intermediate activations after the forward pass</li>\n<li>Recomputes them on-the-fly during the backward pass</li>\n</ul>\n<p>A modified <a href=\"https://arxiv.org/abs/2412.19437\">DualPipe</a> schedule overlaps this recomputation with gradient communication, hiding latency.</p>\n<h3 id=\"results\">Results</h3>\n<p>With these optimizations, <strong>expansion rate n=4 runs with only 6.7% training overhead</strong> compared to the baseline\u2014proving complex topological routing is practical at scale.</p>\n<hr />\n<h2 id=\"empirical-validation\">Empirical Validation</h2>\n<p>The theoretical guarantees translate to real improvements:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Standard HC</th>\n<th>mHC</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Amax Gain (deep nets)</td>\n<td>~3000</td>\n<td>~1.6</td>\n</tr>\n<tr>\n<td>Training stability</td>\n<td>Frequent spikes</td>\n<td>Stable</td>\n</tr>\n<tr>\n<td>GSM8K benchmark</td>\n<td>Baseline</td>\n<td>Better</td>\n</tr>\n<tr>\n<td>MATH benchmark</td>\n<td>Baseline</td>\n<td>Better</td>\n</tr>\n</tbody>\n</table>\n<p>The mHC-27B model (based on DeepSeek-V3 architecture) outperforms both standard ResNet and unconstrained HC on mathematical reasoning benchmarks\u2014confirming that the gains come from the architecture itself, not noise from training instability.</p>\n<hr />\n<h2 id=\"trade-offs-and-considerations\">Trade-offs and Considerations</h2>\n<p>mHC is not free:</p>\n<ol>\n<li><strong>Computational overhead</strong>: 6.7% is excellent, but still more than standard residuals</li>\n<li><strong>Implementation complexity</strong>: Requires custom CUDA kernels for efficiency</li>\n<li><strong>Strong inductive bias</strong>: The doubly stochastic constraint is <em>conservation</em>\u2014signal can't be amplified. For tasks genuinely requiring signal amplification, this could be limiting</li>\n</ol>\n<hr />\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ol>\n<li><strong>Residual connections work because of identity mapping</strong>\u2014the ability to pass signals through unchanged</li>\n<li><strong>Hyper-Connections scale width instead of depth</strong>, enabling faster convergence through multi-stream routing</li>\n<li><strong>The flexibility of HC destroys identity mapping</strong>, causing signal explosion in deep networks</li>\n<li><strong>mHC constrains mixing matrices to the Birkhoff Polytope</strong>, mathematically guaranteeing stability</li>\n<li><strong>Sinkhorn-Knopp makes the constraint differentiable</strong>, enabling end-to-end training</li>\n<li><strong>Aggressive infrastructure optimization</strong> (kernel fusion, selective recomputation) makes it practical at scale</li>\n</ol>\n<p>For practitioners: if you're hitting limits with depth scaling and have access to custom kernel development, mHC offers a principled way to scale model capacity through width while maintaining training stability.</p>\n<hr />\n<h2 id=\"references\">References</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2512.24880\">mHC: Manifold-Constrained Hyper-Connections</a> - Xie et al. (DeepSeek)</li>\n<li><a href=\"https://arxiv.org/abs/1512.03385\">Deep Residual Learning for Image Recognition</a> - He et al. (ResNet)</li>\n<li><a href=\"https://arxiv.org/abs/2409.19606\">Hyper-Connections</a> - Original HC paper</li>\n<li><a href=\"https://arxiv.org/abs/2504.17577\">TileLang</a> - CUDA kernel optimization framework</li>\n<li><a href=\"https://arxiv.org/abs/2412.19437\">DualPipe</a> - Pipeline parallelism scheduler for DeepSeek-V3</li>\n<li><a href=\"https://arxiv.org/abs/2304.14802\">ResiDual</a> - Dual residual path architecture</li>\n</ul>", "image": null, "date_modified": "2026-01-21T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Paper Review"]}, {"id": "https://slavadubrov.github.io/blog/2026/01/11/enterprise-rag-challenge-3-winning-approaches-for-autonomous-ai-agents/", "url": "https://slavadubrov.github.io/blog/2026/01/11/enterprise-rag-challenge-3-winning-approaches-for-autonomous-ai-agents/", "title": "Enterprise RAG Challenge 3: Winning Approaches for Autonomous AI Agents", "content_html": "<h1 id=\"enterprise-rag-challenge-3-winning-approaches-for-autonomous-ai-agents\">Enterprise RAG Challenge 3: Winning Approaches for Autonomous AI Agents</h1>\n<p>The Enterprise RAG Challenge 3 (ECR3) just concluded, and the results reveal powerful insights for building production-grade AI agents. Out of 524 teams and over 341,000 agent runs, only <strong>0.4%</strong> achieved a perfect score\u2014making this one of the most demanding benchmarks in the field. With the leaderboard and solution write-ups now public, I analyzed the winning approaches to distill the patterns that separated top performers from the rest.</p>\n<p>This article breaks down what ECR3 is, what tasks were involved, and how the best teams solved them.</p>\n<!-- more -->\n\n<p><strong>TL;DR</strong>: Multi-agent pipelines with specialized agents outperformed monolithic designs. The top team used evolutionary prompt engineering running through 80+ automated iterations. Winners built intelligent guardrails (pre-flight security checks, in-loop validators, post-execution guards) and invested heavily in context strategy\u2014quality over quantity.</p>\n<h2 id=\"what-is-the-enterprise-rag-challenge\">What is the Enterprise RAG Challenge?</h2>\n<p>The Enterprise RAG Challenge 3 is a <a href=\"https://erc.timetoact-group.at/\">large-scale, crowdsourced research project</a> that tests how autonomous AI agents can solve complex business tasks. Unlike static benchmarks, ECR3 uses the <strong>Agentic Enterprise Simulation (AGES)</strong>\u2014a dynamic, discrete-event simulation that provides a <a href=\"https://erc.timetoact-group.at/benchmarks/erc3-prod\">realistic enterprise API</a>.</p>\n<h3 id=\"why-ecr3-matters\">Why ECR3 Matters</h3>\n<p>Through AGES, agents interact with a corporate ecosystem containing:</p>\n<ul>\n<li><strong>Employee profiles</strong> with specific skills and departments</li>\n<li><strong>Projects</strong> with team assignments and customer relationships</li>\n<li><strong>Corporate wiki</strong> with business rules and permission hierarchies</li>\n<li><strong>Time tracking and financial operations</strong></li>\n</ul>\n<p>Each task runs in a unique, isolated simulation instance with fresh data\u2014agents cannot memorize the environment.</p>\n<h3 id=\"the-difficulty-bar\">The Difficulty Bar</h3>\n<p>The <a href=\"https://erc.timetoact-group.at/benchmarks/erc3-prod\">statistics</a> paint a stark picture of the challenge's complexity:</p>\n<table>\n<thead>\n<tr>\n<th>Metric</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Registered Teams</strong></td>\n<td><a href=\"https://www.timetoact-group.at/en/insights/the-winner-of-the-enterprise-rag-challenge\">524</a></td>\n</tr>\n<tr>\n<td><strong>Total Agent Runs</strong></td>\n<td><a href=\"https://www.timetoact-group.at/en/insights/the-winner-of-the-enterprise-rag-challenge\">341,000+</a></td>\n</tr>\n<tr>\n<td><strong>Available Tasks</strong></td>\n<td><a href=\"https://erc.timetoact-group.at/benchmarks/erc3-prod\">103 unique business tasks</a></td>\n</tr>\n<tr>\n<td><strong>Perfect Score (100.0)</strong></td>\n<td>Only <a href=\"https://erc.timetoact-group.at/benchmarks/erc3-prod\"><strong>0.4%</strong> of teams</a></td>\n</tr>\n<tr>\n<td><strong>Score \u2265 0.9</strong></td>\n<td>Only <a href=\"https://erc.timetoact-group.at/benchmarks/erc3-prod\"><strong>1.1%</strong> of teams</a></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"types-of-tasks\">Types of Tasks</h3>\n<p>Agents faced diverse challenges requiring:</p>\n<ul>\n<li><strong>Multi-hop reasoning</strong>: Cross-reference employee skills with project assignments</li>\n<li><strong>Permission validation</strong>: Prevent unauthorized salary changes or data access</li>\n<li><strong>Ambiguous queries</strong>: Handle multilingual and paraphrased user requests</li>\n<li><strong>Strict output compliance</strong>: Include mandatory entity links in responses</li>\n</ul>\n<hr />\n<h2 id=\"key-findings-what-won-the-challenge\">Key Findings: What Won the Challenge</h2>\n<p>Before diving into specific architectures, four high-level insights emerged:</p>\n<p><strong>The Four Pillars of ECR3 Success:</strong></p>\n<ol>\n<li><strong>Multi-agent pipelines outperform monolithic agents</strong> \u2014 Separation of concerns wins</li>\n<li><strong>Success is defined by guardrails, not freedom</strong> \u2014 The most \"constrained\" agents scored highest</li>\n<li><strong>Automated evolution beats manual engineering</strong> \u2014 Self-improving prompts through 80+ generations</li>\n<li><strong>Context strategy is the primary performance lever</strong> \u2014 Quality over quantity in the context window</li>\n</ol>\n<hr />\n<h2 id=\"top-winning-approaches\">Top Winning Approaches</h2>\n<p>The following five architectures represent distinct strategies\u2014from fully automated prompt evolution to hybrid retrieval systems. Each offers unique insights you can adapt for your own agents.</p>\n<h3 id=\"1-evolutionary-prompt-engineering-team-vzs9fl-aostrikov\">1. Evolutionary Prompt Engineering (Team VZS9FL / @aostrikov)</h3>\n<p>The <a href=\"https://erc.timetoact-group.at/assets/erc3.html\">highest-scoring approach</a> completely automated prompt engineering through a self-improvement loop.</p>\n<p><img alt=\"Evolutionary Prompt Engineering Pipeline\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/evolutionary_pipeline.svg\" /></p>\n<p><strong>Core Philosophy</strong>: Instead of manually tuning prompts, let agents learn from their own failures.</p>\n<p><strong>The Three-Agent Pipeline</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Agent</th>\n<th>Role</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Main Agent</strong></td>\n<td>Runs benchmark, logs all actions and failures</td>\n</tr>\n<tr>\n<td><strong>Analyzer Agent</strong></td>\n<td>Reviews failed tasks, formulates hypotheses about root causes</td>\n</tr>\n<tr>\n<td><strong>Versioner Agent</strong></td>\n<td>Generates new prompt version incorporating learnings</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Key Result</strong>: The production prompt was the <a href=\"https://erc.timetoact-group.at/assets/erc3.html\"><strong>80th auto-generated version</strong></a>\u2014each iteration systematically patching failure patterns. This discovered fixes that would be nearly impossible to identify through manual engineering.</p>\n<p><strong>Technical Stack</strong>: <a href=\"https://erc.timetoact-group.at/assets/erc3.html\">claude-opus-4.5 with Anthropic Python SDK</a> native Tool Use.</p>\n<hr />\n<h3 id=\"2-multi-agent-sequential-pipeline-team-lcnxuy-andrey_aiweapps\">2. Multi-Agent Sequential Pipeline (Team Lcnxuy / @andrey_aiweapps)</h3>\n<p>This architecture rejected monolithic agents in favor of specialized, sequential workflows.</p>\n<p><img alt=\"Multi-Agent Sequential Pipeline\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/multi_agent_pipeline.svg\" /></p>\n<p><strong>Core Philosophy</strong>: Separation of concerns\u2014each agent optimized for a single task.</p>\n<p><strong>The 4-Stage Pipeline</strong>:</p>\n<ol>\n<li><strong>Security Gate Agent</strong>: Pre-execution check validating permissions against wiki rules before main loop begins</li>\n<li><strong>Context Extraction Agent</strong>: Surfaces critical rules from massive prompts, preloads user/project/customer data</li>\n<li><strong>Execution Agent</strong>: ReAct-style planning with 5 internal phases (Identity \u2192 Threat Detection \u2192 Info Gathering \u2192 Access Validation \u2192 Execution)</li>\n<li><strong>LinkGeneratorAgent</strong>: Embedded in response tool, automatically parses context to include required entity links</li>\n</ol>\n<p><strong>Key Insight</strong>: The embedded LinkGenerator solved one of the <a href=\"https://erc.timetoact-group.at/assets/erc3.html\">most common failure modes</a>\u2014providing correct answers but missing mandatory reference links.</p>\n<p><strong>Technical Stack</strong>: <a href=\"https://github.com/BrainBlend-AI/atomic-agents\"><code>atomic-agents</code></a> and <a href=\"https://github.com/instructor-ai/instructor\"><code>instructor</code></a> frameworks with gpt-5.1-codex-max, gpt-4.1, and claude-sonnet-4.5.</p>\n<hr />\n<h3 id=\"3-schema-guided-reasoning-with-step-validation-team-nln7dw-ilia-ris\">3. Schema-Guided Reasoning with Step Validation (Team NLN7Dw / Ilia Ris)</h3>\n<p>This architecture combined SGR with ultra-high-speed inference and real-time validation.</p>\n<p><img alt=\"SGR with Step Validation\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/sgr_validation.svg\" /></p>\n<p><strong>Core Philosophy</strong>: Make many rapid, validated decisions rather than one slow, deliberated one.</p>\n<p><strong>Key Components</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>StepValidator</strong></td>\n<td>Scrutinizes each proposed step\u2014if flawed, sends back for rework with specific comments</td>\n</tr>\n<tr>\n<td><strong>Context Management</strong></td>\n<td>Full plan from previous turn + compressed history for older turns</td>\n</tr>\n<tr>\n<td><strong>Dynamic Enrichment</strong></td>\n<td>Auto-pulls user profile, projects, customers; LLM filters to inject only task-relevant data</td>\n</tr>\n<tr>\n<td><strong>Auto-pagination Wrappers</strong></td>\n<td>All list endpoints return complete results automatically</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Speed Advantage</strong>: Running on <a href=\"https://cerebras.ai/\">Cerebras</a> at ~3,000 tokens/second, enabling rapid \"rethinking\" that outperformed slower, single-pass reasoning from larger models.</p>\n<p><strong>Technical Stack</strong>: <a href=\"https://cerebras.ai/\">gpt-oss-120b on Cerebras</a>, customized SGR NextStep implementation.</p>\n<hr />\n<h3 id=\"4-enricher-guard-system-team-j8gvbi-mishka\">4. Enricher &amp; Guard System (Team J8Gvbi / @mishka)</h3>\n<p>This architecture augmented SGR with non-blocking \"intelligent hints\" and a sophisticated guard system.</p>\n<p><img alt=\"Enricher &amp; Guard System\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/enricher_guard_system.svg\" /></p>\n<p><strong>Core Philosophy</strong>: Analyze API results in real-time to guide the agent's next steps.</p>\n<p><strong>The Enricher System</strong>:</p>\n<p>Over <a href=\"https://erc.timetoact-group.at/assets/erc3.html\"><strong>20 enrichers</strong></a> analyzed API responses and injected contextual hints:</p>\n<div class=\"highlight\"><pre><span></span><code>RoleEnricher: &quot;You are LEAD of this project, proceed with update.&quot;\nPaginationHintEnricher: &quot;next_offset=5 means MORE results! MUST paginate.&quot;\n</code></pre></div>\n<p><strong>Three-Mode Guard System</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Mode</th>\n<th>Behavior</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Hard block</strong></td>\n<td>Impossible actions blocked permanently</td>\n</tr>\n<tr>\n<td><strong>Soft block</strong></td>\n<td>Risky actions blocked on first attempt, allowed on retry</td>\n</tr>\n<tr>\n<td><strong>Soft hint</strong></td>\n<td>Guidance without blocking</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Hybrid RAG Wiki System</strong>: Three-stream search combining Regex, Semantic, and Keyword lookups\u2014using the best retrieval method for each query type.</p>\n<p><strong>Technical Stack</strong>: qwen/qwen3-235b-a22b-2507 on <a href=\"https://github.com/langchain-ai/langchain\">LangChain</a> SGR framework.</p>\n<hr />\n<h3 id=\"5-plan-execute-repl-team-key_concept_parallel\">5. Plan-Execute REPL (Team key_concept_parallel)</h3>\n<p>This architecture embraced strict separation between planning and execution, using a code-generating loop\u2014a REPL (Read-Eval-Print Loop) where the agent plans a step, generates code, executes it, and decides what to do next.</p>\n<p><img alt=\"Plan-Execute REPL Architecture\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/plan_execute_repl.svg\" /></p>\n<p><strong>Core Philosophy</strong>: Different models for different capabilities\u2014planners plan, coders code.</p>\n<p><strong>Multi-Model Architecture</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Stage</th>\n<th>Model</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Planning</td>\n<td>openai/gpt-5.1</td>\n</tr>\n<tr>\n<td>Code Generation</td>\n<td>deepseek/deepseek-v3.2</td>\n</tr>\n<tr>\n<td>Post-Step Decision</td>\n<td>openai/gpt-4.1</td>\n</tr>\n<tr>\n<td>Final Response</td>\n<td>openai/gpt-4.1</td>\n</tr>\n</tbody>\n</table>\n<p><strong>The Step Completion REPL</strong>:</p>\n<ol>\n<li>Planner creates high-level step</li>\n<li>Code-gen model writes Python script to achieve the step</li>\n<li>Script executes in isolated context</li>\n<li>Decision model analyzes result: continue / abort / replan</li>\n</ol>\n<p><strong>Key Advantage</strong>: The decision and repair loop enabled recovery from partial failures through dynamic replanning.</p>\n<hr />\n<h2 id=\"common-patterns-of-excellence\">Common Patterns of Excellence</h2>\n<p>Beyond individual architectures, several patterns appeared consistently across top performers.</p>\n<h3 id=\"context-management-the-primary-performance-lever\">Context Management: The Primary Performance Lever</h3>\n<p>Top teams recognized that agent performance is fundamentally limited by context quality.</p>\n<p><img alt=\"Context Management Strategies\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/context_strategies.svg\" /></p>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Approach</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Rule Distillation</strong></td>\n<td>Pre-process wiki via LLM into <a href=\"https://erc.timetoact-group.at/assets/erc3.html\">~320 token summary</a></td>\n<td>Lean prompts, fast startup</td>\n</tr>\n<tr>\n<td><strong>Aggressive Preloading</strong></td>\n<td>Load user/project/customer data before execution</td>\n<td>Minimizing tool calls</td>\n</tr>\n<tr>\n<td><strong>Hybrid RAG</strong></td>\n<td>Regex + Semantic + Keyword search streams</td>\n<td>Complex retrieval needs</td>\n</tr>\n<tr>\n<td><strong>History Compression</strong></td>\n<td>Keep recent turns full, compress older history</td>\n<td>Long conversations</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><strong>Trade-off Insight</strong>: <a href=\"https://erc.timetoact-group.at/assets/erc3.html\">Teams Kc7F2N and f1Uixf</a> found that context quality beats quantity. Team f1Uixf discovered that history compression actually degraded performance\u2014they kept full history, relying on long-context capabilities instead.</p>\n</blockquote>\n<hr />\n<h3 id=\"guardrail-architecture-the-autonomy-paradox\">Guardrail Architecture: The Autonomy Paradox</h3>\n<p>The most successful autonomous agents were the most <strong>effectively constrained</strong>.</p>\n<p><img alt=\"Guardrail Architecture\" src=\"../../../../assets/2026-01-09-ecr3-winning-approaches/guardrail_architecture.svg\" /></p>\n<table>\n<thead>\n<tr>\n<th>Guardrail Type</th>\n<th>When</th>\n<th>Example</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Pre-Execution Gates</strong></td>\n<td>Before main loop starts</td>\n<td>Security Gate Agent validates permissions against wiki rules</td>\n</tr>\n<tr>\n<td><strong>In-Loop Validators</strong></td>\n<td>During reasoning</td>\n<td>StepValidator checks each proposed action, triggers rework if flawed</td>\n</tr>\n<tr>\n<td><strong>Post-Execution Guards</strong></td>\n<td>Before final submission</td>\n<td>Three-Mode Guard System validates response completeness</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h3 id=\"intelligent-tool-wrappers\">Intelligent Tool Wrappers</h3>\n<p>Top teams built abstraction layers around raw API calls:</p>\n<ul>\n<li><strong>Auto-pagination</strong>: Wrappers automatically loop through all pages, returning complete datasets</li>\n<li><strong>Fuzzy normalization</strong>: Translate \"Willingness to travel\" \u2192 <code>will_travel</code> API field</li>\n<li><strong>Specialized reasoning tools</strong>: <code>think</code>, <code>plan</code>, and <code>critic</code> tools for controlled deliberation</li>\n</ul>\n<hr />\n<h2 id=\"common-failure-modes-mitigations\">Common Failure Modes &amp; Mitigations</h2>\n<p>Even top agents shared common blind spots. Here's what went wrong and how winners fixed it:</p>\n<table>\n<thead>\n<tr>\n<th>Failure Mode</th>\n<th>Description</th>\n<th>Architectural Fix</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Permission Bypass</strong></td>\n<td>Executing restricted actions without verifying user permissions</td>\n<td>Pre-execution Security Gate Agent; mandatory Identity \u2192 Permissions \u2192 Execution sequence</td>\n</tr>\n<tr>\n<td><strong>Missing Entity Links</strong></td>\n<td>Correct text answer but missing required reference links</td>\n<td>Embedded LinkGeneratorAgent in response tool</td>\n</tr>\n<tr>\n<td><strong>Pagination Exhaustion</strong></td>\n<td>Processing only first page of list results</td>\n<td>Auto-pagination wrappers for all list endpoints</td>\n</tr>\n<tr>\n<td><strong>Tool-Calling Loops</strong></td>\n<td>Stuck repeatedly calling same tool with minor variations</td>\n<td>Turn limits; reasoning-focused models (Qwen3)</td>\n</tr>\n<tr>\n<td><strong>Context Overloading</strong></td>\n<td>Filling context with irrelevant wiki sections</td>\n<td>Rule distillation; dynamic context filtering</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<p>What can you apply from these approaches when building your own agents?</p>\n<ol>\n<li>\n<p><strong>Embrace Multi-Agent Systems</strong>\n   Monolithic agents are consistently outperformed. Top teams used 3\u20135 specialized agents for security, context extraction, planning, and execution.</p>\n</li>\n<li>\n<p><strong>Prioritize Automated Refinement</strong>\n   The winning team's production prompt was the <strong>80th auto-generated version</strong>\u2014each iteration systematically patching failure patterns that manual engineering would likely miss.</p>\n</li>\n<li>\n<p><strong>Build Intelligent Guardrails, Not Just Agents</strong>\n   The most autonomous agents were the most constrained. Winners implemented three-layer validation: pre-flight security checks, in-loop critics, and post-execution guards.</p>\n</li>\n<li>\n<p><strong>Abstract Complexity with Tool Wrappers</strong>\n   Handle pagination, data enrichment, and fuzzy matching in wrappers. One team built <strong>20+ enrichers</strong> analyzing API responses in real-time.</p>\n</li>\n<li>\n<p><strong>Invest in Context Strategy</strong>\n   Combine rule distillation (~320 token summaries), aggressive preloading, and dynamic filtering. Speed matters\u2014one team ran at <strong>~3,000 tokens/second</strong> enabling rapid replanning.</p>\n</li>\n</ol>\n<hr />\n<h2 id=\"references\">References</h2>\n<ul>\n<li><a href=\"https://erc.timetoact-group.at/\">Enterprise RAG Challenge 3 - Official Site</a></li>\n<li><a href=\"https://erc.timetoact-group.at/assets/erc3.html\">ECR3 Leaderboards</a></li>\n<li><a href=\"https://erc.timetoact-group.at/benchmarks/erc3-prod\">Benchmark: erc3-prod</a></li>\n<li><a href=\"https://erc.timetoact-group.at/releases\">Release Notes</a></li>\n<li><a href=\"https://www.timetoact-group.at/en/insights/the-winner-of-the-enterprise-rag-challenge\">The Winner of the Enterprise RAG Challenge</a></li>\n</ul>", "image": null, "date_modified": "2026-01-11T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Agentic AI"]}, {"id": "https://slavadubrov.github.io/blog/2026/01/04/the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production/", "url": "https://slavadubrov.github.io/blog/2026/01/04/the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production/", "title": "The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production", "content_html": "<h1 id=\"the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production\">The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production</h1>\n<p>Fine-tuning has become the secret weapon for building specialized AI applications. While general-purpose models like GPT-4 and Claude excel at broad tasks, fine-tuning transforms them into laser-focused experts for your specific domain. This guide walks you through everything you need to know\u2014from understanding when to fine-tune to deploying your custom model.</p>\n<!-- more -->\n\n<h2 id=\"strategic-decision-fine-tuning-vs-alternatives\">Strategic Decision: Fine-Tuning vs Alternatives</h2>\n<p>Before investing GPU hours and engineering time, you need to answer a fundamental question: <strong>is fine-tuning the right solution for your problem?</strong></p>\n<p><img alt=\"Decision Flowchart\" src=\"../../../../assets/2026-01-04-finetuning-guide/decision_flowchart.svg\" /></p>\n<h3 id=\"fine-tuning-vs-rag\">Fine-Tuning vs RAG</h3>\n<p>Do not fine-tune just to add \"knowledge.\" Here's when to use each approach:</p>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Fine-Tuning</th>\n<th>RAG (Retrieval-Augmented Generation)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Core Function</strong></td>\n<td>Alters internal weights to teach skills, styles, or behaviors</td>\n<td>Provides external, up-to-date context at inference time</td>\n</tr>\n<tr>\n<td><strong>Best For</strong></td>\n<td>\u2022 Specific conversational styles<br>\u2022 Complex instruction following<br>\u2022 Domain-specific reasoning</td>\n<td>\u2022 Rapidly changing data (news, stock prices)<br>\u2022 Reducing hallucinations (grounding)<br>\u2022 Citing sources</td>\n</tr>\n<tr>\n<td><strong>Knowledge Handling</strong></td>\n<td>Internalizes patterns, not facts</td>\n<td>Retrieves facts from external knowledge base</td>\n</tr>\n<tr>\n<td><strong>Update Frequency</strong></td>\n<td>Requires retraining for updates</td>\n<td>Updates immediately with new documents</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"fine-tuning-vs-prompt-engineering\">Fine-Tuning vs Prompt Engineering</h3>\n<p>Modern LLMs are remarkably responsive to well-crafted prompts. Before fine-tuning, exhaust prompt engineering options:</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Fine-Tuning</th>\n<th>Prompt Engineering</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Setup Cost</strong></td>\n<td>High (data curation, GPU compute, iteration)</td>\n<td>Low (iterative prompt refinement)</td>\n</tr>\n<tr>\n<td><strong>Flexibility</strong></td>\n<td>Locked after training</td>\n<td>Change anytime without retraining</td>\n</tr>\n<tr>\n<td><strong>Format/Style</strong></td>\n<td>Best for complex, consistent output formats</td>\n<td>Good for simple formatting with few-shot examples</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>Lower (no long system prompts)</td>\n<td>Higher (context tax on every request)</td>\n</tr>\n<tr>\n<td><strong>Best For</strong></td>\n<td>Complex behaviors, distillation, cost at scale</td>\n<td>Rapid iteration, changing requirements</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>[!TIP] <strong>Try Prompting First</strong>\nStart with few-shot examples in your prompt. If you don't get consistent behavior, try SGR before jumping to fine-tuning.</p>\n</blockquote>\n<h3 id=\"fine-tuning-vs-schema-guided-reasoning-sgr\">Fine-Tuning vs Schema-Guided Reasoning (SGR)</h3>\n<p>Libraries like <code>xgrammar</code> and <code>outlines</code> constrain model outputs at inference time using Finite State Machines. They work with base models out of the box\u2014<strong>no training required</strong>.</p>\n<p><strong>SGR isn't just about structured outputs.</strong> Its primary value is <strong>consistency and reliability</strong>. When prompting alone produces inconsistent results, SGR enforces deterministic output patterns without the cost and complexity of fine-tuning.</p>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>SGR (No Fine-Tuning)</th>\n<th>Fine-Tuning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Setup</strong></td>\n<td>Immediate\u2014define schema, deploy</td>\n<td>Requires data curation, GPU compute, iteration</td>\n</tr>\n<tr>\n<td><strong>Consistency</strong></td>\n<td>Guaranteed structure, reliable patterns</td>\n<td>Learned behavior (may still vary)</td>\n</tr>\n<tr>\n<td><strong>Flexibility</strong></td>\n<td>Change schema anytime without retraining</td>\n<td>Locked after training</td>\n</tr>\n<tr>\n<td><strong>Latency</strong></td>\n<td>Slight overhead (model may \"fight\" schema)</td>\n<td>Lower (model naturally outputs format)</td>\n</tr>\n<tr>\n<td><strong>Best For</strong></td>\n<td>Structured outputs, consistent behavior</td>\n<td>Complex reasoning, deep behavioral changes</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Recommendation:</strong></p>\n<ol>\n<li><strong>Start with prompting</strong> \u2014 Simple few-shot examples for basic formatting</li>\n<li><strong>Add SGR if inconsistent</strong> \u2014 Use <code>xgrammar</code> or <code>outlines</code> to guarantee output structure and reliability</li>\n<li><strong>Fine-tune as last resort</strong> \u2014 Only when you need deep behavioral changes that schema constraints can't achieve</li>\n</ol>\n<h3 id=\"quick-reference-matching-problems-to-solutions\">Quick Reference: Matching Problems to Solutions</h3>\n<table>\n<thead>\n<tr>\n<th>Challenge</th>\n<th>Best Solution</th>\n<th>Why?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Missing knowledge</strong></td>\n<td>RAG</td>\n<td>Models hallucinate facts. Retrieval provides grounded, up-to-date context</td>\n</tr>\n<tr>\n<td><strong>Wrong format/tone</strong></td>\n<td>Prompt Engineering</td>\n<td>Modern models follow style instructions well via few-shot examples</td>\n</tr>\n<tr>\n<td><strong>Inconsistent outputs</strong></td>\n<td>SGR (xgrammar)</td>\n<td>Guaranteed structure and reliability without training</td>\n</tr>\n<tr>\n<td><strong>Complex behavioral changes</strong></td>\n<td>Fine-Tuning (SFT)</td>\n<td>Deep persona, reasoning patterns, or multi-step workflows</td>\n</tr>\n<tr>\n<td><strong>Safety/preference</strong></td>\n<td>Alignment (DPO)</td>\n<td>When outputs are correct but don't match preferences</td>\n</tr>\n<tr>\n<td><strong>Latency/cost at scale</strong></td>\n<td>Distillation (SFT)</td>\n<td>Train smaller student model on larger teacher's outputs</td>\n</tr>\n<tr>\n<td><strong>Reduce model size</strong></td>\n<td>Quantization</td>\n<td>No training\u2014compress weights (FP16\u2192INT4) for faster inference</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"the-economic-case\">The Economic Case</h3>\n<p>Fine-tuning shines in <strong>high-volume, stable-requirement scenarios</strong>. Consider this: a robust RAG system might require 2,000 tokens of context on every call (system prompt + retrieved docs + few-shot examples). That's your \"context tax\" on every request.</p>\n<p>A fine-tuned model can internalize those instructions, reducing your prompt from 2,000 tokens to 50. At scale, this pays for the training compute within weeks.</p>\n<blockquote>\n<p>[!TIP] <strong>The Hybrid Approach</strong>\nThe industry sweet spot is often a fine-tuned smaller model (8B params) combined with lightweight RAG for facts. This often outperforms prompting a massive model (70B+) in both accuracy and cost.</p>\n</blockquote>\n<hr />\n<h2 id=\"types-of-fine-tuning\">Types of Fine-Tuning</h2>\n<p>Before diving into the lifecycle, understand the three main approaches to fine-tuning:</p>\n<p><img alt=\"Fine-Tuning Types\" src=\"../../../../assets/2026-01-04-finetuning-guide/finetuning_types.svg\" /></p>\n<h3 id=\"1-continued-pre-training-unsupervised\">1. Continued Pre-training (Unsupervised)</h3>\n<p>Continued pre-training extends the base model's knowledge by training on additional raw text <strong>without labels</strong>. The model simply learns to predict the next token, just like during original pre-training.</p>\n<p><strong>When to use:</strong></p>\n<ul>\n<li>Your domain has specialized vocabulary the base model doesn't know (medical, legal, financial)</li>\n<li>You have large amounts of domain text but no labeled examples</li>\n<li>The base model struggles with domain-specific terminology</li>\n</ul>\n<p><strong>Example:</strong> Training on millions of clinical notes so the model understands medical abbreviations, drug names, and clinical workflows.</p>\n<h3 id=\"2-supervised-fine-tuning-sft\">2. Supervised Fine-Tuning (SFT)</h3>\n<p>SFT trains on labeled <strong>(input, output) pairs</strong>. You show the model exactly what output you expect for each input.</p>\n<p><strong>When to use:</strong></p>\n<ul>\n<li>You have a specific task with clear input/output format</li>\n<li>Quality labeled data is available (even small amounts)</li>\n<li>You need consistent, predictable behavior</li>\n</ul>\n<p><strong>Example:</strong> Training on (SQL query description, SQL code) pairs for Text-to-SQL conversion.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;input&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Get all users who signed up last month&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;output&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;SELECT * FROM users WHERE signup_date &gt;= DATE_SUB(NOW(), INTERVAL 1 MONTH)&quot;</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<h3 id=\"3-instruction-tuning\">3. Instruction Tuning</h3>\n<p>Instruction tuning is a <strong>special case of SFT</strong> designed to make models follow diverse natural language instructions. The training data consists of (instruction, response) pairs across many different tasks.</p>\n<p><strong>When to use:</strong></p>\n<ul>\n<li>You want a general-purpose assistant (like ChatGPT or Claude)</li>\n<li>The model needs to handle varied, open-ended requests</li>\n<li>You're building a chat interface</li>\n</ul>\n<p><strong>Example:</strong> Training on thousands of diverse instructions like \"Summarize this article,\" \"Write a poem about X,\" \"Explain Y in simple terms.\"</p>\n<h3 id=\"comparison\">Comparison</h3>\n<table>\n<thead>\n<tr>\n<th>Aspect</th>\n<th>Continued Pre-training</th>\n<th>SFT</th>\n<th>Instruction Tuning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Data</strong></td>\n<td>Raw text</td>\n<td>(input, output) pairs</td>\n<td>(instruction, response) pairs</td>\n</tr>\n<tr>\n<td><strong>Labels</strong></td>\n<td>None (unsupervised)</td>\n<td>Task-specific</td>\n<td>Diverse tasks</td>\n</tr>\n<tr>\n<td><strong>Goal</strong></td>\n<td>Domain knowledge</td>\n<td>Specific task behavior</td>\n<td>Follow any instruction</td>\n</tr>\n<tr>\n<td><strong>Data Volume</strong></td>\n<td>Large (millions of tokens)</td>\n<td>Small-Medium (500-10k examples)</td>\n<td>Medium-Large (10k-100k examples)</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>[!NOTE] <strong>Most Common Approach</strong>\nIn practice, most practitioners use <strong>SFT</strong> for specific tasks or <strong>Instruction Tuning</strong> for chat applications. Continued pre-training is rarer because it requires massive amounts of domain text and is computationally expensive.</p>\n</blockquote>\n<hr />\n<h2 id=\"the-7-stage-fine-tuning-lifecycle\">The 7-Stage Fine-Tuning Lifecycle</h2>\n<p>Fine-tuning isn't a single action\u2014it's a structured lifecycle. Understanding this pipeline is critical for success.</p>\n<p><img alt=\"7-Stage Pipeline\" src=\"../../../../assets/2026-01-04-finetuning-guide/pipeline_7_stages.svg\" /></p>\n<p>Each stage builds on the previous one:</p>\n<ol>\n<li><strong>Data Preparation</strong> \u2014 Clean, deduplicate, and format your data (highest leverage step)</li>\n<li><strong>Model Selection</strong> \u2014 Choose the right base model and load weights</li>\n<li><strong>Training Setup</strong> \u2014 Configure hardware, hyperparameters, and optimization strategy</li>\n<li><strong>Fine-Tuning</strong> \u2014 Run SFT, DPO, or ORPO training</li>\n<li><strong>Evaluation</strong> \u2014 Benchmark performance and validate quality</li>\n<li><strong>Deployment</strong> \u2014 Export and serve your model</li>\n<li><strong>Monitoring</strong> \u2014 Track performance, maintain, and iterate</li>\n</ol>\n<blockquote>\n<p>[!WARNING] <strong>Data is the Foundation</strong>\nStage 1 (Dataset Preparation) is the highest leverage step. Flaws in your data cannot be fixed by algorithms later. <strong>Quality over quantity</strong> \u2014 500-1,000 carefully curated examples often outperform 50,000 noisy ones.</p>\n</blockquote>\n<hr />\n<h2 id=\"stage-1-data-preparation-data-is-the-foundation\">Stage 1: Data Preparation \u2014 \"Data is the Foundation\"</h2>\n<p>This is where most fine-tuning projects succeed or fail. The industry has moved far beyond simple \"clean and format\" scripts.</p>\n<p><img alt=\"Data Pipeline\" src=\"../../../../assets/2026-01-04-finetuning-guide/data_pipeline.svg\" /></p>\n<h3 id=\"the-5-stage-data-pipeline\">The 5-Stage Data Pipeline</h3>\n<p>Modern production pipelines use tools like <strong>DataTrove</strong> (Hugging Face) and <strong>Distilabel</strong> (Argilla) rather than custom scripts:</p>\n<h4 id=\"1-ingestion-filtering\">1. Ingestion &amp; Filtering</h4>\n<ul>\n<li><strong>Action</strong>: Remove \"refusals\" (e.g., \"I cannot answer that\"), broken UTF-8, non-target languages</li>\n<li><strong>Tools</strong>: Trafilatura (extraction) + FastText (language ID)</li>\n</ul>\n<h4 id=\"2-pii-scrubbing-enterprise-critical\">2. PII Scrubbing (Enterprise Critical)</h4>\n<ul>\n<li><strong>Action</strong>: Detect and redact emails, IP addresses, phone numbers before training</li>\n<li><strong>Tools</strong>: Microsoft Presidio or scrubadub</li>\n<li><strong>Why</strong>: Training on customer PII is a critical security failure</li>\n</ul>\n<h4 id=\"3-deduplication-minhash-lsh\">3. Deduplication (MinHash LSH)</h4>\n<ul>\n<li><strong>Action</strong>: Remove near-duplicates to prevent memorization</li>\n<li><strong>Tools</strong>: DataTrove (industry standard for terabyte-scale processing)</li>\n</ul>\n<h4 id=\"4-synthetic-augmentation-the-2025-secret\">4. Synthetic Augmentation (The 2025 Secret)</h4>\n<ul>\n<li><strong>Action</strong>: Use a stronger \"teacher\" model (GPT-4o, DeepSeek-V3) to rewrite raw data into high-quality instruction-response pairs</li>\n<li><strong>Tools</strong>: Distilabel</li>\n<li><strong>Impact</strong>: This step often provides the biggest quality boost</li>\n</ul>\n<h4 id=\"5-formatting\">5. Formatting</h4>\n<ul>\n<li><strong>Action</strong>: Convert to standard formats (Alpaca or ShareGPT)</li>\n</ul>\n<h3 id=\"data-format-examples\">Data Format Examples</h3>\n<p><strong>Alpaca Format</strong> (Instruction-Following):</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;instruction&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Summarize the following text.&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;input&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;The text to be summarized...&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;output&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;This is the summary.&quot;</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<p><strong>ShareGPT/ChatML Format</strong> (Conversational):</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;conversations&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">        </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;from&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;Hello, who are you?&quot;</span><span class=\"w\"> </span><span class=\"p\">},</span>\n<span class=\"w\">        </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;from&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;assistant&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nt\">&quot;value&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;I am a helpful AI assistant.&quot;</span><span class=\"w\"> </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<h3 id=\"key-principles\">Key Principles</h3>\n<ul>\n<li><strong>Quality over Quantity</strong>: 500-1,000 carefully curated examples often outperform 50,000 noisy ones</li>\n<li><strong>Cleanliness</strong>: Remove irrelevant information, normalize text, ensure consistent formatting</li>\n<li><strong>Balance</strong>: Ensure representation across different topics to prevent bias</li>\n<li><strong>Enterprise Critical</strong>: PII scrubbing (Stage 2) is mandatory for production systems</li>\n</ul>\n<hr />\n<h2 id=\"stage-2-model-selection-hardware-requirements\">Stage 2: Model Selection &amp; Hardware Requirements</h2>\n<p>Choosing the right base model and understanding hardware constraints is critical for project success.</p>\n<h3 id=\"hardware-requirements-by-model-size\">Hardware Requirements by Model Size</h3>\n<p>The physics of fine-tuning impose strict memory constraints:</p>\n<table>\n<thead>\n<tr>\n<th>Tier</th>\n<th>Hardware Config</th>\n<th>Capability</th>\n<th>Use Case</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Enterprise Standard</strong></td>\n<td>8x NVIDIA H100 (80GB)</td>\n<td>Full Fine-Tuning</td>\n<td>Training 70B models with long context (32k+) at max speed</td>\n</tr>\n<tr>\n<td><strong>Minimum Viable (Pro)</strong></td>\n<td>4x NVIDIA A100 (80GB)</td>\n<td>QLoRA / LoRA</td>\n<td>Fine-tuning Qwen 72B or Llama 70B in 4-bit</td>\n</tr>\n<tr>\n<td><strong>Local R&amp;D</strong></td>\n<td>4x RTX 6000 Ada (48GB)</td>\n<td>QLoRA</td>\n<td>On-prem workstation for data privacy requirements</td>\n</tr>\n<tr>\n<td><strong>Hobbyist/Indie</strong></td>\n<td>1-2x RTX 3090/4090 (24GB)</td>\n<td>QLoRA</td>\n<td>Fine-tune 7B-32B models with parameter-efficient methods</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>[!TIP] <strong>Consumer GPUs Are More Capable Than You Think</strong>\nWith QLoRA and optimized frameworks like Unsloth, a single RTX 4090 (24GB) can fine-tune models up to 32B parameters. RTX 3090s remain excellent value for 7B-13B model training. The 24GB VRAM sweet spot makes these cards highly capable for serious fine-tuning work.</p>\n<p>[!NOTE] <strong>Why H100s?</strong>\nIt's not just VRAM\u2014it's <strong>FP8 precision</strong>. H100s support native FP8 training, which effectively doubles memory capacity and throughput compared to A100s. For long-context models (128k tokens), FP8 on H100s is often the only way to fit reasonable batch sizes.</p>\n</blockquote>\n<h3 id=\"memory-calculations\">Memory Calculations</h3>\n<p>To fine-tune a 72B model, you need to store:</p>\n<ul>\n<li><strong>Model Weights</strong> (16-bit): ~144 GB</li>\n<li><strong>Gradients &amp; Optimizer States</strong>: ~2-3x the model size (depending on optimizer, e.g., AdamW)</li>\n<li><strong>Activations</strong>: Scales with context length (e.g., 32k tokens)</li>\n</ul>\n<p>This is why QLoRA (4-bit quantization + LoRA adapters) is essential for most teams.</p>\n<hr />\n<h2 id=\"stage-3-training-methods-peft-lora\">Stage 3: Training Methods (PEFT &amp; LoRA)</h2>\n<h3 id=\"full-fine-tuning-vs-peft\">Full Fine-Tuning vs PEFT</h3>\n<p><strong>Full Fine-Tuning (FFT)</strong> updates all model weights. For a 7B model at 16-bit precision, you need roughly 112GB of VRAM just for training. This is prohibitive for most teams.</p>\n<p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> changes the game by updating only a small subset of parameters while freezing the rest.</p>\n<h3 id=\"lora-the-industry-standard\">LoRA: The Industry Standard</h3>\n<p>LoRA (Low-Rank Adaptation) is the foundational PEFT technique. The key insight: weight changes during fine-tuning have low \"intrinsic rank.\"</p>\n<p><img alt=\"LoRA Architecture\" src=\"../../../../assets/2026-01-04-finetuning-guide/lora_architecture.svg\" /></p>\n<p>Instead of updating a massive weight matrix <strong>W</strong> (dimension d\u00d7d), LoRA learns two smaller matrices:</p>\n<ul>\n<li><strong>A</strong> (d \u00d7 r) \u2014 down-projection</li>\n<li><strong>B</strong> (r \u00d7 d) \u2014 up-projection</li>\n</ul>\n<p>The update becomes: <strong>\u0394W = B \u00d7 A</strong></p>\n<p>With rank <code>r=16</code>, this reduces trainable parameters by <strong>~10,000x</strong>, dropping VRAM from 120GB to 16GB.</p>\n<h3 id=\"peft-methods-compared\">PEFT Methods Compared</h3>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>How It Works</th>\n<th>Memory Savings</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>LoRA</strong></td>\n<td>Low-rank matrices injected into frozen weights</td>\n<td>~10x</td>\n<td>General fine-tuning</td>\n</tr>\n<tr>\n<td><strong>QLoRA</strong></td>\n<td>LoRA + 4-bit base model quantization</td>\n<td>~20x</td>\n<td>Consumer GPUs (16-24GB)</td>\n</tr>\n<tr>\n<td><strong>DoRA</strong></td>\n<td>LoRA with magnitude/direction decomposition</td>\n<td>~10x</td>\n<td>When LoRA hits performance ceiling</td>\n</tr>\n<tr>\n<td><strong>HFT</strong></td>\n<td>Freezes half parameters per training round</td>\n<td>~2x</td>\n<td>Balance between FFT and PEFT</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"when-to-use-which\">When to Use Which?</h4>\n<ul>\n<li><strong>LoRA</strong>: Start here. It's fast, memory-efficient, and widely supported</li>\n<li><strong>QLoRA</strong>: When you need to fine-tune 70B models on consumer hardware</li>\n<li><strong>DoRA</strong>: When you need to match full fine-tuning quality on complex reasoning tasks</li>\n<li><strong>HFT</strong>: When you need better performance than LoRA but can't afford full fine-tuning</li>\n</ul>\n<h3 id=\"dora-weight-decomposed-lora\">DoRA: Weight-Decomposed LoRA</h3>\n<p>DoRA (Weight-Decomposed Low-Rank Adaptation) is a novel technique that bridges the performance gap between standard LoRA and full fine-tuning.</p>\n<p><img alt=\"DoRA Architecture\" src=\"../../../../assets/2026-01-04-finetuning-guide/dora_architecture.svg\" /></p>\n<p><strong>How it works:</strong></p>\n<p>Instead of treating weights as a single entity, DoRA decomposes pre-trained weights into two components:</p>\n<ol>\n<li><strong>Magnitude</strong> \u2014 Trainable scalar per column (controls \"strength\")</li>\n<li><strong>Direction</strong> \u2014 Updated with LoRA matrices (controls \"what\")</li>\n</ol>\n<p>The update becomes: <strong>W' = m \u00d7 (V + B \u00d7 A)</strong></p>\n<p>Where:</p>\n<ul>\n<li><code>m</code> = magnitude (trainable)</li>\n<li><code>V</code> = direction (W / ||W||)</li>\n<li><code>B \u00d7 A</code> = LoRA update to direction</li>\n</ul>\n<p><strong>Why it outperforms standard LoRA:</strong></p>\n<ul>\n<li>Richer parameter updates while maintaining efficiency</li>\n<li>Achieves learning outcomes closer to full fine-tuning</li>\n<li>Same memory efficiency (~10x savings)</li>\n<li>Particularly effective on complex reasoning tasks</li>\n</ul>\n<h3 id=\"half-fine-tuning-hft\">Half Fine-Tuning (HFT)</h3>\n<p>HFT offers a unique balance between full fine-tuning and PEFT methods:</p>\n<ul>\n<li><strong>Methodology</strong>: Freezes half of the model's parameters during each fine-tuning round while updating the other half</li>\n<li><strong>Strategy</strong>: The frozen and active halves vary across rounds</li>\n<li><strong>Benefit</strong>: Retains foundational knowledge (frozen params) while acquiring new skills (active params)</li>\n<li><strong>Use case</strong>: When LoRA is insufficient but full fine-tuning is too expensive</li>\n</ul>\n<h3 id=\"adapter-merging-for-multi-task-learning\">Adapter Merging for Multi-Task Learning</h3>\n<p>Instead of fine-tuning a monolithic model for multiple tasks, train separate small adapter modules for each function while keeping the base LLM frozen.</p>\n<p><strong>Merging Methods:</strong></p>\n<ol>\n<li><strong>Concatenation</strong> \u2014 Combines adapter parameters, increasing rank (fast, simple)</li>\n<li><strong>Linear Combination</strong> \u2014 Weighted sum of adapters (more control)</li>\n<li><strong>SVD</strong> \u2014 Matrix decomposition for merging (versatile but slower)</li>\n</ol>\n<p><strong>Example use case:</strong> One adapter for summarization, another for translation, merged into a single multi-task model.</p>\n<hr />\n<h2 id=\"stage-4-fine-tuning-preference-alignment\">Stage 4: Fine-Tuning &amp; Preference Alignment</h2>\n<p>When SFT isn't enough\u2014the model technically answers correctly but in \"wrong\" ways (too verbose, unsafe, wrong tone)\u2014you need preference alignment.</p>\n<p><img alt=\"Alignment Methods\" src=\"../../../../assets/2026-01-04-finetuning-guide/alignment_methods.svg\" /></p>\n<h4 id=\"traditional-approach-rlhf-with-ppo\">Traditional Approach: RLHF with PPO</h4>\n<p>The old standard was a complex 3-stage pipeline:</p>\n<ol>\n<li><strong>SFT</strong> \u2014 Learn the task</li>\n<li><strong>Reward Model</strong> \u2014 Train on human preferences (chosen vs rejected)</li>\n<li><strong>PPO (Proximal Policy Optimization)</strong> \u2014 Reinforcement learning to optimize policy</li>\n</ol>\n<p><strong>Problems:</strong></p>\n<ul>\n<li>Complex to implement and manage</li>\n<li>Computationally expensive (training multiple models)</li>\n<li>Unstable training (hyperparameter sensitive)</li>\n</ul>\n<h4 id=\"modern-streamlined-dpo\">Modern Streamlined: DPO</h4>\n<p><strong>DPO (Direct Preference Optimization)</strong> simplifies preference alignment by eliminating the explicit reward model and RL training loop. While DPO optimizes the same objective as RLHF (reward maximization with KL-divergence constraint), it achieves this through a reparameterized supervised learning objective rather than explicit reinforcement learning:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span>\n    <span class=\"s2\">&quot;prompt&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Explain quantum computing&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;chosen&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Quantum computing uses qubits...&quot;</span><span class=\"p\">,</span>   <span class=\"c1\"># Preferred response</span>\n    <span class=\"s2\">&quot;rejected&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Well, it&#39;s complicated...&quot;</span>        <span class=\"c1\"># Non-preferred response</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<p><strong>Benefits:</strong></p>\n<ul>\n<li>Simpler implementation (no explicit reward model or RL training loop)</li>\n<li>More stable training (supervised learning approach)</li>\n<li>Less compute required</li>\n</ul>\n<p><strong>The PPO vs DPO Debate:</strong></p>\n<p>Recent research suggests the debate isn't settled:</p>\n<ul>\n<li>DPO may yield biased solutions in some scenarios</li>\n<li>Well-tuned PPO can still achieve state-of-the-art results, particularly in complex tasks like code generation</li>\n<li>PPO's explicit reward signal provides more granular guidance for specialized tasks</li>\n</ul>\n<h4 id=\"newest-single-stage-orpo\">Newest Single-Stage: ORPO</h4>\n<p><strong>ORPO (Odds-Ratio Preference Optimization)</strong> is the 2025 recommendation for most use cases. It combines SFT and preference alignment into a <strong>single training stage</strong>.</p>\n<p><strong>How it works:</strong></p>\n<p>ORPO uses a combined loss function that simultaneously:</p>\n<ol>\n<li><strong>Maximizes likelihood</strong> of the chosen response (learning the task)</li>\n<li><strong>Penalizes</strong> the rejected response using an odds-ratio term (learning preferences)</li>\n</ol>\n<p><strong>Key Hyperparameters:</strong></p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">trl</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ORPOConfig</span>\n\n<span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">ORPOConfig</span><span class=\"p\">(</span>\n    <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">8e-6</span><span class=\"p\">,</span>  <span class=\"c1\"># Very low, as recommended by the ORPO paper</span>\n    <span class=\"n\">beta</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>            <span class=\"c1\"># Controls strength of preference penalty</span>\n    <span class=\"c1\"># ... other params</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n<ul>\n<li><strong>Learning Rate</strong>: Use very low values (8e-6) as recommended by the ORPO paper</li>\n<li><strong>Beta</strong>: Controls the strength of the preference penalty (typically 0.1)</li>\n</ul>\n<p><strong>Benefits:</strong></p>\n<ul>\n<li>\u2705 Single training stage (no separate SFT needed)</li>\n<li>\u2705 No reward model required</li>\n<li>\u2705 Fastest path to production</li>\n<li>\u2705 Simpler than DPO (fewer hyperparameters)</li>\n</ul>\n<p><strong>When to use what:</strong></p>\n<ul>\n<li><strong>ORPO</strong>: Start here for most use cases (fastest, simplest)</li>\n<li><strong>DPO</strong>: When you need more control over the alignment process</li>\n<li><strong>PPO</strong>: Only for specialized tasks requiring explicit reward signals (e.g., code generation)</li>\n</ul>\n<hr />\n<h2 id=\"fine-tuning-frameworks\">Fine-Tuning Frameworks</h2>\n<p>The ecosystem has consolidated around four major tools:</p>\n<h3 id=\"unsloth-speed-efficiency-champion\">Unsloth \u2014 Speed &amp; Efficiency Champion</h3>\n<p>Unsloth uses HuggingFace packages (<code>trl</code> and <code>transformers</code>) under the hood but adds additional optimizations:</p>\n<ul>\n<li><strong>Custom Triton kernels</strong> \u2014 Hand-written GPU kernels for attention, RoPE, and cross-entropy that bypass PyTorch overhead</li>\n<li><strong>Memory-efficient backpropagation</strong> \u2014 Recomputes activations during backward pass instead of storing them</li>\n<li><strong>Fused operations</strong> \u2014 Combines multiple operations (layer norm + linear, etc.) into single GPU calls</li>\n<li><strong>4-bit quantization integration</strong> \u2014 Seamless QLoRA with optimized dequantization</li>\n</ul>\n<blockquote>\n<p>[!IMPORTANT] <strong>Import Order Matters</strong>\nBecause Unsloth patches HuggingFace packages, you <strong>must</strong> import and initialize Unsloth's <code>FastLanguageModel</code> <strong>before</strong> importing <code>trl</code> or <code>transformers</code>. Incorrect import order will cause failures.</p>\n</blockquote>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># \u2705 Correct order</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">unsloth</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FastLanguageModel</span>  <span class=\"c1\"># Must be first!</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">trl</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">SFTTrainer</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">transformers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"c1\"># \u274c Wrong order - will fail</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">trl</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">SFTTrainer</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">unsloth</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FastLanguageModel</span>  <span class=\"c1\"># Too late!</span>\n</code></pre></div>\n<p><strong>Best for:</strong> Single-GPU training, prototyping, Colab notebooks, anyone paying for GPU hours.</p>\n<p><strong>Key advantage:</strong> Custom Triton kernels make it 2-5x faster than standard HuggingFace implementations.</p>\n<h3 id=\"axolotl-config-driven-production\">Axolotl \u2014 Config-Driven Production</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># config.yaml - no code required</span>\n<span class=\"nt\">base_model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">meta-llama/Meta-Llama-3-8B</span>\n<span class=\"nt\">adapter</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">qlora</span>\n<span class=\"nt\">lora_r</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">32</span>\n<span class=\"nt\">lora_alpha</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">16</span>\n<span class=\"nt\">datasets</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"nt\">path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">data/my_data.jsonl</span>\n<span class=\"w\">      </span><span class=\"nt\">type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">alpaca</span>\n<span class=\"nt\">sample_packing</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n</code></pre></div>\n<p>Run with: <code>accelerate launch -m axolotl.cli.train config.yaml</code></p>\n<p><strong>Best for:</strong> Production pipelines, multi-GPU clusters, reproducible experiments.</p>\n<p><strong>Key advantage:</strong> YAML configs are version-controllable and shareable.</p>\n<h3 id=\"framework-comparison\">Framework Comparison</h3>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Unsloth</th>\n<th>Axolotl</th>\n<th>TRL</th>\n<th>Torchtune</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Strength</strong></td>\n<td>Speed &amp; efficiency</td>\n<td>Multi-GPU scale</td>\n<td>Ecosystem</td>\n<td>PyTorch native</td>\n</tr>\n<tr>\n<td><strong>Speed</strong></td>\n<td>Fastest (2-5x)</td>\n<td>High</td>\n<td>Moderate</td>\n<td>High</td>\n</tr>\n<tr>\n<td><strong>Multi-GPU</strong></td>\n<td>Growing</td>\n<td>Excellent</td>\n<td>Good</td>\n<td>Excellent</td>\n</tr>\n<tr>\n<td><strong>Config</strong></td>\n<td>Python</td>\n<td>YAML</td>\n<td>Python</td>\n<td>Python</td>\n</tr>\n<tr>\n<td><strong>Best for</strong></td>\n<td>Local/Colab</td>\n<td>Clusters</td>\n<td>Research</td>\n<td>PyTorch purists</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"practical-demo-fine-tuning-with-unsloth\">Practical Demo: Fine-Tuning with Unsloth</h2>\n<p>Let's walk through a complete example using my <a href=\"https://github.com/slavadubrov/unsloth-finetune-demo\">unsloth-finetune-demo</a> repository. This demo fine-tunes Nemotron-Nano for function calling.</p>\n<p><img alt=\"Training Pipeline\" src=\"../../../../assets/2026-01-04-finetuning-guide/training_pipeline.svg\" /></p>\n<h3 id=\"quick-start\">Quick Start</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Clone and setup</span>\ngit<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/slavadubrov/unsloth-finetune-demo.git\n<span class=\"nb\">cd</span><span class=\"w\"> </span>unsloth-finetune-demo\n\n<span class=\"c1\"># Install with uv (recommended)</span>\nuv<span class=\"w\"> </span>sync\n\n<span class=\"c1\"># Run fine-tuning (quick test)</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>finetune<span class=\"w\"> </span>--max-samples<span class=\"w\"> </span><span class=\"m\">1000</span>\n</code></pre></div>\n<h3 id=\"configuration-deep-dive\">Configuration Deep Dive</h3>\n<p>The key configuration lives in <code>config.py</code>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Model &amp; Dataset</span>\n<span class=\"n\">MODEL_NAME</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&quot;</span>  <span class=\"c1\"># 4B params, 128K context</span>\n<span class=\"n\">DATASET_NAME</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;glaiveai/glaive-function-calling-v2&quot;</span>   <span class=\"c1\"># 113K examples</span>\n\n<span class=\"c1\"># LoRA Configuration</span>\n<span class=\"n\">LORA_R</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>        <span class=\"c1\"># Rank - higher = smarter but more VRAM</span>\n<span class=\"n\">LORA_ALPHA</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>    <span class=\"c1\"># Scaling factor - usually 2x LORA_R</span>\n<span class=\"n\">MAX_SEQ_LENGTH</span> <span class=\"o\">=</span> <span class=\"mi\">4096</span>\n\n<span class=\"c1\"># Target all linear layers for best quality</span>\n<span class=\"n\">LORA_TARGET_MODULES</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s2\">&quot;q_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;k_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;v_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;o_proj&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;gate_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;up_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;down_proj&quot;</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n</code></pre></div>\n<blockquote>\n<p>[!NOTE] <strong>The Alpha/Rank Ratio</strong>\nIndustry best practice in 2025: set <strong>alpha = 2 \u00d7 rank</strong> (e.g., rank=16, alpha=32). This provides stronger weight updates without destabilizing training.</p>\n</blockquote>\n<h3 id=\"core-training-code\">Core Training Code</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">unsloth</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FastLanguageModel</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">trl</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">SFTTrainer</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">transformers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">TrainingArguments</span>\n\n<span class=\"c1\"># Load model with 4-bit quantization</span>\n<span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">FastLanguageModel</span><span class=\"o\">.</span><span class=\"n\">from_pretrained</span><span class=\"p\">(</span>\n    <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s2\">&quot;nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">max_seq_length</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">,</span>\n    <span class=\"n\">load_in_4bit</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Add LoRA adapters</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FastLanguageModel</span><span class=\"o\">.</span><span class=\"n\">get_peft_model</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">r</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">lora_alpha</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n    <span class=\"n\">target_modules</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;q_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;k_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;v_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;o_proj&quot;</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;gate_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;up_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;down_proj&quot;</span><span class=\"p\">],</span>\n    <span class=\"n\">use_gradient_checkpointing</span><span class=\"o\">=</span><span class=\"s2\">&quot;unsloth&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Magic sauce for memory</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Train with SFTTrainer</span>\n<span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">SFTTrainer</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span>\n    <span class=\"n\">train_dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span>\n    <span class=\"n\">max_seq_length</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">,</span>\n    <span class=\"n\">packing</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>  <span class=\"c1\"># Crucial for efficiency!</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">TrainingArguments</span><span class=\"p\">(</span>\n        <span class=\"n\">per_device_train_batch_size</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n        <span class=\"n\">gradient_accumulation_steps</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\n        <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">2e-4</span><span class=\"p\">,</span>\n        <span class=\"n\">num_train_epochs</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n        <span class=\"n\">bf16</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n    <span class=\"p\">),</span>\n<span class=\"p\">)</span>\n<span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n</code></pre></div>\n<hr />\n<h2 id=\"fine-tuning-with-axolotl\">Fine-Tuning with Axolotl</h2>\n<blockquote>\n<p>[!NOTE] <strong>Demo Coming Soon</strong>\nI'm currently working on a hands-on Axolotl demo\u2014stay tuned! In the meantime, check out the <a href=\"https://huggingface.co/blog/accelerate-nd-parallel\">Accelerate n-D Parallelism Guide</a> from Hugging Face for multi-GPU training strategies.</p>\n</blockquote>\n<p>For production and multi-GPU setups, Axolotl's config-first approach excels:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># axolotl_config.yaml</span>\n<span class=\"nt\">base_model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">meta-llama/Meta-Llama-3-8B</span>\n<span class=\"nt\">model_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">LlamaForCausalLM</span>\n\n<span class=\"c1\"># QLoRA configuration</span>\n<span class=\"nt\">load_in_4bit</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n<span class=\"nt\">adapter</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">qlora</span>\n<span class=\"nt\">lora_r</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">32</span>\n<span class=\"nt\">lora_alpha</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">16</span>\n<span class=\"nt\">lora_dropout</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.05</span>\n<span class=\"nt\">lora_target_modules</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">q_proj</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">k_proj</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">v_proj</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">o_proj</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">gate_proj</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">up_proj</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">down_proj</span>\n\n<span class=\"c1\"># Dataset</span>\n<span class=\"nt\">datasets</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"nt\">path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">data/training_data.jsonl</span>\n<span class=\"w\">      </span><span class=\"nt\">type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">alpaca</span>\n\n<span class=\"c1\"># Training settings</span>\n<span class=\"nt\">sequence_len</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4096</span>\n<span class=\"nt\">sample_packing</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span><span class=\"w\"> </span><span class=\"c1\"># Critical for speed!</span>\n<span class=\"nt\">micro_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">2</span>\n<span class=\"nt\">gradient_accumulation_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">4</span>\n<span class=\"nt\">learning_rate</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.0002</span>\n<span class=\"nt\">num_epochs</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">3</span>\n\n<span class=\"c1\"># Hardware</span>\n<span class=\"nt\">bf16</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n<span class=\"nt\">flash_attention</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">true</span>\n</code></pre></div>\n<p>Run training:</p>\n<div class=\"highlight\"><pre><span></span><code>accelerate<span class=\"w\"> </span>launch<span class=\"w\"> </span>-m<span class=\"w\"> </span>axolotl.cli.train<span class=\"w\"> </span>axolotl_config.yaml\n</code></pre></div>\n<hr />\n<h2 id=\"stage-5-evaluation\">Stage 5: Evaluation</h2>\n<p>Training is easy; knowing if it worked is hard. Evaluation should happen before deployment.</p>\n<h3 id=\"automated-benchmarks\">Automated Benchmarks</h3>\n<p>Use <code>lm-evaluation-harness</code> for standardized testing:</p>\n<div class=\"highlight\"><pre><span></span><code>lm_eval<span class=\"w\"> </span>--model<span class=\"w\"> </span>hf<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--model_args<span class=\"w\"> </span><span class=\"nv\">pretrained</span><span class=\"o\">=</span>./outputs/merged-model<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--tasks<span class=\"w\"> </span>hellaswag,arc_easy,mmlu<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--batch_size<span class=\"w\"> </span><span class=\"m\">8</span>\n</code></pre></div>\n<h3 id=\"llm-as-judge\">LLM-as-Judge</h3>\n<p>For subjective quality, use a larger model to evaluate:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">judge_prompt</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">Rate this response from 1-5 on:</span>\n<span class=\"s2\">- Relevance</span>\n<span class=\"s2\">- Accuracy</span>\n<span class=\"s2\">- Formatting</span>\n\n<span class=\"s2\">Response: </span><span class=\"si\">{model_output}</span>\n<span class=\"s2\">Expected: </span><span class=\"si\">{ground_truth}</span>\n<span class=\"s2\">&quot;&quot;&quot;</span>\n</code></pre></div>\n<h3 id=\"domain-specific-eval\">Domain-Specific Eval</h3>\n<p>Create a held-out test set of real examples from your use case. This is the most important evaluation\u2014generic benchmarks won't tell you if your function-calling model actually works.</p>\n<hr />\n<h2 id=\"stage-6-deployment-output-formats\">Stage 6: Deployment &amp; Output Formats</h2>\n<p>After training, you have three export options:</p>\n<p><img alt=\"Output Formats\" src=\"../../../../assets/2026-01-04-finetuning-guide/output_formats.svg\" /></p>\n<h3 id=\"1-lora-adapter-default\">1. LoRA Adapter (Default)</h3>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>finetune<span class=\"w\">  </span><span class=\"c1\"># Saves ~100-500MB adapter</span>\n</code></pre></div>\n<ul>\n<li><strong>Size:</strong> ~100-500 MB</li>\n<li><strong>Best for:</strong> Development, testing, multiple adapters on one base model</li>\n<li><strong>Flexibility:</strong> Swap adapters without re-downloading the base model</li>\n</ul>\n<h3 id=\"2-merged-model\">2. Merged Model</h3>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>finetune<span class=\"w\"> </span>--merge<span class=\"w\">  </span><span class=\"c1\"># Creates standalone ~8-16GB model</span>\n</code></pre></div>\n<ul>\n<li><strong>Size:</strong> ~8-16 GB (full 16-bit weights)</li>\n<li><strong>Best for:</strong> Sharing on HuggingFace, vLLM serving, simple deployment</li>\n<li><strong>Trade-off:</strong> Larger storage, but no separate base model needed</li>\n</ul>\n<h3 id=\"3-gguf-format\">3. GGUF Format</h3>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>finetune<span class=\"w\"> </span>--gguf<span class=\"w\"> </span>q4_k_m<span class=\"w\">  </span><span class=\"c1\"># Creates ~2-4GB quantized model</span>\n</code></pre></div>\n<ul>\n<li><strong>Size:</strong> ~2-4 GB (Q4_K_M quantization)</li>\n<li><strong>Best for:</strong> CPU inference, Ollama, llama.cpp, edge deployment</li>\n<li><strong>Options:</strong> <code>q4_k_m</code> (balanced), <code>q5_k_m</code> (higher quality), <code>q8_0</code> (near-lossless)</li>\n</ul>\n<hr />\n<h2 id=\"stage-7-serving-monitoring\">Stage 7: Serving &amp; Monitoring</h2>\n<h3 id=\"with-vllm-production\">With vLLM (Production)</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Requires merged model format</span>\nvllm<span class=\"w\"> </span>serve<span class=\"w\"> </span>./outputs/unsloth-nemotron-function-calling-merged<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--host<span class=\"w\"> </span><span class=\"m\">0</span>.0.0.0<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--port<span class=\"w\"> </span><span class=\"m\">8000</span><span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--max-model-len<span class=\"w\"> </span><span class=\"m\">4096</span>\n</code></pre></div>\n<p>Query via OpenAI-compatible API:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">openai</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">OpenAI</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">OpenAI</span><span class=\"p\">(</span><span class=\"n\">base_url</span><span class=\"o\">=</span><span class=\"s2\">&quot;http://localhost:8000/v1&quot;</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s2\">&quot;dummy&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">chat</span><span class=\"o\">.</span><span class=\"n\">completions</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;unsloth-nemotron-function-calling-merged&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"p\">[{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Book a flight to Tokyo&quot;</span><span class=\"p\">}]</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"with-ollama-local\">With Ollama (Local)</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Create Modelfile</span>\n<span class=\"nb\">echo</span><span class=\"w\"> </span><span class=\"s1\">&#39;FROM ./outputs/unsloth-nemotron-function-calling-gguf/model-q4_k_m.gguf&#39;</span><span class=\"w\"> </span>&gt;<span class=\"w\"> </span>Modelfile\n\n<span class=\"c1\"># Import to Ollama</span>\nollama<span class=\"w\"> </span>create<span class=\"w\"> </span>my-function-model<span class=\"w\"> </span>-f<span class=\"w\"> </span>Modelfile\n\n<span class=\"c1\"># Run</span>\nollama<span class=\"w\"> </span>run<span class=\"w\"> </span>my-function-model\n</code></pre></div>\n<h3 id=\"with-llamacpp-cpu\">With llama.cpp (CPU)</h3>\n<div class=\"highlight\"><pre><span></span><code>./main<span class=\"w\"> </span>-m<span class=\"w\"> </span>./outputs/model-q4_k_m.gguf<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>-p<span class=\"w\"> </span><span class=\"s2\">&quot;What&#39;s the weather in Tokyo?&quot;</span><span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--ctx-size<span class=\"w\"> </span><span class=\"m\">4096</span>\n</code></pre></div>\n<hr />\n<h2 id=\"key-takeaways\">Key Takeaways</h2>\n<ol>\n<li><strong>Follow the 7-stage lifecycle</strong> \u2014 Data Preparation is the highest leverage step</li>\n<li><strong>Don't default to fine-tuning</strong> \u2014 Try RAG and prompting first; use the decision framework</li>\n<li><strong>Data is the foundation</strong> \u2014 Use modern pipelines (DataTrove, Distilabel) with PII scrubbing for enterprise</li>\n<li><strong>Start with QLoRA</strong> \u2014 Fine-tune 70B models on consumer GPUs (4x A100 minimum for production)</li>\n<li><strong>Use ORPO for alignment</strong> \u2014 Single-stage training is faster and simpler than DPO or PPO</li>\n<li><strong>Consider DoRA</strong> \u2014 When LoRA hits performance ceiling on complex reasoning tasks</li>\n<li><strong>Sample packing</strong> is the single biggest training speedup</li>\n<li><strong>Start with Unsloth</strong> for prototyping, <strong>Axolotl</strong> for production</li>\n<li><strong>Export to GGUF</strong> for local/edge deployment</li>\n</ol>\n<hr />\n<h2 id=\"references\">References</h2>\n<h3 id=\"papers-research\">Papers &amp; Research</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a></li>\n<li><a href=\"https://arxiv.org/abs/2305.14314\">QLoRA: Efficient Finetuning of Quantized LLMs</a></li>\n<li><a href=\"https://arxiv.org/abs/2402.09353\">DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>\n<li><a href=\"https://arxiv.org/abs/2305.18290\">DPO: Direct Preference Optimization</a></li>\n<li><a href=\"https://arxiv.org/abs/2403.07691\">ORPO: Odds Ratio Preference Optimization</a></li>\n<li><a href=\"https://arxiv.org/abs/1707.06347\">PPO: Proximal Policy Optimization Algorithms</a> \u2014 OpenAI, 2017</li>\n<li><a href=\"https://arxiv.org/abs/2404.18466\">HFT: Half Fine-Tuning for Large Language Models</a> \u2014 Mitigating catastrophic forgetting</li>\n</ul>\n<h3 id=\"data-processing-tools\">Data Processing Tools</h3>\n<ul>\n<li><a href=\"https://github.com/huggingface/datatrove\">DataTrove</a> \u2014 Hugging Face data processing at scale</li>\n<li><a href=\"https://github.com/argilla-io/distilabel\">Distilabel</a> \u2014 Synthetic data generation (Argilla)</li>\n<li><a href=\"https://github.com/adbar/trafilatura\">Trafilatura</a> \u2014 Web text extraction and crawling</li>\n<li><a href=\"https://fasttext.cc/\">FastText</a> \u2014 Facebook AI language identification (supports 217 languages)</li>\n<li><a href=\"https://github.com/microsoft/presidio\">Microsoft Presidio</a> \u2014 PII detection and anonymization</li>\n<li><a href=\"https://github.com/LeapBeyond/scrubadub\">scrubadub</a> \u2014 Python library for PII removal</li>\n</ul>\n<h3 id=\"schema-guided-reasoning-sgr\">Schema-Guided Reasoning (SGR)</h3>\n<ul>\n<li><a href=\"https://github.com/mlc-ai/xgrammar\">xgrammar</a> \u2014 Constrained decoding with FSMs</li>\n<li><a href=\"https://github.com/dottxt-ai/outlines\">outlines</a> \u2014 Structured generation for LLMs</li>\n</ul>\n<h3 id=\"training-frameworks\">Training Frameworks</h3>\n<ul>\n<li><a href=\"https://docs.unsloth.ai/\">Unsloth</a> \u2014 Speed &amp; efficiency champion (2-5x faster)</li>\n<li><a href=\"https://github.com/axolotl-ai-cloud/axolotl\">Axolotl</a> \u2014 Config-driven multi-GPU training</li>\n<li><a href=\"https://huggingface.co/docs/trl\">TRL (Transformer Reinforcement Learning)</a> \u2014 Hugging Face RL training</li>\n<li><a href=\"https://github.com/pytorch/torchtune\">Torchtune</a> \u2014 PyTorch-native fine-tuning library</li>\n</ul>\n<h3 id=\"inference-deployment\">Inference &amp; Deployment</h3>\n<ul>\n<li><a href=\"https://docs.vllm.ai/\">vLLM</a> \u2014 High-throughput LLM serving engine</li>\n<li><a href=\"https://ollama.ai/\">Ollama</a> \u2014 Local LLM runner for Mac/Windows/Linux</li>\n<li><a href=\"https://github.com/ggerganov/llama.cpp\">llama.cpp</a> \u2014 CPU/GPU inference with GGUF format</li>\n</ul>\n<h3 id=\"evaluation\">Evaluation</h3>\n<ul>\n<li><a href=\"https://github.com/EleutherAI/lm-evaluation-harness\">lm-evaluation-harness</a> \u2014 EleutherAI standardized LLM benchmarking</li>\n</ul>\n<h3 id=\"guides-resources\">Guides &amp; Resources</h3>\n<ul>\n<li><a href=\"https://github.com/slavadubrov/unsloth-finetune-demo\">Demo Repository</a> \u2014 Practical fine-tuning example</li>\n<li><a href=\"https://notebooklm.google.com/notebook/f6bfdb56-8949-4929-87e4-ab6dee31a4a8\">LLM Fine-Tuning. Theoretical Intuition and Practical Implementation</a> \u2014 NotebookLM research notebook</li>\n<li><a href=\"https://huggingface.co/blog/accelerate-nd-parallel\">Accelerate n-D Parallelism Guide</a> \u2014 Hugging Face multi-GPU training strategies</li>\n</ul>", "image": null, "date_modified": "2026-01-04T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["AI Engineering"]}, {"id": "https://slavadubrov.github.io/blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/", "url": "https://slavadubrov.github.io/blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/", "title": "Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines", "content_html": "<h1 id=\"schema-guided-reasoning-on-vllm-turning-llms-into-reliable-business-logic-engines\">Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines</h1>\n<h2 id=\"tldr\">TL;DR</h2>\n<blockquote>\n<p><strong>Schema-Guided Reasoning (SGR)</strong> is a technique that forces LLMs to reason through predefined steps by enforcing structured output schemas. Instead of hoping the model follows your formatting instructions, you <strong>guarantee</strong> it with constrained decoding. Combined with vLLM's xgrammar backend, you get 100% valid JSON output with near-zero latency overhead.</p>\n</blockquote>\n<p><strong>The problem</strong>: You build an LLM-powered agent. It works in demos. In production, it outputs malformed JSON, skips reasoning steps, and gives inconsistent responses. You add retry loops, validation layers, larger models. Costs explode.</p>\n<p><strong>The fix</strong>: Define your reasoning topology as a Pydantic schema. Let xgrammar enforce it at the token generation level. The LLM physically cannot produce invalid output.</p>\n<!-- more -->\n\n<hr />\n<h2 id=\"what-is-schema-guided-reasoning\">What is Schema-Guided Reasoning?</h2>\n<p>Schema-Guided Reasoning (SGR) is a technique pioneered by <a href=\"https://abdullin.com/schema-guided-reasoning/\">Rinat Abdullin</a> that guides LLMs to produce structured, clear, and predictable outputs by enforcing reasoning through predefined steps.</p>\n<p>Instead of allowing free-form text completion (which can be inconsistent or ambiguous), the schema acts as a strict guideline. By creating a specific schema (or structured template), you explicitly define:</p>\n<ul>\n<li><strong>What steps</strong> the model must go through (preventing skipped reasoning)</li>\n<li><strong>In which order</strong> it must reason (ensuring logical flow)</li>\n<li><strong>Where it should focus</strong> attention (improving depth and accuracy)</li>\n</ul>\n<p>Think of it as giving the model a \"cognitive checklist\" that it <strong>must</strong> follow.</p>\n<p><img alt=\"SGR Overview\" src=\"../../../../assets/2025-12-25-sgr-vllm/sgr_overview.svg\" /></p>\n<h3 id=\"why-sgr-matters\">Why SGR Matters</h3>\n<p>The core insight is simple but powerful:</p>\n<blockquote>\n<p><strong>Schema = Cognitive Scaffold</strong></p>\n</blockquote>\n<p>When you define a schema with fields like <code>churn_analysis</code>, <code>margin_math</code>, and then <code>max_discount_percent</code>, the model is <strong>forced</strong> to populate these fields in order. It cannot jump to the discount decision without first analyzing the data.</p>\n<p>This translates to:</p>\n<ul>\n<li><strong>Reproducible reasoning</strong> \u2014 consistent inference across repeated runs</li>\n<li><strong>Auditable outputs</strong> \u2014 every reasoning step is explicit and inspectable</li>\n<li><strong>Debuggable &amp; testable</strong> \u2014 intermediate outputs can be evaluated against test datasets</li>\n<li><strong>Works with smaller models</strong> \u2014 the schema \"holds the hand\" of weaker models</li>\n<li><strong>5-10% accuracy boost</strong> \u2014 <a href=\"https://abdullin.com/schema-guided-reasoning/\">commonly observed</a> in production deployments</li>\n</ul>\n<hr />\n<h2 id=\"sgr-vs-chain-of-thought-vs-prompt-engineering\">SGR vs Chain of Thought vs Prompt Engineering</h2>\n<p>Let's be precise about what makes SGR different from the approaches you're probably already using.</p>\n<p><img alt=\"SGR Comparison\" src=\"../../../../assets/2025-12-25-sgr-vllm/sgr_comparison.svg\" /></p>\n<h3 id=\"the-comparison\">The Comparison</h3>\n<table>\n<thead>\n<tr>\n<th>Feature</th>\n<th>Prompt Engineering</th>\n<th>Chain of Thought</th>\n<th>Schema-Guided Reasoning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Output Structure</strong></td>\n<td>Variable text</td>\n<td>Free-form prose</td>\n<td>Rigid JSON/Pydantic</td>\n</tr>\n<tr>\n<td><strong>Control Mechanism</strong></td>\n<td>Semantic persuasion (\"Please output JSON\")</td>\n<td>Heuristic prompting (\"Let's think step by step\")</td>\n<td>Constrained decoding (grammar-based)</td>\n</tr>\n<tr>\n<td><strong>Reasoning Flow</strong></td>\n<td>Model determines</td>\n<td>Model determines</td>\n<td>Developer determines (schema topology)</td>\n</tr>\n<tr>\n<td><strong>Auditability</strong></td>\n<td>Low (requires parsing)</td>\n<td>Low (requires reading prose)</td>\n<td>High (field-level inspection)</td>\n</tr>\n<tr>\n<td><strong>Integration</strong></td>\n<td>Difficult (regex parsing)</td>\n<td>Difficult (variable format)</td>\n<td>Trivial (native object deserialization)</td>\n</tr>\n<tr>\n<td><strong>Error Rate</strong></td>\n<td>High (format variability)</td>\n<td>Moderate (hallucination of format)</td>\n<td>Near-zero (syntax enforced by engine)</td>\n</tr>\n<tr>\n<td><strong>Model Requirement</strong></td>\n<td>Strong instruction following</td>\n<td>Strong reasoning capability</td>\n<td>Works with smaller models too</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"prompt-engineering-semantic-persuasion\">Prompt Engineering: Semantic Persuasion</h3>\n<div class=\"highlight\"><pre><span></span><code>Please analyze the customer data and output your response as valid JSON\nwith the following structure: {&quot;discount&quot;: &lt;number&gt;, &quot;reason&quot;: &lt;string&gt;}\nBe careful with the formatting!\n</code></pre></div>\n<p><strong>The problem</strong>: You're <em>hoping</em> the model's semantic understanding of \"output JSON\" outweighs its tendency to be conversational. A model update, temperature change, or different few-shot examples can break your parser.</p>\n<h3 id=\"chain-of-thought-better-reasoning-same-structure-problems\">Chain of Thought: Better Reasoning, Same Structure Problems</h3>\n<div class=\"highlight\"><pre><span></span><code>Let&#39;s think step by step:\n1. First, I&#39;ll analyze the customer&#39;s churn risk...\n2. Then I&#39;ll calculate the margin...\n3. Therefore, I recommend a 15% discount.\n</code></pre></div>\n<p>CoT improves reasoning <strong>accuracy</strong> but makes structure <strong>worse</strong>. The output is unpredictable prose that's nearly impossible to parse reliably. You end up needing a second LLM call to extract structured data from the reasoning.</p>\n<h3 id=\"sgr-structured-chain-of-thought\">SGR: Structured Chain of Thought</h3>\n<p>SGR doesn't abandon CoT's insight that intermediate reasoning improves accuracy. It <strong>formalizes</strong> it:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PricingLogic</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"c1\"># 1. Data Analysis (must complete before decision)</span>\n    <span class=\"n\">churn_analysis</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Analyze churn_probability&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">financial_analysis</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Analyze cart_value and margin&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 2. Math Enforcement (explicit calculation)</span>\n    <span class=\"n\">margin_math</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Calculate: &#39;Cart $X * Y% = $Z&#39;&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 3. Decision Constraint (bounded by prior analysis)</span>\n    <span class=\"n\">max_discount_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Max allowed discount&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 4. Final Output</span>\n    <span class=\"n\">offer_code</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">customer_message</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n</code></pre></div>\n<p>The model <strong>cannot</strong> output <code>max_discount_percent</code> without first populating <code>churn_analysis</code>, <code>financial_analysis</code>, and <code>margin_math</code>. The schema enforces the reasoning order.</p>\n<hr />\n<h2 id=\"sgr-patterns\">SGR Patterns</h2>\n<p>SGR enables three foundational patterns for controlling LLM reasoning. These can be combined for complex workflows.</p>\n<p><img alt=\"SGR Patterns\" src=\"../../../../assets/2025-12-25-sgr-vllm/sgr_patterns.svg\" /></p>\n<h3 id=\"1-cascade-sequential-reasoning-steps\">1. Cascade: Sequential Reasoning Steps</h3>\n<p>Cascade ensures the model follows predefined reasoning steps in order. Each field must be completed before the next.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Literal</span><span class=\"p\">,</span> <span class=\"n\">Annotated</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">annotated_types</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Ge</span><span class=\"p\">,</span> <span class=\"n\">Le</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">CandidateEvaluation</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Evaluate a job candidate with enforced reasoning order.&quot;&quot;&quot;</span>\n\n    <span class=\"c1\"># Step 1: Summarize (forces context awareness)</span>\n    <span class=\"n\">brief_candidate_summary</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n\n    <span class=\"c1\"># Step 2: Rate (bounded integer)</span>\n    <span class=\"n\">rate_skill_match</span><span class=\"p\">:</span> <span class=\"n\">Annotated</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Ge</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">Le</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)]</span>\n\n    <span class=\"c1\"># Step 3: Decide (constrained choices)</span>\n    <span class=\"n\">final_recommendation</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s2\">&quot;hire&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;reject&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;hold&quot;</span><span class=\"p\">]</span>\n</code></pre></div>\n<p><strong>Use cases</strong>: Candidate evaluation, document classification, compliance analysis, medical diagnosis</p>\n<p><strong>Key insight</strong>: The model must complete <code>brief_candidate_summary</code> before it can rate, and must rate before it can recommend. No shortcuts allowed.</p>\n<hr />\n<h3 id=\"2-routing-semantic-switch-statement\">2. Routing: Semantic Switch Statement</h3>\n<p>Routing forces the model to explicitly choose one path from multiple options. This is implemented using <code>Union</code> types.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Literal</span><span class=\"p\">,</span> <span class=\"n\">Union</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">FeatureLookup</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Route to database lookup.&quot;&quot;&quot;</span>\n    <span class=\"n\">rationale</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tool_name</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s2\">&quot;fetch_user_features&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;fetch_user_features&quot;</span>\n    <span class=\"n\">user_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">GeneralResponse</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Standard response for non-pricing queries.&quot;&quot;&quot;</span>\n    <span class=\"n\">tool_name</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s2\">&quot;respond&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;respond&quot;</span>\n    <span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">RouterSchema</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;The model must pick exactly ONE branch.&quot;&quot;&quot;</span>\n    <span class=\"n\">action</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">FeatureLookup</span><span class=\"p\">,</span> <span class=\"n\">GeneralResponse</span><span class=\"p\">]</span>\n</code></pre></div>\n<p><strong>Use cases</strong>: Intent classification, tool selection, support triage, multi-agent dispatch</p>\n<p><strong>Key insight</strong>: The <code>Literal</code> type with a discriminator field (like <code>tool_name</code>) ensures the model commits to one branch and fills in the required fields for that specific path.</p>\n<hr />\n<h3 id=\"3-cycle-repeated-reasoning-with-lists\">3. Cycle: Repeated Reasoning with Lists</h3>\n<p>Cycle forces the model to produce multiple items, with constraints on minimum and maximum count.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Literal</span><span class=\"p\">,</span> <span class=\"n\">Annotated</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">annotated_types</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">MinLen</span><span class=\"p\">,</span> <span class=\"n\">MaxLen</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">RiskFactor</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">explanation</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">severity</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s2\">&quot;low&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;medium&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;high&quot;</span><span class=\"p\">]</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">RiskAssessment</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Generate 2-4 risk factors.&quot;&quot;&quot;</span>\n    <span class=\"n\">factors</span><span class=\"p\">:</span> <span class=\"n\">Annotated</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">RiskFactor</span><span class=\"p\">],</span> <span class=\"n\">MinLen</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">MaxLen</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">)]</span>\n</code></pre></div>\n<p><strong>Use cases</strong>: Risk assessment, issue extraction, parallel tool calls, multi-step planning</p>\n<p><strong>Key insight</strong>: The <code>MinLen</code> and <code>MaxLen</code> annotations force the model to generate at least 2 but no more than 4 items. Combined with Routing, this enables parallel tool dispatch.</p>\n<hr />\n<h2 id=\"making-sgr-work-constrained-decoding\">Making SGR Work: Constrained Decoding</h2>\n<p>The patterns above are powerful, but they're just Pydantic schemas \u2014 how do we actually <strong>enforce</strong> them? The answer is <strong>Constrained Decoding</strong> (also called Structured Output).</p>\n<p>Constrained Decoding works by modifying the token generation process. Instead of allowing the model to freely sample from its vocabulary, the decoding engine applies a <strong>grammar mask</strong> that blocks tokens that would violate the schema. This happens at the inference engine level, not in your application code.</p>\n<blockquote>\n<p>[!TIP]\nSGR doesn't require \"reasoning models\" (like o1 or DeepSeek-R1). It works well with instruction-tuned models, and especially well with models distilled from reasoning models.</p>\n</blockquote>\n<h3 id=\"supported-cloud-providers\">Supported Cloud Providers</h3>\n<p>Most modern LLM providers now support Structured Outputs via constrained decoding:</p>\n<table>\n<thead>\n<tr>\n<th>Provider</th>\n<th>Support</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>OpenAI</strong></td>\n<td><a href=\"https://platform.openai.com/docs/guides/structured-outputs\">Structured Outputs</a> (including Azure). <a href=\"https://abdullin.com/schema-guided-reasoning/\">GPT-5 uses JSON Schema via llguidance</a></td>\n</tr>\n<tr>\n<td><strong>Google/Gemini</strong></td>\n<td><a href=\"https://ai.google.dev/gemini-api/docs/structured-output\">JSON Schema</a> support since Nov 2025 (Pydantic and Zod)</td>\n</tr>\n<tr>\n<td><strong>Mistral</strong></td>\n<td><a href=\"https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/\">Custom Structured Output</a></td>\n</tr>\n<tr>\n<td><strong>Grok</strong></td>\n<td><a href=\"https://docs.x.ai/docs/guides/structured-outputs\">Structured Outputs</a> for multiple models</td>\n</tr>\n<tr>\n<td><strong>Fireworks AI</strong></td>\n<td><a href=\"https://docs.fireworks.ai/structured-responses/structured-response-formatting\">JSON Schema</a></td>\n</tr>\n<tr>\n<td><strong>Cerebras</strong></td>\n<td><a href=\"https://inference-docs.cerebras.ai/capabilities/structured-outputs\">Structured Outputs</a></td>\n</tr>\n<tr>\n<td><strong>OpenRouter</strong></td>\n<td>Depends on downstream provider, maps to JSON Schema</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"supported-inference-engines\">Supported Inference Engines</h3>\n<p>For self-hosted models, most modern inference engines support constrained decoding:</p>\n<table>\n<thead>\n<tr>\n<th>Engine</th>\n<th>Backend</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>vLLM</strong></td>\n<td><a href=\"https://github.com/mlc-ai/xgrammar\">xgrammar</a> or <a href=\"https://github.com/guidance-ai/llguidance\">guidance</a></td>\n</tr>\n<tr>\n<td><strong>SGLang</strong></td>\n<td><a href=\"https://github.com/dottxt-ai/outlines\">Outlines</a>, <a href=\"https://github.com/mlc-ai/xgrammar\">XGrammar</a>, or <a href=\"https://github.com/guidance-ai/llguidance\">llguidance</a></td>\n</tr>\n<tr>\n<td><strong>TensorRT-LLM</strong></td>\n<td><a href=\"https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-api/llm_guided_decoding.py\">GuidedDecoding</a></td>\n</tr>\n<tr>\n<td><strong>Ollama</strong></td>\n<td><a href=\"https://ollama.com/blog/structured-outputs\">Structured Outputs</a></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"why-this-article-focuses-on-vllm-xgrammar\">Why This Article Focuses on vLLM + xgrammar</h3>\n<p>For this article, we'll dive deep into <strong>vLLM with the xgrammar backend</strong> because:</p>\n<ol>\n<li><strong>Production-grade</strong>: vLLM is the most widely deployed open-source LLM inference engine</li>\n<li><strong>Zero-overhead</strong>: xgrammar is implemented in C++ with near-zero latency impact</li>\n<li><strong>OpenAI-compatible API</strong>: Easy migration from cloud to self-hosted</li>\n<li><strong>Full schema support</strong>: Handles complex nested schemas, unions, and recursive structures</li>\n</ol>\n<p>Let's look at how xgrammar actually enforces these schemas at the token level.</p>\n<hr />\n<h2 id=\"how-xgrammar-enforces-schemas\">How xgrammar Enforces Schemas</h2>\n<p>Now let's get precise about <strong>when</strong> and <strong>how</strong> xgrammar enforces your schema. Understanding this helps you debug and tune your SGR workflows.</p>\n<p><img alt=\"xgrammar Enforcement\" src=\"../../../../assets/2025-12-25-sgr-vllm/xgrammar_enforcement.svg\" /></p>\n<h3 id=\"where-does-masking-happen\">Where Does Masking Happen?</h3>\n<p>Here's the key insight: <strong>xgrammar modifies the output logits AFTER the model's forward pass, BEFORE sampling</strong>. It does not change the model itself \u2014 it filters what tokens can be selected.</p>\n<p>The standard LLM inference loop looks like this:</p>\n<div class=\"highlight\"><pre><span></span><code>1. Input tokens \u2192 GPU Forward Pass \u2192 Logits (probability scores for all ~128K tokens)\n2. Logits \u2192 Sampling (temperature, top-p, etc.) \u2192 Next Token\n3. Repeat until done\n</code></pre></div>\n<p>xgrammar inserts itself between steps 1 and 2:</p>\n<div class=\"highlight\"><pre><span></span><code>1. Input tokens \u2192 GPU Forward Pass \u2192 Raw Logits\n2. Raw Logits \u2192 xgrammar Logits Processor \u2192 Masked Logits\n3. Masked Logits \u2192 Sampling \u2192 Next Token (guaranteed valid)\n4. Repeat until done\n</code></pre></div>\n<p>The critical point: <strong>the model computes its full probability distribution on the GPU first</strong>. Then xgrammar, running on CPU, applies a bitmask to the logits before sampling. Invalid tokens get their logits set to <code>-\u221e</code>, which makes their probability exactly 0 after softmax.</p>\n<h3 id=\"the-two-phase-process\">The Two-Phase Process</h3>\n<p>xgrammar's efficiency comes from splitting the work into two phases:</p>\n<h4 id=\"phase-1-grammar-compilation-one-time-before-inference\">Phase 1: Grammar Compilation (one-time, before inference)</h4>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># This happens once per schema</span>\n<span class=\"n\">tokenizer_info</span> <span class=\"o\">=</span> <span class=\"n\">xgr</span><span class=\"o\">.</span><span class=\"n\">TokenizerInfo</span><span class=\"o\">.</span><span class=\"n\">from_huggingface</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"p\">)</span>\n<span class=\"n\">grammar_compiler</span> <span class=\"o\">=</span> <span class=\"n\">xgr</span><span class=\"o\">.</span><span class=\"n\">GrammarCompiler</span><span class=\"p\">(</span><span class=\"n\">tokenizer_info</span><span class=\"p\">)</span>\n<span class=\"n\">compiled_grammar</span> <span class=\"o\">=</span> <span class=\"n\">grammar_compiler</span><span class=\"o\">.</span><span class=\"n\">compile_json_schema</span><span class=\"p\">(</span><span class=\"n\">schema_json</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>During compilation, xgrammar:</p>\n<ol>\n<li>Converts your JSON Schema to a Context-Free Grammar (CFG)</li>\n<li>Builds a Pushdown Automaton (PDA) \u2014 like a state machine with a stack for handling nested structures like <code>{\"a\": {\"b\": {\"c\": ...}}}</code></li>\n<li>Pre-computes which tokens are valid at each grammar position (the \"adaptive token mask cache\")</li>\n<li>Categorizes tokens as \"context-independent\" (can be pre-checked) or \"context-dependent\" (must be checked at runtime based on stack state)</li>\n</ol>\n<blockquote>\n<p>[!NOTE]\nAbout 99% of tokens are context-independent and can be cached (<a href=\"https://arxiv.org/abs/2411.15100\">XGrammar paper</a>). This is why xgrammar is so fast \u2014 most validity checks are just cache lookups.</p>\n</blockquote>\n<h4 id=\"phase-2-runtime-mask-generation-every-token\">Phase 2: Runtime Mask Generation (every token)</h4>\n<p>At each generation step:</p>\n<ol>\n<li>The <code>GrammarMatcher</code> tracks the current position in the grammar</li>\n<li>It retrieves the pre-computed mask for context-independent tokens (cache lookup)</li>\n<li>It runs the PDA to check the remaining context-dependent tokens</li>\n<li>It combines these into a final bitmask and applies it to the logits</li>\n</ol>\n<h3 id=\"why-pushdown-automata-matter\">Why Pushdown Automata Matter</h3>\n<p>You might wonder: why not just use regex? The answer is <strong>nesting</strong>.</p>\n<p>A regular expression (which is a Finite State Machine) cannot reliably match structures like:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;user&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;profile&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;settings&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;theme&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;dark&quot;</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"p\">}</span><span class=\"w\"> </span><span class=\"p\">}</span>\n</code></pre></div>\n<p>The problem is matching the closing braces <code>}}}</code> \u2014 you need to \"remember\" how many you opened. A Pushdown Automaton has a <strong>stack</strong> that tracks this context, enabling it to handle arbitrary nesting depth.</p>\n<p>This is why xgrammar can enforce complex schemas with Union types, nested objects, and recursive structures \u2014 capabilities that simpler regex-based approaches cannot match.</p>\n<h3 id=\"concrete-example-generating-a-float-field\">Concrete Example: Generating a Float Field</h3>\n<p>When the model is generating <code>\"max_discount_percent\":</code>, xgrammar knows from the schema that a <code>float</code> comes next. The mask:</p>\n<ul>\n<li><strong>Allows</strong> (probability unchanged): <code>0</code>, <code>1</code>, <code>2</code>, ..., <code>9</code>, <code>.</code>, <code>-</code></li>\n<li><strong>Blocks</strong> (probability \u2192 0): <code>\"</code>, <code>{</code>, <code>[</code>, <code>true</code>, <code>false</code>, <code>null</code>, and all 128K+ other tokens</li>\n</ul>\n<p>The model's forward pass might have assigned high probability to the word <code>\"fifteen\"</code>. But after xgrammar's mask, that token has probability 0. The model <strong>must</strong> output digits.</p>\n<h3 id=\"performance-why-near-zero-overhead\">Performance: Why \"Near-Zero Overhead\"?</h3>\n<p>Three factors make xgrammar fast:</p>\n<ol>\n<li>\n<p><strong>Parallel execution</strong>: Mask computation (CPU) overlaps with the next forward pass (GPU). While the GPU computes logits for token N+1, the CPU computes the mask for token N.</p>\n</li>\n<li>\n<p><strong>Caching</strong>: 99%+ of token validity is pre-computed during grammar compilation. Runtime checks are mostly cache lookups.</p>\n</li>\n<li>\n<p><strong>C++ implementation</strong>: The hot path is optimized C++, not Python. The mask is applied directly to logits in-place.</p>\n</li>\n</ol>\n<p>In benchmarks, xgrammar often shows <strong>negligible overhead</strong> \u2014 and sometimes structured generation is <em>faster</em> than unconstrained generation because the constrained vocabulary reduces sampling complexity.</p>\n<hr />\n<h2 id=\"practical-implementation-with-vllm\">Practical Implementation with vLLM</h2>\n<p>Let's look at a complete implementation using the <a href=\"https://github.com/slavadubrov/sgr-discount-manager\">sgr-discount-manager</a> project \u2014 a demo that shows SGR patterns for dynamic pricing.</p>\n<p><img alt=\"Agent Workflow\" src=\"../../../../assets/2025-12-25-sgr-vllm/agent_workflow.svg\" /></p>\n<h3 id=\"project-structure\">Project Structure</h3>\n<div class=\"highlight\"><pre><span></span><code>sgr/\n\u251c\u2500\u2500 agent.py            # Main orchestration\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 schemas.py      # Pydantic SGR schemas\n\u251c\u2500\u2500 prompts/\n\u2502   \u251c\u2500\u2500 routing.py      # Phase 1 prompts\n\u2502   \u2514\u2500\u2500 pricing.py      # Phase 3 prompts\n\u251c\u2500\u2500 store/\n\u2502   \u2514\u2500\u2500 hybrid_store.py # Hot/Cold data retrieval\n\u2514\u2500\u2500 utils/\n    \u2514\u2500\u2500 llm_client.py   # LLM client wrapper with xgrammar\n</code></pre></div>\n<h3 id=\"step-1-define-your-schemas\">Step 1: Define Your Schemas</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># sgr/models/schemas.py</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Literal</span><span class=\"p\">,</span> <span class=\"n\">Union</span>\n\n\n<span class=\"c1\"># --- Phase 1: Routing (Union for branching) ---</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">FeatureLookup</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Route to DB lookup if pricing context is needed.&quot;&quot;&quot;</span>\n    <span class=\"n\">rationale</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tool_name</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s2\">&quot;fetch_user_features&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;fetch_user_features&quot;</span>\n    <span class=\"n\">user_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">GeneralResponse</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Standard response for non-pricing queries.&quot;&quot;&quot;</span>\n    <span class=\"n\">tool_name</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s2\">&quot;respond&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;respond&quot;</span>\n    <span class=\"n\">content</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">RouterSchema</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">action</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">FeatureLookup</span><span class=\"p\">,</span> <span class=\"n\">GeneralResponse</span><span class=\"p\">]</span>\n\n\n<span class=\"c1\"># --- Phase 2: Pricing Logic (Cascade for sequential reasoning) ---</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PricingLogic</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Strict reasoning topology for dynamic pricing.</span>\n<span class=\"sd\">    Fields are ordered to enforce the analysis\u2192decision flow.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"c1\"># 1. Data Analysis (Reflection)</span>\n    <span class=\"n\">churn_analysis</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Analyze churn_probability (High &gt; 0.7).&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">financial_analysis</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Analyze cart_value and profit_margin.&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 2. Hard Math Enforcement</span>\n    <span class=\"n\">margin_math</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Calculate absolute profit: &#39;Cart $200 * 0.20 Margin = $40&#39;.&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 3. The Decision Constraint</span>\n    <span class=\"n\">max_discount_percent</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Max allowed discount %. NEVER exceed margin.&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 4. Final Output</span>\n    <span class=\"n\">offer_code</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Generated code (e.g. SAVE20).&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">customer_message</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The final polite offer text.&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"step-2-create-the-llm-client-with-xgrammar\">Step 2: Create the LLM Client with xgrammar</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># sgr/utils/llm_client.py</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">openai</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">OpenAI</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">TypeVar</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n\n<span class=\"n\">T</span> <span class=\"o\">=</span> <span class=\"n\">TypeVar</span><span class=\"p\">(</span><span class=\"s2\">&quot;T&quot;</span><span class=\"p\">,</span> <span class=\"n\">bound</span><span class=\"o\">=</span><span class=\"n\">BaseModel</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">LLMClient</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Wrapper for vLLM with xgrammar-enforced structured generation.&quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">base_url</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;http://localhost:8000/v1&quot;</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">OpenAI</span><span class=\"p\">(</span><span class=\"n\">base_url</span><span class=\"o\">=</span><span class=\"n\">base_url</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s2\">&quot;EMPTY&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_get_available_model</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">_get_available_model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Auto-detect the model running on vLLM server.&quot;&quot;&quot;</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">models</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">list</span><span class=\"p\">()</span>\n            <span class=\"k\">if</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"n\">models</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">id</span>\n        <span class=\"k\">except</span> <span class=\"ne\">Exception</span><span class=\"p\">:</span>\n            <span class=\"k\">pass</span>\n        <span class=\"k\">return</span> <span class=\"s2\">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">run_sgr</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">messages</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">],</span> <span class=\"n\">schema_class</span><span class=\"p\">:</span> <span class=\"nb\">type</span><span class=\"p\">[</span><span class=\"n\">T</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"n\">T</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Run inference with Schema-Guided Response constraints.</span>\n\n<span class=\"sd\">        Uses vLLM&#39;s guided_json with xgrammar backend to enforce</span>\n<span class=\"sd\">        strict schema constraints at the token generation level.</span>\n<span class=\"sd\">        &quot;&quot;&quot;</span>\n        <span class=\"n\">schema_dict</span> <span class=\"o\">=</span> <span class=\"n\">schema_class</span><span class=\"o\">.</span><span class=\"n\">model_json_schema</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># Enhance system message with schema for model guidance</span>\n        <span class=\"n\">enhanced_messages</span> <span class=\"o\">=</span> <span class=\"n\">messages</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">enhanced_messages</span> <span class=\"ow\">and</span> <span class=\"n\">enhanced_messages</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">:</span>\n            <span class=\"n\">schema_json</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"n\">schema_dict</span><span class=\"p\">,</span> <span class=\"n\">indent</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n            <span class=\"n\">enhanced_messages</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n                <span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"p\">(</span>\n                    <span class=\"n\">enhanced_messages</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;content&quot;</span><span class=\"p\">]</span>\n                    <span class=\"o\">+</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n\\n</span><span class=\"s2\">Respond with JSON matching this schema:</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"n\">schema_json</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n                <span class=\"p\">),</span>\n            <span class=\"p\">}</span>\n\n        <span class=\"c1\"># The magic: vLLM&#39;s guided_json with xgrammar backend</span>\n        <span class=\"n\">completion</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">chat</span><span class=\"o\">.</span><span class=\"n\">completions</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span>\n            <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">,</span>\n            <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">enhanced_messages</span><span class=\"p\">,</span>\n            <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>  <span class=\"c1\"># Low temp for deterministic reasoning</span>\n            <span class=\"n\">extra_body</span><span class=\"o\">=</span><span class=\"p\">{</span>\n                <span class=\"s2\">&quot;guided_json&quot;</span><span class=\"p\">:</span> <span class=\"n\">schema_dict</span><span class=\"p\">,</span>  <span class=\"c1\"># Pydantic schema as dict</span>\n                <span class=\"s2\">&quot;guided_decoding_backend&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;xgrammar&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Hardware-enforced</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"n\">raw_response</span> <span class=\"o\">=</span> <span class=\"n\">completion</span><span class=\"o\">.</span><span class=\"n\">choices</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">message</span><span class=\"o\">.</span><span class=\"n\">content</span>\n        <span class=\"k\">return</span> <span class=\"n\">schema_class</span><span class=\"o\">.</span><span class=\"n\">model_validate_json</span><span class=\"p\">(</span><span class=\"n\">raw_response</span><span class=\"p\">)</span>\n</code></pre></div>\n<blockquote>\n<p>[!NOTE]\nThe <code>guided_json</code> parameter accepts a JSON Schema dict. Combined with <code>guided_decoding_backend: \"xgrammar\"</code>, this ensures the LLM can only generate tokens that form valid JSON matching your schema.</p>\n</blockquote>\n<h3 id=\"step-3-orchestrate-the-agent\">Step 3: Orchestrate the Agent</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># sgr/agent.py</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">.models.schemas</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">PricingLogic</span><span class=\"p\">,</span> <span class=\"n\">RouterSchema</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">.prompts.routing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">build_routing_prompt</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">.prompts.pricing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">build_pricing_context_prompt</span><span class=\"p\">,</span> <span class=\"n\">ASSISTANT_FETCH_MESSAGE</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">.store.hybrid_store</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">HybridFeatureStore</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">.utils.llm_client</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">LLMClient</span>\n\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">pricing_agent</span><span class=\"p\">(</span><span class=\"n\">user_query</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">user_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Process a pricing query with three-phase SGR workflow.&quot;&quot;&quot;</span>\n\n    <span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">LLMClient</span><span class=\"p\">()</span>\n    <span class=\"n\">feature_store</span> <span class=\"o\">=</span> <span class=\"n\">HybridFeatureStore</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># Build conversation history</span>\n    <span class=\"n\">history</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"n\">build_routing_prompt</span><span class=\"p\">(</span><span class=\"n\">user_id</span><span class=\"p\">)},</span>\n        <span class=\"p\">{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"n\">user_query</span><span class=\"p\">},</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"c1\"># --- Phase 1: Routing (Uses RouterSchema) ---</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;\ud83e\udd16 Processing: &#39;</span><span class=\"si\">{</span><span class=\"n\">user_query</span><span class=\"si\">}</span><span class=\"s2\">&#39; for </span><span class=\"si\">{</span><span class=\"n\">user_id</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">decision</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">run_sgr</span><span class=\"p\">(</span><span class=\"n\">history</span><span class=\"p\">,</span> <span class=\"n\">RouterSchema</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;\ud83d\udccd Routing decision: </span><span class=\"si\">{</span><span class=\"n\">decision</span><span class=\"o\">.</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">tool_name</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">decision</span><span class=\"o\">.</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">tool_name</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;respond&quot;</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">decision</span><span class=\"o\">.</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">content</span>\n\n    <span class=\"c1\"># --- Phase 2: Context Retrieval ---</span>\n    <span class=\"k\">if</span> <span class=\"n\">decision</span><span class=\"o\">.</span><span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">tool_name</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;fetch_user_features&quot;</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;\ud83d\udd0d Fetching features for </span><span class=\"si\">{</span><span class=\"n\">user_id</span><span class=\"si\">}</span><span class=\"s2\">...&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"n\">feature_store</span><span class=\"o\">.</span><span class=\"n\">get_user_context</span><span class=\"p\">(</span><span class=\"n\">user_id</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">context</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"s2\">&quot;Error: User profile not found.&quot;</span>\n\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;   [Data] LTV: $</span><span class=\"si\">{</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;user_ltv&#39;</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\"> | &quot;</span>\n              <span class=\"sa\">f</span><span class=\"s2\">&quot;Margin: </span><span class=\"si\">{</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;cart_profit_margin&#39;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"mi\">100</span><span class=\"si\">}</span><span class=\"s2\">%&quot;</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Inject context into conversation</span>\n        <span class=\"n\">history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;assistant&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"n\">ASSISTANT_FETCH_MESSAGE</span><span class=\"p\">})</span>\n        <span class=\"n\">history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">({</span>\n            <span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"n\">build_pricing_context_prompt</span><span class=\"p\">(</span>\n                <span class=\"n\">churn_prob</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;churn_probability&quot;</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">),</span>\n                <span class=\"n\">cart_val</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;current_cart_value&quot;</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">),</span>\n                <span class=\"n\">margin</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;cart_profit_margin&quot;</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">),</span>\n                <span class=\"n\">user_ltv</span><span class=\"o\">=</span><span class=\"n\">context</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;user_ltv&quot;</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span>\n            <span class=\"p\">),</span>\n        <span class=\"p\">})</span>\n\n        <span class=\"c1\"># --- Phase 3: SGR Logic Execution (Uses PricingLogic) ---</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\ud83e\udde0 Calculating Offer (Schema Enforced)...&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">offer</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">run_sgr</span><span class=\"p\">(</span><span class=\"n\">history</span><span class=\"p\">,</span> <span class=\"n\">PricingLogic</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Audit log \u2014 the SGR benefit: explicit reasoning traces</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;   [Audit] Math: </span><span class=\"si\">{</span><span class=\"n\">offer</span><span class=\"o\">.</span><span class=\"n\">margin_math</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;   [Audit] Max Allowed: </span><span class=\"si\">{</span><span class=\"n\">offer</span><span class=\"o\">.</span><span class=\"n\">max_discount_percent</span><span class=\"si\">}</span><span class=\"s2\">%&quot;</span><span class=\"p\">)</span>\n\n        <span class=\"k\">return</span> <span class=\"n\">offer</span><span class=\"o\">.</span><span class=\"n\">customer_message</span>\n\n    <span class=\"k\">return</span> <span class=\"s2\">&quot;I&#39;m sorry, I couldn&#39;t process your request.&quot;</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">pricing_agent</span><span class=\"p\">(</span><span class=\"s2\">&quot;I want a discount or I&#39;m leaving!&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;user_102&quot;</span><span class=\"p\">)</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">\ud83d\udcac Final Reply: </span><span class=\"si\">{</span><span class=\"n\">response</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"step-4-run-vllm-with-xgrammar\">Step 4: Run vLLM with xgrammar</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Start vLLM server with xgrammar backend (default in recent versions)</span>\npython<span class=\"w\"> </span>-m<span class=\"w\"> </span>vllm.entrypoints.openai.api_server<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--model<span class=\"w\"> </span>Qwen/Qwen2.5-7B-Instruct<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">    </span>--port<span class=\"w\"> </span><span class=\"m\">8000</span>\n\n<span class=\"c1\"># Run the agent</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>python<span class=\"w\"> </span>-m<span class=\"w\"> </span>sgr.agent\n</code></pre></div>\n<h3 id=\"example-output\">Example Output</h3>\n<div class=\"highlight\"><pre><span></span><code>\ud83e\udd16 Processing: &#39;I want a discount or I&#39;m leaving!&#39; for user_102\n\ud83d\udccd Routing decision: fetch_user_features\n\ud83d\udd0d Fetching features for user_102...\n   [Data] LTV: $1,500 | Margin: 20%\n\ud83e\udde0 Calculating Offer (Schema Enforced)...\n   [Audit] Math: Cart $200 * 0.20 Margin = $40\n   [Audit] Max Allowed: 15.0%\n\n\ud83d\udcac Final Reply: We value your loyalty! Here&#39;s a special 15% discount\n   with code SAVE15. This reflects our appreciation for your continued\n   business with us.\n</code></pre></div>\n<p>The audit log shows exactly how the model reasoned: it calculated the margin ($40 on a $200 cart at 20% margin), and correctly bounded the discount to stay within the profit constraint.</p>\n<hr />\n<h2 id=\"best-practices\">Best Practices</h2>\n<h3 id=\"schema-design\">Schema Design</h3>\n<ol>\n<li><strong>Order fields by reasoning flow</strong>: Put analysis fields before decision fields</li>\n<li><strong>Use descriptive Field descriptions</strong>: They guide the model's attention</li>\n<li><strong>Constrain with Literal and Annotated</strong>: Use <code>Literal[\"a\", \"b\"]</code> for enums, <code>Annotated[int, Ge(1), Le(10)]</code> for bounds</li>\n<li><strong>Keep schemas focused</strong>: One schema per reasoning phase, compose with multiple calls</li>\n</ol>\n<h3 id=\"vllm-configuration\">vLLM Configuration</h3>\n<ol>\n<li><strong>Use low temperature</strong> (0.1-0.3) for deterministic reasoning</li>\n<li><strong>Let xgrammar handle structure</strong>: Don't over-engineer prompts for formatting</li>\n<li><strong>Monitor token usage</strong>: SGR typically uses fewer tokens than CoT (no verbose prose)</li>\n</ol>\n<h3 id=\"production-considerations\">Production Considerations</h3>\n<ol>\n<li><strong>Schema versioning</strong>: Track schema changes like API versions</li>\n<li><strong>Fallback handling</strong>: Even with SGR, network/server errors need graceful handling</li>\n<li><strong>Audit logging</strong>: Log raw SGR outputs for compliance and debugging</li>\n<li><strong>Test with edge cases</strong>: Ensure schemas handle boundary conditions</li>\n</ol>\n<hr />\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Schema-Guided Reasoning bridges the gap between the flexibility of LLMs and the reliability requirements of production systems. By defining your reasoning topology as a Pydantic schema and letting xgrammar enforce it, you get:</p>\n<ul>\n<li><strong>Guaranteed valid output</strong> \u2014 no retry loops, no parsing failures</li>\n<li><strong>Explicit reasoning traces</strong> \u2014 every step is auditable</li>\n<li><strong>Smaller model viability</strong> \u2014 the schema compensates for weaker instruction-following</li>\n<li><strong>Lower costs</strong> \u2014 fewer tokens, no retries, smaller models work</li>\n</ul>\n<p>The <a href=\"https://github.com/slavadubrov/sgr-discount-manager\">sgr-discount-manager</a> demo shows how these patterns work in practice. Clone it, run it, and adapt the schemas for your use case.</p>\n<hr />\n<h2 id=\"references\">References</h2>\n<h3 id=\"sgr-framework\">SGR Framework</h3>\n<ul>\n<li><a href=\"https://abdullin.com/schema-guided-reasoning/\">Schema-Guided Reasoning (SGR)</a> \u2014 Rinat Abdullin's original framework</li>\n<li><a href=\"https://abdullin.com/schema-guided-reasoning/patterns\">SGR Patterns</a> \u2014 Cascade, Routing, Cycle patterns</li>\n</ul>\n<h3 id=\"xgrammar\">xgrammar</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2411.15100\">XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</a> \u2014 Yixin Dong et al., arXiv:2411.15100 (technical paper with benchmarks)</li>\n<li><a href=\"https://github.com/mlc-ai/xgrammar\">xgrammar GitHub</a> \u2014 Fast, flexible structured generation library</li>\n<li><a href=\"https://xgrammar.mlc.ai/docs/\">xgrammar Documentation</a> \u2014 Official docs with quick start guide</li>\n<li><a href=\"https://xgrammar.mlc.ai/docs/start/quick_start\">xgrammar Quick Start</a> \u2014 Getting started with xgrammar</li>\n<li><a href=\"https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar\">Achieving Efficient Structured Generation with XGrammar</a> \u2014 MLC blog post on xgrammar internals</li>\n</ul>\n<h3 id=\"vllm\">vLLM</h3>\n<ul>\n<li><a href=\"https://docs.vllm.ai/en/latest/features/structured_outputs.html\">vLLM Structured Outputs</a> \u2014 Official documentation</li>\n</ul>\n<h3 id=\"demo-project\">Demo Project</h3>\n<ul>\n<li><a href=\"https://github.com/slavadubrov/sgr-discount-manager\">sgr-discount-manager</a> \u2014 Working demo with all code examples from this post</li>\n</ul>", "image": null, "date_modified": "2025-12-28T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Agentic AI"]}, {"id": "https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/", "url": "https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/", "title": "Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It", "content_html": "<h1 id=\"context-engineering-in-the-agenticai-era-and-how-to-cook-it\">Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It</h1>\n<h2 id=\"tldr\">TL;DR</h2>\n<blockquote>\n<p><em>Context engineering</em> (the <strong>context layer</strong>) is the pipeline that selects, structures, and governs <strong>what the model sees at the moment of decision</strong>: <strong>Instructions, Examples, Knowledge, Memory, Skills, Tools, Guardrails</strong>. Agentic systems live or die by this layer. Below is a field\u2011tested blueprint and patterns.</p>\n</blockquote>\n<p><strong>The problem</strong>: You build an agent. It works in demos, fails in production. Why? The model gets the wrong context at the wrong time\u2014stale memory, irrelevant docs, no safety checks, ambiguous instructions.</p>\n<p><strong>The fix</strong>: Design the context layer deliberately. This guide shows you how.</p>\n<!-- more -->\n\n<hr />\n<h2 id=\"why-now\">Why now</h2>\n<p>Picture this: your customer support agent runs for three weeks. It handles 200 tickets. Then it suddenly starts hallucinating product details, mixing up customers, and calling the wrong APIs. The model didn't get worse\u2014the context did.</p>\n<p>Here's why context engineering became critical in 2025:</p>\n<ul>\n<li>\n<p><strong>Agents moved from chat to action.</strong> Multi\u2011step planning, tool use, and sub\u2011agents raised the bar for <em>repeatable context assembly</em> vs. one\u2011off prompts. A single bad context decision can cascade through a 10\u2011step plan.</p>\n</li>\n<li>\n<p><strong>Memory and standards arrived.</strong> Centralized user/org memory (and standards like MCP) make it feasible to load personal/org context <em>safely</em>\u2014if you design the layer properly. Without governance, you leak PII or overload the window.</p>\n</li>\n<li>\n<p><strong>Retrieval matured.</strong> Hybrid search, reranking, and graph\u2011aware retrieval (e.g., GraphRAG) reduce hallucinations and token waste. But only if you route queries to the right retrieval strategy.</p>\n</li>\n<li>\n<p><strong>Value focus shifted.</strong> Many \"agentic\" pilots stall not because of model quality but because of weak context design/governance. A deliberate context layer is the fix.</p>\n</li>\n</ul>\n<hr />\n<h2 id=\"key-concepts-for-beginners\">Key Concepts for Beginners</h2>\n<p>Before we dive in, let's define a few terms that will appear frequently:</p>\n<ul>\n<li><strong>Context Window</strong>: The \"working memory\" of the model. It's the maximum amount of text (measured in tokens) the model can process at once. If you exceed it, the model crashes or forgets the beginning.</li>\n<li><strong>Tokens</strong>: The basic units of text for an LLM. Roughly, 1,000 tokens \u2248 750 words.</li>\n<li><strong>Attention Budget</strong>: Language models process tokens through attention mechanisms that create pairwise relationships between all tokens. For n tokens, this creates n\u00b2 relationships. As context grows, this budget gets stretched thin\u2014meaning information in the middle of context receives less attention than information at the beginning or end.</li>\n<li><strong>Embeddings</strong>: Numerical representations of text. We use them to search for \"meaning\" rather than just keywords (e.g., searching for \"dog\" might find \"puppy\").</li>\n<li><strong>JSON Schema</strong>: A standard way to describe the structure of JSON data. It allows us to force the model to output specific fields (like <code>{\"answer\": \"...\", \"citations\": [...]}</code>).</li>\n<li><strong>MCP (Model Context Protocol)</strong>: An open standard that enables AI models to interact with external data and tools securely. Think of it as a \"USB port\" for AI apps to connect to your local files, databases, or Slack.</li>\n</ul>\n<p><img alt=\"Context Window Composition\" src=\"../../../../assets/2025-10-05-context-engineering/context_window_composition.svg\" /></p>\n<hr />\n<h2 id=\"what-is-the-context-layer\">What is the context layer?</h2>\n<blockquote>\n<p>A <strong>pipeline + policy</strong> that (1) <strong>selects &amp; structures</strong> inputs per step, (2) <strong>applies controls</strong> (format/safety/policy), and (3) <strong>feeds</strong> the model/agent with <strong>just\u2011enough, just\u2011in\u2011time</strong> context.</p>\n</blockquote>\n<p>Think of it as the assembly line that prepares exactly what the model needs to make a good decision\u2014nothing more, nothing less.</p>\n<p><strong>Context is a finite resource.</strong> Like humans with limited working memory, language models have an attention budget that depletes as context grows. Every new token consumes some of this budget. The engineering problem is finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.</p>\n<p>There's no single canonical definition. Different teams ship different stacks. But a practical, shared decomposition is:</p>\n<p><img alt=\"Context Layer Architecture\" src=\"../../../../assets/2025-10-05-context-engineering/context_layer_components.svg\" /></p>\n<h3 id=\"1-instructions\">1) Instructions</h3>\n<p><strong>What</strong>: A durable <strong>contract</strong> for behavior: role, tone, constraints, output schema, evaluation goals. Modern models respect instruction <strong>hierarchies</strong> (system &gt; developer &gt; user).</p>\n<h4 id=\"when-to-use-instructions\">When to use Instructions</h4>\n<ul>\n<li>You need <strong>consistent output</strong> (reports, SQL, API calls, JSON).</li>\n<li>You must apply policy (e.g., redact PII, reject unsupported asks).</li>\n</ul>\n<h4 id=\"instruction-patterns\">Instruction Patterns</h4>\n<ul>\n<li><strong>Role &amp; policy blocks</strong>: keep <em>rules</em> separate from the user task.</li>\n<li><strong>Structured outputs</strong>: JSON Schema \u2192 deterministic downstream.</li>\n<li><strong>Instruction hierarchy</strong>: split <em>system</em>, <em>developer</em>, <em>user</em> explicitly.</li>\n</ul>\n<p>Plain example (policy block)</p>\n<p><img alt=\"Instruction Hierarchy\" src=\"../../../../assets/2025-10-05-context-engineering/instruction_hierarchy.svg\" /></p>\n<div class=\"highlight\"><pre><span></span><code>SYSTEM RULES\n- Role: support assistant for ACME.\n- Always output valid JSON per AnswerSchema.\n- If a request needs account data, ask for the account ID.\n- Never include secrets or internal URLs.\n</code></pre></div>\n<hr />\n<h4 id=\"schemaguided-reasoning-sgr\">Schema\u2011Guided Reasoning (SGR)</h4>\n<p><strong>What</strong>: Drive the agent with JSON Schemas for the plan, tool arguments, intermediate results, and the final answer. The model emits/consumes JSON at each step; your code validates it.</p>\n<p><strong>Why</strong>: Reduces ambiguity, makes retries/repairs deterministic, and improves safety by enforcing types and required fields throughout the loop.</p>\n<p><strong>How it works</strong>:</p>\n<ol>\n<li>Define schemas for <code>Plan</code>, <code>ToolArgs</code>, <code>StepResult</code>, and <code>FinalAnswer</code>.</li>\n<li>At each agent step, the model outputs JSON matching one of these schemas.</li>\n<li>Your code validates the JSON before proceeding.</li>\n<li>If validation fails, attempt one automatic repair (e.g., add missing required fields with defaults).</li>\n<li>If repair fails, refuse and log the error.</li>\n</ol>\n<p><strong>Concrete example</strong>: Instead of the model saying \"I'll search for the customer's tickets\", it outputs:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;action&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;call_tool&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;tool&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;search_tickets&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\"> </span><span class=\"nt\">&quot;customer_id&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;A-123&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"nt\">&quot;limit&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">10</span><span class=\"w\"> </span><span class=\"p\">},</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;expected_schema&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;TicketList&quot;</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<p>Your code validates <code>args</code> against the tool's schema <em>before</em> calling the API.</p>\n<hr />\n<h3 id=\"2-examples\">2) Examples</h3>\n<p><strong>What</strong>: A few short input\u2192output examples that show the exact format, tone, and steps the model should follow.</p>\n<h4 id=\"when-to-use-examples\">When to use Examples</h4>\n<ul>\n<li>You need the model to match a <strong>specific template</strong> (tables, JSON, SQL, API calls).</li>\n<li>You want <strong>domain\u2011specific</strong> phrasing/labels or consistent tone.</li>\n</ul>\n<h4 id=\"example-patterns\">Example Patterns</h4>\n<ul>\n<li><strong>Canonical demos</strong>: show the <em>exact</em> target structure.</li>\n<li><strong>Bad vs. good</strong>: contrast common mistakes with the desired result.</li>\n<li><strong>Schema\u2011first + examples</strong>: pair your JSON Schema with 2\u20133 short demos.</li>\n<li><strong>Keep it short</strong>: many small, focused demos beat one long example.</li>\n</ul>\n<hr />\n<h3 id=\"3-knowledge\">3) Knowledge</h3>\n<p><strong>What</strong>: Grounding via retrieval (vector + keyword), reranking, graphs, web, or enterprise sources.</p>\n<h4 id=\"when-to-use-knowledge\">When to use Knowledge</h4>\n<ul>\n<li>You need <strong>fresh or private facts</strong>.</li>\n<li>You want <strong>cited, defensible</strong> answers.</li>\n</ul>\n<h4 id=\"knowledge-patterns\">Knowledge Patterns</h4>\n<ul>\n<li><strong>Hybrid retrieval</strong> (BM25 + dense) with <strong>reranker</strong> to shrink tokens.</li>\n<li><strong>Graph\u2011aware</strong> retrieval (GraphRAG) for cross\u2011doc relations.</li>\n<li><strong>Adaptive RAG</strong>: route between <em>no retrieval</em>, <em>single\u2011shot</em>, and <em>iterative</em>.</li>\n</ul>\n<p><img alt=\"Adaptive Retrieval Router\" src=\"../../../../assets/2025-10-05-context-engineering/retrieval_router.svg\" /></p>\n<p><strong>Params that matter</strong>:</p>\n<ul>\n<li><strong>Chunking</strong>: split by semantic boundary (paragraphs, sections) &gt; fixed size.</li>\n<li><strong>top\u2011k</strong>: start with 10\u201320 for hybrid, rerank to 3\u20135.</li>\n<li><strong>MMR (diversity) \u03bb</strong>: 0.7 as default.</li>\n<li><strong>Citations</strong>: Always include source references and quotes.</li>\n</ul>\n<hr />\n<h3 id=\"4-memory\">4) Memory</h3>\n<p><strong>What</strong>: Durable context across turns/sessions: <strong>short\u2011term</strong> (conversation state), <strong>long\u2011term</strong> (user/app facts), <strong>episodic</strong> (events), <strong>semantic</strong> (facts/entities).</p>\n<h4 id=\"when-to-use-memory\">When to use Memory</h4>\n<ul>\n<li>You want personalization and continuity.</li>\n<li>Multiple agents coordinate over days/weeks.</li>\n</ul>\n<h4 id=\"memory-patterns\">Memory Patterns</h4>\n<ul>\n<li><strong>Entity memories</strong> (names, IDs, preferences) + expiry policies.</li>\n<li><strong>Scoped retrieval</strong> from long-term store (vector/kv/graph).</li>\n<li><strong>Compression integration</strong>: When short-term memory grows large, apply <a href=\"#context-compression-strategies\">Context Compression Strategies</a> <a href=\"#references\">[7]</a>.</li>\n</ul>\n<p><img alt=\"Memory Scoping\" src=\"../../../../assets/2025-10-05-context-engineering/memory_scoping.svg\" /></p>\n<p><strong>Expiry rules</strong>:</p>\n<ul>\n<li>Preferences: 365 days</li>\n<li>Episodic events: 90 days</li>\n<li>Short-term state: clear after session ends</li>\n<li>Entities: no expiry, but require periodic validation</li>\n</ul>\n<hr />\n<h3 id=\"5-skills\">5) Skills</h3>\n<p><strong>What</strong>: Composable domain expertise that agents discover and load dynamically. <a href=\"https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\">Agent Skills</a> are a framework introduced by Anthropic for equipping agents with specialized capabilities by capturing and sharing procedural knowledge.</p>\n<blockquote>\n<p>Building a skill for an agent is like putting together an onboarding guide for a new hire.\n\u2014 Anthropic Engineering Blog</p>\n</blockquote>\n<h4 id=\"when-to-use-skills\">When to use Skills</h4>\n<ul>\n<li>You need <strong>domain-specific expertise</strong> (PDF manipulation, git operations, data analysis).</li>\n<li>You want <strong>reusable procedures</strong> across agents or organizations.</li>\n<li>You need to <strong>specialize</strong> an agent without hardcoding behaviors.</li>\n</ul>\n<h4 id=\"what-is-a-skill\">What is a Skill?</h4>\n<p>A Skill is an organized folder containing instructions, scripts, and resources that agents can discover and load dynamically:</p>\n<ul>\n<li><strong>SKILL.md file</strong>: Contains name, description (in YAML frontmatter), and the skill's instructions</li>\n<li><strong>Additional files</strong>: Scripts, references, templates that the skill can reference</li>\n<li><strong>Code</strong>: Python scripts or other executables the agent can run as tools</li>\n</ul>\n<h4 id=\"skills-pattern-progressive-disclosure\">Skills Pattern: Progressive Disclosure</h4>\n<p>The key design principle is <strong>progressive disclosure</strong>\u2014loading information only when needed. See <a href=\"#the-progressive-disclosure-principle\">The Progressive Disclosure Principle</a> for the general pattern.</p>\n<ol>\n<li><strong>Level 1 (Startup)</strong>: Only skill names and descriptions are loaded into context</li>\n<li><strong>Level 2 (Activation)</strong>: When relevant, the full SKILL.md is loaded</li>\n<li><strong>Level 3+ (Deep dive)</strong>: Additional referenced files loaded only as needed</li>\n</ol>\n<p>This means skill content is effectively unbounded\u2014agents don't need to load everything into context at once.</p>\n<div class=\"highlight\"><pre><span></span><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Context Window                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 System Prompt                                           \u2502\n\u2502 \u251c\u2500\u2500 Core instructions                                   \u2502\n\u2502 \u2514\u2500\u2500 Skill metadata (name + description only)           \u2502\n\u2502     \u2022 pdf: &quot;Manipulate PDF documents&quot;                   \u2502\n\u2502     \u2022 git: &quot;Advanced git operations&quot;                    \u2502\n\u2502     \u2022 context-compression: &quot;Manage long sessions&quot;       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [User triggers task requiring PDF skill]                \u2502\n\u2502                                                         \u2502\n\u2502 \u2192 Agent reads pdf/SKILL.md into context                 \u2502\n\u2502 \u2192 Agent reads pdf/forms.md (only if filling forms)      \u2502\n\u2502 \u2192 Agent executes pdf/extract_fields.py (without loading)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></div>\n<h4 id=\"skill-best-practices\">Skill Best Practices</h4>\n<p>From Anthropic's guidelines:</p>\n<ol>\n<li><strong>Start with evaluation</strong>: Identify gaps by running agents on representative tasks, build skills to address shortcomings</li>\n<li><strong>Structure for scale</strong>: Split large SKILL.md into separate files, keeping paths separate for mutually exclusive contexts</li>\n<li><strong>Think from Claude's perspective</strong>: Monitor skill usage, iterate on name/description to improve triggering accuracy</li>\n<li><strong>Iterate with Claude</strong>: Ask Claude to capture successful approaches into reusable skill content</li>\n</ol>\n<h4 id=\"security-considerations\">Security Considerations</h4>\n<p>Skills can introduce vulnerabilities since they provide new capabilities through instructions and code:</p>\n<ul>\n<li>Install skills only from trusted sources</li>\n<li>Audit skills before use\u2014review bundled files, code dependencies, and network connections</li>\n<li>Pay attention to instructions that connect to external sources</li>\n</ul>\n<hr />\n<h3 id=\"6-tools\">6) Tools</h3>\n<p><strong>What</strong>: Function calls to fetch data or take actions (APIs, DB, search, file ops, \"computer use\").</p>\n<h4 id=\"when-to-use-tools\">When to use Tools</h4>\n<ul>\n<li>You want <strong>deterministic</strong> side\u2011effects and data fidelity.</li>\n<li>You orchestrate <strong>plan \u2192 call \u2192 verify \u2192 continue</strong> loops.</li>\n</ul>\n<h4 id=\"tool-patterns\">Tool Patterns</h4>\n<ul>\n<li><strong>Tool\u2011first planning</strong> + <strong>post\u2011call validators</strong>.</li>\n<li><strong>Structured outputs</strong> between steps.</li>\n<li><strong>Fallbacks</strong> when tools fail (retry \u2192 degrade \u2192 human\u2011in\u2011loop).</li>\n</ul>\n<p><img alt=\"Tool Execution Loop\" src=\"../../../../assets/2025-10-05-context-engineering/tool_execution_loop.svg\" /></p>\n<p><strong>Key concepts</strong>:</p>\n<ul>\n<li><strong>Idempotent</strong>: safe to retry without side effects (GET=yes, POST/DELETE=no).</li>\n<li><strong>Postconditions</strong>: checks after each call (non_empty_result, status==\"ok\", valid_json).</li>\n<li><strong>Fallback chain</strong>: retry \u2192 degrade gracefully \u2192 human-in-loop.</li>\n</ul>\n<p><strong>A Note on MCP (Model Context Protocol)</strong></p>\n<p>MCP is becoming the standard for how agents connect to tools and data <a href=\"#references\">[6]</a>. Instead of custom API wrappers for every service, you run an \"MCP Server\" for each. Your agent automatically discovers available tools and resources.</p>\n<p><img alt=\"MCP Architecture\" src=\"../../../../assets/2025-10-05-context-engineering/mcp_context_architecture.svg\" /></p>\n<hr />\n<h3 id=\"7-guardrails\">7) Guardrails</h3>\n<p><strong>What</strong>: Input/output validation, safety filters, jailbreak defense, schema enforcement, content policy.</p>\n<h4 id=\"when-to-use-guardrails\">When to use Guardrails</h4>\n<ul>\n<li>You need compliance/brand integrity.</li>\n<li>You want <strong>typed, correct</strong> outputs and safe behavior.</li>\n</ul>\n<h4 id=\"guardrail-patterns\">Guardrail Patterns</h4>\n<ul>\n<li><strong>Programmable rails</strong> (policy rules + actions).</li>\n<li><strong>Schema + semantic validators</strong> (types, regex, evals).</li>\n<li><strong>Central policy + observability</strong> (dashboards, red\u2011teaming).</li>\n</ul>\n<p><img alt=\"Guardrails Flow\" src=\"../../../../assets/2025-10-05-context-engineering/guardrails_flow.svg\" /></p>\n<p><strong>Repair vs refuse flow</strong>:</p>\n<ul>\n<li><strong>Schema violations</strong>: Attempt automatic repair once. If repair fails, refuse with clear error.</li>\n<li><strong>Policy violations</strong>: Refuse immediately (no repair attempt). Suggest safe alternative.</li>\n</ul>\n<p><strong>Common guardrail types</strong>:</p>\n<ol>\n<li><strong>Input guards</strong>: PII detection, prompt injection defense, toxicity filters</li>\n<li><strong>Output guards</strong>: schema validation, content policy, factual consistency</li>\n<li><strong>Tool guards</strong>: rate limiting, permission checks, cost thresholds</li>\n<li><strong>Memory guards</strong>: PII redaction before storage, expiry enforcement</li>\n</ol>\n<hr />\n<h3 id=\"concrete-example-support-bot-answering-a-ticket\">Concrete example: support bot answering a ticket</h3>\n<p>Let's see how all these components work together. When a customer asks \"Why is my API key not working?\", the context layer assembles:</p>\n<ul>\n<li><strong>Instructions</strong>: role = helpful support assistant for ACME, cite sources, return JSON {answer, sources, next_steps}.</li>\n<li><strong>Examples</strong>: 2 short Q\u2192A pairs showing tone and JSON shape (one about API keys, one about billing).</li>\n<li><strong>Knowledge</strong>: search the help center and product runbooks for \"API key troubleshooting\"; include relevant quotes.</li>\n<li><strong>Memory</strong>: customer name \"Sam\", account_id \"A-123\", plan \"Pro\", last interaction was \"API key created 3 days ago\".</li>\n<li><strong>Skills</strong>: load <code>ticket-handling</code> skill with troubleshooting procedures, escalation policies, and resolution templates.</li>\n<li><strong>Tools</strong>: <code>search_tickets(customer_id)</code>, <code>check_api_key_status(key)</code>, <code>create_issue(description)</code>.</li>\n<li><strong>Guardrails</strong>: redact any API key values in output; if schema fails, repair once; if policy violated (e.g., requesting to delete production data), refuse politely.</li>\n</ul>\n<p>The model receives all of this structured context, generates an answer, and the guardrails validate it before sending to the customer.</p>\n<hr />\n<h2 id=\"context-fundamentals-deep-dive\">Context Fundamentals Deep Dive</h2>\n<p>Understanding the anatomy of context is prerequisite to effective context engineering. This section provides the foundational knowledge for everything that follows.</p>\n<h3 id=\"the-anatomy-of-context\">The Anatomy of Context</h3>\n<p>Context comprises several distinct components, each with different characteristics and constraints:</p>\n<p><strong>System Prompts</strong></p>\n<p>System prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation.</p>\n<p>The key is finding the right \"altitude\"\u2014specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics. Too low (hardcoded brittle logic) creates fragility; too high (vague guidance) fails to give concrete signals.</p>\n<p>Organize prompts into distinct sections using XML tagging or Markdown headers:</p>\n<div class=\"highlight\"><pre><span></span><code>&lt;BACKGROUND_INFORMATION&gt;\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n&lt;/BACKGROUND_INFORMATION&gt;\n\n&lt;INSTRUCTIONS&gt;\n<span class=\"k\">-</span><span class=\"w\"> </span>Write clean, idiomatic Python code\n<span class=\"k\">-</span><span class=\"w\"> </span>Include type hints for function signatures\n<span class=\"k\">-</span><span class=\"w\"> </span>Add docstrings for public functions\n&lt;/INSTRUCTIONS&gt;\n\n&lt;TOOL_GUIDANCE&gt;\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n&lt;/TOOL_GUIDANCE&gt;\n</code></pre></div>\n<p><strong>Tool Definitions</strong></p>\n<p>Tool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool descriptions collectively steer agent behavior\u2014poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults.</p>\n<p><strong>The consolidation principle</strong>: If a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.</p>\n<p><strong>Retrieved Documents</strong></p>\n<p>Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information. The just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and loads data dynamically.</p>\n<p><strong>Message History</strong></p>\n<p>Message history contains the conversation between user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage. It serves as scratchpad memory where agents track progress and maintain task state.</p>\n<p><strong>Tool Outputs</strong></p>\n<p>Tool outputs are the results of agent actions: file contents, search results, command execution output, API responses. Research shows observations (tool outputs) can reach <strong>over 80% of total context usage</strong> in typical agent trajectories <a href=\"#references\">[4]</a>. This creates pressure for strategies like observation masking and compaction.</p>\n<h3 id=\"context-windows-and-attention-mechanics\">Context Windows and Attention Mechanics</h3>\n<p><strong>The Attention Budget Constraint</strong></p>\n<p>Language models process tokens through attention mechanisms that create pairwise relationships between all tokens. For n tokens, this creates n\u00b2 relationships that must be computed. As context length increases, the model's ability to capture relationships gets stretched thin.</p>\n<p>Models develop attention patterns from training data where shorter sequences predominate. This means models have less experience with context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.</p>\n<p><img alt=\"Progressive Disclosure\" src=\"../../../../assets/2025-10-05-context-engineering/progressive_disclosure.svg\" /></p>\n<h4 id=\"the-progressive-disclosure-principle\">The Progressive Disclosure Principle</h4>\n<p>Progressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions\u2014sufficient to know when a skill might be relevant. Full content loads only when activated for specific tasks.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"gh\"># Instead of loading all documentation at once:</span>\n\n<span class=\"gh\"># Step 1: Load summary</span>\n\ndocs/api_summary.md # Lightweight overview\n\n<span class=\"gh\"># Step 2: Load specific section as needed</span>\n\ndocs/api/endpoints.md # Only when API calls needed\ndocs/api/authentication.md # Only when auth context needed\n</code></pre></div>\n<h3 id=\"context-quality-versus-quantity\">Context Quality Versus Quantity</h3>\n<p>The assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens.</p>\n<p>Several factors create pressure for context efficiency:</p>\n<ul>\n<li><strong>Cost</strong>: Processing cost grows disproportionately with context length\u2014not just double for double the tokens, but exponentially more in time and computing resources.</li>\n<li><strong>Degradation</strong>: Model performance degrades beyond certain context lengths even when the window technically supports more.</li>\n<li><strong>Latency</strong>: Long inputs remain expensive even with prefix caching.</li>\n</ul>\n<p><strong>The guiding principle</strong>: Informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not.</p>\n<hr />\n<h2 id=\"context-degradation-patterns\">Context Degradation Patterns</h2>\n<p>Language models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems.</p>\n<p><img alt=\"Context Degradation Patterns\" src=\"../../../../assets/2025-10-05-context-engineering/degradation_patterns.svg\" /></p>\n<h3 id=\"the-lost-in-middle-phenomenon\">The Lost-in-Middle Phenomenon</h3>\n<p>The most well-documented degradation pattern is \"lost-in-middle,\" where models demonstrate U-shaped attention curves <a href=\"#references\">[1]</a>. Information at the beginning and end of context receives reliable attention, while information in the middle suffers from <strong>10-40% lower recall accuracy</strong>.</p>\n<p><strong>Why it happens</strong>: Models allocate massive attention to the first token (often the BOS token) to stabilize internal states, creating an \"attention sink\" that consumes attention budget <a href=\"#references\">[2]</a>. As context grows, middle tokens fail to garner sufficient attention weight.</p>\n<p><strong>Practical fix</strong>: Place critical information at the beginning or end of context. Use summary structures that surface key information at attention-favored positions.</p>\n<p><img alt=\"Attention U-Curve\" src=\"../../../../assets/2025-10-05-context-engineering/attention_u_curve.svg\" /></p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"gh\"># Organize context with critical info at edges</span>\n\n[CURRENT TASK] # At start\n\n<span class=\"k\">-</span><span class=\"w\"> </span>Goal: Generate quarterly report\n<span class=\"k\">-</span><span class=\"w\"> </span>Deadline: End of week\n\n[DETAILED CONTEXT] # Middle (less attention)\n\n<span class=\"k\">-</span><span class=\"w\"> </span>50 pages of data\n<span class=\"k\">-</span><span class=\"w\"> </span>Multiple analysis sections\n<span class=\"k\">-</span><span class=\"w\"> </span>Supporting evidence\n\n[KEY FINDINGS] # At end\n\n<span class=\"k\">-</span><span class=\"w\"> </span>Revenue up 15%\n<span class=\"k\">-</span><span class=\"w\"> </span>Costs down 8%\n<span class=\"k\">-</span><span class=\"w\"> </span>Growth in Region A\n</code></pre></div>\n<p><strong>Prompt duplication technique</strong>: For critical instructions that must not be missed, duplicate them at both the beginning AND end of the context. Since models attend strongly to both edges, placing the same instruction in both positions ensures it gets proper attention regardless of context length. This is particularly useful for:</p>\n<ul>\n<li>System constraints that must always be followed</li>\n<li>Output format requirements</li>\n<li>Safety policies and content guidelines</li>\n</ul>\n<div class=\"highlight\"><pre><span></span><code><span class=\"gh\"># Example: Duplicating critical instructions</span>\n\n[SYSTEM - START]\nCRITICAL: Always respond in JSON format. Never include PII.\n\n[... long context with documents, history, tools ...]\n\n[SYSTEM - REMINDER]\nCRITICAL: Always respond in JSON format. Never include PII.\n</code></pre></div>\n<h3 id=\"context-poisoning\">Context Poisoning</h3>\n<p>Context poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.</p>\n<p><strong>How poisoning occurs</strong>:</p>\n<ol>\n<li>Tool outputs contain errors or unexpected formats</li>\n<li>Retrieved documents have incorrect or outdated information</li>\n<li>Model-generated summaries introduce hallucinations that persist</li>\n</ol>\n<p><strong>Detection symptoms</strong>:</p>\n<ul>\n<li>Degraded output quality on tasks that previously succeeded</li>\n<li>Tool misalignment (agents call wrong tools or parameters)</li>\n<li>Persistent hallucinations despite correction attempts</li>\n</ul>\n<p><strong>Recovery</strong>: Truncate context to before the poisoning point, explicitly note the poisoning and request re-evaluation, or restart with clean context preserving only verified information.</p>\n<h3 id=\"context-distraction\">Context Distraction</h3>\n<p>Context distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge.</p>\n<p>Research shows that <strong>even a single irrelevant document</strong> reduces performance on tasks involving relevant documents <a href=\"#references\">[8]</a>. The effect follows a step function\u2014the presence of any distractor triggers degradation.</p>\n<p><strong>Key insight</strong>: Models cannot \"skip\" irrelevant context. They must attend to everything provided, creating distraction even when irrelevant information is clearly not useful.</p>\n<p><strong>Mitigation</strong>: Apply relevance filtering before loading documents. Use namespacing to make irrelevant sections easy to ignore. Consider whether information needs to be in context or can be accessed through tool calls.</p>\n<h3 id=\"context-confusion\">Context Confusion</h3>\n<p>Context confusion arises when irrelevant information influences responses in ways that degrade quality. If you put something in context, the model has to pay attention to it\u2014it may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints from different contexts.</p>\n<p><strong>Signs</strong>: Responses address the wrong aspect of queries, tool calls seem appropriate for different tasks, outputs mix requirements from multiple sources.</p>\n<p><strong>Fix</strong>: Explicit task segmentation (different tasks get different context windows), clear transitions between task contexts, state management that isolates context.</p>\n<h3 id=\"context-clash\">Context Clash</h3>\n<p>Context clash develops when accumulated information directly conflicts, creating contradictory guidance. This differs from poisoning where one piece is incorrect\u2014in clash, multiple correct pieces contradict each other.</p>\n<p><strong>Common sources</strong>: Multi-source retrieval with contradictory information, version conflicts (outdated and current information both in context), perspective conflicts (valid but incompatible viewpoints).</p>\n<p><strong>Resolution</strong>: Explicit conflict marking, priority rules establishing which source takes precedence, version filtering to exclude outdated information.</p>\n<h3 id=\"model-specific-degradation-thresholds\">Model-Specific Degradation Thresholds</h3>\n<p>Research provides concrete data on when performance degradation begins. Note that \"effective context length\" (where models maintain optimal performance) is often significantly smaller than advertised maximum context windows <a href=\"#references\">[3]</a>:</p>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Max Context</th>\n<th>Effective Context</th>\n<th>Degradation Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GPT-4 Turbo</td>\n<td>128K tokens</td>\n<td>~32K tokens</td>\n<td>Retrieval degrades after 32K, accuracy suffers beyond 64K</td>\n</tr>\n<tr>\n<td>GPT-4o</td>\n<td>128K tokens</td>\n<td>~8K tokens</td>\n<td>Complex NIAH accuracy drops from 99% to 70% at 32K</td>\n</tr>\n<tr>\n<td>Claude 3.5 Sonnet</td>\n<td>200K tokens</td>\n<td>~4K tokens</td>\n<td>Complex NIAH accuracy drops from 88% to 30% at 32K</td>\n</tr>\n<tr>\n<td>Gemini 1.5 Pro</td>\n<td>1M tokens</td>\n<td>~128K tokens</td>\n<td>99% NIAH recall at 1M, best long-context performance</td>\n</tr>\n<tr>\n<td>Gemini 2.0 Flash</td>\n<td>1M tokens</td>\n<td>~32K tokens</td>\n<td>Complex NIAH accuracy drops from 94% to 48% at 32K</td>\n</tr>\n</tbody>\n</table>\n<p><em>Sources: RULER benchmark <a href=\"#references\">[3]</a>, NoLiMa benchmark <a href=\"#references\">[9]</a>, Google technical reports</em></p>\n<p><strong>Key finding</strong> <a href=\"#references\">[3]</a>: Only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding\u2014complex reasoning tasks show much steeper degradation.</p>\n<hr />\n<h2 id=\"context-compression-strategies\">Context Compression Strategies</h2>\n<blockquote>\n<p><strong>Terminology note</strong>: Context compression is an umbrella category that includes several techniques: summarization (for conversation history and memory), observation masking (for tool outputs), and selective trimming <a href=\"#references\">[5]</a>. The memory summarization referenced in the <a href=\"#4-memory\">Memory</a> section is one application of these broader compression strategies.</p>\n</blockquote>\n<p>When agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. <strong>The correct optimization target is tokens per task</strong> <a href=\"#references\">[4]</a>: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.</p>\n<p><img alt=\"Compression Strategies\" src=\"../../../../assets/2025-10-05-context-engineering/compression_strategies.svg\" /></p>\n<h3 id=\"when-compression-is-needed\">When Compression is Needed</h3>\n<p>Activate compression when:</p>\n<ul>\n<li>Agent sessions exceed context window limits</li>\n<li>Agents \"forget\" what files they modified</li>\n<li>Debugging long-running coding or debugging sessions</li>\n<li>Performance degrades in extended conversations</li>\n</ul>\n<h3 id=\"three-production-ready-approaches\">Three Production-Ready Approaches</h3>\n<p><strong>1. Anchored Iterative Summarization</strong></p>\n<p>Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary.</p>\n<p><strong>Key insight</strong>: Structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"gu\">## Session Intent</span>\n\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n<span class=\"gu\">## Root Cause</span>\n\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n<span class=\"gu\">## Files Modified</span>\n\n<span class=\"k\">-</span><span class=\"w\"> </span>auth.controller.ts: No changes (read only)\n<span class=\"k\">-</span><span class=\"w\"> </span>config/redis.ts: Fixed connection pooling configuration\n<span class=\"k\">-</span><span class=\"w\"> </span>services/session.service.ts: Added retry logic for transient failures\n<span class=\"k\">-</span><span class=\"w\"> </span>tests/auth.test.ts: Updated mock setup\n\n<span class=\"gu\">## Test Status</span>\n\n14 passing, 2 failing (mock setup issues)\n\n<span class=\"gu\">## Next Steps</span>\n\n<span class=\"k\">1.</span> Fix remaining test failures (mock session service)\n<span class=\"k\">2.</span> Run full test suite\n<span class=\"k\">3.</span> Deploy to staging\n</code></pre></div>\n<p><strong>2. Opaque Compression</strong></p>\n<p>Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Use when maximum token savings required and re-fetching costs are low.</p>\n<p><strong>3. Regenerative Full Summary</strong></p>\n<p>Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles.</p>\n<h3 id=\"compression-comparison\">Compression Comparison</h3>\n<p>Research from Factory.ai <a href=\"#references\">[4]</a> compared compression strategies on real production agent sessions:</p>\n<table>\n<thead>\n<tr>\n<th>Method</th>\n<th>Compression Ratio</th>\n<th>Quality Score</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Anchored Iterative</td>\n<td>98.6%</td>\n<td>3.70/5</td>\n<td>Long sessions, file tracking</td>\n</tr>\n<tr>\n<td>Regenerative</td>\n<td>98.7%</td>\n<td>3.44/5</td>\n<td>Clear phase boundaries</td>\n</tr>\n<tr>\n<td>Opaque</td>\n<td>99.3%</td>\n<td>3.35/5</td>\n<td>Maximum savings, short sessions</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Understanding the metrics:</strong></p>\n<ul>\n<li>\n<p><strong>Compression Ratio (98.6%)</strong>: The percentage of tokens removed. A 98.6% ratio means if you had 100,000 tokens of conversation history, only 1,400 tokens remain after compression (98,600 were removed).</p>\n</li>\n<li>\n<p><strong>Quality Score (3.70/5)</strong>: Measured via probe-based evaluation\u2014after compression, the agent is asked questions that require recalling specific details from the truncated history (e.g., \"What files did we modify?\", \"What was the error message?\"). A score of 3.70/5 means the agent answered ~74% of probes correctly.</p>\n</li>\n<li>\n<p><strong>0.7% additional tokens</strong>: Comparing Anchored Iterative (98.6%) to Opaque (99.3%), the difference is 0.7%. For a 100K token session: Anchored keeps 1,400 tokens, Opaque keeps only 700 tokens. That extra 700 tokens (0.7% of original) buys 0.35 quality points (3.70 vs 3.35)\u2014meaning significantly better recall of task-critical details.</p>\n</li>\n</ul>\n<h3 id=\"the-artifact-trail-problem\">The Artifact Trail Problem</h3>\n<p><strong>Artifact trail integrity is the weakest dimension</strong> across all compression methods, scoring 2.2-2.5 out of 5.0. Coding agents need to know which files were created, modified, read\u2014and compression often loses this.</p>\n<p><strong>Recommendation</strong>: Implement a separate artifact index or explicit file-state tracking in agent scaffolding, beyond general summarization.</p>\n<h3 id=\"compression-trigger-strategies\">Compression Trigger Strategies</h3>\n<table>\n<thead>\n<tr>\n<th>Strategy</th>\n<th>Trigger Point</th>\n<th>Trade-off</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Fixed threshold</td>\n<td>70-80% utilization</td>\n<td>Simple but may compress early</td>\n</tr>\n<tr>\n<td>Sliding window</td>\n<td>Keep last N turns + summary</td>\n<td>Predictable context size</td>\n</tr>\n<tr>\n<td>Importance-based</td>\n<td>Compress low-relevance first</td>\n<td>Complex but preserves signal</td>\n</tr>\n<tr>\n<td>Task-boundary</td>\n<td>Compress at task completions</td>\n<td>Clean summaries, unpredictable timing</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"probe-based-evaluation\">Probe-Based Evaluation</h3>\n<p>Traditional metrics like ROUGE fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.</p>\n<p><strong>Probe-based evaluation</strong> directly measures quality by asking questions after compression:</p>\n<table>\n<thead>\n<tr>\n<th>Probe Type</th>\n<th>What It Tests</th>\n<th>Example Question</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Recall</td>\n<td>Factual retention</td>\n<td>\"What was the original error message?\"</td>\n</tr>\n<tr>\n<td>Artifact</td>\n<td>File tracking</td>\n<td>\"Which files have we modified?\"</td>\n</tr>\n<tr>\n<td>Continuation</td>\n<td>Task planning</td>\n<td>\"What should we do next?\"</td>\n</tr>\n<tr>\n<td>Decision</td>\n<td>Reasoning chain</td>\n<td>\"What did we decide about the Redis issue?\"</td>\n</tr>\n</tbody>\n</table>\n<p>If compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.</p>\n<hr />\n<h2 id=\"context-optimization-techniques\">Context Optimization Techniques</h2>\n<p>Context optimization extends effective capacity through strategic compression, masking, caching, and partitioning. <strong>These techniques build on the <a href=\"#context-compression-strategies\">Context Compression Strategies</a> covered earlier</strong>, applying them systematically for production use. Effective optimization can <strong>double or triple effective context capacity</strong> without requiring larger models.</p>\n<p><img alt=\"Optimization Techniques\" src=\"../../../../assets/2025-10-05-context-engineering/optimization_techniques.svg\" /></p>\n<h3 id=\"compaction-strategies\">Compaction Strategies</h3>\n<p>Compaction summarizes context contents when approaching limits, then reinitializes with the summary. This distills contents in a high-fidelity manner, enabling continued operation with minimal degradation.</p>\n<p><strong>Priority for compression</strong>:</p>\n<ol>\n<li>Tool outputs \u2192 replace with summaries</li>\n<li>Old turns \u2192 summarize early conversation</li>\n<li>Retrieved docs \u2192 summarize if recent versions exist</li>\n<li><strong>Never compress</strong>: system prompt</li>\n</ol>\n<p><strong>Effective summaries preserve different elements by type</strong>:</p>\n<ul>\n<li><strong>Tool outputs</strong>: Preserve key findings, metrics, conclusions. Remove verbose raw output.</li>\n<li><strong>Conversational turns</strong>: Preserve decisions, commitments, context shifts. Remove filler.</li>\n<li><strong>Retrieved documents</strong>: Preserve key facts and claims. Remove supporting elaboration.</li>\n</ul>\n<h3 id=\"observation-masking\">Observation Masking</h3>\n<p>Tool outputs can comprise <strong>over 80% of token usage</strong> <a href=\"#references\">[4]</a>. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value.</p>\n<p><strong>Masking decision matrix</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Action</th>\n<th>Reasoning</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Current task observations</td>\n<td>Never mask</td>\n<td>Critical to current work</td>\n</tr>\n<tr>\n<td>Most recent turn</td>\n<td>Never mask</td>\n<td>Immediately relevant</td>\n</tr>\n<tr>\n<td>Active reasoning</td>\n<td>Never mask</td>\n<td>In-progress thought</td>\n</tr>\n<tr>\n<td>3+ turns ago</td>\n<td>Consider masking</td>\n<td>Purpose likely served</td>\n</tr>\n<tr>\n<td>Repeated outputs</td>\n<td>Always mask</td>\n<td>Redundant</td>\n</tr>\n<tr>\n<td>Boilerplate</td>\n<td>Always mask</td>\n<td>Low signal</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"kv-cache-optimization\">KV-Cache Optimization</h3>\n<p>The KV-cache stores Key and Value tensors computed during inference. Caching across requests with identical prefixes avoids recomputation, dramatically reducing cost and latency.</p>\n<p><strong>Optimize for caching</strong>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Stable content first (cacheable)</span>\n<span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">system_prompt</span><span class=\"p\">,</span> <span class=\"n\">tool_definitions</span><span class=\"p\">]</span>\n<span class=\"c1\"># Frequently reused elements</span>\n<span class=\"n\">context</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"n\">reused_templates</span><span class=\"p\">]</span>\n<span class=\"c1\"># Unique elements last</span>\n<span class=\"n\">context</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"n\">unique_content</span><span class=\"p\">]</span>\n</code></pre></div>\n<p><strong>Design for cache stability</strong>: Avoid dynamic content like timestamps in prompts, use consistent formatting, keep structure stable across sessions.</p>\n<h3 id=\"context-partitioning\">Context Partitioning</h3>\n<p>The most aggressive optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.</p>\n<p><strong>Result aggregation</strong>: Validate all partitions completed, merge compatible results, summarize if still too large.</p>\n<h3 id=\"optimization-decision-framework\">Optimization Decision Framework</h3>\n<p><strong>When to optimize</strong>:</p>\n<ul>\n<li>Context utilization exceeds 70%</li>\n<li>Response quality degrades in extended conversations</li>\n<li>Costs increase due to long contexts</li>\n<li>Latency increases with conversation length</li>\n</ul>\n<p><strong>What to apply</strong>:</p>\n<ul>\n<li>Tool outputs dominate \u2192 observation masking</li>\n<li>Retrieved documents dominate \u2192 summarization or partitioning</li>\n<li>Message history dominates \u2192 compaction with summarization</li>\n<li>Multiple components \u2192 combine strategies</li>\n</ul>\n<p><strong>Target metrics</strong>:</p>\n<ul>\n<li>Compaction: 50-70% token reduction with &lt;5% quality degradation</li>\n<li>Masking: 60-80% reduction in masked observations</li>\n<li>Cache optimization: 70%+ hit rate for stable workloads</li>\n</ul>\n<hr />\n<h2 id=\"how-to-cook-it-stepbystep\">How to cook it (step\u2011by\u2011step)</h2>\n<p>Here's a practical recipe to implement the context layer in your agentic system. Start simple, then add complexity only when needed.</p>\n<h3 id=\"step-1-write-the-contract\">Step 1: Write the contract</h3>\n<p>Define what your agent must do and how it should behave.</p>\n<p><strong>Actions</strong>:</p>\n<ul>\n<li>Write system-level policies: role, constraints, safety rules</li>\n<li>Write developer guidelines: output format, tone, citation requirements</li>\n<li>Define JSON Schemas for all outputs: <code>AnswerSchema</code>, <code>PlanSchema</code>, <code>StepResultSchema</code></li>\n</ul>\n<h3 id=\"step-2-pick-retrieval-strategy\">Step 2: Pick retrieval strategy</h3>\n<p>Start with hybrid retrieval (BM25 + vector) + reranker.</p>\n<p><strong>Decision tree</strong>:</p>\n<ul>\n<li>Query is general knowledge? \u2192 No retrieval (parametric)</li>\n<li>Query needs fresh/private facts? \u2192 Single-shot RAG (hybrid + rerank)</li>\n<li>Query is complex/multi-part? \u2192 Iterative RAG (break into subqueries)</li>\n</ul>\n<h3 id=\"step-3-design-memory\">Step 3: Design memory</h3>\n<p>Split short-term (conversation state) from long-term (user facts, history).</p>\n<p><strong>Actions</strong>:</p>\n<ul>\n<li>Short-term: store conversation state, last few turns. Clear after session.</li>\n<li>Long-term: store user entities, preferences, episodic events.</li>\n<li>Set expiry rules: preferences 365d, episodic 90d, short-term session-only.</li>\n<li>Add PII redaction before storing anything.</li>\n</ul>\n<h3 id=\"step-4-specify-tools\">Step 4: Specify tools</h3>\n<p>Define clear tool signatures with validation and fallback strategies.</p>\n<p><strong>Actions</strong>:</p>\n<ul>\n<li>For each tool: write clear docstring, input schema, output schema</li>\n<li>Mark idempotency: is it safe to retry?</li>\n<li>Define postconditions and fallback chains</li>\n<li>Validate tool arguments against schema <em>before</em> calling</li>\n</ul>\n<h3 id=\"step-5-install-guardrails\">Step 5: Install guardrails</h3>\n<p>Add input and output validation, safety filters, and policy enforcement.</p>\n<p><strong>Quick checklist</strong>:</p>\n<ul>\n<li>[ ] Redact PII (emails, SSNs, credit cards) before processing</li>\n<li>[ ] Validate all outputs against JSON Schema</li>\n<li>[ ] Block prompt injection attempts</li>\n<li>[ ] Rate limit tool calls</li>\n<li>[ ] Log all policy violations for auditing</li>\n</ul>\n<h3 id=\"step-6-add-observability-evals\">Step 6: Add observability &amp; evals</h3>\n<p>Instrument your context layer so you can debug and improve it.</p>\n<p><strong>Essential traces</strong>:</p>\n<ul>\n<li>Which context sources loaded?</li>\n<li>Token counts: input, output, cost</li>\n<li>Retrieval metrics: query, top-k results, sources cited</li>\n<li>Tool calls: which tools, arguments, results, failures</li>\n<li>Guardrail triggers: input blocks, output repairs, policy refusals</li>\n</ul>\n<p><strong>Metrics to track</strong>:</p>\n<ol>\n<li><strong>Exactness (schema validity)</strong>: Target 99%+</li>\n<li><strong>Groundedness (citation rate)</strong>: Target 90%+ for knowledge queries</li>\n<li><strong>Latency</strong>: Target &lt;2s p95</li>\n<li><strong>Cost</strong>: Target &lt;$0.05 per query</li>\n</ol>\n<h3 id=\"step-7-iterate\">Step 7: Iterate</h3>\n<p>Start with basics. Add advanced patterns only when you hit clear limits.</p>\n<p><strong>When to add</strong>:</p>\n<ul>\n<li><strong>Reflections</strong>: when error rate &gt; 5%</li>\n<li><strong>Planners</strong>: when tasks require &gt; 3 sequential steps</li>\n<li><strong>Sub-agents</strong>: when you have distinct domains</li>\n</ul>\n<hr />\n<h2 id=\"antipatterns\">Anti\u2011patterns</h2>\n<p>Common mistakes that kill agentic systems. Avoid these.</p>\n<h3 id=\"1-stuff-the-window\">1. Stuff-the-window</h3>\n<p><strong>What</strong>: Dump every possible document, memory, and example into context on every query.</p>\n<p><strong>Why it fails</strong>: Context rot. Signal-to-noise ratio collapses. See <a href=\"#context-degradation-patterns\">Context Degradation Patterns</a> for details on distraction and confusion.</p>\n<p><strong>Fix</strong>: Route adaptively. Use compression and masking. See <a href=\"#context-optimization-techniques\">Context Optimization Techniques</a>.</p>\n<hr />\n<h3 id=\"2-unvalidated-tool-results\">2. Unvalidated tool results</h3>\n<p><strong>What</strong>: Agent calls a tool, gets back data, and immediately feeds it to the model without checking.</p>\n<p><strong>Why it fails</strong>: Malformed data crashes downstream logic. Null results cause hallucinations. This is a primary vector for <a href=\"#context-poisoning\">Context Poisoning</a>.</p>\n<p><strong>Fix</strong>: Always validate tool results against schema and postconditions.</p>\n<hr />\n<h3 id=\"3-one-shot-everything\">3. One-shot everything</h3>\n<p><strong>What</strong>: Cram system policy, developer guidelines, examples, user query, memory, and knowledge into a single monolithic prompt.</p>\n<p><strong>Why it fails</strong>: No separation of concerns. Context window fills with duplicate boilerplate.</p>\n<p><strong>Fix</strong>: Separate durable instructions from step-specific context. Use <a href=\"#kv-cache-optimization\">KV-Cache Optimization</a> patterns.</p>\n<hr />\n<h3 id=\"4-unbounded-memory\">4. Unbounded memory</h3>\n<p><strong>What</strong>: Store every user interaction forever. Load all of it on every query.</p>\n<p><strong>Why it fails</strong>: Context fills with stale, irrelevant memories. Privacy risks. See <a href=\"#the-lost-in-middle-phenomenon\">Lost-in-Middle</a> for why middle content gets ignored anyway.</p>\n<p><strong>Fix</strong>: Set retention policies. Implement scoped retrieval. Redact PII.</p>\n<hr />\n<h3 id=\"5-rag-everywhere\">5. RAG everywhere</h3>\n<p><strong>What</strong>: Retrieve documents for every single query, even \"What is 2+2?\" or \"Hello\".</p>\n<p><strong>Why it fails</strong>: Wastes latency and cost. Retrieval can inject noise that causes <a href=\"#context-distraction\">Context Distraction</a>.</p>\n<p><strong>Fix</strong>: Implement adaptive RAG routing. Use classifiers or heuristics.</p>\n<hr />\n<h3 id=\"6-ignoring-guardrail-triggers\">6. Ignoring guardrail triggers</h3>\n<p><strong>What</strong>: Log guardrail violations but never review them.</p>\n<p><strong>Why it fails</strong>: You miss real attacks. You miss UX issues. Schema repairs shouldn't be frequent\u2014if they are, your instructions are unclear.</p>\n<p><strong>Fix</strong>: Review guardrail triggers weekly.</p>\n<hr />\n<h3 id=\"7-no-evals\">7. No evals</h3>\n<p><strong>What</strong>: Ship context layer changes without testing them.</p>\n<p><strong>Why it fails</strong>: Silent regressions. No way to compare variants objectively.</p>\n<p><strong>Fix</strong>: Define 5\u201310 eval scenarios before shipping. Run on every change. Use <a href=\"#probe-based-evaluation\">Probe-Based Evaluation</a> to catch compression quality issues.</p>\n<hr />\n<h2 id=\"quick-wins-ship-these-today\">Quick wins: ship these today</h2>\n<p>If you already have an agent in production and want immediate improvements, start here. Each takes &lt; 1 day.</p>\n<h3 id=\"1-add-output-schema-validation\">1. Add output schema validation</h3>\n<p><strong>Impact</strong>: Catch the majority of errors before they reach users.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">jsonschema</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">validate</span><span class=\"p\">,</span> <span class=\"n\">ValidationError</span>\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">validate_output</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">validate</span><span class=\"p\">(</span><span class=\"n\">instance</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">schema</span><span class=\"o\">=</span><span class=\"n\">ANSWER_SCHEMA</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">output</span>\n    <span class=\"k\">except</span> <span class=\"n\">ValidationError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"n\">repaired</span> <span class=\"o\">=</span> <span class=\"n\">auto_repair</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">)</span>\n        <span class=\"n\">validate</span><span class=\"p\">(</span><span class=\"n\">instance</span><span class=\"o\">=</span><span class=\"n\">repaired</span><span class=\"p\">,</span> <span class=\"n\">schema</span><span class=\"o\">=</span><span class=\"n\">ANSWER_SCHEMA</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">repaired</span>\n</code></pre></div>\n<h3 id=\"2-instrument-basic-tracing\">2. Instrument basic tracing</h3>\n<p><strong>Impact</strong>: Debug significantly faster when things break.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">({</span>\n    <span class=\"s2\">&quot;request_id&quot;</span><span class=\"p\">:</span> <span class=\"n\">request_id</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;query&quot;</span><span class=\"p\">:</span> <span class=\"n\">query</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;context_loaded&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;instructions&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s2\">&quot;memory&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"s2\">&quot;knowledge&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">},</span>\n    <span class=\"s2\">&quot;tokens&quot;</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s2\">&quot;input&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1200</span><span class=\"p\">,</span> <span class=\"s2\">&quot;output&quot;</span><span class=\"p\">:</span> <span class=\"mi\">150</span><span class=\"p\">},</span>\n    <span class=\"s2\">&quot;latency_ms&quot;</span><span class=\"p\">:</span> <span class=\"mi\">1120</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;result&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;success&quot;</span>\n<span class=\"p\">}))</span>\n</code></pre></div>\n<h3 id=\"3-split-system-vs-user-messages\">3. Split system vs user messages</h3>\n<p><strong>Impact</strong>: Reduce token waste significantly by enabling <a href=\"#kv-cache-optimization\">KV-Cache Optimization</a>.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"p\">{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"n\">SYSTEM_POLICY</span> <span class=\"o\">+</span> <span class=\"n\">DEVELOPER_GUIDELINES</span><span class=\"p\">},</span>\n    <span class=\"p\">{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;Query: </span><span class=\"si\">{</span><span class=\"n\">query</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">Memory: </span><span class=\"si\">{</span><span class=\"n\">memory</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s2\">Knowledge: </span><span class=\"si\">{</span><span class=\"n\">knowledge</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</code></pre></div>\n<h3 id=\"4-add-citation-requirements\">4. Add citation requirements</h3>\n<p><strong>Impact</strong>: Build trust, enable auditing, reduce hallucinations.</p>\n<h3 id=\"5-set-memory-expiry\">5. Set memory expiry</h3>\n<p><strong>Impact</strong>: Prevent context pollution and privacy risks.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">load_memory</span><span class=\"p\">(</span><span class=\"n\">customer_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n    <span class=\"n\">entries</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">get_memory</span><span class=\"p\">(</span><span class=\"n\">customer_id</span><span class=\"p\">)</span>\n    <span class=\"n\">now</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"n\">k</span><span class=\"p\">:</span> <span class=\"n\">v</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">entries</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">v</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;expires_at&quot;</span><span class=\"p\">,</span> <span class=\"n\">now</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"n\">now</span>\n    <span class=\"p\">}</span>\n</code></pre></div>\n<hr />\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Context engineering is the discipline that separates demo agents from production agents. The model didn't get worse\u2014the context did. By understanding:</p>\n<ul>\n<li><strong>Fundamentals</strong>: Context is finite, attention is limited, progressive disclosure is key</li>\n<li><strong>Degradation</strong>: Lost-in-middle, poisoning, distraction, confusion, clash</li>\n<li><strong>Compression</strong>: Tokens-per-task over tokens-per-request, structured summaries</li>\n<li><strong>Optimization</strong>: Compaction, masking, caching, partitioning</li>\n</ul>\n<p>...you can build agents that work reliably at scale.</p>\n<p>Start with the quick wins. Add complexity only when you hit clear limits. And always measure\u2014you can't improve what you don't trace.</p>\n<hr />\n<p><em>This article incorporates content from the Agent Skills for Context Engineering collection, a set of reusable knowledge modules for building better AI agents.</em></p>\n<hr />\n<h2 id=\"references\">References</h2>\n<ol>\n<li>\n<p><strong>Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp; Liang, P. (2023).</strong> \"Lost in the Middle: How Language Models Use Long Contexts.\" <em>arXiv preprint arXiv:2307.03172</em>. <a href=\"https://arxiv.org/abs/2307.03172\">https://arxiv.org/abs/2307.03172</a></p>\n<ul>\n<li>Key finding: 10-40% lower recall accuracy for information in the middle of context vs. beginning/end.</li>\n</ul>\n</li>\n<li>\n<p><strong>Xiao, G., Tian, Y., Chen, B., Han, S., &amp; Lewis, M. (2023).</strong> \"Efficient Streaming Language Models with Attention Sinks.\" <em>ICLR 2024</em>. <a href=\"https://arxiv.org/abs/2309.17453\">https://arxiv.org/abs/2309.17453</a></p>\n<ul>\n<li>Introduces the \"attention sink\" phenomenon where LLMs allocate disproportionate attention to initial tokens.</li>\n</ul>\n</li>\n<li>\n<p><strong>Hsieh, C. Y., et al. (2024).</strong> \"RULER: What's the Real Context Size of Your Long-Context Language Models?\" <em>COLM 2024</em>. <a href=\"https://arxiv.org/abs/2404.06654\">https://arxiv.org/abs/2404.06654</a></p>\n<ul>\n<li>Key finding: Only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens.</li>\n</ul>\n</li>\n<li>\n<p><strong>Factory.ai Research. (2025).</strong> \"Evaluating Context Compression for AI Agents.\" <a href=\"https://www.factory.ai/blog/evaluating-context-compression\">https://www.factory.ai/blog/evaluating-context-compression</a></p>\n<ul>\n<li>Source for compression strategy comparisons, tokens-per-task optimization, and probe-based evaluation methodology.</li>\n</ul>\n</li>\n<li>\n<p><strong>Li, Y., et al. (2023).</strong> \"Compressing Context to Enhance Inference Efficiency of Large Language Models.\" <em>EMNLP 2023</em>.</p>\n<ul>\n<li>Research on selective context pruning using self-information metrics.</li>\n</ul>\n</li>\n<li>\n<p><strong>Anthropic. (2024).</strong> \"Model Context Protocol (MCP) Specification.\" <a href=\"https://modelcontextprotocol.io/\">https://modelcontextprotocol.io/</a></p>\n<ul>\n<li>Official specification for the MCP standard for AI-tool integration.</li>\n</ul>\n</li>\n<li>\n<p><strong>LangChain/LangGraph. (2024).</strong> \"How to add memory to the prebuilt ReAct agent.\" <a href=\"https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/\">https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/</a>\n   \u2014 Demonstrates that summarization is one technique for managing memory within context limits.</p>\n</li>\n<li>\n<p><strong>Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., &amp; Berant, J. (2024).</strong> \"Making Retrieval-Augmented Language Models Robust to Irrelevant Context.\" <em>ICLR 2024</em>. <a href=\"https://arxiv.org/abs/2310.01558\">https://arxiv.org/abs/2310.01558</a></p>\n<ul>\n<li>Key finding: Even a single irrelevant document can significantly reduce RAG performance, creating a \"distracting effect.\"</li>\n</ul>\n</li>\n<li>\n<p><strong>Maekawa, S., et al. (2025).</strong> \"NoLiMa: Long-Context Evaluation Beyond Literal Matching.\" <em>ICML 2025</em>. <a href=\"https://arxiv.org/abs/2502.05167\">https://arxiv.org/abs/2502.05167</a></p>\n<ul>\n<li>Key finding: GPT-4o effective context ~8K tokens, Claude 3.5 Sonnet ~4K tokens when latent reasoning is required (vs. literal matching). At 32K tokens, GPT-4o drops from 99.3% to 69.7% accuracy.</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"additional-resources\">Additional Resources</h3>\n<ul>\n<li><strong>Anthropic Claude Documentation</strong>: Best practices for long-context usage</li>\n<li><strong>OpenAI Cookbook</strong>: Strategies for managing context windows</li>\n<li><strong>LangChain Documentation</strong>: Memory and retrieval patterns</li>\n<li><strong>LlamaIndex Documentation</strong>: RAG and chunking strategies</li>\n</ul>", "image": null, "date_modified": "2025-12-27T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Agentic AI"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/", "url": "https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/", "title": "Choosing the Right Open-Source LLM Variant & File Format", "content_html": "<h1 id=\"choosing-the-right-open-source-llm-variant-file-format\">Choosing the Right Open-Source LLM Variant &amp; File Format</h1>\n<hr />\n<h2 id=\"why-do-open-source-llms-have-so-many-confusing-names\">Why do open-source LLMs have so many confusing names?</h2>\n<p>You've probably seen model names like <code>Llama-3.1-8B-Instruct.Q4_K_M.gguf</code> or <code>Mistral-7B-v0.3-A3B.awq</code> and wondered what all those suffixes mean. It looks like a secret code, but the short answer is: <strong>they tell you two critical things.</strong></p>\n<p>Open-source LLMs vary along <strong>two independent dimensions</strong>:</p>\n<ol>\n<li><strong>Model variant</strong> \u2013 the suffix in the name (<code>-Instruct</code>, <code>-Distill</code>, <code>-A3B</code>, etc.) describes <em>how</em> the model was trained and <em>what</em> it's optimized for.</li>\n<li><strong>File format</strong> \u2013 the extension (<code>.gguf</code>, <code>.gptq</code>, <code>.awq</code>, etc.) describes <em>how</em> the weights are stored and <em>where</em> they run best (CPU, GPU, mobile, etc.).</li>\n</ol>\n<p>Think of it like this: the <strong>model variant is the recipe</strong>, and the <strong>file format is the container</strong>. You can put the same soup (recipe) into a thermos, a bowl, or a takeout box (container) depending on where you plan to eat it.</p>\n<p><img alt=\"LLM Variant vs Format\" src=\"../../../../assets/2025-05-11-llm-variant-guide/llm_variant_vs_format.svg\" /></p>\n<p>Understanding both dimensions helps you avoid downloading 20 GB of the wrong model at midnight and then spending hours debugging CUDA errors.</p>\n<!-- more -->\n\n<hr />\n<h2 id=\"model-variants-explained-the-recipe\">Model variants explained (the recipe)</h2>\n<p>This is about the <em>brain</em> of the model. How was it taught?</p>\n<h3 id=\"base-models\">Base models</h3>\n<p><strong>What it is:</strong> The raw, pre-trained model straight from the training run. Think of it as the unfiltered brain that learned language patterns from massive text datasets (the entire internet) but hasn't been taught to follow instructions. It just predicts the next word.</p>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>You're planning to <strong>fine-tune</strong> it for your specific domain.</li>\n<li>You're doing research and need the \"pure\" foundation.</li>\n<li>You want maximum creative freedom (no safety guardrails).</li>\n</ul>\n<p><strong>Trade-offs:</strong> Won't reliably follow instructions. If you ask \"What is the capital of France?\", it might reply \"and what is the capital of Germany?\" because it thinks it's completing a list of questions.</p>\n<h3 id=\"instruct-chat-models\">Instruct / Chat models</h3>\n<p><strong>What it is:</strong> A base model that went through additional training (Supervised Fine-Tuning + RLHF) to understand and follow human instructions. This is what most people actually want when they say \"I want an LLM.\"</p>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Building chatbots, AI agents, or RAG applications.</li>\n<li>Function calling and tool use.</li>\n<li>Day-to-day coding assistance.</li>\n<li><strong>95% of production use cases.</strong></li>\n</ul>\n<p><strong>Trade-offs:</strong> Slightly larger and slower than base models due to the extra training layers. May be less \"creative\" due to alignment training that makes it more predictable and helpful.</p>\n<h3 id=\"reasoning-cot-models-new\">Reasoning / CoT Models (New!)</h3>\n<p><strong>What it is:</strong> A new breed of models (like DeepSeek-R1 or o1-derivatives) trained with \"Chain of Thought\" (CoT) reinforcement learning. They \"think\" before they speak, generating internal reasoning tokens to solve complex logic, math, or coding problems before outputting the final answer.</p>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Complex coding tasks and debugging.</li>\n<li>Math problems and logic puzzles.</li>\n<li>When you need the model to double-check its work and avoid hallucinations.</li>\n</ul>\n<p><strong>Trade-offs:</strong> <strong>Slower inference</strong>. They generate many \"thought\" tokens that you might not see but still have to wait for. They can also be overly verbose for simple \"hello world\" tasks.</p>\n<h3 id=\"distilled-models\">Distilled models</h3>\n<p><strong>What it is:</strong> A smaller \"student\" model trained to mimic the behavior of a larger \"teacher\" model. Think of it as compressed knowledge\u2014you get 70-80% of the performance at 30-50% of the size.</p>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Mobile or edge devices with limited resources.</li>\n<li>Cost-sensitive SaaS where every millisecond counts.</li>\n<li>High-throughput scenarios where you need to serve many requests.</li>\n</ul>\n<p><strong>Trade-offs:</strong> Some loss in complex reasoning ability, but excellent efficiency. The token-per-watt ratio is hard to beat.</p>\n<h3 id=\"moe-mixture-of-experts-a3b-a22b-etc\">MoE (Mixture-of-Experts): A3B, A22B, etc.</h3>\n<p><strong>What it is:</strong> A clever architecture where the model has many \"expert\" sub-networks, but only activates a subset for each token. \"A3B\" means \"3 billion parameters active\" out of a much larger total (often 30B+).</p>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>You want \"big model\" smarts but only have 12-24 GB VRAM.</li>\n<li>You need the reasoning power of a 30B model but with 7B inference costs.</li>\n<li>You're running locally and want the best performance-per-memory ratio.</li>\n</ul>\n<p><strong>Trade-offs:</strong> Takes more disk space (you're storing all the experts). Not every inference framework supports MoE routing yet\u2014check compatibility first.</p>\n<p><img alt=\"Choosing Model Variant\" src=\"../../../../assets/2025-05-11-llm-variant-guide/choose_model_variant.svg\" /></p>\n<blockquote>\n<p><strong>Rule of thumb:</strong></p>\n<ul>\n<li>Start with an <strong>Instruct</strong> model\u2014it's what most people need.</li>\n<li>Hit memory or latency limits? Try a <strong>Distilled</strong> or <strong>MoE</strong> variant.</li>\n<li>Need to solve a complex riddle? Try a <strong>Reasoning</strong> model.</li>\n</ul>\n</blockquote>\n<hr />\n<h2 id=\"file-formats-explained-the-container\">File formats explained (the container)</h2>\n<p>Now that you know <em>what</em> kind of model you want, you need to pick <em>how</em> it's packaged. File formats determine where your model runs best and how much memory it needs.</p>\n<h3 id=\"quantization-101-why-do-we-shrink-models\">Quantization 101: Why do we shrink models?</h3>\n<p>Before we talk formats, let's talk <strong>Quantization</strong>.\nStandard models use 16-bit numbers (FP16) for every weight. That's precise but huge.\nQuantization reduces these to 8-bit, 4-bit, or even 2-bit numbers.</p>\n<ul>\n<li><strong>FP16</strong>: 2 bytes per parameter. (13B model \u2248 26 GB)</li>\n<li><strong>4-bit</strong>: 0.5 bytes per parameter. (13B model \u2248 6.5 GB)</li>\n</ul>\n<p>You lose a tiny bit of \"intelligence\" but gain massive speed and memory savings.</p>\n<h3 id=\"gguf-gguf\">GGUF (<code>.gguf</code>)</h3>\n<p><strong>What it is:</strong> The successor to GGML and now the <strong>de-facto standard for local inference</strong>. A single file that contains the model weights, metadata, and even the prompt template.</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li><strong>Apple Silicon (M1/M2/M3)</strong>: It works natively with Metal acceleration.</li>\n<li><strong>CPU Inference</strong>: If you don't have a dedicated GPU.</li>\n<li><strong>Easy Setup</strong>: Works with <code>llama.cpp</code>, Ollama, and LM Studio.</li>\n</ul>\n<p><strong>Why it's great:</strong> One file, works everywhere. Supports multiple quantization levels.</p>\n<h4 id=\"decoding-gguf-names-q3_k_m-q5_k_m-etc\">Decoding GGUF Names (Q3_K_M, Q5_K_M, etc.)</h4>\n<p>You'll often see a long list of files like <code>Q3_K_M</code>, <code>Q4_K_S</code>, <code>Q5_K_M</code>. These aren't random; they are specific \"K-quant\" formats.</p>\n<p><strong>How to read <code>Q3_K_M</code>:</strong></p>\n<ul>\n<li><strong>Q3</strong>: Average <strong>3-bit</strong> quantization for the weights.</li>\n<li><strong>K</strong>: Uses the <strong>K-quant</strong> scheme (a newer, smarter quantization method that uses non-uniform precision).</li>\n<li><strong>M</strong>: <strong>Medium</strong> block size. This refers to the internal layout (<code>S</code> = Small, <code>L</code> = Large).</li>\n</ul>\n<p><strong>Which one should you pick?</strong></p>\n<ul>\n<li><strong>Q3_K_M (The \"Budget\" Choice)</strong>: Use when you are tight on memory. It has a noticeable quality drop but allows you to run larger models on weaker hardware.</li>\n<li><strong>Q4_K_M (The \"Standard\")</strong>: The sweet spot. Best balance of speed, size, and perplexity. Indistinguishable from uncompressed for most tasks.</li>\n<li><strong>Q5_K_M (The \"Premium\")</strong>: Use if you have VRAM to spare. Quality is very close to FP16 / Q8 while still being quite compact.</li>\n</ul>\n<blockquote>\n<p><strong>Pro Tip:</strong> It is often better to run a <strong>larger model at lower quantization</strong> (e.g., Llama-70B at Q3) than a <strong>smaller model at high quantization</strong> (e.g., Llama-8B at Q8). Intelligence scales with parameter count more than precision.</p>\n</blockquote>\n<h3 id=\"gptq-safetensors-configjson\">GPTQ (<code>.safetensors</code> + <code>config.json</code>)</h3>\n<p><strong>What it is:</strong> Post-training quantization optimized specifically for <strong>Nvidia GPUs</strong>. It uses second-order information to minimize accuracy loss when compressing.</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li><strong>Production Servers</strong>: Running on Linux with Nvidia GPUs (CUDA).</li>\n<li><strong>High Throughput</strong>: Very fast inference at 4-bit.</li>\n<li><strong>ExLlamaV2</strong>: Can be run with the ExLlamaV2 loader for extreme speed.</li>\n</ul>\n<p><strong>Watch out for:</strong> Requires a GPU. Won't run efficiently on CPU or Mac.</p>\n<h3 id=\"awq-safetensors\">AWQ (<code>.safetensors</code>)</h3>\n<p><strong>What it is:</strong> Activation-Aware Weight Quantization. It analyzes which weights matter most during inference and preserves their precision better than naive quantization.</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li><strong>Accuracy</strong>: Often matches FP16 accuracy more closely than GPTQ at 4-bit.</li>\n<li><strong>vLLM</strong>: Supported natively by the vLLM serving engine.</li>\n</ul>\n<p><strong>Why it's great:</strong> It's \"smarter\" quantization. If you care about squeezing every drop of quality out of a 4-bit model, AWQ is often the winner.</p>\n<h3 id=\"pytorch-safetensors-fp16bf16\">PyTorch / Safetensors (FP16/BF16)</h3>\n<p><strong>What it is:</strong> Full-precision weights with no quantization. The original format most models are released in.</p>\n<p><strong>Best for:</strong></p>\n<ul>\n<li><strong>Cloud inference</strong> with powerful GPUs (A100, H100).</li>\n<li><strong>Fine-tuning</strong> and continued training.</li>\n<li>When accuracy is paramount and memory isn't a constraint.</li>\n</ul>\n<p><strong>Trade-offs:</strong> Largest memory and disk footprint. A 70B model in FP16 needs ~140 GB VRAM!</p>\n<p><img alt=\"Choosing File Format\" src=\"../../../../assets/2025-05-11-llm-variant-guide/choose_file_format.svg\" /></p>\n<blockquote>\n<p><strong>Tip:</strong> When in doubt, start with <strong>GGUF Q4_K_M</strong>. It's the Swiss Army knife of LLM formats\u2014runs on 8GB VRAM GPUs, modern CPUs, and everything in between. You can always optimize later.</p>\n</blockquote>\n<hr />\n<h2 id=\"how-to-actually-run-these-serving-engines\">How to actually run these? (Serving Engines)</h2>\n<p>You have the file. Now what? You need an engine to run it.</p>\n<ol>\n<li>\n<p><strong>Ollama</strong>: The easiest CLI tool.</p>\n<ul>\n<li><em>Uses</em>: GGUF.</li>\n<li><em>Good for</em>: Mac, Linux, Windows, Local development.</li>\n<li><em>Command</em>: <code>ollama run llama3</code></li>\n</ul>\n</li>\n<li>\n<p><strong>LM Studio</strong>: A beautiful GUI application.</p>\n<ul>\n<li><em>Uses</em>: GGUF.</li>\n<li><em>Good for</em>: Beginners, testing models visually, Mac/Windows.</li>\n</ul>\n</li>\n<li>\n<p><strong>vLLM</strong>: The production standard.</p>\n<ul>\n<li><em>Uses</em>: AWQ, GPTQ, Safetensors (FP16).</li>\n<li><em>Good for</em>: High-performance servers, deploying APIs, Linux/Docker.</li>\n<li><em>Note</em>: Doesn't support GGUF well (yet).</li>\n</ul>\n</li>\n<li>\n<p><strong>Llama.cpp</strong>: The engine behind Ollama.</p>\n<ul>\n<li><em>Uses</em>: GGUF.</li>\n<li><em>Good for</em>: Low-level integration, running on Raspberry Pis, Android, etc.</li>\n</ul>\n</li>\n</ol>\n<hr />\n<h2 id=\"putting-it-all-together-a-decision-framework\">Putting it all together: a decision framework</h2>\n<p>Here's a practical flowchart to help you choose. Start with your constraints (hardware and use case), then pick the appropriate combination.</p>\n<p><img alt=\"LLM Decision Tree\" src=\"../../../../assets/2025-05-11-llm-variant-guide/llm_decision_tree.svg\" /></p>\n<h3 id=\"quick-recommendations-by-scenario\">Quick recommendations by scenario</h3>\n<p><strong>Scenario 1: Building a chatbot on a MacBook Pro (16GB RAM)</strong></p>\n<ul>\n<li><strong>Model:</strong> Instruct (Llama-3-8B or Mistral-7B)</li>\n<li><strong>Format:</strong> GGUF Q4_K_M</li>\n<li><strong>Why:</strong> Runs smoothly on CPU/Metal, fits in memory, one-file simplicity.</li>\n</ul>\n<p><strong>Scenario 2: RAG system on a server with RTX 4090 (24GB VRAM)</strong></p>\n<ul>\n<li><strong>Model:</strong> Instruct or MoE (Mixtral 8x7B or Qwen-14B)</li>\n<li><strong>Format:</strong> EXL2 (via ExLlamaV2) or AWQ 4-bit</li>\n<li><strong>Why:</strong> Maximizes the 24GB VRAM. EXL2 is blazing fast on Nvidia cards.</li>\n</ul>\n<p><strong>Scenario 3: Fine-tuning for domain-specific use on cloud GPU</strong></p>\n<ul>\n<li><strong>Model:</strong> Base</li>\n<li><strong>Format:</strong> FP16 Safetensors</li>\n<li><strong>Why:</strong> You need full precision for training. Start with the unaligned base model.</li>\n</ul>\n<p><strong>Scenario 4: High-throughput API with cost constraints</strong></p>\n<ul>\n<li><strong>Model:</strong> Distilled (DeepSeek-Distill or similar)</li>\n<li><strong>Format:</strong> AWQ 4-bit running on vLLM</li>\n<li><strong>Why:</strong> vLLM + AWQ offers incredible throughput (tokens/sec) per dollar.</li>\n</ul>\n<hr />\n<h2 id=\"common-pitfalls-and-misconceptions\">Common pitfalls and misconceptions</h2>\n<h3 id=\"all-4-bit-models-are-the-same-quality\">\"All 4-bit models are the same quality\"</h3>\n<p><strong>Not true.</strong> A QAT 4-bit model (trained in 4-bit) often beats an 8-bit post-training quantized model. The <em>method</em> matters. AWQ typically preserves more accuracy than naive GPTQ.</p>\n<h3 id=\"moe-models-work-with-any-inference-engine\">\"MoE models work with any inference engine\"</h3>\n<p><strong>Not yet.</strong> <code>llama.cpp</code> handles MoE routing well. Support in other engines varies. Always check compatibility before downloading a 50GB MoE model.</p>\n<h3 id=\"distilled-models-are-just-smaller-versions\">\"Distilled models are just smaller versions\"</h3>\n<p><strong>Nope.</strong> A distilled 7B model can outperform a vanilla 13B model because it learned from a much larger teacher (often 70B+). It's compressed <em>knowledge</em>, not just compressed <em>parameters</em>.</p>\n<h3 id=\"i-should-quantize-my-qat-model-further-to-save-space\">\"I should quantize my QAT model further to save space\"</h3>\n<p><strong>Don't.</strong> QAT models were already trained in low-bit precision. Quantizing them again usually degrades quality significantly. Use them as-is.</p>\n<h3 id=\"bigger-is-always-better\">\"Bigger is always better\"</h3>\n<p><strong>Context matters.</strong> A well-tuned 8B Instruct model often outperforms a poorly-aligned 70B base model for specific tasks. Match the model variant to your use case\u2014size isn't everything.</p>\n<hr />\n<h2 id=\"tldr-just-tell-me-what-to-download\">TL;DR - Just tell me what to download</h2>\n<blockquote>\n<p><strong>If you just want something that works:</strong></p>\n<ol>\n<li>Download a <strong><code>&lt;model-name&gt;-Instruct.Q4_K_M.gguf</code></strong> file from Hugging Face.</li>\n<li>Run it with <strong>Ollama</strong> or <strong>LM Studio</strong>.</li>\n<li>If it's too slow \u2192 try a smaller model or Distilled variant.</li>\n<li>If you're out of memory \u2192 try a Q3_K_M quantization.</li>\n<li>If quality isn't good enough \u2192 move up to Q5_K_M or switch to a larger model.</li>\n</ol>\n<p>Start simple, optimize only when needed. The defaults are good enough for 90% of use cases.</p>\n</blockquote>", "image": null, "date_modified": "2025-11-27T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["AI Engineering"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/", "url": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/", "title": "Quick-guide on Running LLMs Locally on macOS", "content_html": "<h1 id=\"quick-guide-on-running-llms-locally-on-macos\">Quick-guide on Running LLMs Locally on macOS</h1>\n<p>Running Large Language Models (LLMs) locally on your Mac is a game-changer. It means <strong>faster responses</strong>, <strong>complete privacy</strong>, and <strong>zero API bills</strong>. But with so many tools popping up every week, which one should you choose?</p>\n<p>This guide breaks down the top options\u2014from dead-simple menu bar apps to full-control command-line tools. We'll cover what makes each special, their trade-offs, and how to get started.</p>\n<!-- more -->\n\n<h2 id=\"why-run-locally\">Why Run Locally?</h2>\n<p>Before we dive into the tools, let's look at the benefits:</p>\n<ul>\n<li><strong>\ud83d\udd12 Privacy</strong>: Your data and prompts never leave your machine. Perfect for sensitive work.</li>\n<li><strong>\u26a1 Speed</strong>: No network latency. On Apple Silicon, responses can be faster than cloud APIs.</li>\n<li><strong>\ud83d\udcb0 Cost</strong>: One-time download. No monthly subscriptions or token fees.</li>\n<li><strong>\u2708\ufe0f Offline</strong>: Work from a plane, a cabin, or a coffee shop with spotty Wi-Fi.</li>\n</ul>\n<h3 id=\"key-concepts-for-beginners\">Key Concepts for Beginners</h3>\n<p>If you're new to local LLMs, here are three terms you'll see often:</p>\n<ol>\n<li><strong>Inference</strong>: The act of \"running\" the model to generate text.</li>\n<li><strong>Quantization (GGUF)</strong>: A technique to shrink model sizes with minimal quality loss. You'll see filenames like <code>llama-3-8b-Q4_K_M.gguf</code>. The <code>Q4</code> means \"4-bit quantization\"\u2014it uses less RAM than the full 16-bit model.</li>\n<li><strong>Apple Silicon (Metal)</strong>: Apple's M1/M2/M3 chips have \"Unified Memory,\" allowing the CPU and GPU to share RAM. This makes Macs uniquely powerful for running huge models that would require expensive dedicated GPUs on a PC.</li>\n</ol>\n<h3 id=\"prerequisites\">Prerequisites</h3>\n<ul>\n<li><strong>Hardware</strong>: A Mac with Apple Silicon (M1, M2, M3, or M4) is highly recommended. Intel Macs work but will be significantly slower.</li>\n<li><strong>RAM</strong>:<ul>\n<li><strong>8GB</strong>: Can run small models (Mistral 7B, Llama 3 8B) comfortably.</li>\n<li><strong>16GB+</strong>: Recommended for larger models and multitasking.</li>\n</ul>\n</li>\n<li><strong>Disk Space</strong>: Models take up space! Plan for ~10-20GB for a good starter library.</li>\n</ul>\n<p><img alt=\"Local LLM Architecture\" src=\"../../../../assets/2025-05-10-guide-llm-macos-tools/local_llm_flow.svg\" /></p>\n<hr />\n<h2 id=\"1-ollama-the-just-works-option\">1. Ollama - The \"Just Works\" Option</h2>\n<p><strong>Download:</strong> <a href=\"https://ollama.com/download/mac\">ollama.com</a></p>\n<p>Think of Ollama as the \"Docker for LLMs.\" It wraps the complex engine (<code>llama.cpp</code>) in a sleek, native package. You install it, run one command, and you're chatting. It handles all the messy details like model downloading and hardware acceleration automatically.</p>\n<h3 id=\"best-for\">Best For</h3>\n<ul>\n<li><strong>Beginners</strong> who want to get started in 5 minutes.</li>\n<li><strong>Developers</strong> who want a simple CLI tool.</li>\n</ul>\n<h3 id=\"example-workflow\">Example Workflow</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># 1. Download and run Llama 3 (it auto-downloads if needed)</span>\nollama<span class=\"w\"> </span>run<span class=\"w\"> </span>llama3\n\n<span class=\"c1\"># 2. Use it in your code via the local API</span>\ncurl<span class=\"w\"> </span>http://localhost:11434/api/generate<span class=\"w\"> </span>-d<span class=\"w\"> </span><span class=\"s1\">&#39;{</span>\n<span class=\"s1\">  &quot;model&quot;: &quot;llama3&quot;,</span>\n<span class=\"s1\">  &quot;prompt&quot;: &quot;Explain quantum computing to a 5-year-old&quot;,</span>\n<span class=\"s1\">  &quot;stream&quot;: false</span>\n<span class=\"s1\">}&#39;</span>\n</code></pre></div>\n<h3 id=\"pros-cons\">Pros &amp; Cons</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">\u2705 Pros</th>\n<th style=\"text-align: left;\">\u274c Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">Easiest setup (Drag-and-drop <code>.dmg</code>)</td>\n<td style=\"text-align: left;\">Core application is closed-source</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Great CLI (<code>ollama list</code>, <code>ollama pull</code>)</td>\n<td style=\"text-align: left;\">Less granular control over generation parameters</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Huge library of pre-configured models</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"2-lm-studio-the-visual-explorer\">2. LM Studio - The Visual Explorer</h2>\n<p><strong>Download:</strong> <a href=\"https://lmstudio.ai\">lmstudio.ai</a></p>\n<p>LM Studio is for those who prefer a beautiful Graphical User Interface (GUI) over a terminal. It features a built-in \"App Store\" style browser for models, letting you search HuggingFace directly. It also supports Apple's native MLX format, which can be faster on some Macs.</p>\n<h3 id=\"best-for_1\">Best For</h3>\n<ul>\n<li><strong>Visual learners</strong> who want to explore and test different models.</li>\n<li><strong>Developers</strong> needing an OpenAI-compatible local server.</li>\n</ul>\n<h3 id=\"example-workflow_1\">Example Workflow</h3>\n<p>LM Studio has a great Python SDK, but it also provides a local server that mimics OpenAI's API, meaning you can use standard libraries:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Using the official LM Studio SDK</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">lmstudio</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">LMStudio</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">LMStudio</span><span class=\"p\">()</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">complete</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;llama-3-8b&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">prompt</span><span class=\"o\">=</span><span class=\"s2\">&quot;Write a haiku about debugging.&quot;</span>\n<span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"pros-cons_1\">Pros &amp; Cons</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">\u2705 Pros</th>\n<th style=\"text-align: left;\">\u274c Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">Beautiful, easy-to-use interface</td>\n<td style=\"text-align: left;\">GUI is closed-source</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Native support for both GGUF and MLX models</td>\n<td style=\"text-align: left;\">Larger download (~750MB)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Built-in RAG (Chat with your PDFs)</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"3-llamacpp-the-power-users-tool\">3. llama.cpp - The Power User's Tool</h2>\n<p><strong>Repo:</strong> <a href=\"https://github.com/ggml-org/llama.cpp\">github.com/ggml-org/llama.cpp</a></p>\n<p>This is the engine that powers almost everyone else. If you want <strong>maximum performance</strong>, <strong>bleeding-edge features</strong>, or to embed an LLM into your own C++ application, this is the source. It's bare-metal, lightweight, and incredibly powerful.</p>\n<h3 id=\"best-for_2\">Best For</h3>\n<ul>\n<li><strong>Engineers</strong> and <strong>Power Users</strong>.</li>\n<li>Running on older or constrained hardware.</li>\n</ul>\n<h3 id=\"example-workflow_2\">Example Workflow</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># 1. Install via Homebrew</span>\nbrew<span class=\"w\"> </span>install<span class=\"w\"> </span>llama.cpp\n\n<span class=\"c1\"># 2. Download a model manually (e.g., from HuggingFace)</span>\nhuggingface-cli<span class=\"w\"> </span>download<span class=\"w\"> </span>TheBloke/Llama-3-8B-Instruct-GGUF<span class=\"w\"> </span>--local-dir<span class=\"w\"> </span>.\n\n<span class=\"c1\"># 3. Run inference with full control</span>\nllama-cli<span class=\"w\"> </span>-m<span class=\"w\"> </span>llama-3-8b-instruct.Q4_K_M.gguf<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>-p<span class=\"w\"> </span><span class=\"s2\">&quot;Write a python script to sort a list&quot;</span><span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>-n<span class=\"w\"> </span><span class=\"m\">512</span><span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>--temp<span class=\"w\"> </span><span class=\"m\">0</span>.7<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>--ctx-size<span class=\"w\"> </span><span class=\"m\">4096</span>\n</code></pre></div>\n<h3 id=\"pros-cons_2\">Pros &amp; Cons</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">\u2705 Pros</th>\n<th style=\"text-align: left;\">\u274c Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">Ultimate control over every parameter</td>\n<td style=\"text-align: left;\">Steep learning curve (CLI only)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">MIT Licensed (Open Source)</td>\n<td style=\"text-align: left;\">Manual model management</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Extremely lightweight (&lt;30MB)</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"4-gpt4all-privacy-first-rag\">4. GPT4All - Privacy-First &amp; RAG</h2>\n<p><strong>Download:</strong> <a href=\"https://gpt4all.io\">gpt4all.io</a></p>\n<p>GPT4All focuses heavily on <strong>privacy</strong> and <strong>documents</strong>. Its standout feature is \"LocalDocs,\" which lets you point the app at a folder of PDFs, notes, or code, and chat with them instantly. It runs completely offline with no telemetry.</p>\n<h3 id=\"best-for_3\">Best For</h3>\n<ul>\n<li><strong>Privacy advocates</strong>.</li>\n<li>Users who want to <strong>chat with their own documents</strong> (RAG) easily.</li>\n</ul>\n<h3 id=\"pros-cons_3\">Pros &amp; Cons</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">\u2705 Pros</th>\n<th style=\"text-align: left;\">\u274c Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">\"LocalDocs\" RAG is excellent and easy</td>\n<td style=\"text-align: left;\">GUI-only (no headless mode)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Completely offline &amp; private</td>\n<td style=\"text-align: left;\">Heavier resource usage than Ollama</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Cross-platform (Mac, Windows, Linux)</td>\n<td style=\"text-align: left;\"></td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"5-koboldcpp-for-storytellers\">5. KoboldCPP - For Storytellers</h2>\n<p><strong>Repo:</strong> <a href=\"https://github.com/LostRuins/koboldcpp\">github.com/LostRuins/koboldcpp</a></p>\n<p>A fork of <code>llama.cpp</code> tailored for <strong>creative writing</strong> and <strong>Role-Playing Games (RPGs)</strong>. It features a web interface designed for long-form text generation, with tools to manage \"World Info,\" character memory, and story consistency.</p>\n<h3 id=\"best-for_4\">Best For</h3>\n<ul>\n<li><strong>Writers</strong>, <strong>Novelists</strong>, and <strong>RPG players</strong>.</li>\n</ul>\n<h3 id=\"example-workflow_3\">Example Workflow</h3>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># 1. Download the single binary</span>\nwget<span class=\"w\"> </span>https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-mac.zip\n\n<span class=\"c1\"># 2. Run it (launches a web server)</span>\n./koboldcpp<span class=\"w\"> </span>--model<span class=\"w\"> </span>llama-3-8b.gguf<span class=\"w\"> </span>--port<span class=\"w\"> </span><span class=\"m\">5001</span><span class=\"w\"> </span>--smartcontext\n</code></pre></div>\n<h3 id=\"pros-cons_4\">Pros &amp; Cons</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">\u2705 Pros</th>\n<th style=\"text-align: left;\">\u274c Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">Best-in-class tools for creative writing</td>\n<td style=\"text-align: left;\">Niche UI (not great for coding/chat)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Single file executable (no installation)</td>\n<td style=\"text-align: left;\">AGPL license (restrictive for commercial use)</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"honorable-mention-mlx-lm\">Honorable Mention: MLX-LM</h2>\n<p>If you are a Python developer specifically targeting Apple Silicon, check out <strong><a href=\"https://github.com/ml-explore/mlx-examples/tree/main/llms\">MLX-LM</a></strong> by Apple. It's a framework optimized specifically for the M-series chips. While less \"user-friendly\" than Ollama, it's often the fastest way to run models if you're comfortable with Python.</p>\n<hr />\n<h2 id=\"summary-which-tool-is-right-for-you\">Summary: Which Tool is Right for You?</h2>\n<p>Here is a quick decision tree to help you decide:</p>\n<p><img alt=\"Decision Tree\" src=\"../../../../assets/2025-05-10-guide-llm-macos-tools/tool_decision_tree.svg\" /></p>\n<h3 id=\"quick-comparison-table\">Quick Comparison Table</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Tool</th>\n<th style=\"text-align: left;\">Interface</th>\n<th style=\"text-align: left;\">Difficulty</th>\n<th style=\"text-align: left;\">Best Feature</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>Ollama</strong></td>\n<td style=\"text-align: left;\">CLI / Menu Bar</td>\n<td style=\"text-align: left;\">\u2b50 (Easy)</td>\n<td style=\"text-align: left;\">\"Just Works\" experience</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>LM Studio</strong></td>\n<td style=\"text-align: left;\">GUI</td>\n<td style=\"text-align: left;\">\u2b50 (Easy)</td>\n<td style=\"text-align: left;\">Model discovery &amp; UI</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>GPT4All</strong></td>\n<td style=\"text-align: left;\">GUI</td>\n<td style=\"text-align: left;\">\u2b50 (Easy)</td>\n<td style=\"text-align: left;\">Chat with local docs (RAG)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>KoboldCPP</strong></td>\n<td style=\"text-align: left;\">Web UI</td>\n<td style=\"text-align: left;\">\u2b50\u2b50 (Medium)</td>\n<td style=\"text-align: left;\">Creative writing tools</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>llama.cpp</strong></td>\n<td style=\"text-align: left;\">CLI</td>\n<td style=\"text-align: left;\">\u2b50\u2b50\u2b50 (Hard)</td>\n<td style=\"text-align: left;\">Raw performance &amp; control</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"final-thoughts\">Final Thoughts</h2>\n<p>You can't really go wrong with any of these. They all run locally, they all respect your privacy, and they all leverage the incredible power of Apple Silicon.</p>\n<ul>\n<li><strong>Start with Ollama</strong> if you just want to see what the fuss is about.</li>\n<li><strong>Try LM Studio</strong> if you want to browse models visually.</li>\n<li><strong>Dive into llama.cpp</strong> if you want to understand how it all works under the hood.</li>\n</ul>", "image": null, "date_modified": "2025-11-26T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["AI Engineering"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/", "url": "https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/", "title": "Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025", "content_html": "<h1 id=\"scaling-large-language-models-practical-multi-gpu-and-multi-node-strategies-for-2025\">Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</h1>\n<p>The race to build bigger, better language models continues at breakneck speed. Today's state-of-the-art models require massive computing resources that no single GPU can handle. Whether you're training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.</p>\n<p>This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face's <a href=\"https://huggingface.co/spaces/nanotron/ultrascale-playbook\">Ultra-Scale Playbook</a>.</p>\n<!-- more -->\n\n<h2 id=\"prerequisites\">Prerequisites</h2>\n<p>Before diving in, you should be familiar with:</p>\n<ul>\n<li><strong>Basic Deep Learning</strong>: Backpropagation, gradients, and optimizers (AdamW).</li>\n<li><strong>Transformer Architecture</strong>: Attention mechanisms, Feed-Forward Networks (FFN).</li>\n<li><strong>PyTorch Basics</strong>: <code>nn.Module</code>, <code>DataLoader</code>, and the training loop.</li>\n</ul>\n<h2 id=\"why-scaling-matters\">Why Scaling Matters</h2>\n<p>Modern LLMs have outgrown single GPUs. Here's why scaling is no longer optional:</p>\n<ul>\n<li><strong>Model size</strong>: A 70B parameter model needs ~140GB in FP16 format - that's nearly 2x what an A100 (80GB) can hold</li>\n<li><strong>Training time</strong>: Even with 8 top-tier A100 GPUs, training a 13B model from scratch takes weeks</li>\n<li><strong>Context length</strong>: Long contexts (32k+ tokens) easily exceed single-GPU memory limits</li>\n<li><strong>Inference speed</strong>: For production workloads, distributing inference reduces latency and increases throughput</li>\n</ul>\n<p>The solution? Split the workload across multiple GPUs. Let's explore how.</p>\n<h2 id=\"1-parallelism-techniques-explained-simply\">1. Parallelism Techniques Explained Simply</h2>\n<h3 id=\"11-data-parallelism-dp\">1.1 Data Parallelism (DP)</h3>\n<p><strong>The idea:</strong> Multiple workers with identical instruction manuals (the model), each working on different examples.</p>\n<p><strong>How it works:</strong></p>\n<ol>\n<li>Each GPU gets a complete copy of the model</li>\n<li>Each GPU processes different batches of data</li>\n<li>After computing gradients, all GPUs synchronize by averaging their gradients</li>\n<li>Everyone updates their model copy with the averaged gradients</li>\n</ol>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Your model fits comfortably on a single GPU</li>\n<li>You want to process more data faster</li>\n<li>You need the simplest distributed setup with minimal code changes</li>\n</ul>\n<p><strong>Limitation:</strong> Memory inefficient - every GPU stores the full model, so you're not saving memory, just increasing throughput.</p>\n<p><img alt=\"Data Parallelism\" src=\"../../../../assets/2025-05-04-scaling-llms/data_parallelism.svg\" /></p>\n<p><strong>Tools</strong>: <a href=\"https://pytorch.org/docs/stable/notes/ddp.html\">PyTorch DDP</a>, <a href=\"https://horovod.ai/\">Horovod</a>.</p>\n<h3 id=\"12-fully-sharded-data-parallelism-fsdp\">1.2 Fully Sharded Data Parallelism (FSDP)</h3>\n<p><strong>The idea:</strong> Like Data Parallelism, but memory-efficient. Each worker keeps only part of the instruction manual and borrows pages from colleagues when needed.</p>\n<p><strong>How it works:</strong></p>\n<ol>\n<li>Model parameters, gradients, and optimizer states are <strong>sharded</strong> (split) across all GPUs</li>\n<li>During forward pass: each GPU gathers the parameters it needs from other GPUs</li>\n<li>After using them, it discards those borrowed parameters to save memory</li>\n<li>During backward pass: same gathering happens for gradient computation</li>\n<li>After backward pass: gradients are reduced and each GPU updates only its own parameter shard</li>\n</ol>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Your model is too large for a single GPU (typically &gt;10B parameters)</li>\n<li>You want to train bigger models without changing your code much</li>\n<li>You're working on a single machine with multiple GPUs</li>\n</ul>\n<p><strong>Real-world impact:</strong> FSDP lets you train models 4-8x larger than what fits on one GPU.</p>\n<p><img alt=\"FSDP\" src=\"../../../../assets/2025-05-04-scaling-llms/fsdp.svg\" /></p>\n<blockquote>\n<p>[!NOTE] &gt; <strong>Understanding ZeRO Stages</strong>\nFSDP is often described in terms of \"ZeRO stages\" (Zero Redundancy Optimizer):</p>\n<ul>\n<li><strong>Stage 1</strong>: Shard optimizer states only (4x memory savings).</li>\n<li><strong>Stage 2</strong>: Shard gradients + optimizer states (8x memory savings).</li>\n<li><strong>Stage 3</strong>: Shard parameters + gradients + optimizer states (Linear memory savings with N GPUs).</li>\n</ul>\n<p>PyTorch FSDP defaults to Stage 3 behavior.</p>\n</blockquote>\n<h4 id=\"example-enabling-fsdp-in-pytorch\">Example: Enabling FSDP in PyTorch</h4>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">torch.distributed.fsdp</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FullyShardedDataParallel</span> <span class=\"k\">as</span> <span class=\"n\">FSDP</span>\n\n<span class=\"c1\"># 1. Wrap your model</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MyLLM</span><span class=\"p\">()</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">FSDP</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 2. Train as usual</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n<span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n<span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n<span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre></div>\n<p><strong>Tools</strong>: <a href=\"https://pytorch.org/docs/stable/fsdp.html\">PyTorch FSDP</a>, <a href=\"https://www.deepspeed.ai/tutorials/zero/\">DeepSpeed ZeRO-3</a>.</p>\n<h3 id=\"13-tensor-parallelism-tp\">1.3 Tensor Parallelism (TP)</h3>\n<p><strong>The idea:</strong> Split individual layers across GPUs - like dividing a massive spreadsheet calculation where each person computes a few columns.</p>\n<p><strong>How it works:</strong></p>\n<ol>\n<li>Take a single layer's weight matrix and split it into chunks</li>\n<li>Each GPU gets one chunk and computes its portion of the output</li>\n<li>Results are combined (via all-reduce or concatenation) before passing to the next layer</li>\n<li>This happens at <strong>every</strong> layer in the model</li>\n</ol>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Individual layers are too large even with FSDP (e.g., huge attention or FFN layers)</li>\n<li>You have fast GPU-to-GPU connections (NVLink/NVSwitch)</li>\n<li>You're working within a single node (TP doesn't scale well across nodes due to communication overhead)</li>\n</ul>\n<p><strong>Sweet spot:</strong> TP degree of 2-8 within a single machine with NVLink.</p>\n<p><img alt=\"Tensor Parallelism\" src=\"../../../../assets/2025-05-04-scaling-llms/tensor_parallelism.svg\" /></p>\n<p><strong>Tools</strong>: <a href=\"https://github.com/NVIDIA/Megatron-LM\">Megatron-LM</a>, <a href=\"https://github.com/NVIDIA/TensorRT-LLM\">TensorRT-LLM</a>, <a href=\"https://github.com/hpcaitech/ColossalAI\">ColossalAI</a>.</p>\n<h3 id=\"14-pipeline-parallelism-pp\">1.4 Pipeline Parallelism (PP)</h3>\n<p><strong>The idea:</strong> Split the model vertically by layers - like an assembly line where each station handles specific layers.</p>\n<p><strong>How it works:</strong></p>\n<ol>\n<li>Divide your model into stages (e.g., layers 1-10, 11-20, 21-30)</li>\n<li>Assign each stage to a different GPU</li>\n<li>Send micro-batches through the pipeline: GPU 1 processes batch 1, sends output to GPU 2, then starts on batch 2</li>\n<li>Multiple micro-batches flow through simultaneously to keep all GPUs busy</li>\n</ol>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Very deep models that don't fit on available GPUs even with FSDP</li>\n<li>Multi-node training where inter-node bandwidth is limited</li>\n<li>Combined with TP and FSDP for massive models</li>\n</ul>\n<p><strong>Challenge:</strong> Pipeline \"bubbles\" (idle time) at the start and end of each batch. Use multiple micro-batches to minimize this.</p>\n<p><img alt=\"Pipeline Parallelism\" src=\"../../../../assets/2025-05-04-scaling-llms/pipeline_parallelism.svg\" /></p>\n<p><strong>Tools</strong>: <a href=\"https://www.deepspeed.ai/tutorials/pipeline/\">DeepSpeed PP</a>, <a href=\"https://github.com/NVIDIA/Megatron-LM\">Megatron-LM</a>, <a href=\"https://arxiv.org/abs/1811.06965\">GPipe</a>.</p>\n<h3 id=\"15-context-parallelism-cp\">1.5 Context Parallelism (CP)</h3>\n<p><strong>The idea:</strong> For handling extremely long sequences - different people read different paragraphs of a book, then share key information.</p>\n<p><strong>How it works:</strong></p>\n<ol>\n<li>Split a long sequence (e.g., 64K tokens) across multiple GPUs (e.g., 4 GPUs \u00d7 16K tokens each)</li>\n<li>Each GPU runs self-attention on its local chunk</li>\n<li>GPUs exchange keys and values to compute cross-attention (how tokens in one chunk relate to tokens in other chunks)</li>\n<li>Results are merged to produce the final output</li>\n</ol>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>Processing very long contexts (64K, 128K, or even 1M+ tokens)</li>\n<li>Document analysis, long-form code generation, or book-length reasoning</li>\n<li>When context length is the bottleneck, not model size</li>\n</ul>\n<p><strong>Real-world impact:</strong> Context Parallelism enables 100K+ token processing on consumer hardware that would otherwise max out at 8K tokens.</p>\n<p><img alt=\"Context Parallelism\" src=\"../../../../assets/2025-05-04-scaling-llms/context_parallelism.svg\" /></p>\n<p><strong>Tools</strong>: <a href=\"https://github.com/huggingface/picotron\">Picotron</a>, <a href=\"https://github.com/huggingface/nanotron\">Nanotron</a>.</p>\n<h3 id=\"16-expert-parallelism-mixture-of-experts-moe\">1.6 Expert Parallelism (Mixture of Experts - MoE)</h3>\n<p><strong>The idea:</strong> Specialized consultants - instead of activating the entire model for every input, each token gets routed only to the \"experts\" it needs.</p>\n<p><strong>How it works:</strong></p>\n<ol>\n<li>Replace dense feed-forward layers with multiple \"expert\" networks (e.g., 8 or 64 experts)</li>\n<li>A gating network decides which experts (usually top-2) should process each token</li>\n<li>Only those selected experts activate for that token</li>\n<li>Different experts can live on different GPUs</li>\n</ol>\n<p><strong>When to use it:</strong></p>\n<ul>\n<li>You want a model with 100B+ total parameters but only want to activate 13B per token</li>\n<li>You need better parameter efficiency than dense models</li>\n<li>You're okay with more complex training dynamics</li>\n</ul>\n<p><strong>Real-world examples:</strong> Mixtral-8x7B (56B total params, 13B active), Grok, DeepSeek-V2.</p>\n<p><strong>Trade-off:</strong> More parameters with less compute per token, but training can be trickier due to load balancing between experts.</p>\n<p><img alt=\"Mixture of Experts\" src=\"../../../../assets/2025-05-04-scaling-llms/moe.svg\" /></p>\n<p><strong>Tools</strong>: <a href=\"https://github.com/huggingface/picotron\">Picotron</a>, <a href=\"https://github.com/huggingface/nanotron\">Nanotron</a>.</p>\n<h3 id=\"quick-comparison-which-parallelism-should-you-use\">Quick Comparison: Which Parallelism Should You Use?</h3>\n<table>\n<thead>\n<tr>\n<th>Technique</th>\n<th>What It Splits</th>\n<th>Best For</th>\n<th>Memory Savings</th>\n<th>Communication Cost</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Data Parallelism (DP)</strong></td>\n<td>Data batches</td>\n<td>Models that fit on 1 GPU</td>\n<td>None (copies model)</td>\n<td>Low (only gradients)</td>\n</tr>\n<tr>\n<td><strong>FSDP</strong></td>\n<td>Model + optimizer + gradients</td>\n<td>Models too big for 1 GPU</td>\n<td>High (4-8x)</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Tensor Parallelism (TP)</strong></td>\n<td>Individual layers</td>\n<td>Huge layers, fast GPUs</td>\n<td>Medium</td>\n<td>High (per layer)</td>\n</tr>\n<tr>\n<td><strong>Pipeline Parallelism (PP)</strong></td>\n<td>Layer groups (stages)</td>\n<td>Very deep models</td>\n<td>Medium</td>\n<td>Low (between stages)</td>\n</tr>\n<tr>\n<td><strong>Context Parallelism (CP)</strong></td>\n<td>Sequence length</td>\n<td>Long contexts (64K+ tokens)</td>\n<td>High (for activations)</td>\n<td>Medium</td>\n</tr>\n<tr>\n<td><strong>Expert Parallelism (MoE)</strong></td>\n<td>Experts in MoE layers</td>\n<td>Massive sparse models</td>\n<td>None (more params, less FLOPs)</td>\n<td>Medium</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Rule of thumb:</strong> Start with FSDP. Add TP if individual layers are too big. Add PP if you need multiple nodes. Add CP if context length is your bottleneck.</p>\n<h2 id=\"2-practical-training-strategies\">2. Practical Training Strategies</h2>\n<p>Now that you understand the techniques, here's what to actually do based on your hardware setup.</p>\n<h3 id=\"21-single-machine-2-8-gpus\">2.1 Single Machine (2-8 GPUs)</h3>\n<p><strong>Recommended approach:</strong> FSDP, optionally + TP</p>\n<p><strong>What to do:</strong></p>\n<ol>\n<li>Start with pure FSDP using PyTorch FSDP or DeepSpeed ZeRO-2/ZeRO-3</li>\n<li>If your model has huge attention or FFN layers that still don't fit, add TP=2</li>\n<li>Use Hugging Face <code>accelerate</code> or PyTorch <code>torchrun</code> for easy setup</li>\n</ol>\n<p><strong>Hardware-specific tips:</strong></p>\n<ul>\n<li>Consumer GPUs (RTX 4090, etc.) with PCIe: Stick to TP=1 or TP=2 max</li>\n<li>Server GPUs (A100, H100) with NVLink: You can efficiently use TP=2 to TP=4</li>\n<li>8 GPUs in one box: FSDP alone often works great for models up to 70B</li>\n</ul>\n<h3 id=\"22-small-cluster-2-16-nodes-128-gpus\">2.2 Small Cluster (2-16 nodes, \u2264128 GPUs)</h3>\n<p><strong>Recommended approach:</strong> 2D or 3D parallelism (TP + FSDP, optionally + PP)</p>\n<p><strong>What to do:</strong></p>\n<ol>\n<li>Use TP within each node (e.g., TP=4 or TP=8 per node with NVLink)</li>\n<li>Use FSDP across nodes for data parallelism</li>\n<li>If your model is extremely deep, add PP to split it vertically across nodes</li>\n</ol>\n<p><strong>Why this works:</strong></p>\n<ul>\n<li>Fast intra-node connections (NVLink) handle TP's high communication needs</li>\n<li>Slower inter-node connections (InfiniBand) only need to sync FSDP shards</li>\n<li>Minimizes cross-node bandwidth requirements</li>\n</ul>\n<p><strong>Pro tip:</strong> When using Pipeline Parallelism, set your number of micro-batches to at least 4\u00d7 your pipeline degree to keep GPUs busy and minimize \"bubbles.\"</p>\n<h3 id=\"23-large-cluster-hundreds-or-thousands-of-gpus\">2.3 Large Cluster (Hundreds or Thousands of GPUs)</h3>\n<p><strong>Recommended approach:</strong> 4D parallelism (DP \u00d7 TP \u00d7 PP \u00d7 CP)</p>\n<p><strong>What to do:</strong></p>\n<ol>\n<li>Combine all four parallelism strategies to handle the largest models</li>\n<li>Carefully map parallelism strategies to your hardware topology</li>\n<li>Use tools like Megatron-LM or Nanotron that support 4D parallelism out of the box</li>\n</ol>\n<p><strong>When you need this:</strong></p>\n<ul>\n<li>Training models with 70B+ parameters and 32K+ context windows</li>\n<li>Pretraining from scratch (not fine-tuning)</li>\n<li>Production-scale model training at big labs</li>\n</ul>\n<p><strong>Performance expectations:</strong></p>\n<ul>\n<li>With good InfiniBand networking: ~70-80% scaling efficiency</li>\n<li>With excellent setup and tuning: ~85% scaling efficiency possible</li>\n</ul>\n<p><strong>Real-world example:</strong> Training a 70B model with 32K context on 512 GPUs:</p>\n<ul>\n<li>TP=8 (within each 8-GPU node)</li>\n<li>PP=4 (pipeline across 4 nodes)</li>\n<li>CP=4 (split context across 4 chunks)</li>\n<li>DP=4 (data parallelism for throughput)</li>\n<li>Total: 8 \u00d7 4 \u00d7 4 \u00d7 4 = 512 GPUs</li>\n</ul>\n<h2 id=\"3-practical-tools-worth-learning\">3. Practical Tools Worth Learning</h2>\n<p>Here's a quick guide to the most useful tools and when to reach for them:</p>\n<table>\n<thead>\n<tr>\n<th>Tool</th>\n<th>When to Use It</th>\n<th>Learning Curve</th>\n<th>Best For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Hugging Face Accelerate</strong></td>\n<td>Any distributed training with minimal code changes</td>\n<td>\u2605\u2606\u2606\u2606\u2606</td>\n<td>Beginners, quick prototypes</td>\n</tr>\n<tr>\n<td><strong>PyTorch FSDP</strong></td>\n<td>Medium-large models (1-30B) on single node</td>\n<td>\u2605\u2605\u2606\u2606\u2606</td>\n<td>Most common use case</td>\n</tr>\n<tr>\n<td><strong>DeepSpeed ZeRO</strong></td>\n<td>Multi-node training with good documentation</td>\n<td>\u2605\u2605\u2605\u2606\u2606</td>\n<td>Production training</td>\n</tr>\n<tr>\n<td><strong>Megatron-LM</strong></td>\n<td>Very large models (70B+), 3D/4D parallelism</td>\n<td>\u2605\u2605\u2605\u2605\u2606</td>\n<td>Advanced/production at scale</td>\n</tr>\n<tr>\n<td><strong>Nanotron</strong></td>\n<td>Learning/research on modern parallelism strategies</td>\n<td>\u2605\u2605\u2605\u2606\u2606</td>\n<td>Education, experimentation</td>\n</tr>\n<tr>\n<td><strong>vLLM</strong></td>\n<td>Fast inference with PagedAttention and KV caching</td>\n<td>\u2605\u2605\u2606\u2606\u2606</td>\n<td>Serving models in production</td>\n</tr>\n<tr>\n<td><strong>TensorRT-LLM</strong></td>\n<td>Maximum inference speed on NVIDIA GPUs</td>\n<td>\u2605\u2605\u2605\u2605\u2606</td>\n<td>Production inference optimization</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"example-accelerate-config-for-fsdp\">Example: Accelerate Config for FSDP</h4>\n<p>To get started with FSDP using Hugging Face Accelerate, you can run <code>accelerate config</code> or create a <code>config.yaml</code> like this:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"nt\">compute_environment</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">LOCAL_MACHINE</span>\n<span class=\"nt\">distributed_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">FSDP</span>\n<span class=\"nt\">fsdp_config</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"nt\">fsdp_auto_wrap_policy</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">TRANSFORMER_BASED_WRAP</span>\n<span class=\"w\">    </span><span class=\"nt\">fsdp_backward_prefetch</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">BACKWARD_PRE</span>\n<span class=\"w\">    </span><span class=\"nt\">fsdp_state_dict_type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">SHARDED_STATE_DICT</span>\n<span class=\"nt\">machine_rank</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0</span>\n<span class=\"nt\">main_process_ip</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n<span class=\"nt\">main_process_port</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n<span class=\"nt\">main_training_function</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">main</span>\n<span class=\"nt\">mixed_precision</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">bf16</span>\n<span class=\"nt\">num_machines</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n<span class=\"nt\">num_processes</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">8</span>\n<span class=\"nt\">use_cpu</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">false</span>\n</code></pre></div>\n<p><strong>My recommendation for getting started:</strong> Start with Hugging Face Accelerate for learning, then graduate to PyTorch FSDP or DeepSpeed when you need more control.</p>\n<h2 id=\"4-making-the-right-choice-a-decision-framework\">4. Making the Right Choice: A Decision Framework</h2>\n<p>Still not sure what to use? Follow this decision tree:</p>\n<p><strong>Step 1: Does your model fit on a single GPU?</strong></p>\n<ul>\n<li>\u2705 <strong>Yes</strong> \u2192 Use standard training (no parallelism needed)</li>\n<li>\u274c <strong>No</strong> \u2192 Continue to Step 2</li>\n</ul>\n<p><strong>Step 2: Do you have multiple GPUs in one machine?</strong></p>\n<ul>\n<li>\u2705 <strong>Yes</strong> \u2192 Start with FSDP</li>\n<li>\u274c <strong>No</strong> \u2192 You'll need a cluster or smaller model (skip to Step 4)</li>\n</ul>\n<p><strong>Step 3: Is FSDP alone enough?</strong></p>\n<ul>\n<li>\u2705 <strong>Yes</strong> \u2192 You're done! Use pure FSDP</li>\n<li>\u274c <strong>No, individual layers are too big</strong> \u2192 Add TP=2 or TP=4</li>\n<li>\u274c <strong>No, context is too long</strong> \u2192 Add CP</li>\n</ul>\n<p><strong>Step 4: Training across multiple nodes?</strong></p>\n<ul>\n<li>Start with: TP within nodes + FSDP across nodes</li>\n<li>If model is very deep: Add PP to split layers across nodes</li>\n<li>If you have 100+ GPUs and long contexts: Consider 4D parallelism (TP + PP + DP + CP)</li>\n</ul>\n<p><strong>Visual decision tree:</strong></p>\n<p><img alt=\"Scaling Decision Tree\" src=\"../../../../assets/2025-05-04-scaling-llms/scaling_decision_tree.svg\" /></p>\n<h2 id=\"5-the-ultra-scale-cheatsheet\">5. The Ultra-Scale Cheatsheet</h2>\n<p>For a comprehensive visual summary, check out this guide from Hugging Face's team:</p>\n<p><img alt=\"Ultra-Scale LLM Cheatsheet\" src=\"https://nanotron-ultrascale-playbook.static.hf.space/assets/images/ultra-cheatsheet.svg\" /></p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Scaling LLMs is both an art and a science. The key takeaways:</p>\n<ol>\n<li><strong>Start simple:</strong> Most people should begin with FSDP. It handles the majority of use cases.</li>\n<li><strong>Add complexity only when needed:</strong> Don't jump straight to 4D parallelism unless you're training at massive scale.</li>\n<li><strong>Match strategy to hardware:</strong> TP works best within nodes, FSDP across nodes, PP for extreme depth.</li>\n<li><strong>Tools matter:</strong> Use Accelerate to learn, FSDP or DeepSpeed for production.</li>\n</ol>\n<p>The techniques here follow logical patterns based on hardware constraints and model architecture. With the right approach, you can scale from a single GPU to thousands, training models that would have been impossible just a few years ago.</p>\n<p><strong>Further resources:</strong></p>\n<ul>\n<li><a href=\"https://huggingface.co/spaces/nanotron/ultrascale-playbook\">Hugging Face Ultra-Scale Playbook</a> - Interactive guide with more details</li>\n<li><a href=\"https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html\">PyTorch FSDP Tutorial</a> - Official getting started guide</li>\n<li><a href=\"https://www.deepspeed.ai/tutorials/\">DeepSpeed Tutorials</a> - Comprehensive DeepSpeed documentation</li>\n</ul>", "image": null, "date_modified": "2025-11-24T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["AI Engineering"]}, {"id": "https://slavadubrov.github.io/blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/", "url": "https://slavadubrov.github.io/blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/", "title": "Quick Guide: Managing Python on macOS with uv", "content_html": "<h2 id=\"quick-start\">Quick Start</h2>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Install uv</span>\nbrew<span class=\"w\"> </span>install<span class=\"w\"> </span>uv\n\n<span class=\"c1\"># For new projects (modern workflow)</span>\nuv<span class=\"w\"> </span>init<span class=\"w\">                </span><span class=\"c1\"># create project structure</span>\nuv<span class=\"w\"> </span>add<span class=\"w\"> </span>pandas<span class=\"w\"> </span>numpy<span class=\"w\">    </span><span class=\"c1\"># add dependencies</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>train.py<span class=\"w\">        </span><span class=\"c1\"># run your script</span>\n\n<span class=\"c1\"># For existing projects (legacy workflow)</span>\nuv<span class=\"w\"> </span>venv<span class=\"w\">                             </span><span class=\"c1\"># create virtual environment</span>\nuv<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-r<span class=\"w\"> </span>requirements.txt<span class=\"w\">  </span><span class=\"c1\"># install dependencies</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>train.py<span class=\"w\">                     </span><span class=\"c1\"># run your script</span>\n\n<span class=\"c1\"># Run tools without installing them</span>\nuvx<span class=\"w\"> </span>ruff<span class=\"w\"> </span>check<span class=\"w\"> </span>.<span class=\"w\">       </span><span class=\"c1\"># run linter</span>\nuvx<span class=\"w\"> </span>black<span class=\"w\"> </span>.<span class=\"w\">            </span><span class=\"c1\"># run formatter</span>\n</code></pre></div>\n<!-- more -->\n\n<h2 id=\"why-uv\">Why uv?</h2>\n<p>If you've been using Python for a while, you're likely familiar with the \"tool fatigue\" of managing <code>pip</code>, <code>virtualenv</code>, <code>pip-tools</code>, <code>pyenv</code>, and <code>poetry</code>.</p>\n<p><strong><code>uv</code> replaces all of them.</strong></p>\n<p>Written in Rust, it is designed to be a drop-in replacement that is <strong>10-100x faster</strong> than existing tools. It unifies your workflow into a single, cohesive experience.</p>\n<p><img alt=\"The uv Ecosystem\" src=\"../../../../assets/2025-04-17-uv-on-macos/uv-ecosystem.svg\" /></p>\n<p>It handles:</p>\n<ul>\n<li><strong>Package management</strong> (replacing <code>pip</code> and <code>pip-tools</code>)</li>\n<li><strong>Python installation</strong> (replacing <code>pyenv</code>)</li>\n<li><strong>Virtual environments</strong> (replacing <code>virtualenv</code> and <code>venv</code>)</li>\n<li><strong>Tool execution</strong> (replacing <code>pipx</code>)</li>\n<li><strong>Project management</strong> (replacing <code>poetry</code> or <code>pdm</code>)</li>\n</ul>\n<hr />\n<h2 id=\"installing-uv\">Installing uv</h2>\n<p>The easiest way to install <code>uv</code> on macOS is via Homebrew:</p>\n<div class=\"highlight\"><pre><span></span><code>brew<span class=\"w\"> </span>install<span class=\"w\"> </span>uv\n</code></pre></div>\n<p><code>uv</code> automatically detects your Mac's architecture (Apple Silicon or Intel), so no extra configuration is needed.</p>\n<p><strong>Keep it updated:</strong></p>\n<div class=\"highlight\"><pre><span></span><code>brew<span class=\"w\"> </span>upgrade<span class=\"w\"> </span>uv\n<span class=\"c1\"># OR</span>\nuv<span class=\"w\"> </span>self<span class=\"w\"> </span>update\n</code></pre></div>\n<hr />\n<h2 id=\"core-concepts\">Core Concepts</h2>\n<p><code>uv</code> simplifies Python development by handling three distinct use cases:</p>\n<ol>\n<li><strong>Projects</strong>: Building an application or library with dependencies.</li>\n<li><strong>Scripts</strong>: Running a single-file Python script with inline dependencies.</li>\n<li><strong>Tools</strong>: Running command-line utilities (like <code>ruff</code> or <code>httpie</code>) globally.</li>\n</ol>\n<h3 id=\"1-modern-project-management\">1. Modern Project Management</h3>\n<p>For new projects, <code>uv</code> uses the standard <code>pyproject.toml</code> for configuration and a cross-platform <code>uv.lock</code> for reproducible builds.</p>\n<p><img alt=\"Modern uv Project Structure\" src=\"../../../../assets/2025-04-17-uv-on-macos/uv-project-structure.svg\" /></p>\n<p><strong>Start a new project:</strong></p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>init<span class=\"w\"> </span>my-project\n<span class=\"nb\">cd</span><span class=\"w\"> </span>my-project\n</code></pre></div>\n<p>This creates a clean project structure with a <code>pyproject.toml</code>, <code>.gitignore</code>, and a <code>hello.py</code>.</p>\n<p><strong>Add dependencies:</strong></p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Add runtime dependencies</span>\nuv<span class=\"w\"> </span>add<span class=\"w\"> </span>pandas<span class=\"w\"> </span>requests\n\n<span class=\"c1\"># Add development dependencies</span>\nuv<span class=\"w\"> </span>add<span class=\"w\"> </span>pytest<span class=\"w\"> </span>ruff<span class=\"w\"> </span>--dev\n</code></pre></div>\n<p><strong>Run your code:</strong></p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>hello.py\n</code></pre></div>\n<p><code>uv</code> automatically manages the virtual environment in <code>.venv</code>. You never need to manually activate it!</p>\n<h3 id=\"2-managing-python-versions\">2. Managing Python Versions</h3>\n<p>Forget <code>pyenv</code>. <code>uv</code> can install and manage Python versions for you, keeping them isolated in <code>~/.cache/uv</code>.</p>\n<p><strong>Install a specific version:</strong></p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>python<span class=\"w\"> </span>install<span class=\"w\"> </span><span class=\"m\">3</span>.12\n</code></pre></div>\n<p><strong>Pin a version for your project:</strong></p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>python<span class=\"w\"> </span>pin<span class=\"w\"> </span><span class=\"m\">3</span>.11\n</code></pre></div>\n<p>This creates a <code>.python-version</code> file. When you run <code>uv run</code>, it will automatically use the pinned version, downloading it if necessary. This ensures your entire team and CI pipeline use the <em>exact same Python version</em>.</p>\n<h3 id=\"3-running-tools-with-uvx\">3. Running Tools with <code>uvx</code></h3>\n<p>Use <code>uvx</code> (an alias for <code>uv tool run</code>) to execute Python command-line tools without polluting your global environment or project dependencies.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Run a linter</span>\nuvx<span class=\"w\"> </span>ruff<span class=\"w\"> </span>check<span class=\"w\"> </span>.\n\n<span class=\"c1\"># Run a formatter</span>\nuvx<span class=\"w\"> </span>black<span class=\"w\"> </span>.\n\n<span class=\"c1\"># Start a temporary Jupyter server</span>\nuvx<span class=\"w\"> </span>--from<span class=\"w\"> </span>jupyterlab<span class=\"w\"> </span>jupyter<span class=\"w\"> </span>lab\n</code></pre></div>\n<p>Each tool runs in its own isolated, temporary environment. It's fast, clean, and safe.</p>\n<hr />\n<h2 id=\"legacy-projects-requirementstxt\">Legacy Projects (requirements.txt)</h2>\n<p>If you have an existing project using <code>requirements.txt</code>, <code>uv</code> works as a drop-in replacement for <code>pip</code> and <code>venv</code>.</p>\n<p><strong>Setup:</strong></p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Create a virtual environment</span>\nuv<span class=\"w\"> </span>venv\n\n<span class=\"c1\"># Install dependencies (lightning fast!)</span>\nuv<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-r<span class=\"w\"> </span>requirements.txt\n</code></pre></div>\n<p><strong>Run:</strong></p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>python<span class=\"w\"> </span>app.py\n</code></pre></div>\n<hr />\n<h2 id=\"performance-notes\">Performance Notes</h2>\n<p>Why is <code>uv</code> so fast?</p>\n<ol>\n<li><strong>Rust</strong>: It's built with performance in mind, without the overhead of Python startup times.</li>\n<li><strong>Global Cache</strong>: It caches built wheels globally. If you've installed <code>numpy</code> in one project, installing it in another is instant (using copy-on-write links on macOS).</li>\n<li><strong>Parallelism</strong>: It downloads and installs packages in parallel, maximizing your bandwidth.</li>\n</ol>\n<hr />\n<h2 id=\"summary\">Summary</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Task</th>\n<th style=\"text-align: left;\">Old Way</th>\n<th style=\"text-align: left;\">The uv Way</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>Install Python</strong></td>\n<td style=\"text-align: left;\"><code>pyenv install 3.12</code></td>\n<td style=\"text-align: left;\"><code>uv python install 3.12</code></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>New Project</strong></td>\n<td style=\"text-align: left;\"><code>mkdir proj &amp;&amp; cd proj &amp;&amp; python -m venv .venv</code></td>\n<td style=\"text-align: left;\"><code>uv init proj</code></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Install Package</strong></td>\n<td style=\"text-align: left;\"><code>pip install pandas &amp;&amp; pip freeze &gt; requirements.txt</code></td>\n<td style=\"text-align: left;\"><code>uv add pandas</code></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Run Script</strong></td>\n<td style=\"text-align: left;\"><code>source .venv/bin/activate &amp;&amp; python script.py</code></td>\n<td style=\"text-align: left;\"><code>uv run script.py</code></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Run Tool</strong></td>\n<td style=\"text-align: left;\"><code>pipx run black</code></td>\n<td style=\"text-align: left;\"><code>uvx black</code></td>\n</tr>\n</tbody>\n</table>\n<p>Switching to <code>uv</code> on macOS is one of the highest-ROI changes you can make to your Python workflow today. It's faster, simpler, and standard-compliant.</p>", "image": null, "date_modified": "2025-11-23T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Python"]}, {"id": "https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/", "url": "https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/", "title": "Quick-Guide on setting up a MacBook for AI Engineering", "content_html": "<h1 id=\"quick-guide-on-setting-up-a-macbook-for-ai-engineering\">Quick-Guide on setting up a MacBook for AI Engineering</h1>\n<p>Setting up a new MacBook for AI development doesn't have to be overwhelming. Here's my streamlined 10-step process to transform a fresh macOS installation into a fully functional AI engineering workstation.</p>\n<!-- more -->\n\n<h2 id=\"1-install-xcode-command-line-tools\">1. Install Xcode Command Line Tools</h2>\n<p>Start by installing the Xcode Command Line Tools. These are essential building blocks for any software development on macOS, including AI and data science work.</p>\n<div class=\"highlight\"><pre><span></span><code>xcode-select<span class=\"w\"> </span>--install\n</code></pre></div>\n<p>This command opens a dialog that walks you through the installation process.</p>\n<h2 id=\"2-install-homebrew\">2. Install Homebrew</h2>\n<p>Next, install <a href=\"https://brew.sh\">Homebrew</a>, the go-to package manager for macOS. It makes installing and managing software incredibly simple. Run this command:</p>\n<div class=\"highlight\"><pre><span></span><code>/bin/bash<span class=\"w\"> </span>-c<span class=\"w\"> </span><span class=\"s2\">&quot;</span><span class=\"k\">$(</span>curl<span class=\"w\"> </span>-fsSL<span class=\"w\"> </span>https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span class=\"k\">)</span><span class=\"s2\">&quot;</span>\n</code></pre></div>\n<p>The installer will guide you through the process and may ask for your password.</p>\n<h2 id=\"3-install-essential-development-tools\">3. Install essential development tools</h2>\n<p>Now let's install the core tools you'll need for AI engineering:</p>\n<div class=\"highlight\"><pre><span></span><code>brew<span class=\"w\"> </span>install<span class=\"w\"> </span>openssl<span class=\"w\"> </span>readline<span class=\"w\"> </span>sqlite3<span class=\"w\"> </span>xz<span class=\"w\"> </span>zlib<span class=\"w\"> </span>pyenv<span class=\"w\"> </span>uv<span class=\"w\"> </span>htop<span class=\"w\"> </span>gitmoji<span class=\"w\"> </span>pandoc<span class=\"w\"> </span>ncdu<span class=\"w\"> </span>tmux\n</code></pre></div>\n<p>Here's what each tool does:</p>\n<p><strong>Python environment:</strong></p>\n<ul>\n<li><a href=\"https://github.com/pyenv/pyenv\">pyenv</a> \u2014 manage multiple Python versions seamlessly</li>\n<li><a href=\"https://github.com/astral-sh/uv\">uv</a> \u2014 fast Python package manager and environment handler</li>\n</ul>\n<p><strong>System libraries:</strong></p>\n<ul>\n<li><a href=\"https://www.openssl.org\">openssl</a> \u2014 SSL/TLS cryptography support</li>\n<li><a href=\"https://tiswww.case.edu/php/chet/readline/rltop.html\">readline</a> \u2014 command-line text editing</li>\n<li><a href=\"https://sqlite.org\">sqlite3</a> \u2014 lightweight embedded database</li>\n<li><a href=\"https://tukaani.org/xz\">xz</a> \u2014 advanced data compression</li>\n<li><a href=\"https://zlib.net\">zlib</a> \u2014 compression library</li>\n</ul>\n<p><strong>Productivity tools:</strong></p>\n<ul>\n<li><a href=\"https://htop.dev\">htop</a> \u2014 visual system monitor and process viewer</li>\n<li><a href=\"https://github.com/tmux/tmux\">tmux</a> \u2014 manage multiple terminal sessions</li>\n<li><a href=\"https://dev.yorhel.nl/ncdu\">ncdu</a> \u2014 analyze disk usage interactively</li>\n<li><a href=\"https://gitmoji.dev\">gitmoji</a> \u2014 add emojis to commit messages</li>\n<li><a href=\"https://pandoc.org\">pandoc</a> \u2014 convert documents between formats</li>\n</ul>\n<blockquote>\n<p><strong>Note:</strong> For more detailed information about using <code>uv</code> for Python development, check out my <a href=\"../../17/quick-guide-managing-python-on-macos-with-uv/\">Quick-Guide on managing Python on macOS with uv</a>.</p>\n</blockquote>\n<h2 id=\"4-choose-your-terminal\">4. Choose your terminal</h2>\n<p>The default macOS Terminal works fine, but I've found better alternatives. I recently switched from <a href=\"https://www.iterm2.com\">iTerm2</a> to <a href=\"https://www.warp.dev/\">Warp</a>. Warp is a modern, Rust-based terminal with built-in AI features that make your workflow smoother.</p>\n<p>You can download Warp from their <a href=\"https://www.warp.dev/\">website</a>.</p>\n<h3 id=\"optional-iterm2-configuration\">Optional: iTerm2 configuration</h3>\n<p>If you prefer the battle-tested iTerm2, here's my recommended setup:</p>\n<p><strong>Enable natural text editing:</strong></p>\n<ol>\n<li>Open Preferences \u2192 Profiles \u2192 Keys \u2192 Key Mappings</li>\n<li>Click the Presets\u2026 dropdown</li>\n<li>Select \"Natural Text Editing\"</li>\n</ol>\n<p><strong>Choose a color theme:</strong></p>\n<ol>\n<li>Browse themes at <a href=\"https://github.com/mbadolato/iTerm2-Color-Schemes\">iTerm2-Color-Schemes</a></li>\n<li>Open Preferences \u2192 Profiles \u2192 Colors \u2192 Color Presets\u2026</li>\n<li>Click Import and select your downloaded theme</li>\n</ol>\n<h2 id=\"5-set-up-zsh-with-oh-my-zsh\">5. Set up Zsh with Oh My Zsh</h2>\n<p>Modern macOS comes with Zsh as the default shell, but we'll enhance it with <a href=\"https://github.com/robbyrussell/oh-my-zsh\">Oh My Zsh</a>, a framework that makes Zsh more powerful and easier to customize:</p>\n<div class=\"highlight\"><pre><span></span><code>brew<span class=\"w\"> </span>install<span class=\"w\"> </span>zsh\nsh<span class=\"w\"> </span>-c<span class=\"w\"> </span><span class=\"s2\">&quot;</span><span class=\"k\">$(</span>curl<span class=\"w\"> </span>-fsSL<span class=\"w\"> </span>https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh<span class=\"k\">)</span><span class=\"s2\">&quot;</span>\n</code></pre></div>\n<p>The Oh My Zsh installer will back up your existing Zsh configuration and set up the new one.</p>\n<h2 id=\"6-add-zsh-plugins-for-superpowers\">6. Add Zsh plugins for superpowers</h2>\n<p>Plugins make your terminal smarter and more productive. Edit your <code>~/.zshrc</code> file to add these plugins:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"nv\">plugins</span><span class=\"o\">=(</span>\n<span class=\"w\">    </span>aws<span class=\"w\"> </span>bgnotify<span class=\"w\"> </span>brew<span class=\"w\"> </span>docker<span class=\"w\"> </span>docker-compose\n<span class=\"w\">    </span>emoji<span class=\"w\"> </span>forklift<span class=\"w\"> </span>gcloud<span class=\"w\"> </span>git<span class=\"w\"> </span><span class=\"nb\">history</span><span class=\"w\"> </span>iterm2\n<span class=\"w\">    </span>keychain<span class=\"w\"> </span>kubectl<span class=\"w\"> </span>macos<span class=\"w\"> </span>pre-commit\n<span class=\"w\">    </span>pyenv<span class=\"w\"> </span>pylint<span class=\"w\"> </span>python<span class=\"w\"> </span>screen<span class=\"w\"> </span>themes\n<span class=\"w\">    </span>tmux<span class=\"w\"> </span>virtualenv<span class=\"w\"> </span>vscode\n<span class=\"w\">    </span>zsh-autosuggestions<span class=\"w\"> </span>zsh-syntax-highlighting\n<span class=\"o\">)</span>\n</code></pre></div>\n<p>You can find detailed descriptions of all plugins in the <a href=\"https://github.com/ohmyzsh/ohmyzsh/wiki/Plugins\">Oh My Zsh plugins wiki</a>.</p>\n<p><strong>Extra installation required:</strong></p>\n<p>The last two plugins need separate installation (but it's quick!):</p>\n<ul>\n<li><a href=\"https://github.com/zsh-users/zsh-autosuggestions\">zsh-autosuggestions</a> \u2014 suggests commands as you type based on your history</li>\n<li><a href=\"https://github.com/zsh-users/zsh-syntax-highlighting\">zsh-syntax-highlighting</a> \u2014 highlights commands in real-time to catch errors</li>\n</ul>\n<p>Follow the installation instructions on their respective GitHub pages.</p>\n<h2 id=\"7-make-your-terminal-beautiful-with-powerlevel10k\">7. Make your terminal beautiful with Powerlevel10k</h2>\n<p><a href=\"https://github.com/romkatv/powerlevel10k\">Powerlevel10k</a> is a gorgeous Zsh theme that displays useful information like your current directory, Git status, Python environment, and more. The best part? It comes with an interactive configuration wizard that walks you through customizing it to your preferences.</p>\n<p>Follow the <a href=\"https://github.com/romkatv/powerlevel10k\">installation instructions</a> on their GitHub page.</p>\n<h3 id=\"font-setup-for-other-editors\">Font setup for other editors</h3>\n<p>If you use VSCode or other editors with integrated terminals, you'll want to use compatible fonts:</p>\n<ol>\n<li>Open your editor's settings</li>\n<li>Search for <code>terminal.integrated.fontFamily</code></li>\n<li>Set it to <code>MesloLGS NF</code> (this font is installed with Powerlevel10k)</li>\n</ol>\n<p>For detailed font setup instructions, check the <a href=\"https://github.com/romkatv/powerlevel10k/blob/master/font.md\">Powerlevel10k font guide</a>.</p>\n<h2 id=\"8-pick-your-code-editor-and-ai-assistant\">8. Pick your code editor and AI assistant</h2>\n<p>For AI engineering, you'll want both a powerful IDE and AI coding assistants. Here's my setup:</p>\n<p><strong>IDE:</strong></p>\n<ul>\n<li><a href=\"https://cursor.sh\">Cursor</a> \u2014 a VSCode fork with native AI pair programming features</li>\n<li><a href=\"https://code.visualstudio.com\">VSCode</a> \u2014 the industry standard with an enormous extension ecosystem</li>\n</ul>\n<p><strong>AI Assistants:</strong></p>\n<ul>\n<li><a href=\"https://openai.com/codex/\">OpenAI Codex</a> \u2014 OpenAI's code generation model for intelligent code completion</li>\n<li><a href=\"https://claude.ai\">Claude</a> \u2014 Anthropic's AI assistant for complex coding tasks and architecture discussions</li>\n</ul>\n<p><strong>My preference:</strong> I use Cursor as my IDE alongside Codex (or Claude Code) running in parallel.</p>\n<h2 id=\"9-additional-developer-tools\">9. Additional developer tools</h2>\n<p>Round out your setup with these applications:</p>\n<ul>\n<li><a href=\"https://github.com/apps/desktop\">GitHub Desktop</a> \u2014 visual Git client for managing repositories</li>\n<li><a href=\"https://www.docker.com\">Docker</a> \u2014 containerization platform (or check out <a href=\"https://spacelift.io/blog/docker-alternatives\">alternatives</a> like Podman)</li>\n<li><a href=\"https://ollama.com\">Ollama</a> or <a href=\"https://lmstudio.ai\">LM Studio</a> \u2014 run large language models locally on your Mac</li>\n</ul>\n<h2 id=\"10-youre-ready-to-build\">10. You're ready to build</h2>\n<p>That's it! You now have a complete AI engineering setup that mirrors what I use daily. This configuration removes the friction between having an idea and building with AI models. From here, you can:</p>\n<ul>\n<li>Start new Python projects with <code>uv</code></li>\n<li>Run local LLMs for development and testing</li>\n<li>Manage your code with Git and GitHub</li>\n<li>Work efficiently in a beautiful, customized terminal</li>\n</ul>", "image": null, "date_modified": "2025-11-04T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/", "url": "https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/", "title": "LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes", "content_html": "<h1 id=\"lorax-playbook-orchestrating-thousands-of-lora-adapters-on-kubernetes\">LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes</h1>\n<p>Serving dozens of fine-tuned large language models used to mean provisioning one GPU per model. <strong>LoRAX (LoRA eXchange)</strong> flips that math on its head: keep a single base model in memory and hot-swap lightweight LoRA adapters per request.</p>\n<p>This guide shows you how LoRAX achieves <strong>near-constant cost per token</strong> regardless of how many fine-tunes you're serving. We'll cover:</p>\n<ul>\n<li><strong>What LoRA is</strong> and why it's a game-changer.</li>\n<li><strong>LoRAX vs. vLLM</strong>: When to use which.</li>\n<li><strong>Kubernetes Deployment</strong>: A production-ready Helm guide.</li>\n<li><strong>API Usage</strong>: REST, Python, and OpenAI-compatible examples.</li>\n</ul>\n<!-- more -->\n\n<h2 id=\"background-what-is-lora\">Background: What is LoRA?</h2>\n<p><strong>Low-Rank Adaptation (LoRA)</strong> is a fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture.</p>\n<p>In simple terms: instead of retraining the entire model (which is slow and produces massive files), LoRA trains a tiny set of \"diffs\" that represent the new knowledge.</p>\n<ul>\n<li><strong>Full Fine-tuning</strong>: Produces a 20GB+ file for a 7B model.</li>\n<li><strong>LoRA Fine-tuning</strong>: Produces a ~100MB adapter file.</li>\n</ul>\n<p>This massive size reduction is what makes dynamic serving possible. You can store thousands of adapters on disk and load them into GPU memory in milliseconds.</p>\n<h2 id=\"the-problem-lorax-solves\">The problem LoRAX solves</h2>\n<p>Traditional multi-model serving is expensive. Each fine-tuned model needs its own GPU memory allocation, which means serving 50 customer-specific models requires 50 separate deployments\u2014or at least 50x the memory. The costs scale linearly with every new variant you add.</p>\n<p>LoRAX is an Apache 2.0 project from <a href=\"https://github.com/predibase/lorax\">Predibase</a> that extends the <a href=\"https://github.com/huggingface/text-generation-inference\">Hugging Face Text Generation Inference server</a> with three critical features: dynamic adapter loading, tiered weight caching, and multi-adapter batching. These let you serve hundreds of tenant-specific LoRA adapters on a single Ampere-class GPU without sacrificing throughput or latency.</p>\n<p>Here's the key insight: LoRA fine-tuning produces small delta weights (adapters) rather than full model copies. LoRAX exploits this by loading just the base model into GPU memory and injecting adapter weights on demand. Unused adapters consume zero VRAM.</p>\n<h2 id=\"how-it-works-four-core-innovations\">How it works: four core innovations</h2>\n<p><strong>1. Dynamic adapter loading</strong></p>\n<p>Adapter weights are injected just-in-time for each request. The base model stays resident in GPU memory while adapters load on the fly without blocking other requests. This means you can catalog thousands of adapters but only pay memory costs for the ones actively serving traffic.</p>\n<p><strong>2. Tiered weight caching</strong></p>\n<p>LoRAX stages adapters across three layers: GPU VRAM for hot adapters, CPU RAM for warm ones, and disk for cold storage. This hierarchy prevents out-of-memory crashes while keeping swap times fast enough that users don't notice the difference.</p>\n<p><strong>3. Continuous multi-adapter batching</strong></p>\n<p>Here's where the magic happens. LoRAX extends continuous batching strategies to work across different adapters in parallel. Requests targeting different fine-tunes can share the same forward pass, keeping the GPU fully utilized. Benchmarks from Predibase show that processing 1M tokens spread across 32 different adapters takes about the same time as 1M tokens on a single model.</p>\n<p><strong>4. Battle-tested foundation</strong></p>\n<p>LoRAX builds on Hugging Face's Text Generation Inference (TGI) server, inheriting production-grade optimizations: FlashAttention 2, paged attention, SGMV kernels for multi-adapter inference, and streaming responses. You get the stability of TGI plus the flexibility of dynamic adapter switching.</p>\n<h3 id=\"the-economics-near-constant-cost-scaling\">The economics: near-constant cost scaling</h3>\n<p>The chart below demonstrates the cost advantage. While traditional dedicated deployments (dark gray) scale linearly\u2014double the models means double the cost\u2014LoRAX (orange) keeps per-token costs nearly flat regardless of how many adapters you serve. Even hosted API fine-tunes from providers like OpenAI (light gray) can't match this efficiency for multi-model scenarios.</p>\n<p><img alt=\"LoRAX cost per million tokens vs number of models\" src=\"https://slavadubrov.github.io/blog/assets/2025-10-22-lorax-serving-guide/lorax-performance.png\" /></p>\n<p><em>Cost per million tokens as the number of fine-tuned models increases. LoRAX maintains near-constant costs through efficient multi-adapter batching, while dedicated deployments scale linearly. Source: <a href=\"https://github.com/predibase/lorax\">LoRAX GitHub</a></em></p>\n<h3 id=\"request-flow-diagram\">Request flow diagram</h3>\n<p><img alt=\"LoRAX Request Flow\" src=\"../../../../assets/2025-10-22-lorax-serving-guide/lorax-request-flow.svg\" /></p>\n<h2 id=\"when-to-use-lorax\">When to use LoRAX</h2>\n<p>LoRAX makes economic and operational sense in specific scenarios. Here's when it shines:</p>\n<p><strong>Multi-tenant SaaS applications</strong></p>\n<p>You're building a platform where each of your 500 customers gets a customized chatbot fine-tuned on their data. Traditional serving would require 500 model deployments. LoRAX serves all 500 from a single GPU by loading the relevant adapter when a customer request arrives.</p>\n<p><strong>Domain-specific expert routers</strong></p>\n<p>Your company maintains specialized LLMs for law, medicine, finance, and engineering. Instead of four separate 13B model deployments, LoRAX runs one base LLaMA 2 13B instance and routes to the appropriate adapter based on the incoming request domain.</p>\n<p><strong>Rapid experimentation and A/B testing</strong></p>\n<p>Testing 10 different fine-tuning approaches in production? With LoRAX you deploy once and switch between variants by changing the <code>adapter_id</code> parameter. No infrastructure changes, no service restarts.</p>\n<p><strong>Resource-constrained or edge deployments</strong></p>\n<p>On-prem installations or edge devices often have limited GPU resources. A single NVIDIA A10G can host a quantized 7B base model plus dozens of task-specific adapters, eliminating the need for one GPU per model.</p>\n<h2 id=\"architecture-memory-hierarchy-and-request-scheduling\">Architecture: memory hierarchy and request scheduling</h2>\n<p>The core of LoRAX is its three-tier memory hierarchy. Understanding this helps you predict performance and plan capacity.</p>\n<p><img alt=\"LoRAX Memory Hierarchy\" src=\"../../../../assets/2025-10-22-lorax-serving-guide/lorax-memory-hierarchy.svg\" /></p>\n<p>LoRAX treats each adapter as a lightweight \"view\" on the shared base model. The scheduler coalesces requests so that serving 32 different adapters can be as fast as serving one\u2014even across a million tokens of throughput. Adapters typically weigh 10-200MB each, compared to multi-gigabyte full models.</p>\n<h2 id=\"deploy-lorax-on-kubernetes\">Deploy LoRAX on Kubernetes</h2>\n<p>LoRAX ships with production-ready Helm charts and Docker images, making Kubernetes deployment straightforward.</p>\n<h3 id=\"prerequisites\">Prerequisites</h3>\n<p>Before you start, ensure you have:</p>\n<ul>\n<li>A Kubernetes cluster with NVIDIA GPUs (Ampere generation or newer: A10, A100, H100)</li>\n<li><a href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\">NVIDIA Container Runtime</a> configured on GPU nodes</li>\n<li><code>kubectl</code> and <code>helm</code> installed locally</li>\n<li>Persistent storage for adapter caches\u2014mount a PersistentVolume to <code>/data</code> in the pod</li>\n</ul>\n<h3 id=\"quick-start-with-the-official-helm-chart\">Quick start with the official Helm chart</h3>\n<p><a href=\"https://helm.sh/\">Helm</a> is the package manager for Kubernetes\u2014it simplifies deploying applications by bundling all the necessary Kubernetes resources (Deployments, Services, ConfigMaps, etc.) into a single \"chart.\" Instead of writing and managing dozens of YAML files manually, you can deploy complex applications with a single command.</p>\n<p>Predibase retired their public Helm repository in late 2024, so the supported workflow is to clone the LoRAX repository and install the chart from disk. Run these commands from your workstation:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Clone the LoRAX repository and switch into it</span>\ngit<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/predibase/lorax.git\n<span class=\"nb\">cd</span><span class=\"w\"> </span>lorax\n\n<span class=\"c1\"># Make sure kubectl can talk to your cluster</span>\nkubectl<span class=\"w\"> </span>config<span class=\"w\"> </span>current-context\nkubectl<span class=\"w\"> </span>get<span class=\"w\"> </span>nodes\n\n<span class=\"c1\"># Build chart dependencies (generates charts/lorax/charts/*.tgz)</span>\nhelm<span class=\"w\"> </span>dependency<span class=\"w\"> </span>update<span class=\"w\"> </span>charts/lorax\n\n<span class=\"c1\"># Optional: render manifests locally to verify everything is templating</span>\nhelm<span class=\"w\"> </span>template<span class=\"w\"> </span>mistral-7b-release<span class=\"w\"> </span>charts/lorax<span class=\"w\"> </span>&gt;<span class=\"w\"> </span>/tmp/lorax-rendered.yaml\n\n<span class=\"c1\"># Deploy with default settings (Mistral-7B-Instruct)</span>\nhelm<span class=\"w\"> </span>upgrade<span class=\"w\"> </span>--install<span class=\"w\"> </span>mistral-7b-release<span class=\"w\"> </span>charts/lorax\n\n<span class=\"c1\"># Watch the pod come up</span>\nkubectl<span class=\"w\"> </span>get<span class=\"w\"> </span>pods<span class=\"w\"> </span>-w\n\n<span class=\"c1\"># Check logs to see model loading progress</span>\nkubectl<span class=\"w\"> </span>logs<span class=\"w\"> </span>-f<span class=\"w\"> </span>deploy/mistral-7b-release-lorax\n</code></pre></div>\n<p>The chart creates a Deployment (one replica by default) and a ClusterIP Service listening on port 80. The first startup downloads the base model from Hugging Face and loads it into GPU memory\u2014this can take a few minutes depending on your network and GPU. Subsequent restarts reuse the cached weights from the persistent volume.</p>\n<blockquote>\n<p><strong>Tip:</strong> If <code>helm upgrade --install</code> returns <code>Kubernetes cluster unreachable</code>, your current kubeconfig context points at a cluster that is offline. Start your local cluster (e.g., Docker Desktop, kind, minikube) or switch to a reachable context with <code>kubectl config use-context</code>. Running <code>kubectl get nodes</code> before deploying helps confirm the API server is available.</p>\n</blockquote>\n<h3 id=\"customize-the-base-model-and-scaling\">Customize the base model and scaling</h3>\n<p>You can swap in a different base model or adjust resources by creating a custom values file. Here's an example <code>llama2-values.yaml</code>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Use LLaMA 2 7B Chat instead of Mistral</span>\n<span class=\"nt\">modelId</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">meta-llama/Llama-2-7b-chat-hf</span>\n\n<span class=\"c1\"># Enable 4-bit quantization to save VRAM</span>\n<span class=\"nt\">modelArgs</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"nt\">quantization</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">&quot;bitsandbytes&quot;</span>\n\n<span class=\"c1\"># Scale to 2 replicas for high availability</span>\n<span class=\"nt\">replicaCount</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">2</span>\n\n<span class=\"c1\"># Request exactly 1 GPU per pod</span>\n<span class=\"nt\">resources</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"nt\">limits</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"nt\">nvidia.com/gpu</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span>\n</code></pre></div>\n<p>Deploy with your custom configuration:</p>\n<div class=\"highlight\"><pre><span></span><code>helm<span class=\"w\"> </span>upgrade<span class=\"w\"> </span>--install<span class=\"w\"> </span>-f<span class=\"w\"> </span>llama2-values.yaml<span class=\"w\"> </span>llama2-chat-release<span class=\"w\"> </span>charts/lorax\n</code></pre></div>\n<p>Run those commands from the cloned <code>lorax/</code> repository so Helm can locate the chart directory.</p>\n<p>LoRAX supports popular open-source models out of the box: LLaMA 2, CodeLlama, Mistral, Mixtral, Qwen, and others. Check the <a href=\"https://github.com/predibase/lorax\">model compatibility list</a> for the latest additions.</p>\n<p><strong>Exposing the service</strong></p>\n<p>The default Service type is ClusterIP, which only allows access within the cluster. For external traffic, either:</p>\n<ul>\n<li>Create a LoadBalancer Service (on cloud providers)</li>\n<li>Set up an Ingress with TLS termination</li>\n<li>Place an API gateway in front for authentication and rate limiting</li>\n</ul>\n<p><strong>Cleanup</strong></p>\n<p>When you're done testing, free up the GPU resources:</p>\n<div class=\"highlight\"><pre><span></span><code>helm<span class=\"w\"> </span>uninstall<span class=\"w\"> </span>mistral-7b-release\n</code></pre></div>\n<p>This removes the Deployment, Service, and all pods. Cached model weights remain in the PersistentVolume unless you delete that separately.</p>\n<h2 id=\"working-with-the-lorax-apis\">Working with the LoRAX APIs</h2>\n<p>Once deployed, LoRAX exposes three ways to interact with it: a REST API compatible with Hugging Face TGI, a Python client library, and an OpenAI-compatible endpoint. All three methods support dynamic adapter switching.</p>\n<h3 id=\"rest-api\">REST API</h3>\n<p>The <code>/generate</code> endpoint accepts JSON payloads with your prompt and optional parameters. Using the base model without any adapter:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Basic request to the base model (no adapter)</span>\ncurl<span class=\"w\"> </span>-X<span class=\"w\"> </span>POST<span class=\"w\"> </span>http://localhost:8080/generate<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>-H<span class=\"w\"> </span><span class=\"s2\">&quot;Content-Type: application/json&quot;</span><span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>-d<span class=\"w\"> </span><span class=\"s1\">&#39;{</span>\n<span class=\"s1\">    &quot;inputs&quot;: &quot;Write a short poem about the sea.&quot;,</span>\n<span class=\"s1\">    &quot;parameters&quot;: {</span>\n<span class=\"s1\">      &quot;max_new_tokens&quot;: 64,</span>\n<span class=\"s1\">      &quot;temperature&quot;: 0.7</span>\n<span class=\"s1\">    }</span>\n<span class=\"s1\">  }&#39;</span>\n</code></pre></div>\n<p>The response includes the generated text and metadata like token counts and timing information.</p>\n<p><strong>Loading a specific adapter</strong></p>\n<p>Add an <code>adapter_id</code> parameter to target a fine-tuned model. Here's an example using a math-specialized adapter:</p>\n<div class=\"highlight\"><pre><span></span><code>curl<span class=\"w\"> </span>-X<span class=\"w\"> </span>POST<span class=\"w\"> </span>http://localhost:8080/generate<span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>-H<span class=\"w\"> </span><span class=\"s2\">&quot;Content-Type: application/json&quot;</span><span class=\"w\"> </span><span class=\"se\">\\</span>\n<span class=\"w\">  </span>-d<span class=\"w\"> </span><span class=\"s1\">&#39;{</span>\n<span class=\"s1\">    &quot;inputs&quot;: &quot;Natalia sold 48 clips in April, and then half as many in May. How many clips did she sell in total?&quot;,</span>\n<span class=\"s1\">    &quot;parameters&quot;: {</span>\n<span class=\"s1\">      &quot;max_new_tokens&quot;: 64,</span>\n<span class=\"s1\">      &quot;adapter_id&quot;: &quot;vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k&quot;</span>\n<span class=\"s1\">    }</span>\n<span class=\"s1\">  }&#39;</span>\n</code></pre></div>\n<p>On the first call with a new <code>adapter_id</code>, LoRAX downloads the adapter from Hugging Face Hub and caches it under <code>/data</code>. Subsequent requests use the cached version. You can also load adapters from local paths by specifying <code>\"adapter_source\": \"local\"</code> alongside a file path.</p>\n<h3 id=\"python-client\">Python client</h3>\n<p>For programmatic access, install the <code>lorax-client</code> package:</p>\n<div class=\"highlight\"><pre><span></span><code>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>lorax-client\n</code></pre></div>\n<p>The client wraps the REST API with a clean interface:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">lorax</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Client</span>\n\n<span class=\"c1\"># Connect to your LoRAX instance (default port 8080)</span>\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"s2\">&quot;http://localhost:8080&quot;</span><span class=\"p\">)</span>\n\n<span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;Explain the significance of the moon landing in 1969.&quot;</span>\n\n<span class=\"c1\"># 1. Generate using the base model (no adapter loaded)</span>\n<span class=\"n\">base_response</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"p\">,</span> <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Base model:&quot;</span><span class=\"p\">,</span> <span class=\"n\">base_response</span><span class=\"o\">.</span><span class=\"n\">generated_text</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 2. Generate using a fine-tuned adapter</span>\n<span class=\"c1\"># The adapter_id can be a Hugging Face repo ID or a local path</span>\n<span class=\"n\">adapter_response</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span>\n    <span class=\"n\">prompt</span><span class=\"p\">,</span>\n    <span class=\"n\">max_new_tokens</span><span class=\"o\">=</span><span class=\"mi\">80</span><span class=\"p\">,</span>\n    <span class=\"n\">adapter_id</span><span class=\"o\">=</span><span class=\"s2\">&quot;alignment-handbook/zephyr-7b-dpo-lora&quot;</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;With adapter:&quot;</span><span class=\"p\">,</span> <span class=\"n\">adapter_response</span><span class=\"o\">.</span><span class=\"n\">generated_text</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>The client supports streaming responses, adjusting decoding parameters (temperature, top-p, repetition penalty), and accessing token-level details. Check the <a href=\"https://github.com/predibase/lorax\">client reference</a> for advanced usage patterns.</p>\n<h3 id=\"openai-compatible-endpoint\">OpenAI-compatible endpoint</h3>\n<p>LoRAX implements the OpenAI Chat Completions API under the <code>/v1</code> path. This lets you drop LoRAX into tools that expect OpenAI's API format\u2014LangChain, Semantic Kernel, or custom applications.</p>\n<p>Use the <code>model</code> field to specify which adapter to load:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">openai</span>\n\n<span class=\"c1\"># Point the OpenAI client at LoRAX</span>\n<span class=\"n\">openai</span><span class=\"o\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;EMPTY&quot;</span>  <span class=\"c1\"># LoRAX doesn&#39;t require an API key by default</span>\n<span class=\"n\">openai</span><span class=\"o\">.</span><span class=\"n\">api_base</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;http://localhost:8080/v1&quot;</span>\n\n<span class=\"c1\"># The model parameter becomes the adapter_id</span>\n<span class=\"c1\"># This allows seamless integration with tools like LangChain</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">openai</span><span class=\"o\">.</span><span class=\"n\">ChatCompletion</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;alignment-handbook/zephyr-7b-dpo-lora&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"p\">{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;You are a friendly chatbot who speaks like a pirate.&quot;</span><span class=\"p\">},</span>\n        <span class=\"p\">{</span><span class=\"s2\">&quot;role&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;content&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;How many parrots can a person own?&quot;</span><span class=\"p\">},</span>\n    <span class=\"p\">],</span>\n    <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s2\">&quot;choices&quot;</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s2\">&quot;message&quot;</span><span class=\"p\">][</span><span class=\"s2\">&quot;content&quot;</span><span class=\"p\">])</span>\n</code></pre></div>\n<p>This compatibility unlocks two powerful use cases:</p>\n<ol>\n<li><strong>Drop-in replacement:</strong> Migrate existing applications from OpenAI's hosted models to your own infrastructure by changing one configuration line</li>\n<li><strong>Tool integration:</strong> Use LoRAX with any framework that supports OpenAI's API without custom adapters</li>\n</ol>\n<p>Note that the first request to a new adapter may have higher latency while LoRAX downloads and loads it. Plan for this in user-facing applications by preloading popular adapters or showing loading states.</p>\n<h2 id=\"trade-offs-to-consider\">Trade-offs to consider</h2>\n<h3 id=\"what-lorax-does-well\">What LoRAX does well</h3>\n<p><strong>Dramatic cost reduction for multi-model scenarios</strong></p>\n<p>Serve hundreds or thousands of fine-tuned models on a single GPU. Traditional approaches would require separate deployments for each model, multiplying infrastructure costs linearly. LoRAX keeps costs nearly constant as you add adapters.</p>\n<p><strong>Zero memory waste</strong></p>\n<p>Adapters are loaded just-in-time when requests arrive. Unused models consume no VRAM. This means you can maintain a catalog of 1,000+ specialized models but only pay for the handful actively serving traffic at any moment.</p>\n<p><strong>Production-grade performance</strong></p>\n<p>Continuous multi-adapter batching keeps latency and throughput comparable to single-model serving. Predibase benchmarks show that serving 32 different adapters simultaneously adds minimal overhead compared to serving one model.</p>\n<p><strong>Proven foundation</strong></p>\n<p>Built on Hugging Face TGI, LoRAX inherits battle-tested optimizations: FlashAttention 2, paged attention, streaming token generation, and SGMV kernels for efficient multi-adapter inference.</p>\n<p><strong>Deployment maturity</strong></p>\n<p>Ships with Docker images, Helm charts, Prometheus metrics, and OpenTelemetry tracing. The Apache 2.0 license means you can use it commercially without restrictions.</p>\n<p><strong>Broad model support</strong></p>\n<p>Works with popular open-source architectures: LLaMA 2, CodeLlama, Mistral, Mixtral, Qwen, and more. Supports quantization (4-bit via bitsandbytes, GPTQ, AWQ) to reduce memory footprint.</p>\n<h3 id=\"limitations-and-constraints\">Limitations and constraints</h3>\n<p><strong>Tied to LoRA-based fine-tuning</strong></p>\n<p>All your adapters must come from LoRA-style fine-tuning of the same base model. Full fine-tunes that produce standalone models won't work without conversion. If you have completely different model architectures, you'll need separate LoRAX deployments for each base.</p>\n<p><strong>Cold start latency</strong></p>\n<p>The first request after startup loads the base model into GPU memory (can take 30-90 seconds for larger models). First-time adapter requests also incur a download delay if pulling from Hugging Face. Plan for this with health checks and preloading strategies.</p>\n<p><strong>Cache thrashing under bursty load</strong></p>\n<p>If traffic suddenly hits dozens of different adapters, LoRAX may shuffle weights between GPU, CPU RAM, and disk. While adapter swaps are fast (~10ms from RAM), a very large working set can cause temporary slowdowns. Monitor GPU memory and adapter cache hit rates.</p>\n<p><strong>Fast-moving project</strong></p>\n<p>LoRAX forked from TGI in late 2023 and evolves rapidly. Expect frequent updates and occasional breaking changes as the maintainers track upstream TGI improvements and add new features. Pin versions carefully in production.</p>\n<h2 id=\"alternatives-lorax-vs-vllm\">Alternatives: LoRAX vs. vLLM</h2>\n<p><a href=\"https://github.com/vllm-project/vllm\">vLLM</a> is another popular high-throughput serving engine that recently added multi-LoRA support. How do they compare?</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Feature</th>\n<th style=\"text-align: left;\">LoRAX</th>\n<th style=\"text-align: left;\">vLLM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>Primary Focus</strong></td>\n<td style=\"text-align: left;\"><strong>Massive Scale</strong>: Serving hundreds/thousands of adapters.</td>\n<td style=\"text-align: left;\"><strong>High Throughput</strong>: Maximum tokens/sec for fewer active adapters.</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Architecture</strong></td>\n<td style=\"text-align: left;\"><strong>Dynamic Swapping</strong>: Aggressively offloads to CPU/disk.</td>\n<td style=\"text-align: left;\"><strong>Batching</strong>: Optimized for concurrent execution of active adapters.</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Best For</strong></td>\n<td style=\"text-align: left;\"><strong>Long-tail SaaS</strong>: 1000s of tenants, sporadic usage.</td>\n<td style=\"text-align: left;\"><strong>High-traffic tiers</strong>: 5-10 heavily used adapters.</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Base</strong></td>\n<td style=\"text-align: left;\">Hugging Face TGI</td>\n<td style=\"text-align: left;\">Custom Paged Attention Engine</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Choose LoRAX if:</strong> You have a \"long tail\" of adapters (e.g., one per user) where most are idle at any given time. LoRAX's tiered caching excels here.</p>\n<p><strong>Choose vLLM if:</strong> You have a small set of highly active adapters and raw throughput is your top priority.</p>\n<h2 id=\"getting-started-a-practical-roadmap\">Getting started: a practical roadmap</h2>\n<p>If LoRAX fits your use case, here's how to move from prototype to production:</p>\n<h3 id=\"1-start-small\">1. Start small</h3>\n<p>Deploy LoRAX with the base model you're already using and 3-5 representative adapters. Verify that adapter loading works and measure baseline latency for your workload.</p>\n<h3 id=\"2-measure-and-profile\">2. Measure and profile</h3>\n<ul>\n<li>Track adapter cache hit rates and GPU memory usage under realistic traffic patterns</li>\n<li>Identify your \"hot\" adapters (top 20% by request volume) and consider preloading them at startup</li>\n<li>Measure P50, P95, and P99 latency for both cached and cold adapter loads</li>\n</ul>\n<h3 id=\"3-optimize-for-your-workload\">3. Optimize for your workload</h3>\n<ul>\n<li>If you have a few very popular adapters, increase GPU memory allocation to keep more adapters hot</li>\n<li>If you have long-tail usage across hundreds of adapters, tune the tiered cache settings to balance RAM and disk</li>\n<li>Use quantization (4-bit bitsandbytes or GPTQ) if VRAM is tight</li>\n</ul>\n<h3 id=\"4-scale-horizontally\">4. Scale horizontally</h3>\n<p>Once you understand single-instance behavior, add replicas for high availability. Place a load balancer in front that routes based on <code>adapter_id</code> to improve cache locality\u2014requests for the same adapter hitting the same replica means better cache utilization.</p>\n<h3 id=\"5-monitor-continuously\">5. Monitor continuously</h3>\n<p>Set up dashboards for GPU utilization, adapter cache metrics, and request latency broken down by adapter. Watch for cache thrashing during traffic spikes and adjust your scaling strategy accordingly.</p>\n<p>With LoRAX, orchestrating specialized LLM experiences becomes a matter of routing adapter IDs\u2014not provisioning endless GPUs. The economics shift from linear scaling to near-constant costs, making multi-model serving viable even for small teams.</p>", "image": null, "date_modified": "2025-10-22T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["AI Engineering"]}, {"id": "https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/", "url": "https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/", "title": "Domain-driven design for AI agents: a beginner-friendly guide", "content_html": "<h1 id=\"domain-driven-design-for-ai-agents-a-beginner-friendly-guide\">Domain-driven design for AI agents: a beginner-friendly guide</h1>\n<h2 id=\"tldr\">TL;DR</h2>\n<blockquote>\n<p>Domain-driven design (DDD) gives AI agent teams a shared language, clear boundaries, and code that mirrors the real world. Use it to tame prompt spaghetti, enforce business rules, and evolve systems without breaking everything.</p>\n</blockquote>\n<p><img alt=\"DDD Layered Architecture\" src=\"../../../../assets/2025-10-20-domain-driven-design-ai-agents/ddd_layered_architecture.svg\" /></p>\n<!-- more -->\n\n<hr />\n<h2 id=\"why-domain-driven-design-matters-for-ai-agents\">Why domain-driven design matters for AI agents</h2>\n<p>AI agent projects fail for a surprising reason: not because the code is bad, but because developers and domain experts can't understand each other. You've seen it\u2014business teams ask for a \"policy check\" and get back a <code>process_data()</code> method. Nobody knows what it does, so requirements drift and systems calcify.</p>\n<p>Domain-driven design (DDD) fixes this by putting the <strong>business domain</strong> at the center. Not the database schema. Not the prompt template. The actual real-world process you're trying to model. This alignment delivers three immediate wins:</p>\n<ul>\n<li><strong>Shared language.</strong> Everyone\u2014product, ops, engineering\u2014uses the same words. When compliance says \"refund request\", that's what appears in your code, prompts, and documentation.</li>\n<li><strong>Focused scope.</strong> You build what matters: the core workflows, compliance rules, and critical metrics. Not mountains of glue code that break when requirements shift.</li>\n<li><strong>Adaptability.</strong> When policies change (and they will), you update one well-defined slice instead of hunting through a monolithic tangle.</li>\n</ul>\n<p>This matters most in complex domains where rules evolve constantly\u2014think finance, healthcare, operations, or any regulated industry. DDD gives you a fighting chance to keep up.</p>\n<hr />\n<h2 id=\"strategic-building-blocks\">Strategic building blocks</h2>\n<p>DDD isn't one big idea\u2014it's a toolkit of patterns that work together. It's often split into two parts:</p>\n<ol>\n<li><strong>Strategic Design</strong>: The \"big picture\" stuff. Defining boundaries, teams, and how systems talk. This is crucial for multi-agent systems.</li>\n<li><strong>Tactical Design</strong>: The code-level patterns (Entities, Aggregates). This keeps your agent's internal logic clean.</li>\n</ol>\n<p>Here are the core concepts you'll use every day.</p>\n<h3 id=\"ubiquitous-language\">Ubiquitous language</h3>\n<p>This is the shared vocabulary that shows up everywhere: in meetings, documentation, prompts, and method names. No translation layers between \"business speak\" and \"code speak.\"</p>\n<p>If compliance says \"policy check\", your method is <code>run_policy_check()</code>, not <code>process_data()</code>. If doctors say \"admit patient\", you write <code>admit_patient()</code>, not <code>add_user()</code>.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PatientRegistry</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">admit_patient</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">patient_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Admit a patient to the registry - term used by medical staff.&quot;&quot;&quot;</span>\n        <span class=\"o\">...</span>\n</code></pre></div>\n<p>This eliminates translation gaps and makes code self-documenting. When requirements change, the language change is obvious and localized.</p>\n<h3 id=\"bounded-contexts\">Bounded contexts</h3>\n<p>Large systems need explicit boundaries. Why? Because the same word means different things in different parts of the business.</p>\n<p>Take \"product\" in e-commerce. In the <strong>Inventory</strong> context, a product is a catalog item with SKUs and stock counts. In the <strong>Billing</strong> context, it's a line item with pricing rules and tax calculations. In <strong>Order Management</strong>, it's a quantity and delivery promise.</p>\n<p>Bounded contexts let each subdomain have its own definition without conflict. Translation layers or interfaces connect them when they need to talk.</p>\n<p>Bounded contexts let each subdomain have its own definition without conflict. Translation layers or interfaces connect them when they need to talk.</p>\n<p><img alt=\"Bounded Contexts\" src=\"../../../../assets/2025-10-20-domain-driven-design-ai-agents/ddd_bounded_contexts.svg\" /></p>\n<p>This keeps each model lean and prevents the \"one size fits all\" model that becomes unwieldy as complexity grows.</p>\n<h3 id=\"entities-and-value-objects\">Entities and value objects</h3>\n<p>These are the basic building blocks of your domain model. Understanding the difference is key.</p>\n<p><strong>Entities</strong> have identity that persists over time. A <code>Task</code> with ID <code>123</code> is the same task even if you change its description, status, or due date. Two entities are equal if they have the same ID, regardless of their attributes.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">SupportTicket</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">ticket_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>  <span class=\"c1\"># This is the identity</span>\n    <span class=\"n\">customer</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">issue</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">status</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;OPEN&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">close</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">status</span> <span class=\"o\">!=</span> <span class=\"s2\">&quot;OPEN&quot;</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&quot;Ticket already closed&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;CLOSED&quot;</span>\n</code></pre></div>\n<p><strong>Value objects</strong> have no identity\u2014they're defined entirely by their attributes. Two <code>TimeSlot</code> objects with the same start and end times are interchangeable. Value objects are immutable; instead of changing them, you create new ones.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TimeSlot</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">start</span><span class=\"p\">:</span> <span class=\"nb\">str</span>  <span class=\"c1\"># e.g., &quot;2025-10-18 09:00&quot;</span>\n    <span class=\"n\">end</span><span class=\"p\">:</span> <span class=\"nb\">str</span>    <span class=\"c1\"># e.g., &quot;2025-10-18 10:00&quot;</span>\n\n    <span class=\"nd\">@property</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">duration</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">int</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Compute duration from start to end</span>\n        <span class=\"o\">...</span>\n</code></pre></div>\n<p>Use entities for things that have lifecycles (<code>Order</code>, <code>User</code>, <code>AgentSession</code>). Use value objects for descriptions and measurements (<code>EmailAddress</code>, <code>Priority</code>, <code>Location</code>).</p>\n<h3 id=\"aggregates\">Aggregates</h3>\n<p>Aggregates are clusters of related entities and value objects that get treated as one unit. Think of them as consistency boundaries\u2014within an aggregate, business rules must always hold true.</p>\n<p>Every aggregate has one <strong>aggregate root</strong>\u2014an entity that controls access to everything inside. Want to modify something in the aggregate? Go through the root. This enforces invariants and prevents invalid states.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">datetime</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">date</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Task</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Plan</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>  <span class=\"c1\"># This is the aggregate root</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Task</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">add_task</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">task</span><span class=\"p\">:</span> <span class=\"n\">Task</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Business rule enforced here: no duplicate task IDs</span>\n        <span class=\"k\">if</span> <span class=\"nb\">any</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">id</span> <span class=\"o\">==</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">id</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span><span class=\"p\">):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&quot;Task ID already exists&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>External code never touches the <code>tasks</code> list directly\u2014it always calls <code>add_task()</code>. This guarantees the \"no duplicate IDs\" rule can never be violated. When you save to a database, you typically save the entire aggregate at once.</p>\n<h3 id=\"repositories\">Repositories</h3>\n<p>Repositories abstract away persistence. To your domain code, it feels like working with an in-memory collection\u2014no SQL queries, no database sessions, just clean methods like <code>save()</code> and <code>get()</code>.</p>\n<p>This separation has real benefits:</p>\n<ul>\n<li><strong>Domain logic stays clean.</strong> It doesn't care if data lives in Postgres, MongoDB, or a JSON file.</li>\n<li><strong>Testing is trivial.</strong> Swap in an in-memory repository for tests without touching domain code.</li>\n<li><strong>Storage can evolve.</strong> Switch from SQLite to Redis without rewriting business rules.</li>\n</ul>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">abc</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ABC</span><span class=\"p\">,</span> <span class=\"n\">abstractmethod</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Task</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Plan</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Task</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">PlanRepository</span><span class=\"p\">(</span><span class=\"n\">ABC</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Domain layer defines the interface.&quot;&quot;&quot;</span>\n    <span class=\"nd\">@abstractmethod</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">plan</span><span class=\"p\">:</span> <span class=\"n\">Plan</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"o\">...</span>\n\n    <span class=\"nd\">@abstractmethod</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">plan_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Plan</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"o\">...</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">InMemoryPlanRepository</span><span class=\"p\">(</span><span class=\"n\">PlanRepository</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Infrastructure layer provides the implementation.&quot;&quot;&quot;</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Plan</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">plan</span><span class=\"p\">:</span> <span class=\"n\">Plan</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"p\">[</span><span class=\"n\">plan</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">plan</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">plan_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Plan</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">storage</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">plan_id</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Your domain code only knows about <code>PlanRepository</code> (the interface). The infrastructure layer plugs in the actual implementation.</p>\n<h3 id=\"domain-events\">Domain events</h3>\n<p>Domain events capture important things that happen in your system. They're named in past tense\u2014<code>OrderPlaced</code>, <code>TaskCompleted</code>, <code>PaymentFailed</code>\u2014because they represent facts.</p>\n<p>Events make implicit side effects explicit. Instead of one module directly calling another when something happens, the domain raises an event. Other parts of the system subscribe and react independently.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">datetime</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">datetime</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskCompleted</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">completed_at</span><span class=\"p\">:</span> <span class=\"n\">datetime</span>\n</code></pre></div>\n<p>When a task finishes, you emit <code>TaskCompleted</code>. A notification service might listen for this event and send an email. A reporting service might log it for analytics. The important part: the task aggregate doesn't need to know about emails or analytics. It just announces what happened.</p>\n<p>This decouples workflows and makes cross-context communication clean. It's especially powerful in multi-agent systems where agents react to each other's events.</p>\n<hr />\n<h2 id=\"translating-ddd-to-agent-architectures\">Translating DDD to agent architectures</h2>\n<p>AI agents deal with complexity\u2014multi-step workflows, unreliable LLM outputs, evolving requirements. DDD's patterns map surprisingly well to these challenges. Here's how the concepts translate:</p>\n<h3 id=\"bounded-contexts-become-agents-or-skills\">Bounded contexts become agents or skills</h3>\n<p>Each agent (or major capability) is a bounded context. A research orchestrator might coordinate three specialized agents:</p>\n<ul>\n<li><strong>Trends Agent</strong> \u2014 gathers market data using its own vocabulary and tools</li>\n<li><strong>Compliance Agent</strong> \u2014 runs policy checks with regulatory terminology</li>\n<li><strong>Cost Agent</strong> \u2014 estimates expenses with finance-specific rules</li>\n</ul>\n<p>Each has its own model, terminology, and invariants. They communicate through well-defined interfaces or events.</p>\n<p>Each has its own model, terminology, and invariants. They communicate through well-defined interfaces or events.</p>\n<p><img alt=\"Agent Orchestration\" src=\"../../../../assets/2025-10-20-domain-driven-design-ai-agents/agent_orchestration.svg\" /></p>\n<p>Even in a single-agent system, you might define internal contexts\u2014a Planning module and an Execution module, each with its own domain model.</p>\n<h3 id=\"prompts-honor-the-ubiquitous-language\">Prompts honor the ubiquitous language</h3>\n<p>Use domain terms in system prompts, tool descriptions, and function signatures. If compliance experts say \"policy check\", that exact phrase appears in your prompts and code. This keeps humans and agents synchronized and makes the system easier to review and debug.</p>\n<h3 id=\"state-becomes-explicit-entities\">State becomes explicit entities</h3>\n<p>LLMs are often stateless, but complex agents maintain state\u2014conversation sessions, goals, intermediate results, tool outputs. Model these as entities or value objects:</p>\n<ul>\n<li><code>ConversationSession</code> entity with ID and message history</li>\n<li><code>Task</code> entity representing units of work</li>\n<li><code>ToolOutput</code> value object for immutable results</li>\n</ul>\n<p>Explicit modeling enables validation, business rules, and reuse. You can enforce rules like \"a task can't be completed until dependencies finish\" directly in the entity methods.</p>\n<h3 id=\"aggregates-express-agent-plans\">Aggregates express agent plans</h3>\n<p>A <code>Plan</code> aggregate root can govern task lists, enforce limits, and maintain priorities. When an LLM proposes adding 50 tasks, the aggregate enforces a maximum of 10. When it suggests duplicate work, the aggregate rejects it. This keeps AI proposals within business constraints.</p>\n<h3 id=\"domain-events-drive-orchestration\">Domain events drive orchestration</h3>\n<p>Agents raise events\u2014<code>ResearchCompleted</code>, <code>ThresholdExceeded</code>, <code>PolicyViolationDetected</code>. Other agents or services listen and react without tight coupling. This event-driven approach is the future of scalable agent systems: it lets multiple agents collaborate in real-time without being hard-wired together.</p>\n<h3 id=\"business-rules-wrap-ai-actions\">Business rules wrap AI actions</h3>\n<p>LLM outputs flow through domain services or entity methods. If an LLM suggests a refund amount beyond policy limits, your <code>RefundRequest</code> value object validates and rejects it. The AI can improvise, but business rules have the final say. This keeps agents safe and aligned with policy.</p>\n<h3 id=\"the-anti-corruption-layer-acl\">The Anti-Corruption Layer (ACL)</h3>\n<p>When working with LLMs, you are dealing with a probabilistic, creative, and occasionally chaotic entity. Your domain model, however, must be deterministic and safe. You cannot let the raw output of an LLM leak directly into your domain logic.</p>\n<p>Enter the <strong>Anti-Corruption Layer (ACL)</strong>.</p>\n<p><img alt=\"LLM Domain Interaction\" src=\"../../../../assets/2025-10-20-domain-driven-design-ai-agents/llm_domain_interaction.svg\" /></p>\n<p>The ACL acts as a gatekeeper. It translates the \"wild\" output of the LLM into the \"strict\" language of your domain.</p>\n<ol>\n<li><strong>Ingest</strong>: Receive raw text or JSON from the LLM.</li>\n<li><strong>Validate</strong>: Use Pydantic models to check structure and types.</li>\n<li><strong>Sanitize</strong>: Ensure values fall within acceptable ranges (e.g., no negative prices).</li>\n<li><strong>Translate</strong>: Convert DTOs (Data Transfer Objects) into Domain Entities.</li>\n</ol>\n<p>If validation fails, the ACL rejects the data\u2014often sending an error message back to the LLM so it can correct itself. This loop ensures that <strong>only valid data ever touches your core business logic</strong>.</p>\n<hr />\n<h2 id=\"example-a-task-assistant-modeled-with-ddd\">Example: a task assistant modeled with DDD</h2>\n<p>Let's build a personal task assistant that handles requests like \"Remind me to buy milk tomorrow\" or \"What's on my to-do list?\" We'll apply DDD principles step by step.</p>\n<h3 id=\"1-map-the-contexts\">1. Map the contexts</h3>\n<p>Start by breaking the problem into subdomains:</p>\n<ul>\n<li><strong>Task Management</strong> \u2014 handling to-do items and reminders (core domain)</li>\n<li><strong>Scheduling</strong> \u2014 calendar events and meetings</li>\n<li><strong>Notifications</strong> \u2014 sending alerts and emails</li>\n</ul>\n<p>We'll focus on Task Management first. The others can evolve as separate bounded contexts or companion agents.</p>\n<p>We'll focus on Task Management first. The others can evolve as separate bounded contexts or companion agents.</p>\n<p><img alt=\"Task Assistant Context Map\" src=\"../../../../assets/2025-10-20-domain-driven-design-ai-agents/task_assistant_context_map.svg\" /></p>\n<h3 id=\"2-speak-the-same-language\">2. Speak the same language</h3>\n<p>Establish the vocabulary with domain experts (or just common sense for personal tasks): \"task\", \"deadline\", \"reminder\", \"priority\". Use these exact terms everywhere\u2014prompt templates, method names, UI labels. No translation layers.</p>\n<h3 id=\"3-capture-entities-value-objects-and-events\">3. Capture entities, value objects, and events</h3>\n<p>Now model the core concepts:</p>\n<ul>\n<li><strong>Entity:</strong> <code>Task</code> with identity (<code>id</code>) and mutable state (<code>completed</code>)</li>\n<li><strong>Value object:</strong> <code>Priority</code> enum (immutable, defined by its value)</li>\n<li><strong>Domain event:</strong> <code>TaskCompletedEvent</code> to signal when work finishes</li>\n</ul>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">datetime</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">datetime</span><span class=\"p\">,</span> <span class=\"n\">date</span><span class=\"p\">,</span> <span class=\"n\">timezone</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">enum</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Enum</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Priority</span><span class=\"p\">(</span><span class=\"n\">Enum</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Value object: priority is defined by its value alone.&quot;&quot;&quot;</span>\n    <span class=\"n\">LOW</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n    <span class=\"n\">NORMAL</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n    <span class=\"n\">HIGH</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskCompletedEvent</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Domain event: announces a task was completed.&quot;&quot;&quot;</span>\n    <span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">time</span><span class=\"p\">:</span> <span class=\"n\">datetime</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Task</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Entity: identity persists even as attributes change.&quot;&quot;&quot;</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">created_at</span><span class=\"p\">:</span> <span class=\"n\">datetime</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">(</span><span class=\"n\">timezone</span><span class=\"o\">.</span><span class=\"n\">utc</span><span class=\"p\">))</span>\n    <span class=\"n\">due_date</span><span class=\"p\">:</span> <span class=\"n\">date</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">priority</span><span class=\"p\">:</span> <span class=\"n\">Priority</span> <span class=\"o\">=</span> <span class=\"n\">Priority</span><span class=\"o\">.</span><span class=\"n\">NORMAL</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">mark_completed</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">TaskCompletedEvent</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Business rule: can&#39;t complete an already-completed task.&quot;&quot;&quot;</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">completed</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&quot;Task is already completed.&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">completed</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n        <span class=\"k\">return</span> <span class=\"n\">TaskCompletedEvent</span><span class=\"p\">(</span><span class=\"n\">task_id</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">id</span><span class=\"p\">,</span> <span class=\"n\">time</span><span class=\"o\">=</span><span class=\"n\">datetime</span><span class=\"o\">.</span><span class=\"n\">now</span><span class=\"p\">(</span><span class=\"n\">timezone</span><span class=\"o\">.</span><span class=\"n\">utc</span><span class=\"p\">))</span>\n</code></pre></div>\n<p>Notice how business rules live in the entity methods, not scattered across prompt templates.</p>\n<h3 id=\"4-shape-the-aggregate\">4. Shape the aggregate</h3>\n<p>The <code>TaskList</code> is our aggregate root. It holds multiple <code>Task</code> entities and enforces consistency rules across them. All modifications go through the root's methods.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">datetime</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">date</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Task</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">due_date</span><span class=\"p\">:</span> <span class=\"n\">date</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskList</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Aggregate root: enforces invariants across all tasks.&quot;&quot;&quot;</span>\n    <span class=\"n\">owner</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Task</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">add_task</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">task</span><span class=\"p\">:</span> <span class=\"n\">Task</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Business rule: no duplicate tasks on the same day.&quot;&quot;&quot;</span>\n        <span class=\"k\">if</span> <span class=\"nb\">any</span><span class=\"p\">(</span>\n            <span class=\"n\">existing</span><span class=\"o\">.</span><span class=\"n\">description</span> <span class=\"o\">==</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">description</span>\n            <span class=\"ow\">and</span> <span class=\"n\">existing</span><span class=\"o\">.</span><span class=\"n\">due_date</span> <span class=\"o\">==</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">due_date</span>\n            <span class=\"k\">for</span> <span class=\"n\">existing</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span>\n        <span class=\"p\">):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&quot;A similar task on that date already exists.&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_pending</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Task</span><span class=\"p\">]:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Query helper: find tasks that aren&#39;t done yet.&quot;&quot;&quot;</span>\n        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"n\">task</span> <span class=\"k\">for</span> <span class=\"n\">task</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">completed</span><span class=\"p\">]</span>\n\n\n<span class=\"n\">TaskList</span><span class=\"o\">.</span><span class=\"n\">model_rebuild</span><span class=\"p\">()</span>  <span class=\"c1\"># Resolve forward references for Pydantic.</span>\n</code></pre></div>\n<p>External code never manipulates <code>tasks</code> directly\u2014it always goes through <code>add_task()</code> or other root methods. This guarantees the \"no duplicates\" rule holds.</p>\n<h3 id=\"5-wrap-persistence-in-a-repository\">5. Wrap persistence in a repository</h3>\n<p>The repository abstracts storage. Domain code doesn't know if tasks live in memory, a database, or a JSON file.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Task</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskList</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">owner</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Task</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">TaskList</span><span class=\"o\">.</span><span class=\"n\">model_rebuild</span><span class=\"p\">()</span>  <span class=\"c1\"># Resolve forward references for Pydantic.</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskRepository</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Abstracts task storage - in-memory implementation for simplicity.&quot;&quot;&quot;</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_data</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">TaskList</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_task_list</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">owner</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">TaskList</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Retrieve a user&#39;s task list, or create a new empty one.&quot;&quot;&quot;</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_data</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">owner</span><span class=\"p\">,</span> <span class=\"n\">TaskList</span><span class=\"p\">(</span><span class=\"n\">owner</span><span class=\"o\">=</span><span class=\"n\">owner</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">save_task_list</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">task_list</span><span class=\"p\">:</span> <span class=\"n\">TaskList</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Persist changes to the task list.&quot;&quot;&quot;</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_data</span><span class=\"p\">[</span><span class=\"n\">task_list</span><span class=\"o\">.</span><span class=\"n\">owner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">task_list</span>\n</code></pre></div>\n<p>In production, you'd swap this for a database implementation\u2014say, using <a href=\"https://www.sqlalchemy.org/\">SQLAlchemy</a> or Postgres\u2014without touching the domain logic.</p>\n<h3 id=\"6-run-the-flow\">6. Run the flow</h3>\n<p>Here's how everything fits together when a user makes a request:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">datetime</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">date</span><span class=\"p\">,</span> <span class=\"n\">timedelta</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">uuid</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">uuid4</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">pydantic</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">Task</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">description</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">due_date</span><span class=\"p\">:</span> <span class=\"n\">date</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">completed</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskList</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">owner</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Task</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">add_task</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">task</span><span class=\"p\">:</span> <span class=\"n\">Task</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"nb\">any</span><span class=\"p\">(</span>\n            <span class=\"n\">existing</span><span class=\"o\">.</span><span class=\"n\">description</span> <span class=\"o\">==</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">description</span>\n            <span class=\"ow\">and</span> <span class=\"n\">existing</span><span class=\"o\">.</span><span class=\"n\">due_date</span> <span class=\"o\">==</span> <span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">due_date</span>\n            <span class=\"k\">for</span> <span class=\"n\">existing</span> <span class=\"ow\">in</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span>\n        <span class=\"p\">):</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&quot;A similar task on that date already exists.&quot;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tasks</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">TaskRepository</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_data</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">TaskList</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_task_list</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">owner</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">TaskList</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_data</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">owner</span><span class=\"p\">,</span> <span class=\"n\">TaskList</span><span class=\"p\">(</span><span class=\"n\">owner</span><span class=\"o\">=</span><span class=\"n\">owner</span><span class=\"p\">))</span>\n\n    <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">save_task_list</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">task_list</span><span class=\"p\">:</span> <span class=\"n\">TaskList</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_data</span><span class=\"p\">[</span><span class=\"n\">task_list</span><span class=\"o\">.</span><span class=\"n\">owner</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">task_list</span>\n\n\n<span class=\"n\">TaskList</span><span class=\"o\">.</span><span class=\"n\">model_rebuild</span><span class=\"p\">()</span>  <span class=\"c1\"># Resolve forward references for Pydantic.</span>\n\n\n<span class=\"c1\"># User says: &quot;Remind me to buy milk tomorrow&quot;</span>\n<span class=\"c1\"># (In reality, an LLM would parse this into structured data)</span>\n<span class=\"n\">user_input</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;Remind me to buy milk tomorrow&quot;</span>\n<span class=\"n\">intent</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;add_task&quot;</span>\n\n<span class=\"c1\"># Initialize repository</span>\n<span class=\"n\">repo</span> <span class=\"o\">=</span> <span class=\"n\">TaskRepository</span><span class=\"p\">()</span>\n\n<span class=\"k\">if</span> <span class=\"n\">intent</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;add_task&quot;</span><span class=\"p\">:</span>\n    <span class=\"c1\"># 1. Load the user&#39;s task list</span>\n    <span class=\"n\">task_list</span> <span class=\"o\">=</span> <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">get_task_list</span><span class=\"p\">(</span><span class=\"n\">owner</span><span class=\"o\">=</span><span class=\"s2\">&quot;User123&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 2. Create a new task entity</span>\n    <span class=\"n\">task</span> <span class=\"o\">=</span> <span class=\"n\">Task</span><span class=\"p\">(</span>\n        <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">uuid4</span><span class=\"p\">()),</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;buy milk&quot;</span><span class=\"p\">,</span>\n        <span class=\"n\">due_date</span><span class=\"o\">=</span><span class=\"n\">date</span><span class=\"o\">.</span><span class=\"n\">today</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"n\">timedelta</span><span class=\"p\">(</span><span class=\"n\">days</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"c1\"># 3. Domain layer enforces business rules</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">task_list</span><span class=\"o\">.</span><span class=\"n\">add_task</span><span class=\"p\">(</span><span class=\"n\">task</span><span class=\"p\">)</span>\n        <span class=\"n\">repo</span><span class=\"o\">.</span><span class=\"n\">save_task_list</span><span class=\"p\">(</span><span class=\"n\">task_list</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Task &#39;</span><span class=\"si\">{</span><span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">description</span><span class=\"si\">}</span><span class=\"s2\">&#39; added for </span><span class=\"si\">{</span><span class=\"n\">task</span><span class=\"o\">.</span><span class=\"n\">due_date</span><span class=\"si\">}</span><span class=\"s2\">.&quot;</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">exc</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Sorry, I couldn&#39;t add that task: </span><span class=\"si\">{</span><span class=\"n\">exc</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Notice the separation of concerns:</p>\n<ul>\n<li><strong>LLM layer</strong> parses natural language into structured data (intent + parameters)</li>\n<li><strong>Domain layer</strong> enforces business rules through entity methods</li>\n<li><strong>Repository layer</strong> handles persistence without leaking into domain logic</li>\n</ul>\n<p>The LLM can be creative with parsing, but the domain ensures consistency. If the LLM tries to add a duplicate task, the aggregate root rejects it\u2014no special-casing needed in prompts.</p>\n<hr />\n<h2 id=\"tooling-to-bring-the-model-to-life\">Tooling to bring the model to life</h2>\n<p>DDD doesn't require special frameworks, but certain tools make implementation smoother\u2014especially for AI agents.</p>\n<h3 id=\"fastapi\">FastAPI</h3>\n<p><a href=\"https://fastapi.tiangolo.com/\">FastAPI</a> pairs beautifully with DDD. Use routers to separate bounded contexts (<code>/tasks</code>, <code>/schedule</code>), Pydantic models for request/response validation, and dependency injection to wire up repositories.</p>\n<p>Structure your project in layers:</p>\n<div class=\"highlight\"><pre><span></span><code>project/\n\u251c\u2500\u2500 domain/          # Pure business logic (entities, aggregates, value objects)\n\u251c\u2500\u2500 application/     # Use cases and command handlers\n\u251c\u2500\u2500 infrastructure/  # Repositories, databases, external APIs\n\u2514\u2500\u2500 interface/       # FastAPI routers and HTTP contracts\n</code></pre></div>\n<p>This layering (sometimes called \"onion architecture\") keeps changes from rippling through your codebase. Swap the database? Touch only <code>infrastructure/</code>. Change the UI? Touch only <code>interface/</code>.</p>\n<h3 id=\"pydantic-and-pydantic-ai\">Pydantic and Pydantic AI</h3>\n<p><a href=\"https://docs.pydantic.dev/latest/\">Pydantic</a> enforces invariants and validates data at runtime. Use it for entities, value objects, and especially for validating LLM outputs.</p>\n<p><a href=\"https://ai.pydantic.dev/\">Pydantic AI</a> takes this further: it ensures LLM responses conform to your domain schemas. Define an <code>AddTaskCommand</code> with required fields, and Pydantic AI validates that the LLM's JSON output matches before you act on it. This brings structure to the chaotic world of AI outputs.</p>\n<p>Another excellent tool is <strong><a href=\"https://github.com/jxnl/instructor\">Instructor</a></strong>, which patches OpenAI (and other) clients to return Pydantic models directly. It's a lightweight way to implement your Anti-Corruption Layer.</p>\n<h3 id=\"ddd-helper-libraries\">DDD helper libraries</h3>\n<ul>\n<li><strong><a href=\"https://pypi.org/project/dddesign/\">DDDesign</a></strong> \u2014 provides base classes for entities, repositories, and value objects built on Pydantic</li>\n<li><strong><a href=\"https://github.com/proteanhq/protean\">Protean</a></strong> \u2014 a full framework for DDD, CQRS, and event sourcing if you want something that comes with a lot of ready-made features out of the box</li>\n</ul>\n<p>Most Python developers skip these and use vanilla classes with Pydantic, but they're worth exploring for large projects.</p>\n<h3 id=\"event-driven-tooling\">Event-driven tooling</h3>\n<p>For domain events, consider:</p>\n<ul>\n<li><strong><a href=\"https://github.com/jek/blinker\">blinker</a></strong> \u2014 lightweight in-process event dispatcher</li>\n<li><strong><a href=\"https://github.com/redis/redis-py\">redis-py Pub/Sub</a></strong> or <strong><a href=\"https://www.rabbitmq.com/\">RabbitMQ</a></strong> \u2014 for distributed events across services or agents</li>\n<li><strong>asyncio event patterns</strong> \u2014 if you're already async</li>\n</ul>\n<p>Events are crucial for multi-agent orchestration. An agent emits <code>ResearchCompleted</code>, others react\u2014no tight coupling.</p>\n<h3 id=\"agent-frameworks\">Agent frameworks</h3>\n<p><a href=\"https://python.langchain.com/\">LangChain</a>, <a href=\"https://langchain-ai.github.io/langgraph/\">LangGraph</a>, <a href=\"https://github.com/deepset-ai/haystack\">Haystack</a>, <a href=\"https://github.com/microsoft/semantic-kernel\">Semantic Kernel</a>, <a href=\"https://github.com/run-llama/llama_index\">LlamaIndex</a>, <a href=\"https://github.com/microsoft/autogen\">AutoGen</a>, <a href=\"https://ai.google.dev/gemini-api/docs/agent-developer-kit\">Google ADK</a>, <a href=\"https://github.com/huggingface/smolagents\">smolagents</a>, and <a href=\"https://github.com/joaomdmoura/crewAI\">CrewAI</a> provide structure for modern agent workflows. Use them within your domain layer, but wrap them in your own interfaces. That way, swapping frameworks doesn't break your business logic.</p>\n<h3 id=\"testing\">Testing</h3>\n<p>One of DDD's biggest wins: your domain layer tests without the whole stack running.</p>\n<ul>\n<li><strong><a href=\"https://docs.pytest.org/en/stable/\">PyTest</a></strong> for unit tests on entities and aggregates</li>\n<li><strong>Fake repositories</strong> (in-memory) for integration tests</li>\n<li><strong>LLM stubs</strong> that return predetermined outputs</li>\n</ul>\n<p>Your domain code should never require a live LLM to test. The LLM is an implementation detail\u2014your tests validate business rules.</p>\n<hr />\n<h2 id=\"getting-started-checklist\">Getting started checklist</h2>\n<p>Ready to apply DDD to your next agent project? Here's your roadmap:</p>\n<ol>\n<li><strong>Interview domain experts.</strong> Draft the ubiquitous language\u2014the vocabulary everyone will use. Document it.</li>\n<li><strong>Map bounded contexts.</strong> Draw the subdomains and mark where they need to talk to each other. Start with one core context.</li>\n<li><strong>Model entities and value objects.</strong> What things have identity? What things are just values? Bake invariants into their methods.</li>\n<li><strong>Define aggregate roots.</strong> Bundle related entities under one root that enforces consistency rules.</li>\n<li><strong>Create repository interfaces.</strong> Don't implement storage yet\u2014just define <code>save()</code> and <code>get()</code> methods. Keep the domain clean.</li>\n<li><strong>Emit domain events.</strong> For meaningful changes (order placed, task completed), raise events. Wire listeners later as needed.</li>\n<li><strong>Wrap LLM outputs in schemas.</strong> Use Pydantic models to enforce contracts. Don't let free-form text leak into your domain.</li>\n<li><strong>Add orchestration.</strong> Build application services that coordinate agents via structured commands or events.</li>\n</ol>\n<p>The golden rule: <strong>start with the domain, not the tech stack.</strong> Understand the business problem first. Model it explicitly. Then let the AI tooling serve that model\u2014not the other way around.</p>", "image": null, "date_modified": "2025-10-20T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Agentic AI"]}, {"id": "https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/", "url": "https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/", "title": "Building a Custom FeatureStoreLite MCP Server Using uv", "content_html": "<h1 id=\"building-a-custom-featurestorelite-mcp-server-using-uv\">Building a Custom FeatureStoreLite MCP Server Using uv</h1>\n<p><em>A step-by-step guide that shows how to create your own lightweight feature store MCP server from scratch using FastMCP, run it through <strong>uv</strong>, and integrate it with Claude Desktop.</em></p>\n<!-- more -->\n\n<hr />\n<h2 id=\"1-introduction\">1. Introduction</h2>\n<p>The <strong>Model Context Protocol (MCP)</strong> is an open standard that enables AI assistants (like Claude) to connect to external data and tools. Instead of building custom integrations for every tool, MCP provides a universal language for AI models to interact with your world.</p>\n<p>In this tutorial, we will build a <strong>FeatureStoreLite</strong> MCP server. This server will act as a bridge between an LLM and a feature store (a database of precomputed ML features), allowing the LLM to query and retrieve feature vectors for users, products, or documents.</p>\n<h3 id=\"why-build-this\">Why build this?</h3>\n<p>Imagine you are an ML engineer debugging a pipeline. Instead of writing SQL queries or Python scripts to check feature values, you can simply ask Claude: <em>\"What is the feature vector for user_123?\"</em> or <em>\"Show me the metadata for product_abc\"</em>.</p>\n<h3 id=\"why-use-uv\">Why use <code>uv</code>?</h3>\n<p>We will use <strong><a href=\"https://docs.astral.sh/uv/\">uv</a></strong>, an extremely fast Python package installer and project manager. It simplifies dependency management and makes running our server reproducible and fast.</p>\n<h3 id=\"architecture-overview\">Architecture Overview</h3>\n<p>Here is how the components interact:</p>\n<p><img alt=\"MCP Architecture\" src=\"../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/mcp_architecture.svg\" /></p>\n<ol>\n<li><strong>User</strong>: Asks a question in natural language.</li>\n<li><strong>Claude Desktop</strong>: The MCP Client that interprets the question and decides which tool to call.</li>\n<li><strong>MCP Server</strong>: Our Python application running <code>FastMCP</code> that exposes tools (<code>get_feature</code>, <code>store_feature</code>).</li>\n<li><strong>SQLite</strong>: The backing storage for our feature vectors.</li>\n</ol>\n<hr />\n<h2 id=\"2-setup-and-installation\">2. Setup and Installation</h2>\n<h3 id=\"21-install-uv\">2.1. Install <code>uv</code></h3>\n<p>If you haven't installed <code>uv</code> yet, get it now. It's a game-changer for Python development.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># macOS/Linux</span>\ncurl<span class=\"w\"> </span>-LsSf<span class=\"w\"> </span>https://astral.sh/uv/install.sh<span class=\"w\"> </span><span class=\"p\">|</span><span class=\"w\"> </span>sh\n\n<span class=\"c1\"># Or via Homebrew</span>\nbrew<span class=\"w\"> </span>install<span class=\"w\"> </span>uv\n</code></pre></div>\n<h3 id=\"22-initialize-the-project\">2.2. Initialize the Project</h3>\n<p>Create a new directory and initialize a Python project. <code>uv init</code> creates a <code>pyproject.toml</code> for you.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># Create project directory</span>\nmkdir<span class=\"w\"> </span>mcp-featurestore\n<span class=\"nb\">cd</span><span class=\"w\"> </span>mcp-featurestore\n\n<span class=\"c1\"># Initialize Python project</span>\nuv<span class=\"w\"> </span>init\n\n<span class=\"c1\"># Add the MCP SDK with CLI tools</span>\nuv<span class=\"w\"> </span>add<span class=\"w\"> </span><span class=\"s2\">&quot;mcp[cli]&quot;</span>\n</code></pre></div>\n<hr />\n<h2 id=\"3-building-the-server\">3. Building the Server</h2>\n<p>We will split our application into two files:</p>\n<ol>\n<li><code>database.py</code>: Handles SQLite operations.</li>\n<li><code>featurestore_server.py</code>: The MCP server definition.</li>\n</ol>\n<h3 id=\"31-the-database-layer-databasepy\">3.1. The Database Layer (<code>database.py</code>)</h3>\n<p>This module manages the SQLite connection and provides helper functions. We'll also seed it with some dummy data so we have something to query.</p>\n<p>Create <code>database.py</code>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># database.py</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">sqlite3</span>\n\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_db_path</span><span class=\"p\">()</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Get the database path - always in the script&#39;s directory&quot;&quot;&quot;</span>\n    <span class=\"n\">script_dir</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">dirname</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">abspath</span><span class=\"p\">(</span><span class=\"vm\">__file__</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">script_dir</span><span class=\"p\">,</span> <span class=\"s2\">&quot;features.db&quot;</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">init_db</span><span class=\"p\">()</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Initialize the feature store database with table and sample data&quot;&quot;&quot;</span>\n    <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">get_db_path</span><span class=\"p\">())</span>\n    <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">        CREATE TABLE IF NOT EXISTS features (</span>\n<span class=\"s2\">            key TEXT PRIMARY KEY,</span>\n<span class=\"s2\">            vector TEXT NOT NULL,</span>\n<span class=\"s2\">            metadata TEXT,</span>\n<span class=\"s2\">            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</span>\n<span class=\"s2\">        )</span>\n<span class=\"s2\">    &quot;&quot;&quot;</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Sample data for experimentation</span>\n    <span class=\"n\">example_features</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"p\">(</span>\n            <span class=\"s2\">&quot;user_123&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;[0.1, 0.2, -0.5, 0.8, 0.3, -0.1, 0.9, -0.4]&quot;</span><span class=\"p\">,</span>\n            <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">({</span><span class=\"s2\">&quot;type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;user&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;id&quot;</span><span class=\"p\">:</span> <span class=\"mi\">123</span><span class=\"p\">,</span> <span class=\"s2\">&quot;segment&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;premium&quot;</span><span class=\"p\">}),</span>\n        <span class=\"p\">),</span>\n        <span class=\"p\">(</span>\n            <span class=\"s2\">&quot;product_abc&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;[0.7, -0.3, 0.4, 0.1, -0.8, 0.6, 0.2, -0.5]&quot;</span><span class=\"p\">,</span>\n            <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">({</span><span class=\"s2\">&quot;type&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;product&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;id&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;abc&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;category&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;electronics&quot;</span><span class=\"p\">}),</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"c1\"># Insert if not exists</span>\n    <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"p\">,</span> <span class=\"n\">metadata</span> <span class=\"ow\">in</span> <span class=\"n\">example_features</span><span class=\"p\">:</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span>\n                <span class=\"s2\">&quot;INSERT INTO features (key, vector, metadata) VALUES (?, ?, ?)&quot;</span><span class=\"p\">,</span>\n                <span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"p\">,</span> <span class=\"n\">metadata</span><span class=\"p\">),</span>\n            <span class=\"p\">)</span>\n        <span class=\"k\">except</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">IntegrityError</span><span class=\"p\">:</span>\n            <span class=\"k\">pass</span>  <span class=\"c1\"># Already exists</span>\n\n    <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span>\n    <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_db_connection</span><span class=\"p\">()</span> <span class=\"o\">-&gt;</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">Connection</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;Get a database connection&quot;&quot;&quot;</span>\n    <span class=\"k\">return</span> <span class=\"n\">sqlite3</span><span class=\"o\">.</span><span class=\"n\">connect</span><span class=\"p\">(</span><span class=\"n\">get_db_path</span><span class=\"p\">())</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">init_db</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;\u2705 Database initialized successfully!&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<p>Initialize the database:</p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>python<span class=\"w\"> </span>database.py\n</code></pre></div>\n<h3 id=\"32-the-mcp-server-featurestore_serverpy\">3.2. The MCP Server (<code>featurestore_server.py</code>)</h3>\n<p>Now for the exciting part. We use <code>FastMCP</code> to define our server. It uses decorators to turn standard Python functions into MCP Tools and Resources.</p>\n<p>Create <code>featurestore_server.py</code>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># featurestore_server.py</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">mcp.server.fastmcp</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FastMCP</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">database</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">get_db_connection</span><span class=\"p\">,</span> <span class=\"n\">init_db</span>\n\n<span class=\"c1\"># Initialize the MCP Server</span>\n<span class=\"n\">mcp</span> <span class=\"o\">=</span> <span class=\"n\">FastMCP</span><span class=\"p\">(</span><span class=\"s2\">&quot;FeatureStoreLite&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Ensure DB is ready when server starts</span>\n<span class=\"n\">init_db</span><span class=\"p\">()</span>\n\n\n<span class=\"nd\">@mcp</span><span class=\"o\">.</span><span class=\"n\">resource</span><span class=\"p\">(</span><span class=\"s2\">&quot;schema://main&quot;</span><span class=\"p\">)</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_schema</span><span class=\"p\">()</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Resource: Provide the database schema.</span>\n<span class=\"sd\">    Resources are passive data that LLMs can read like files.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">get_db_connection</span><span class=\"p\">()</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">schema</span> <span class=\"o\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span>\n            <span class=\"s2\">&quot;SELECT sql FROM sqlite_master WHERE type=&#39;table&#39;&quot;</span>\n        <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"s2\">&quot;</span><span class=\"se\">\\n</span><span class=\"s2\">&quot;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">sql</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">sql</span> <span class=\"ow\">in</span> <span class=\"n\">schema</span> <span class=\"k\">if</span> <span class=\"n\">sql</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"ow\">or</span> <span class=\"s2\">&quot;No tables found.&quot;</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n\n<span class=\"nd\">@mcp</span><span class=\"o\">.</span><span class=\"n\">tool</span><span class=\"p\">()</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">store_feature</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">metadata</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Tool: Store a feature vector.</span>\n<span class=\"sd\">    Tools are executable functions that LLMs can call to perform actions.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">get_db_connection</span><span class=\"p\">()</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Validate that vector is valid JSON</span>\n        <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">vector</span><span class=\"p\">)</span>\n\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span>\n            <span class=\"s2\">&quot;INSERT OR REPLACE INTO features (key, vector, metadata) VALUES (?, ?, ?)&quot;</span><span class=\"p\">,</span>\n            <span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">vector</span><span class=\"p\">,</span> <span class=\"n\">metadata</span><span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">commit</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;Successfully stored feature &#39;</span><span class=\"si\">{</span><span class=\"n\">key</span><span class=\"si\">}</span><span class=\"s2\">&#39;&quot;</span>\n    <span class=\"k\">except</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">JSONDecodeError</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"s2\">&quot;Error: Vector must be a valid JSON array string (e.g., &#39;[0.1, 0.2]&#39;)&quot;</span>\n    <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;Error: </span><span class=\"si\">{</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n\n<span class=\"nd\">@mcp</span><span class=\"o\">.</span><span class=\"n\">tool</span><span class=\"p\">()</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_feature</span><span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Tool: Retrieve a feature vector by key.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">get_db_connection</span><span class=\"p\">()</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">row</span> <span class=\"o\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span>\n            <span class=\"s2\">&quot;SELECT vector, metadata FROM features WHERE key = ?&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">key</span><span class=\"p\">,)</span>\n        <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">()</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">row</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span>\n                <span class=\"p\">{</span>\n                    <span class=\"s2\">&quot;key&quot;</span><span class=\"p\">:</span> <span class=\"n\">key</span><span class=\"p\">,</span>\n                    <span class=\"s2\">&quot;vector&quot;</span><span class=\"p\">:</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span>\n                    <span class=\"s2\">&quot;metadata&quot;</span><span class=\"p\">:</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">loads</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"k\">if</span> <span class=\"n\">row</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">else</span> <span class=\"kc\">None</span><span class=\"p\">,</span>\n                <span class=\"p\">},</span>\n                <span class=\"n\">indent</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n            <span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s2\">&quot;Feature &#39;</span><span class=\"si\">{</span><span class=\"n\">key</span><span class=\"si\">}</span><span class=\"s2\">&#39; not found.&quot;</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n\n<span class=\"nd\">@mcp</span><span class=\"o\">.</span><span class=\"n\">tool</span><span class=\"p\">()</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">list_features</span><span class=\"p\">()</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot;</span>\n<span class=\"sd\">    Tool: List all available feature keys.</span>\n<span class=\"sd\">    &quot;&quot;&quot;</span>\n    <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">get_db_connection</span><span class=\"p\">()</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">&quot;SELECT key FROM features&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">()</span>\n        <span class=\"k\">return</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">([</span><span class=\"n\">row</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">rows</span><span class=\"p\">])</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n    <span class=\"n\">mcp</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n</code></pre></div>\n<hr />\n<h2 id=\"4-testing-with-mcp-inspector\">4. Testing with MCP Inspector</h2>\n<p>Before connecting to Claude, use the <strong>MCP Inspector</strong> to verify your server works. This web interface lets you test tools and view resources.</p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>mcp<span class=\"w\"> </span>dev<span class=\"w\"> </span>featurestore_server.py\n</code></pre></div>\n<p>This command starts the server and opens the Inspector in your browser (usually <code>http://localhost:5173</code> or similar).</p>\n<p><img alt=\"Inspector\" src=\"../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/inspector.png\" style=\"width:600px;max-width:100%;height:auto;\" /></p>\n<p>Try calling <code>get_feature</code> with <code>key=\"user_123\"</code> in the Inspector to confirm it returns the JSON data.</p>\n<hr />\n<h2 id=\"5-connecting-to-claude-desktop\">5. Connecting to Claude Desktop</h2>\n<p>Now let's connect our server to Claude Desktop so we can talk to it.</p>\n<h3 id=\"51-configure-claude\">5.1. Configure Claude</h3>\n<p>Edit your Claude Desktop configuration file:</p>\n<ul>\n<li><strong>macOS</strong>: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li>\n<li><strong>Windows</strong>: <code>%APPDATA%/Claude/claude_desktop_config.json</code></li>\n</ul>\n<p>Add your server to the <code>mcpServers</code> object:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"nt\">&quot;mcpServers&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">        </span><span class=\"nt\">&quot;featurestore&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;command&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&quot;uv&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">            </span><span class=\"nt\">&quot;args&quot;</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">                </span><span class=\"s2\">&quot;run&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                </span><span class=\"s2\">&quot;--with&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                </span><span class=\"s2\">&quot;mcp[cli]&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                </span><span class=\"s2\">&quot;mcp&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                </span><span class=\"s2\">&quot;run&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">                </span><span class=\"s2\">&quot;/ABSOLUTE/PATH/TO/mcp-featurestore/featurestore_server.py&quot;</span>\n<span class=\"w\">            </span><span class=\"p\">]</span>\n<span class=\"w\">        </span><span class=\"p\">}</span>\n<span class=\"w\">    </span><span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<blockquote>\n<p><strong>\u26a0\ufe0f Important</strong>: You must use the <strong>absolute path</strong> to your <code>featurestore_server.py</code> file.</p>\n</blockquote>\n<h3 id=\"52-how-the-interaction-works\">5.2. How the Interaction Works</h3>\n<p>When you ask Claude a question, the following workflow occurs:</p>\n<p><img alt=\"MCP Workflow\" src=\"../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/mcp_workflow.svg\" /></p>\n<ol>\n<li>Claude sees the available tools (<code>get_feature</code>, <code>list_features</code>, etc.).</li>\n<li>It determines that your question requires data from the feature store.</li>\n<li>It constructs a tool call and sends it to your server.</li>\n<li>Your server executes the Python function and returns the result.</li>\n<li>Claude uses that result to answer your question.</li>\n</ol>\n<h3 id=\"53-example-queries\">5.3. Example Queries</h3>\n<p>Restart Claude Desktop and try these prompts:</p>\n<ol>\n<li>\n<p>\"List all available features.\"\n   <img alt=\"Question 2\" src=\"../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/question-2.png\" style=\"width:600px;max-width:100%;height:auto;\" /></p>\n</li>\n<li>\n<p>\"Get the feature vector for user_123.\"\n   <img alt=\"Question 3\" src=\"../../../../assets/2025-06-10-mcp-server-tutorial-with-uv/question-3.png\" style=\"width:600px;max-width:100%;height:auto;\" /></p>\n</li>\n<li>\n<p>\"Store a new feature for 'new_item' with vector [0.5, 0.5] and metadata {'type': 'test'}.\"</p>\n</li>\n</ol>\n<hr />\n<h2 id=\"6-troubleshooting\">6. Troubleshooting</h2>\n<p>If things aren't working, check these common issues:</p>\n<ul>\n<li>\n<p><strong>\"Server connection failed\"</strong>:</p>\n<ul>\n<li>Check the logs: <code>tail -f ~/Library/Logs/Claude/mcp.log</code> (macOS).</li>\n<li>Ensure you used the <strong>absolute path</strong> in the config file.</li>\n<li>Verify <code>uv</code> is in your system PATH or use the full path to the <code>uv</code> binary.</li>\n</ul>\n</li>\n<li>\n<p><strong>\"Tool execution error\"</strong>:</p>\n<ul>\n<li>Use the <strong>Inspector</strong> (<code>uv run mcp dev ...</code>) to debug the specific tool.</li>\n<li>Check if your <code>database.py</code> is creating the <code>features.db</code> file in the correct location.</li>\n</ul>\n</li>\n</ul>\n<hr />\n<h2 id=\"7-conclusion\">7. Conclusion</h2>\n<p>You've just built a functional MCP server that extends Claude's capabilities! This pattern\u2014using <code>FastMCP</code> for the server and <code>uv</code> for execution\u2014is a powerful way to build robust AI tools quickly.</p>\n<h2 id=\"references\">References</h2>\n<ul>\n<li><a href=\"https://github.com/slavadubrov/mcp-featurestore\">The repo of this tutorial example</a></li>\n<li><a href=\"https://www.anthropic.com/news/model-context-protocol\">Introduction to MCP</a></li>\n<li><a href=\"https://github.com/modelcontextprotocol/python-sdk\">MCP Python SDK</a></li>\n<li><a href=\"https://claude.ai/download\">Claude Desktop</a></li>\n<li><a href=\"https://docs.astral.sh/uv/\">uv</a></li>\n</ul>", "image": null, "date_modified": "2025-06-10T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/", "url": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/", "title": "Quick-guide on Local Stable-Diffusion Toolkits for macOS", "content_html": "<h1 id=\"quick-guide-on-local-stable-diffusion-toolkits-for-macos\">Quick-guide on Local Stable-Diffusion Toolkits for macOS</h1>\n<p>Running generative AI models locally is a game-changer. It means <strong>zero cloud costs</strong>, <strong>no censorship</strong>, <strong>total privacy</strong>, and <strong>unlimited experimentation</strong>. Whether you're generating character portraits, architectural concepts, or just having fun, your Mac is more than capable of handling the workload thanks to Apple Silicon.</p>\n<p>But with so many tools available, where do you start?</p>\n<p>Below is a practical guide to the best macOS-ready interfaces. Each tool wraps the same powerful Stable Diffusion models but offers a completely different experience\u2014from \"Apple-like\" simplicity to \"developer-grade\" control.</p>\n<!-- more -->\n\n<h2 id=\"how-these-tools-work\">How these tools work</h2>\n<p>At their core, all these applications do the same thing: they provide a user interface (UI) for the Stable Diffusion models. They handle the complex \"plumbing\"\u2014loading heavy model weights, managing memory, and talking to your Mac's GPU.</p>\n<p><img alt=\"AI Tools Workflow\" src=\"../../../../assets/2025-05-10-guide-ai-image-tools/ai_tools_workflow.svg\" /></p>\n<p>Because they all share the same underlying architecture, you can usually share <strong>model files</strong> (<code>.safetensors</code>) between them. Download a model once, and try it in different apps to see which workflow suits you.</p>\n<h3 id=\"the-apple-silicon-advantage\">The Apple Silicon Advantage</h3>\n<p>Why is the Mac so good for this? It comes down to <strong>Unified Memory</strong>. Unlike a PC with a separate graphics card (where you might have only 8GB or 12GB of VRAM), your Mac's GPU has access to your <em>entire</em> system RAM.</p>\n<p><img alt=\"Apple Silicon Stack\" src=\"../../../../assets/2025-05-10-guide-ai-image-tools/apple_silicon_stack.svg\" /></p>\n<p>This means a MacBook Pro with 32GB or 64GB of RAM can load massive models (like SDXL or Flux) that would bring a typical gaming PC to its knees.</p>\n<hr />\n<h2 id=\"1-draw-things-the-powerhouse-app\">1. Draw Things: The Powerhouse App</h2>\n<ul>\n<li><strong>Download:</strong> <a href=\"https://apps.apple.com/de/app/draw-things-offline-ai-art/id6444050820?l=en-GB\">App Store Link</a></li>\n<li><strong>What it is:</strong> A native iOS/macOS app that feels like a professional design tool. It's surprisingly powerful, supporting ControlNet, Inpainting, and LoRAs right out of the box.</li>\n<li><strong>Best for:</strong> Users who want a <strong>native app experience</strong> (no terminal!) but don't want to sacrifice advanced features.</li>\n<li><strong>Pros:</strong><ul>\n<li><strong>Native Performance:</strong> Highly optimized for Apple Silicon.</li>\n<li><strong>Feature Rich:</strong> Supports Inpainting, Outpainting, ControlNet, and scriptable workflows.</li>\n<li><strong>Offline First:</strong> Runs completely offline after downloading models.</li>\n</ul>\n</li>\n<li><strong>Cons:</strong><ul>\n<li>UI can be a bit dense on smaller screens (it's designed to scale from iPhone to Mac).</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"2-diffusionbee-the-one-click-wonder\">2. DiffusionBee: The \"One-Click\" Wonder</h2>\n<ul>\n<li><strong>Download:</strong> <a href=\"https://diffusionbee.com/download\">diffusionbee.com</a></li>\n<li><strong>What it is:</strong> The simplest way to run Stable Diffusion on a Mac. It strips away all the jargon. You don't \"load a checkpoint\"; you just select a style.</li>\n<li><strong>Best for:</strong> Absolute beginners who just want to make cool images <em>now</em>.</li>\n<li><strong>Pros:</strong><ul>\n<li><strong>Zero Setup:</strong> Download the DMG, drag to Applications, run.</li>\n<li><strong>Clean UI:</strong> Very \"Apple-like\" design.</li>\n<li><strong>Built-in Tools:</strong> Includes simple upscaling and background removal.</li>\n</ul>\n</li>\n<li><strong>Cons:</strong><ul>\n<li><strong>Limited Control:</strong> You can't easily tweak advanced sampler settings or complex node pipelines.</li>\n<li><strong>Slower Updates:</strong> New features (like the latest ControlNet models) take longer to arrive.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-comfyui-the-node-based-lab\">3. ComfyUI: The Node-Based Lab</h2>\n<ul>\n<li><strong>Download:</strong> <a href=\"https://www.comfy.org/download\">comfy.org</a></li>\n<li><strong>What it is:</strong> A visual programming environment. Instead of sliders, you connect \"nodes\" with wires to build your image generation pipeline.</li>\n<li><strong>Best for:</strong> Power users, technical artists, and anyone who wants to understand <em>exactly</em> how the image is being made.</li>\n<li><strong>Pros:</strong><ul>\n<li><strong>Ultimate Control:</strong> Build custom workflows for specific tasks (e.g., \"Generate image -&gt; Upscale -&gt; Face Restore\").</li>\n<li><strong>Speed:</strong> Often faster than other UIs because it executes only what's needed.</li>\n<li><strong>Ecosystem:</strong> Thousands of custom nodes created by the community.</li>\n</ul>\n</li>\n<li><strong>Cons:</strong><ul>\n<li><strong>Steep Learning Curve:</strong> It looks like a bowl of spaghetti until you learn to read it.</li>\n<li><strong>Setup:</strong> Requires some comfort with Python/Terminal (though installers exist).</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"4-stable-diffusion-webui-automatic1111\">4. Stable Diffusion WebUI (AUTOMATIC1111)</h2>\n<ul>\n<li><strong>Install Guide:</strong> <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon\">Installation on Apple Silicon</a></li>\n<li><strong>What it is:</strong> The \"Swiss Army Knife\" of Stable Diffusion. It runs in your browser and has a tab for everything.</li>\n<li><strong>Best for:</strong> Enthusiasts who want to use the latest community extensions immediately.</li>\n<li><strong>Pros:</strong><ul>\n<li><strong>Extensions:</strong> If a new AI technique is released today, A1111 will have an extension for it tomorrow.</li>\n<li><strong>Tutorials:</strong> The vast majority of YouTube tutorials use this interface.</li>\n</ul>\n</li>\n<li><strong>Cons:</strong><ul>\n<li><strong>Clunky UI:</strong> It's functional but chaotic.</li>\n<li><strong>Heavy:</strong> Can be slower and more memory-hungry than ComfyUI or Draw Things.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"5-fooocus-midjourney-on-your-mac\">5. Fooocus: Midjourney on Your Mac</h2>\n<ul>\n<li><strong>Repo:</strong> <a href=\"https://github.com/lllyasviel/Fooocus\">github.com/lllyasviel/Fooocus</a></li>\n<li><strong>What it is:</strong> An interface designed to mimic the ease of Midjourney. It automates all the technical choices (samplers, steps, refiners) so you can focus on the prompt.</li>\n<li><strong>Best for:</strong> High-quality results with minimal tweaking.</li>\n<li><strong>Pros:</strong><ul>\n<li><strong>Smart Defaults:</strong> It \"just works\" and produces beautiful images.</li>\n<li><strong>Minimalist:</strong> No overwhelming sliders.</li>\n</ul>\n</li>\n<li><strong>Cons:</strong><ul>\n<li><strong>Less Customization:</strong> Harder to force it to do something specific if it fights you.</li>\n<li><strong>Performance:</strong> Often optimized for NVIDIA GPUs first, so Mac performance can vary.</li>\n</ul>\n</li>\n</ul>\n<hr />\n<h2 id=\"comparison-table\">Comparison Table</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Tool</th>\n<th style=\"text-align: left;\">Install Difficulty</th>\n<th style=\"text-align: left;\">Interface Style</th>\n<th style=\"text-align: left;\">Best For</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>Draw Things</strong></td>\n<td style=\"text-align: left;\">\u2605\u2606\u2606\u2606\u2606 (App Store)</td>\n<td style=\"text-align: left;\">Native App (Pro)</td>\n<td style=\"text-align: left;\"><strong>The Sweet Spot</strong> (Power + Ease)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>DiffusionBee</strong></td>\n<td style=\"text-align: left;\">\u2605\u2606\u2606\u2606\u2606 (DMG)</td>\n<td style=\"text-align: left;\">Native App (Simple)</td>\n<td style=\"text-align: left;\"><strong>Beginners</strong> &amp; Casual Use</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>ComfyUI</strong></td>\n<td style=\"text-align: left;\">\u2605\u2605\u2605\u2606\u2606 (Python)</td>\n<td style=\"text-align: left;\">Node Graph</td>\n<td style=\"text-align: left;\"><strong>Complex Workflows</strong> &amp; Automation</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>A1111 WebUI</strong></td>\n<td style=\"text-align: left;\">\u2605\u2605\u2605\u2605\u2606 (Terminal)</td>\n<td style=\"text-align: left;\">Browser Dashboard</td>\n<td style=\"text-align: left;\"><strong>Extensions</strong> &amp; Community Support</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Fooocus</strong></td>\n<td style=\"text-align: left;\">\u2605\u2605\u2605\u2606\u2606 (Python)</td>\n<td style=\"text-align: left;\">Minimalist</td>\n<td style=\"text-align: left;\"><strong>Midjourney-style</strong> Prompting</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"decision-flowchart\">Decision Flowchart</h2>\n<p>Not sure which one to pick? Follow this path:</p>\n<p><img alt=\"Decision Tree\" src=\"../../../../assets/2025-05-10-guide-ai-image-tools/ai_tool_decision_tree.svg\" /></p>\n<hr />\n<h2 id=\"essential-tips-for-mac-users\">Essential Tips for Mac Users</h2>\n<h3 id=\"1-system-requirements\">1. System Requirements</h3>\n<ul>\n<li><strong>RAM is King:</strong><ul>\n<li><strong>8GB:</strong> Doable for basic 512x512 images, but expect slowness and crashes with newer models (SDXL).</li>\n<li><strong>16GB:</strong> The comfortable minimum. You can run most things, including SDXL.</li>\n<li><strong>32GB+:</strong> The dream. You can keep multiple models loaded and multitask while generating.</li>\n</ul>\n</li>\n<li><strong>Storage:</strong> AI models are huge (2GB - 6GB each). Get an external SSD if your Mac is low on space.</li>\n</ul>\n<h3 id=\"2-where-to-get-models\">2. Where to get Models</h3>\n<p>The software is just the engine; you need fuel (models).</p>\n<ul>\n<li><strong><a href=\"https://civitai.com\">Civitai</a>:</strong> The largest community for models. Look for \"Checkpoints\" that are compatible with SD 1.5 or SDXL.</li>\n<li><strong><a href=\"https://huggingface.co\">Hugging Face</a>:</strong> The \"GitHub of AI\". More technical, but the official source for base models from Stability AI.</li>\n<li><strong>File Types:</strong> Always look for <code>.safetensors</code> files. Avoid <code>.ckpt</code> files if possible, as they can theoretically contain malicious code (though rare).</li>\n</ul>\n<h3 id=\"3-start-simple\">3. Start Simple</h3>\n<p>Don't try to install ComfyUI on day one. Start with <strong>DiffusionBee</strong> or <strong>Draw Things</strong>. Get a feel for how prompting works. Once you hit a wall (\"I wish I could control the pose of this character...\"), <em>then</em> look into ControlNet and more advanced tools.</p>", "image": null, "date_modified": "2025-05-10T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/", "url": "https://slavadubrov.github.io/blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/", "title": "The Ultimate Guide to `pyproject.toml`", "content_html": "<h1 id=\"the-ultimate-guide-to-pyprojecttoml\">The Ultimate Guide to <code>pyproject.toml</code></h1>\n<h2 id=\"tldr\">TL;DR</h2>\n<p>Think of <code>pyproject.toml</code> as the <strong><code>package.json</code> for Python</strong>. It's a single configuration file that holds your project's metadata, dependencies, and tool settings. Whether you use <code>.venv</code>, <code>pyenv</code>, or <code>uv</code>, this one file simplifies development and makes collaboration smoother.</p>\n<!-- more -->\n\n<h2 id=\"what-is-pyprojecttoml\">What is <code>pyproject.toml</code>?</h2>\n<p><code>pyproject.toml</code> is a standardized configuration file that lives at the root of your Python project. It uses the TOML format (think INI files but better) and is backed by official Python Enhancement Proposals (PEPs).</p>\n<p>The file evolved in two key stages:</p>\n<ul>\n<li><strong><a href=\"https://peps.python.org/pep-0518/\">PEP 518</a></strong> (2016) introduced the <code>[build-system]</code> table so build tools could declare their requirements in a standard way.</li>\n<li><strong><a href=\"https://peps.python.org/pep-0621/\">PEP 621</a></strong> (2020) added the <code>[project]</code> table for core package metadata\u2014name, version, dependencies, and more.</li>\n</ul>\n<p>Today, most Python developer tools (Black, isort, pytest, Ruff, mypy) read their configuration from <code>[tool.*]</code> sections in this file, making it the central hub for your entire project setup.</p>\n<p><img alt=\"Structure\" src=\"../../../../assets/2025-05-08-guide-pyproject-toml/pyproject_structure.svg\" /></p>\n<h2 id=\"why-should-you-care\">Why should you care?</h2>\n<h3 id=\"1-one-file-to-rule-them-all\">1. One file to rule them all</h3>\n<p>Before <code>pyproject.toml</code>, you'd juggle <code>setup.py</code>, <code>setup.cfg</code>, <code>requirements.txt</code>, <code>MANIFEST.in</code>, and various dotfiles (<code>.flake8</code>, <code>.coveragerc</code>). Now everything lives in one place.</p>\n<h3 id=\"2-backend-agnostic-builds\">2. Backend-agnostic builds</h3>\n<p>When you run <code>pip install .</code>, pip reads <code>pyproject.toml</code> and automatically installs whatever build tools your project needs (setuptools, flit, hatchling, etc.). You are no longer tied to <code>setuptools</code>.</p>\n<h3 id=\"3-universal-tool-configuration\">3. Universal tool configuration</h3>\n<p>Linters, formatters, test runners, and type checkers all know to look here for their settings. Your IDE, CI pipeline, and teammates all read from the same source of truth.</p>\n<p><img alt=\"Tool Ecosystem\" src=\"../../../../assets/2025-05-08-guide-pyproject-toml/tool_ecosystem.svg\" /></p>\n<h2 id=\"anatomy-of-a-pyprojecttoml\">Anatomy of a <code>pyproject.toml</code></h2>\n<p>Here's what a typical file looks like with the three main sections:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># 1. Build system - tells pip/build how to package your project</span>\n<span class=\"k\">[build-system]</span>\n<span class=\"n\">requires</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">&quot;hatchling&quot;</span><span class=\"p\">]</span>\n<span class=\"n\">build-backend</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;hatchling.build&quot;</span>\n\n<span class=\"c1\"># 2. Project metadata and dependencies</span>\n<span class=\"k\">[project]</span>\n<span class=\"n\">name</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;awesome-app&quot;</span>\n<span class=\"n\">version</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;0.1.0&quot;</span>\n<span class=\"n\">description</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;Short demo of pyproject.toml&quot;</span>\n<span class=\"n\">readme</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;README.md&quot;</span>\n<span class=\"n\">requires-python</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;&gt;=3.12&quot;</span>\n<span class=\"n\">dependencies</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">[</span>\n<span class=\"w\">  </span><span class=\"s2\">&quot;fastapi&gt;=0.111&quot;</span><span class=\"p\">,</span>\n<span class=\"w\">  </span><span class=\"s2\">&quot;uvicorn[standard]&gt;=0.30&quot;</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># Expose CLI commands</span>\n<span class=\"k\">[project.scripts]</span>\n<span class=\"n\">awesome-cli</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;awesome_app.cli:main&quot;</span>\n\n<span class=\"c1\"># Optional dependencies (e.g., for development)</span>\n<span class=\"k\">[project.optional-dependencies]</span>\n<span class=\"n\">dev</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">&quot;pytest&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">&quot;ruff&quot;</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">&quot;mypy&quot;</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># 3. Tool configuration</span>\n<span class=\"k\">[tool.ruff]</span>\n<span class=\"n\">line-length</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">100</span>\n<span class=\"n\">target-version</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;py312&quot;</span>\n\n<span class=\"k\">[tool.pytest.ini_options]</span>\n<span class=\"n\">addopts</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;-ra -q&quot;</span>\n<span class=\"n\">testpaths</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">&quot;tests&quot;</span><span class=\"p\">]</span>\n</code></pre></div>\n<h3 id=\"breaking-it-down\">Breaking it down</h3>\n<h4 id=\"build-system\"><code>[build-system]</code></h4>\n<p>Required if you want to package/distribute your project. Tells pip and build tools (like <code>python -m build</code>) which backend to use.</p>\n<h4 id=\"project\"><code>[project]</code></h4>\n<p>Your package metadata. This is where dependencies live instead of <code>requirements.txt</code>.</p>\n<ul>\n<li><strong><code>dependencies</code></strong>: The runtime requirements for your package.</li>\n<li><strong><code>optional-dependencies</code></strong>: Groups of extra dependencies (e.g., <code>dev</code>, <code>test</code>, <code>docs</code>).</li>\n<li><strong><code>scripts</code></strong>: Creates executable commands. In the example above, installing the package creates an <code>awesome-cli</code> command that runs the <code>main</code> function in <code>awesome_app/cli.py</code>.</li>\n</ul>\n<h4 id=\"tool\"><code>[tool.*]</code></h4>\n<p>Configuration for any tool that supports it. Each tool gets its own namespace (e.g., <code>[tool.pytest.ini_options]</code>, <code>[tool.mypy]</code>).</p>\n<h2 id=\"does-it-replace-requirementstxt\">Does it replace <code>requirements.txt</code>?</h2>\n<p><strong>In modern workflows, yes.</strong> Tools like <a href=\"https://python-poetry.org/\">Poetry</a>, <a href=\"https://pdm-project.org/\">PDM</a>, <a href=\"https://hatch.pypa.io/\">Hatch</a>, and <a href=\"https://github.com/astral-sh/uv\">uv</a> store dependencies directly in the <code>[project]</code> section and generate lockfiles for reproducibility.</p>\n<p>You only need <code>requirements.txt</code> if:</p>\n<ul>\n<li>You're working with legacy deployment systems that expect it.</li>\n<li>You have simple CI scripts that haven't been updated.</li>\n</ul>\n<p>Most modern tools can export a <code>requirements.txt</code> from your <code>pyproject.toml</code> when needed:</p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span><span class=\"nb\">export</span><span class=\"w\"> </span>&gt;<span class=\"w\"> </span>requirements.txt\n</code></pre></div>\n<h2 id=\"choosing-a-build-backend\">Choosing a Build Backend</h2>\n<p>One of the confusing parts of <code>pyproject.toml</code> is choosing a build backend. Here is a quick comparison:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">Backend</th>\n<th style=\"text-align: left;\">Best For</th>\n<th style=\"text-align: left;\">Pros</th>\n<th style=\"text-align: left;\">Cons</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>Hatchling</strong></td>\n<td style=\"text-align: left;\">Modern standard</td>\n<td style=\"text-align: left;\">Fast, extensible, supports plugins</td>\n<td style=\"text-align: left;\">Newer, less legacy support</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Flit</strong></td>\n<td style=\"text-align: left;\">Simple packages</td>\n<td style=\"text-align: left;\">Extremely simple, zero config</td>\n<td style=\"text-align: left;\">Not for complex builds (C extensions)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Setuptools</strong></td>\n<td style=\"text-align: left;\">Legacy / Complex</td>\n<td style=\"text-align: left;\">Supports everything (C extensions, etc.)</td>\n<td style=\"text-align: left;\">Slower, complex configuration</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>Poetry</strong></td>\n<td style=\"text-align: left;\">Poetry users</td>\n<td style=\"text-align: left;\">Integrated with Poetry ecosystem</td>\n<td style=\"text-align: left;\">Locked into Poetry workflow</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Recommendation:</strong> Use <strong>Hatchling</strong> for new pure-Python projects. It's the default for <code>uv</code> and is becoming the industry standard.</p>\n<h2 id=\"migrating-an-existing-project\">Migrating an existing project</h2>\n<p>If you have a legacy Python project, here's how to modernize it:</p>\n<p><img alt=\"Migration Flow\" src=\"../../../../assets/2025-05-08-guide-pyproject-toml/migration_flow.svg\" /></p>\n<p><strong>Step-by-step:</strong></p>\n<ol>\n<li><strong>Add <code>[build-system]</code></strong> - Start with setuptools if you're not sure: <code>requires = [\"setuptools&gt;=61\", \"wheel\"]</code>.</li>\n<li><strong>Move to <code>[project]</code></strong> - Transfer name, version, dependencies from <code>setup.py</code> or <code>setup.cfg</code>.</li>\n<li><strong>Convert dev dependencies</strong> - Put them in <code>[project.optional-dependencies].dev</code>.</li>\n<li><strong>Configure tools</strong> - Add <code>[tool.*]</code> sections for Black, pytest, mypy, etc.</li>\n<li><strong>Handle <code>requirements.txt</code></strong> - Either drop it or generate it from lockfile for legacy systems.</li>\n</ol>\n<p>After migration, you can delete <code>setup.py</code>, <code>setup.cfg</code>, and most config dotfiles.</p>\n<h2 id=\"advanced-features\">Advanced Features</h2>\n<h3 id=\"cli-entry-points\">CLI Entry Points</h3>\n<p>Instead of the old <code>console_scripts</code> in <code>setup.py</code>, use <code>[project.scripts]</code>:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">[project.scripts]</span>\n<span class=\"n\">my-tool</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">&quot;my_package.main:run&quot;</span>\n</code></pre></div>\n<p>When a user installs your package, they can type <code>my-tool</code> in their terminal.</p>\n<h3 id=\"workspaces-monorepos\">Workspaces (Monorepos)</h3>\n<p>Tools like <code>uv</code> and <code>hatch</code> support workspaces, allowing you to manage multiple packages in a single repo.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"k\">[tool.uv.workspace]</span>\n<span class=\"n\">members</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">&quot;packages/*&quot;</span><span class=\"p\">]</span>\n</code></pre></div>\n<p>This allows you to develop multiple interdependent packages and install them all into a single virtual environment for testing.</p>\n<h2 id=\"typical-workflows-with-uv\">Typical Workflows with <code>uv</code></h2>\n<h3 id=\"starting-a-new-project\">Starting a new project</h3>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>init<span class=\"w\"> </span>my_app<span class=\"w\">          </span><span class=\"c1\"># creates folder with pyproject.toml and .venv</span>\n<span class=\"nb\">cd</span><span class=\"w\"> </span>my_app\nuv<span class=\"w\"> </span>add<span class=\"w\"> </span>requests<span class=\"w\"> </span>fastapi<span class=\"w\"> </span><span class=\"c1\"># adds to [project.dependencies] and installs</span>\nuv<span class=\"w\"> </span>run<span class=\"w\"> </span>pytest<span class=\"w\">           </span><span class=\"c1\"># runs tests in the venv</span>\n</code></pre></div>\n<h3 id=\"running-scripts\">Running scripts</h3>\n<p>You can define scripts in <code>pyproject.toml</code> (if using a task runner like <code>poe</code> or <code>hatch</code>) or just use <code>uv run</code>:</p>\n<div class=\"highlight\"><pre><span></span><code>uv<span class=\"w\"> </span>run<span class=\"w\"> </span>python<span class=\"w\"> </span>main.py\n</code></pre></div>\n<h2 id=\"best-practices\">Best Practices</h2>\n<ol>\n<li><strong>Don't pin exact versions in libraries</strong>: Use ranges (e.g., <code>requests&gt;=2.30</code>) so your library doesn't conflict with others.</li>\n<li><strong>Do pin versions in applications</strong>: Use a lockfile (<code>uv.lock</code> or <code>poetry.lock</code>) to ensure reproducible builds.</li>\n<li><strong>Group dev dependencies</strong>: Keep testing, linting, and docs dependencies in separate optional groups (e.g., <code>dev</code>, <code>test</code>, <code>docs</code>).</li>\n<li><strong>Keep it clean</strong>: Don't dump every possible config option in there. Stick to project-wide defaults.</li>\n</ol>\n<hr />\n<p><strong>Bottom line:</strong> <code>pyproject.toml</code> brings Python's project setup into the modern era. Whether you're packaging a library, managing dependencies, or configuring tools, this one file is your command center. Start with <code>uv</code> for the smoothest experience, or integrate it into your existing workflow gradually.</p>", "image": null, "date_modified": "2025-05-08T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Python"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/", "url": "https://slavadubrov.github.io/blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/", "title": "Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc \ud83d\ude80", "content_html": "<h1 id=\"mastering-zsh-startup-zprofile-vs-zshrc\">Mastering Zsh Startup: <code>~/.zprofile</code> vs <code>~/.zshrc</code> \ud83d\ude80</h1>\n<p>If you've ever wondered why your terminal feels slow, or why your environment variables aren't loading where you expect them to, you're likely battling the Zsh startup order.</p>\n<p>The distinction between <code>~/.zprofile</code> and <code>~/.zshrc</code> is one of the most common sources of confusion for developers moving to Zsh (especially on macOS).</p>\n<h2 id=\"tldr\">TL;DR \u26a1</h2>\n<ul>\n<li><strong><code>~/.zprofile</code></strong> is for <strong>Environment Setup</strong>. It runs <em>once</em> when you log in (or open a terminal tab on macOS). Put your <code>PATH</code>, <code>EDITOR</code>, and language version managers (like <code>fnm</code>, <code>pyenv</code>) here.</li>\n<li><strong><code>~/.zshrc</code></strong> is for <strong>Interactive Configuration</strong>. It runs <em>every time</em> you start a new shell instance. Put your aliases, prompt themes, and key bindings here.</li>\n</ul>\n<!-- more -->\n\n<p><img alt=\"Zsh Config Structure\" src=\"../../../../assets/2025-05-07-zshrc-vs-zprofile/zsh_config_structure.svg\" /></p>\n<hr />\n<h2 id=\"the-shell-startup-flow\">The Shell Startup Flow \ud83d\udc1a</h2>\n<p>To understand where to put things, you need to understand <em>when</em> files are loaded. Zsh has a specific hierarchy of configuration files.</p>\n<h3 id=\"login-vs-interactive-shells\">Login vs. Interactive Shells</h3>\n<ol>\n<li><strong>Login Shell</strong>: The first shell you enter after authentication. On macOS, <strong>every new terminal tab or window is a login shell by default</strong>. This is a key difference from Linux, where opening a terminal usually starts a <em>non-login</em> interactive shell.</li>\n<li><strong>Interactive Shell</strong>: Any shell where you can type commands.</li>\n</ol>\n<p>Here is the actual flow of execution when you open a terminal on macOS:</p>\n<p><img alt=\"Zsh Startup Flow\" src=\"../../../../assets/2025-05-07-zshrc-vs-zprofile/zsh_startup_flow.svg\" /></p>\n<h3 id=\"the-loading-order\">The Loading Order</h3>\n<ol>\n<li><strong><code>~/.zshenv</code></strong>: (Optional) Runs for <strong>every</strong> shell script and command. <strong>Avoid putting output or heavy logic here</strong>, as it can break scripts. Use it only for essential environment variables that <em>must</em> exist everywhere (rarely needed for average users).</li>\n<li><strong><code>~/.zprofile</code></strong>: Runs only for <strong>login shells</strong>. This is your \"setup\" phase.</li>\n<li><strong><code>~/.zshrc</code></strong>: Runs for <strong>interactive shells</strong>. This is your \"customization\" phase.</li>\n<li><strong><code>~/.zlogin</code></strong>: (Optional) Runs at the very end of a login shell startup.</li>\n</ol>\n<hr />\n<h2 id=\"what-goes-where\">What Goes Where? \ud83d\udcc1</h2>\n<h3 id=\"1-zprofile-the-environment-layer\">1. <code>~/.zprofile</code>: The Environment Layer \ud83c\udf0d</h3>\n<p>Think of this as the foundation of your house. It sets up the rules of physics (paths, variables) that everything else relies on.</p>\n<p><strong>What belongs here:</strong></p>\n<ul>\n<li><strong><code>PATH</code> modifications</strong>: Adding directories to your executable path.</li>\n<li><strong>Environment Variables</strong>: <code>EDITOR</code>, <code>LANG</code>, <code>GOPATH</code>, <code>JAVA_HOME</code>.</li>\n<li><strong>Tool Initialization</strong>: Things that modify the environment, like <code>pyenv</code>, <code>rbenv</code>, <code>fnm</code>, or <code>cargo</code>.</li>\n</ul>\n<p><strong>Why?</strong>\nThese only need to be calculated once. If you put them in <code>.zshrc</code>, they will be re-calculated every time you open a sub-shell or run a script, which is wasteful and can lead to duplicate entries in your <code>PATH</code>.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># ~/.zprofile</span>\n\n<span class=\"c1\"># 1. Set up your PATH</span>\n<span class=\"c1\"># Ensure local bin is first so your tools override system ones</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"s2\">&quot;</span><span class=\"nv\">$HOME</span><span class=\"s2\">/.local/bin:/opt/homebrew/bin:</span><span class=\"nv\">$PATH</span><span class=\"s2\">&quot;</span>\n\n<span class=\"c1\"># 2. Set global variables</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">EDITOR</span><span class=\"o\">=</span><span class=\"s2\">&quot;nvim&quot;</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">VISUAL</span><span class=\"o\">=</span><span class=\"s2\">&quot;nvim&quot;</span>\n<span class=\"nb\">export</span><span class=\"w\"> </span><span class=\"nv\">LANG</span><span class=\"o\">=</span><span class=\"s2\">&quot;en_US.UTF-8&quot;</span>\n\n<span class=\"c1\"># 3. Initialize Version Managers (The Heavy Lifters)</span>\n<span class=\"c1\"># Doing this here keeps your shell startup snappy!</span>\n<span class=\"nb\">eval</span><span class=\"w\"> </span><span class=\"s2\">&quot;</span><span class=\"k\">$(</span>fnm<span class=\"w\"> </span>env<span class=\"w\"> </span>--use-on-cd<span class=\"k\">)</span><span class=\"s2\">&quot;</span>\n<span class=\"nb\">eval</span><span class=\"w\"> </span><span class=\"s2\">&quot;</span><span class=\"k\">$(</span>pyenv<span class=\"w\"> </span>init<span class=\"w\"> </span>-<span class=\"k\">)</span><span class=\"s2\">&quot;</span>\n</code></pre></div>\n<h3 id=\"2-zshrc-the-interactive-layer\">2. <code>~/.zshrc</code>: The Interactive Layer \ud83c\udfae</h3>\n<p>Think of this as the interior decoration. It makes the house comfortable to live in.</p>\n<p><strong>What belongs here:</strong></p>\n<ul>\n<li><strong>Aliases</strong>: <code>alias g='git'</code>.</li>\n<li><strong>Prompt</strong>: Starship, Powerlevel10k, or Pure.</li>\n<li><strong>Completions</strong>: <code>compinit</code>.</li>\n<li><strong>Key Bindings</strong>: <code>bindkey</code>.</li>\n<li><strong>Shell Options</strong>: <code>setopt autocd</code>, <code>setopt histignorealldups</code>.</li>\n</ul>\n<p><strong>Why?</strong>\nThese settings only matter when a human is typing at the keyboard. A script running in the background doesn't need your fancy prompt or your git aliases.</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># ~/.zshrc</span>\n\n<span class=\"c1\"># 1. Load your prompt (Visuals)</span>\nautoload<span class=\"w\"> </span>-Uz<span class=\"w\"> </span>promptinit<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>promptinit\nprompt<span class=\"w\"> </span>pure\n\n<span class=\"c1\"># 2. Aliases (Shortcuts)</span>\n<span class=\"nb\">alias</span><span class=\"w\"> </span><span class=\"nv\">ll</span><span class=\"o\">=</span><span class=\"s1\">&#39;ls -lah&#39;</span>\n<span class=\"nb\">alias</span><span class=\"w\"> </span><span class=\"nv\">g</span><span class=\"o\">=</span><span class=\"s1\">&#39;git&#39;</span>\n<span class=\"nb\">alias</span><span class=\"w\"> </span><span class=\"nv\">gs</span><span class=\"o\">=</span><span class=\"s1\">&#39;git status&#39;</span>\n\n<span class=\"c1\"># 3. Shell Options (Behavior)</span>\nsetopt<span class=\"w\"> </span>autocd<span class=\"w\">              </span><span class=\"c1\"># cd by just typing directory name</span>\nsetopt<span class=\"w\"> </span>histignorealldups<span class=\"w\">   </span><span class=\"c1\"># Don&#39;t record duplicate history entries</span>\nsetopt<span class=\"w\"> </span>share_history<span class=\"w\">       </span><span class=\"c1\"># Share history between tabs</span>\n\n<span class=\"c1\"># 4. Completions (The Magic)</span>\nautoload<span class=\"w\"> </span>-Uz<span class=\"w\"> </span>compinit<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>compinit\n</code></pre></div>\n<hr />\n<h2 id=\"why-not-just-put-everything-in-one-file\">Why Not Just Put Everything in One File? \ud83e\udd14</h2>\n<p>You might be asking: <em>\"Why can't I just put my aliases in <code>.zprofile</code> and run them once? Why do I need to reload them?\"</em></p>\n<p>It comes down to <strong>Inheritance</strong> vs. <strong>Re-definition</strong>.</p>\n<h3 id=\"1-environment-variables-inherit\">1. Environment Variables Inherit \ud83e\uddec</h3>\n<p>When you set <code>export EDITOR=\"vim\"</code> in a parent shell (like your login shell), every child process (sub-shells, scripts, programs) inherits that variable. You set it once, and it propagates everywhere. This is why <code>.zprofile</code> is perfect for <code>export</code>.</p>\n<h3 id=\"2-aliases-and-functions-do-not-inherit\">2. Aliases and Functions Do Not Inherit \ud83d\udeab</h3>\n<p>Aliases (<code>alias g='git'</code>) and shell functions are <strong>local</strong> to the current shell instance. They are <em>not</em> passed down to child shells.</p>\n<ul>\n<li>If you define an alias in <code>.zprofile</code>, it exists in your top-level login shell.</li>\n<li>If you then type <code>zsh</code> to start a sub-shell, or run a script, that alias <strong>disappears</strong>.</li>\n<li>To make aliases available everywhere, you <em>must</em> re-define them in every new interactive shell. That is exactly what <code>.zshrc</code> does.</li>\n</ul>\n<h3 id=\"3-scripts-dont-need-human-features\">3. Scripts Don't Need \"Human\" Features \ud83e\udd16</h3>\n<p>When you run a shell script (e.g., <code>./deploy.sh</code>), it starts a new, non-interactive shell.</p>\n<ul>\n<li>It doesn't need your fancy prompt.</li>\n<li>It doesn't need your <code>git</code> aliases.</li>\n<li>It definitely doesn't want to wait for <code>oh-my-zsh</code> to load.</li>\n</ul>\n<p>By keeping interactive config in <code>.zshrc</code>, you ensure that your scripts run fast and clean, without being polluted by your personal customization.</p>\n<hr />\n<h2 id=\"common-pitfalls-best-practices\">Common Pitfalls &amp; Best Practices \ud83d\udeab</h2>\n<h3 id=\"pitfall-1-putting-nvm-or-pyenv-in-zshrc\">\ud83d\uded1 Pitfall 1: Putting <code>nvm</code> or <code>pyenv</code> in <code>.zshrc</code></h3>\n<p><strong>The Symptom:</strong> You open a new terminal tab, and it takes 2-3 seconds before you can type anything.\n<strong>The Cause:</strong> Version managers often have heavy initialization scripts. If you put them in <code>.zshrc</code>, they run every single time.\n<strong>The Fix:</strong> Move them to <code>~/.zprofile</code>.</p>\n<h3 id=\"pitfall-2-growing-path\">\ud83d\uded1 Pitfall 2: Growing <code>PATH</code></h3>\n<p><strong>The Symptom:</strong> Your <code>$PATH</code> variable has the same directories listed 5 times.\n<strong>The Cause:</strong> You have <code>export PATH=\"$HOME/bin:$PATH\"</code> in your <code>.zshrc</code>. Every time you reload the config (<code>source ~/.zshrc</code>) or open a sub-shell, it appends the path again.\n<strong>The Fix:</strong> Move <code>PATH</code> definitions to <code>~/.zprofile</code>.</p>\n<h3 id=\"pro-tip-the-reload-trick\">\ud83d\udca1 Pro Tip: The \"Reload\" Trick</h3>\n<p>If you make changes to <code>~/.zprofile</code>, they won't apply to your current shell immediately because <code>.zprofile</code> is only read at login.\nYou have two options:</p>\n<ol>\n<li>Close the tab and open a new one (easiest).</li>\n<li>Manually source it: <code>source ~/.zprofile</code>.</li>\n</ol>\n<p>For <code>.zshrc</code> changes, you can always just run:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"nb\">source</span><span class=\"w\"> </span>~/.zshrc\n</code></pre></div>\n<hr />\n<h2 id=\"a-robust-configuration-strategy\">A Robust Configuration Strategy \ud83d\udee0\ufe0f</h2>\n<p>If you use multiple machines (e.g., macOS at work, Linux at home), you might want a setup that handles both gracefully.</p>\n<p>Since Linux terminals often start as <em>non-login</em> shells, they might skip <code>~/.zprofile</code>. A common pattern to support both is to source <code>.zprofile</code> from <code>.zshrc</code> if it hasn't been loaded.</p>\n<p><strong>In your <code>~/.zshrc</code>:</strong></p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"c1\"># ~/.zshrc</span>\n\n<span class=\"c1\"># If we are on Linux/Non-login shell, ensure environment is set</span>\n<span class=\"k\">if</span><span class=\"w\"> </span><span class=\"o\">[[</span><span class=\"w\"> </span>-o<span class=\"w\"> </span>interactive<span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span>!<span class=\"w\"> </span>-o<span class=\"w\"> </span>login<span class=\"w\"> </span><span class=\"o\">]]</span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"k\">then</span>\n<span class=\"w\">    </span><span class=\"o\">[[</span><span class=\"w\"> </span>-f<span class=\"w\"> </span>~/.zprofile<span class=\"w\"> </span><span class=\"o\">]]</span><span class=\"w\"> </span><span class=\"o\">&amp;&amp;</span><span class=\"w\"> </span><span class=\"nb\">source</span><span class=\"w\"> </span>~/.zprofile\n<span class=\"k\">fi</span>\n\n<span class=\"c1\"># ... rest of your interactive config</span>\n</code></pre></div>\n<h2 id=\"summary\">Summary</h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">File</th>\n<th style=\"text-align: left;\">Purpose</th>\n<th style=\"text-align: left;\">Examples</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong><code>~/.zshenv</code></strong></td>\n<td style=\"text-align: left;\"><strong>Critical Env Vars</strong></td>\n<td style=\"text-align: left;\"><code>ZDOTDIR</code> (Advanced users only)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong><code>~/.zprofile</code></strong></td>\n<td style=\"text-align: left;\"><strong>Environment Setup</strong></td>\n<td style=\"text-align: left;\"><code>PATH</code>, <code>EDITOR</code>, <code>eval \"$(pyenv init -)\"</code></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong><code>~/.zshrc</code></strong></td>\n<td style=\"text-align: left;\"><strong>Interactive Config</strong></td>\n<td style=\"text-align: left;\"><code>alias</code>, <code>prompt</code>, <code>bindkey</code>, <code>compinit</code></td>\n</tr>\n</tbody>\n</table>\n<p>Keep your <strong>environment</strong> in <code>.zprofile</code> and your <strong>experience</strong> in <code>.zshrc</code>, and you'll have a fast, clean, and reliable terminal experience.</p>", "image": null, "date_modified": "2025-05-07T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/", "url": "https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/", "title": "MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond", "content_html": "<h1 id=\"mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond\">MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</h1>\n<p>The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models. From giant language models like GPT-4 to image diffusion models like Stable Diffusion, these powerful models have fundamentally changed how we build and operate ML systems.</p>\n<p>In this post, I'll explore how ML infrastructure and MLOps practices have evolved to support foundation models. We'll contrast the \"classic\" era of MLOps with modern paradigms, examine what's changed, and look at the new patterns and workflows that have emerged. Think of it as upgrading from a standard toolbox to a fully automated factory\u2014the principles are similar, but the scale and complexity are on a different level.</p>\n<!-- more -->\n\n<h2 id=\"1-the-mlops-landscape-a-few-years-ago-pre-foundation-model-era\">1. The MLOps Landscape a Few Years Ago (Pre-Foundation Model Era)</h2>\n<p>A few years back, MLOps primarily meant applying DevOps principles to machine learning. The goal was simple: automate the model lifecycle from data preparation to deployment and monitoring.</p>\n<p>Back then, ML systems were built around relatively smaller models, often trained from scratch on domain-specific data. Here's what the \"classic\" MLOps era looked like:</p>\n<p><img alt=\"Classic MLOps Pipeline\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/classic_mlops_pipeline.svg\" /></p>\n<h3 id=\"11-end-to-end-pipelines\">1.1. End-to-End Pipelines</h3>\n<p>Teams built end-to-end pipelines for data extraction, training, validation, and deployment. Apache Airflow orchestrated ETL and training workflows, while CI/CD systems ran automated tests and pushed models to production. The focus was on reproducibility and automation: package models in Docker containers, deploy them as REST microservices or batch jobs, and keep everything running smoothly.</p>\n<h3 id=\"12-experiment-tracking-and-model-versioning\">1.2. Experiment Tracking and Model Versioning</h3>\n<p>Managing experiments and versions was critical. Platforms like MLflow and Weights &amp; Biases (W&amp;B) became popular for logging training runs, hyperparameters, and metrics. Data scientists could compare experiments and reliably reproduce results. Models were registered in model registries with version numbers, making rollbacks straightforward when a new model underperformed.</p>\n<h3 id=\"13-continuous-training-cicd\">1.3. Continuous Training &amp; CI/CD</h3>\n<p>Classic MLOps pipelines emphasized continuous integration of new data and models. A typical pipeline might retrain a model nightly or weekly as new data arrived, run a battery of tests, and if tests passed, deploy the new model automatically. Automation tools like Jenkins and GitLab CI/CD ensured that any change in data or code would trigger the pipeline reliably.</p>\n<h3 id=\"14-infrastructure-and-serving\">1.4. Infrastructure and Serving</h3>\n<p>Serving a model in production meant a relatively small footprint\u2014perhaps a few CPU cores or a single GPU for real-time inference. Kubernetes and Docker became the standard for deploying scalable inference services. Monitoring focused on:</p>\n<ul>\n<li><strong>Performance metrics</strong>: latency, throughput, memory usage</li>\n<li><strong>Model metrics</strong>: prediction accuracy, concept drift detection</li>\n<li><strong>System health</strong>: uptime, error rates</li>\n</ul>\n<h3 id=\"15-feature-stores-and-data-management\">1.5. Feature Stores and Data Management</h3>\n<p>For many ML applications (especially in finance or e-commerce), engineered features were as important as models. Feature stores provided a central place to manage features, ensuring consistency between training and serving. The emphasis was on structured data pipelines and feature engineering. Unstructured data like text and images required custom handling outside these stores.</p>\n<p><strong>In summary</strong>: Classic MLOps revolved around small-to-medium models and explicit feature engineering. The tooling was designed for managing many experiments and deployments\u2014scaling out a large number of models for different tasks rather than scaling one enormous model. This paradigm worked well until models started growing dramatically in size and capability.</p>\n<h2 id=\"2-the-paradigm-shift-rise-of-large-scale-foundation-models\">2. The Paradigm Shift: Rise of Large-Scale Foundation Models</h2>\n<p>Around 2018-2020, everything changed. Researchers began introducing foundation models\u2014extremely large models pretrained on vast corpora, capable of being adapted to many tasks.</p>\n<p>The progression was rapid:</p>\n<ul>\n<li><strong>2018-2019</strong>: BERT and GPT-2 showed the power of transfer learning</li>\n<li><strong>2020-2021</strong>: GPT-3 and PaLM demonstrated what massive scale could achieve</li>\n<li><strong>2021-2023</strong>: Image models like DALL-E and Stable Diffusion brought generative AI to the mainstream</li>\n<li><strong>2023-2024</strong>: Foundation models became ubiquitous\u2014available everywhere from Hugging Face to AWS Bedrock</li>\n</ul>\n<p>As one practitioner noted in early 2024: \"Foundational models are everywhere now\u2014a stark change from just two years ago.\"</p>\n<p>This shift created a fundamentally different paradigm. If classic models were like specialized kitchen gadgets (a toaster, a blender), foundation models are like a professional chef who can learn to cook anything with a little instruction.</p>\n<p><img alt=\"Classic MLOps vs LLMOps\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/mlops_vs_llmops.svg\" /></p>\n<p>Here's how foundation models changed ML infrastructure:</p>\n<h3 id=\"21-pretrained-beats-from-scratch\">2.1. Pretrained Beats From Scratch</h3>\n<p>Instead of training models from scratch, teams started with powerful pretrained models and fine-tuned them for specific tasks. This approach:</p>\n<ul>\n<li><strong>Cuts training time</strong> from weeks to hours or days</li>\n<li><strong>Reduces data requirements</strong> from millions to thousands of examples</li>\n<li><strong>Enables smaller teams</strong> to build sophisticated AI applications</li>\n</ul>\n<p>The largest models (with billions of parameters) are often used as-is via APIs or fine-tuned minimally. By 2024, the ML engineer's skillset shifted from \"how to build models\" to \"how to leverage and integrate foundation models\"\u2014treating the model as a service rather than reinventing the wheel.</p>\n<h3 id=\"22-model-size-and-computational-demands\">2.2. Model Size and Computational Demands</h3>\n<p>The sheer scale of these models introduced new challenges. A model with 175 billion parameters cannot be handled with the same infrastructure as one with 50 million parameters.</p>\n<p><strong>Key scaling challenges</strong>:</p>\n<ul>\n<li><strong>Training</strong>: Requires powerful hardware (GPUs, TPUs) and distributed computing</li>\n<li><strong>Model parallelism</strong>: Sharding a single model across multiple GPUs</li>\n<li><strong>Data parallelism</strong>: Synchronizing multiple GPU workers during training</li>\n<li><strong>Inference</strong>: Often requires multiple GPUs or specialized runtimes to keep latency acceptable</li>\n</ul>\n<p>Libraries like DeepSpeed and ZeRO (Zero Redundancy Optimizer) were developed specifically to make training giant models feasible. The infrastructure requirements jumped by orders of magnitude.</p>\n<h3 id=\"23-emergence-of-llmops\">2.3. Emergence of LLMOps</h3>\n<p>It became clear that operating these large models in production required extensions to classic MLOps. This led to <strong>LLMOps</strong> (Large Language Model Operations)\u2014essentially MLOps specialized for large models.</p>\n<p>LLMOps builds on classic MLOps principles but addresses unique challenges:</p>\n<ul>\n<li><strong>Computational resources</strong>: Managing expensive GPU clusters</li>\n<li><strong>Prompt engineering</strong>: Optimizing model behavior through input design</li>\n<li><strong>Safety monitoring</strong>: Detecting bias, harmful content, and data leakage</li>\n<li><strong>Performance management</strong>: Balancing latency, quality, and cost</li>\n</ul>\n<p>Issues that barely registered for smaller models\u2014like producing biased text or leaking training data\u2014became major considerations at LLM scale.</p>\n<p><img alt=\"Nested relationship of MLOps specialties - Machine Learning Ops (outermost), Generative AI Ops, LLM Ops, and Retrieval-Augmented Generation Ops (innermost)\" src=\"https://www.nvidia.com/content/nvidiaGDC/us/en_US/glossary/mlops/_jcr_content/root/responsivegrid/nv_container_1795650_1945302252/nv_image_2134560435_.coreimg.100.1290.jpeg/1741240029246/ai-ops-hierarchy.jpeg\" /></p>\n<p>This diagram from NVIDIA illustrates how general MLOps (outer circle) has branched into specialized subfields like generative AI operations (for all generative models), LLMOps (for large language models), and even RAGOps for retrieval-augmented generation. The concentric circles indicate that these specializations build on the foundation of classic MLOps.</p>\n<h3 id=\"24-foundation-models-as-a-service\">2.4. Foundation Models as a Service</h3>\n<p>Another major shift was the rise of <strong>models as a service</strong>. Instead of deploying their own models, many applications now call external APIs:</p>\n<p><strong>API Providers</strong>:</p>\n<ul>\n<li>OpenAI, Cohere, AI21 Labs offer hosted LLMs</li>\n<li>Google's Vertex AI provides Model Garden with pretrained models</li>\n<li>AWS Bedrock hosts proprietary foundation models</li>\n</ul>\n<p><strong>Model Hubs</strong>:</p>\n<ul>\n<li>Hugging Face hosts thousands of pretrained models</li>\n<li>Models can be downloaded or run in the cloud</li>\n<li>Version control and community sharing became standard</li>\n</ul>\n<p>This changed ML architecture fundamentally. Production pipelines might call external APIs for inference, introducing new considerations:</p>\n<ul>\n<li><strong>Latency</strong>: Network calls add overhead</li>\n<li><strong>Cost</strong>: Pay-per-token pricing models</li>\n<li><strong>Data privacy</strong>: Sending data to third parties</li>\n<li><strong>Vendor lock-in</strong>: Dependency on external services</li>\n</ul>\n<p>But it also saves the massive effort of managing model infrastructure.</p>\n<p><strong>The paradigm shift</strong>: From \"your data + your model code = trained model\" to \"your data + adaptation of a pretrained model = fine-tuned model (or just prompt it with your data).\"</p>\n<h2 id=\"3-new-requirements-and-capabilities-in-modern-ml-infrastructure\">3. New Requirements and Capabilities in Modern ML Infrastructure</h2>\n<p>With foundation models at the center, today's ML infrastructure must support capabilities that were niche or non-existent just a few years ago. Here are the key new requirements:</p>\n<h3 id=\"31-distributed-training-and-model-parallelism\">3.1. Distributed Training and Model Parallelism</h3>\n<p>Training a model with billions of parameters is beyond the capacity of a single machine. Modern ML infrastructure orchestrates distributed training across multiple nodes:</p>\n<p><img alt=\"Distributed Training\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/distributed_training.svg\" /></p>\n<p><strong>Two main approaches</strong>:</p>\n<ul>\n<li><strong>Model parallelism</strong>: Split the model's layers across multiple GPUs (each GPU handles part of the model)</li>\n<li><strong>Data parallelism</strong>: Replicate the model across GPUs and split the training data (synchronize gradients)</li>\n</ul>\n<p><strong>Tools that enable this</strong>:</p>\n<ul>\n<li>PyTorch Lightning, Horovod for general distributed training</li>\n<li>NVIDIA's Megatron-LM for massive transformer models</li>\n<li>Google's JAX/TPU ecosystem for TPU clusters</li>\n</ul>\n<p>A few years ago, most teams trained models on a single server. Now, ML platforms must handle launching jobs on GPU clusters, managing faults, and aggregating gradients from dozens or hundreds of workers seamlessly.</p>\n<h3 id=\"32-efficient-fine-tuning-techniques\">3.2. Efficient Fine-Tuning Techniques</h3>\n<p>Training from scratch is impractical for huge models, but even fine-tuning a multi-billion parameter model can be resource-intensive. This led to parameter-efficient fine-tuning methods:</p>\n<p><strong>Modern fine-tuning approaches</strong>:</p>\n<ul>\n<li><strong>LoRA (Low-Rank Adaptation)</strong>: Updates only a small subset of parameters (adapters) instead of the entire network, dramatically reducing computational cost</li>\n<li><strong>Prompt Tuning</strong>: Optimizes only the prompt embeddings, keeping the model frozen</li>\n<li><strong>Adapter Modules</strong>: Adds small trainable layers between frozen model layers</li>\n</ul>\n<p>Here is a simple example of how you might configure LoRA using the <code>peft</code> library:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">peft</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">LoraConfig</span><span class=\"p\">,</span> <span class=\"n\">get_peft_model</span>\n\n<span class=\"c1\"># Configure LoRA</span>\n<span class=\"n\">peft_config</span> <span class=\"o\">=</span> <span class=\"n\">LoraConfig</span><span class=\"p\">(</span>\n    <span class=\"n\">r</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">,</span>\n    <span class=\"n\">lora_alpha</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">,</span>\n    <span class=\"n\">target_modules</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;q_proj&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;v_proj&quot;</span><span class=\"p\">],</span>\n    <span class=\"n\">lora_dropout</span><span class=\"o\">=</span><span class=\"mf\">0.05</span><span class=\"p\">,</span>\n    <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"s2\">&quot;none&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">task_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;CAUSAL_LM&quot;</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Apply to base model</span>\n<span class=\"c1\"># model = get_peft_model(base_model, peft_config)</span>\n<span class=\"c1\"># model.print_trainable_parameters()</span>\n</code></pre></div>\n<p>ML infrastructure must now support complex workflows: load a base model from a hub, apply fine-tuned weight deltas, and deploy the combined model. Traditional training pipelines evolved significantly to accommodate this multi-step customization.</p>\n<h3 id=\"33-prompt-engineering-management\">3.3. Prompt Engineering &amp; Management</h3>\n<p>One surprising new artifact in modern ML pipelines is <strong>the prompt</strong>. With LLMs, much of the model's behavior is controlled through the text prompt or input format you give it.</p>\n<p>This created an entirely new discipline. Teams now:</p>\n<ul>\n<li>Maintain <strong>prompt libraries</strong> and templates</li>\n<li>Use <strong>version control</strong> for prompts (just like code)</li>\n<li>Run <strong>A/B tests</strong> to compare prompt variants</li>\n<li>Store <strong>prompt versions</strong> alongside model versions</li>\n</ul>\n<p>This is fundamentally different from classic ML, where inputs were just data features\u2014not natural language instructions. Frameworks like LangChain now include prompt optimization as a first-class feature.</p>\n<p><strong>Example prompt evolution</strong>:</p>\n<div class=\"highlight\"><pre><span></span><code>v1: &quot;Classify this text as positive or negative: {text}&quot;\nv2: &quot;You are a sentiment analyzer. Classify: {text}&quot;\nv3: &quot;Analyze sentiment. Return only &#39;positive&#39; or &#39;negative&#39;: {text}&quot;\n</code></pre></div>\n<p>Each version can produce different results, so tracking and testing prompts became as important as tracking model weights.</p>\n<h3 id=\"34-retrieval-augmented-generation-rag\">3.4. Retrieval-Augmented Generation (RAG)</h3>\n<p>Foundation models have a fixed knowledge cutoff and limited context windows. To keep responses accurate and up-to-date, <strong>Retrieval-Augmented Generation (RAG)</strong> has become a best practice.</p>\n<p><strong>How RAG works</strong>:</p>\n<p><img alt=\"RAG Workflow\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/rag_workflow.svg\" /></p>\n<p>Instead of continuously retraining the model on new data (costly and slow), RAG fetches information at query time. The retrieved documents are appended to the prompt as additional context.</p>\n<p>Here's a simplified view of how this looks in code using LangChain:</p>\n<div class=\"highlight\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.vectorstores</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">Pinecone</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.llms</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">OpenAI</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.chains</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">RetrievalQA</span>\n\n<span class=\"c1\"># 1. Load the vector database</span>\n<span class=\"c1\"># vector_db = Pinecone.from_existing_index(&quot;my-index&quot;, embeddings)</span>\n\n<span class=\"c1\"># 2. Initialize the LLM</span>\n<span class=\"c1\"># llm = OpenAI(temperature=0)</span>\n\n<span class=\"c1\"># 3. Create the RAG chain</span>\n<span class=\"c1\"># qa_chain = RetrievalQA.from_chain_type(</span>\n<span class=\"c1\">#     llm=llm,</span>\n<span class=\"c1\">#     chain_type=&quot;stuff&quot;,</span>\n<span class=\"c1\">#     retriever=vector_db.as_retriever()</span>\n<span class=\"c1\"># )</span>\n\n<span class=\"c1\"># 4. Ask a question</span>\n<span class=\"c1\"># response = qa_chain.run(&quot;How does LLMOps differ from MLOps?&quot;)</span>\n</code></pre></div>\n<p><strong>New infrastructure components</strong>:</p>\n<ul>\n<li><strong>Vector databases</strong> (Pinecone, Weaviate, FAISS, Milvus) for fast similarity search on embeddings</li>\n<li><strong>Embedding models</strong> to convert documents into vectors</li>\n<li><strong>Index management</strong> to keep embeddings in sync with the latest data</li>\n</ul>\n<p>In many ways, vector databases have replaced traditional feature stores. Unstructured data and semantic search took center stage over manual feature engineering.</p>\n<h3 id=\"35-data-streaming-and-real-time-data-feeds\">3.5. Data Streaming and Real-Time Data Feeds</h3>\n<p>Modern applications\u2014especially LLM-powered assistants\u2014continuously ingest data: chat conversations, sensor data, event streams. This data needs to update the model's knowledge (via RAG) or trigger responses in real-time.</p>\n<p><strong>The shift</strong>:</p>\n<ul>\n<li><strong>Classic MLOps</strong>: Batch processing (daily/weekly training jobs)</li>\n<li><strong>Modern LLMOps</strong>: Real-time streaming data pipelines</li>\n</ul>\n<p><strong>Technologies driving this</strong>:</p>\n<ul>\n<li><strong>Kafka</strong> and event streaming platforms</li>\n<li><strong>Real-time databases</strong> (Redis, DynamoDB)</li>\n<li><strong>Online feature stores</strong> with continuous updates</li>\n<li><strong>Streaming embeddings</strong> that update vector indexes in real-time</li>\n</ul>\n<p>The boundary between data engineering and MLOps has blurred. Data pipelines now directly feed model inference rather than just training.</p>\n<h3 id=\"36-scalable-and-specialized-serving-infrastructure\">3.6. Scalable and Specialized Serving Infrastructure</h3>\n<p>Serving a massive model is challenging. Modern ML infrastructure must support three key capabilities:</p>\n<p><strong>High-Throughput, Low-Latency Serving</strong></p>\n<p>Interactive applications (chatbots, image generators) demand fast responses. This requires:</p>\n<ul>\n<li><strong>GPU/TPU acceleration</strong> for quick inference</li>\n<li><strong>Model quantization</strong> to reduce precision and speed up serving</li>\n<li><strong>GPU batching</strong> to serve multiple requests in parallel</li>\n<li><strong>Optimized serving engines</strong> like NVIDIA's TensorRT, Triton Inference Server, or DeepSpeed-Inference</li>\n</ul>\n<p><strong>Serverless and Elastic Scaling</strong></p>\n<p>A new trend toward serverless ML has emerged. Platforms like Modal offer \"AWS Lambda but with GPU support\"\u2014you provide code, they handle infrastructure and scaling.</p>\n<p>Benefits:</p>\n<ul>\n<li>No always-running servers</li>\n<li>Compute spins up on-demand</li>\n<li>Scale to zero when idle (pay only per execution)</li>\n<li>Automatic scaling under load</li>\n</ul>\n<p>Tradeoffs:</p>\n<ul>\n<li>Cold-start latency when spinning up</li>\n<li>Managing statelessness</li>\n<li>Less control over infrastructure</li>\n</ul>\n<p>This works well for irregular workloads where managing GPU clusters is overkill.</p>\n<p><strong>Distributed Model Serving</strong></p>\n<p>For models too large for a single GPU, inference itself can be distributed. The model is sharded across multiple machines, each handling part of the forward pass.</p>\n<p>Example: Serving a 175B parameter model on-premises requires multiple GPUs working together. Modern ML infrastructure must launch distributed inference replicas and route requests appropriately.</p>\n<h3 id=\"37-monitoring-observability-and-guardrails\">3.7. Monitoring, Observability, and Guardrails</h3>\n<p>With great power comes great responsibility. Large models can generate incorrect or inappropriate outputs in ways small models never did. Modern ML systems need three layers of monitoring:</p>\n<p><strong>Performance and Reliability</strong></p>\n<p>The basics still matter:</p>\n<ul>\n<li>Latency, throughput, memory usage</li>\n<li>GPU utilization and costs</li>\n<li>Autoscaling policies (scale up under load, fall back to smaller models if needed)</li>\n</ul>\n<p><strong>Output Quality and Safety</strong></p>\n<p>We now monitor the <em>content</em> of outputs:</p>\n<ul>\n<li><strong>Content filtering</strong>: Detect hate speech, PII, harmful content</li>\n<li><strong>Moderation APIs</strong>: Use OpenAI's moderation API or custom filters</li>\n<li><strong>Bias detection</strong>: Continuously evaluate for biased responses</li>\n<li><strong>Guardrails</strong>: Intercept adversarial inputs and ensure outputs stay within bounds</li>\n</ul>\n<p>These \"guardrails\" have become essential in LLMOps\u2014they're not optional.</p>\n<p><strong>Feedback Loops</strong></p>\n<p>Continuous improvement now includes human feedback:</p>\n<ul>\n<li>Collect user interactions (likes, corrections, ratings)</li>\n<li>Use feedback to fine-tune models or adjust prompts</li>\n<li><strong>RLHF (Reinforcement Learning from Human Feedback)</strong>: Explicitly use human ratings to refine behavior</li>\n</ul>\n<p>The infrastructure must support collecting and managing this feedback data securely.</p>\n<hr />\n<p><strong>In summary</strong>: Today's ML infrastructure manages entire ecosystems\u2014base models, fine-tuning adapters, prompt templates, retrieval indexes, monitoring detectors, and more. The complexity is higher, but so is the capability.</p>\n<h2 id=\"4-evolving-system-architecture-and-design-patterns\">4. Evolving System Architecture and Design Patterns</h2>\n<p>Given these new requirements, how are ML systems actually structured today? Here are the key design patterns that have emerged:</p>\n<h3 id=\"41-modular-pipelines-orchestration\">4.1. Modular Pipelines &amp; Orchestration</h3>\n<p><strong>Classic tools</strong> (Kubeflow Pipelines, Apache Airflow) are still used for:</p>\n<ul>\n<li>Fine-tuning workflows</li>\n<li>Batch scoring jobs</li>\n<li>Periodic model retraining</li>\n</ul>\n<p><strong>New tools</strong> have emerged for modern needs:</p>\n<ul>\n<li><strong>Metaflow, Flyte, ZenML</strong>: Pythonic workflows that integrate seamlessly with ML libraries</li>\n<li><strong>Lightweight orchestration</strong>: For low-latency inference, application code often replaces heavyweight workflow engines</li>\n</ul>\n<p>The key difference: engineers no longer need to leave their development environment to manage the flow from data to deployment.</p>\n<h3 id=\"42-model-hubs-and-registries\">4.2. Model Hubs and Registries</h3>\n<p>Model management evolved with centralized hubs:</p>\n<p><strong>External hubs</strong>:</p>\n<ul>\n<li><strong>Hugging Face Hub</strong>: Thousands of models, datasets, and scripts</li>\n<li>One-stop shop for ML components</li>\n<li>Plug-and-play architecture (fetch models at startup)</li>\n</ul>\n<p><strong>Internal registries</strong>:</p>\n<ul>\n<li>MLflow Registry, SageMaker Model Registry for bespoke models</li>\n<li>Combined with external foundation models</li>\n</ul>\n<p><strong>The shift</strong>: Instead of building everything in-house, engineers now plan for how to fine-tune and adapt third-party models. This has accelerated development dramatically.</p>\n<h3 id=\"43-feature-stores-vs-vector-databases\">4.3. Feature Stores vs. Vector Databases</h3>\n<p>The data layer has fundamentally changed:</p>\n<p><img alt=\"Feature Store vs Vector DB\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/feature_store_vs_vector_db.svg\" /></p>\n<p><strong>Traditional feature stores</strong> handled structured data with manual feature engineering.</p>\n<p><strong>Modern vector databases</strong> (Pinecone, Weaviate, Chroma, Milvus) handle:</p>\n<ul>\n<li>High-dimensional embeddings</li>\n<li>Fast similarity search</li>\n<li>Semantic search and deduplication</li>\n<li>RAG for LLMs</li>\n</ul>\n<p>You'll often see both: a vector DB for unstructured semantic lookup and a data warehouse for structured analytics.</p>\n<h3 id=\"44-unified-platforms-end-to-end\">4.4. Unified Platforms (End-to-End)</h3>\n<p>The complexity of modern ML has driven adoption of end-to-end platforms that abstract infrastructure details.</p>\n<p><strong>Cloud platforms</strong> evolved to support foundation models:</p>\n<ul>\n<li><strong>Google Vertex AI</strong>: Auto-distributed training on TPU pods, Model Garden with LLMs, one-click deployment</li>\n<li><strong>AWS SageMaker</strong>: Distributed training, model parallelism, and Bedrock for hosted foundation models</li>\n<li><strong>Azure Machine Learning</strong>: Integrated training, deployment, and monitoring</li>\n</ul>\n<p>These platforms provide managed services like \"fine-tune this 20B parameter model on your data\" or \"embed and index your text data for retrieval.\"</p>\n<p><strong>Open-source and startups</strong>:</p>\n<ul>\n<li><strong>MosaicML</strong> (now Databricks): Efficient training and deployment for large models</li>\n<li><strong>Argilla, Label Studio</strong>: Data labeling and prompt dataset creation</li>\n<li><strong>ClearML, MLflow</strong>: Experiment tracking tied to pipeline execution</li>\n</ul>\n<h3 id=\"45-inference-gateways-and-apis\">4.5. Inference Gateways and APIs</h3>\n<p>The proliferation of model sizes led to <strong>inference gateways</strong>\u2014routers that intelligently direct requests:</p>\n<p><img alt=\"Inference Gateway\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/inference_gateway.svg\" /></p>\n<p><strong>Use cases</strong>:</p>\n<ul>\n<li>Route based on latency requirements</li>\n<li>Different models for different subscription tiers</li>\n<li>A/B testing new models on a fraction of traffic</li>\n<li>Fallback to smaller models under high load</li>\n</ul>\n<p>This decouples the client-facing API from model implementation, allowing seamless model swaps and testing.</p>\n<h3 id=\"46-agentic-systems\">4.6. Agentic Systems</h3>\n<p>A cutting-edge pattern: <strong>AI agents</strong> that dynamically choose sequences of actions to accomplish tasks.</p>\n<p>Unlike static chains, agents can:</p>\n<ul>\n<li>Call external tools (calculators, search engines, databases)</li>\n<li>Decide workflows at runtime based on context</li>\n<li>Invoke different models for different subtasks</li>\n</ul>\n<p><strong>Enabling frameworks</strong>:</p>\n<ul>\n<li>LangChain's agent mode</li>\n<li>OpenAI's function calling</li>\n<li>AutoGPT and similar systems</li>\n</ul>\n<p>This emerging pattern requires new operational practices (sometimes called \"AgentOps\"):</p>\n<ul>\n<li>Robust monitoring to prevent unwanted actions</li>\n<li>Detailed logging to trace decision paths</li>\n<li>Safety guardrails to limit agent capabilities</li>\n</ul>\n<p><img alt=\"Agentic Workflow\" src=\"../../../../assets/2025-05-06-from-mlops-to-llmops/agentic_workflow.svg\" /></p>\n<p>While not yet widespread in production, agentic systems represent the frontier of LLMOps.</p>\n<h2 id=\"5-getting-started-with-llmops\">5. Getting Started with LLMOps</h2>\n<p>If you are new to this field, the ecosystem can feel overwhelming. Here is a recommended path to get your hands dirty:</p>\n<ol>\n<li><strong>Play with APIs</strong>: Start by using OpenAI or Anthropic APIs to understand prompt engineering.</li>\n<li><strong>Build a RAG App</strong>: Use <strong>LangChain</strong> or <strong>LlamaIndex</strong> to build a simple \"Chat with your PDF\" app. This introduces you to vector databases and retrieval.</li>\n<li><strong>Try Fine-Tuning</strong>: Use <strong>Hugging Face</strong> to fine-tune a small model (like Llama-3-8B or Mistral-7B) on a custom dataset using Google Colab.</li>\n<li><strong>Deploy</strong>: Try deploying your fine-tuned model using <strong>vLLM</strong> or <strong>Ollama</strong> locally, then move to a cloud provider.</li>\n</ol>\n<h2 id=\"6-conclusion-from-mlops-to-llmops-and-beyond\">6. Conclusion: From MLOps to LLMOps and Beyond</h2>\n<p>In just a few years, we've witnessed a transformation in how we approach machine learning in production.</p>\n<p><strong>What remains the same</strong>:</p>\n<ul>\n<li>Automation, reproducibility, collaboration</li>\n<li>Focus on reliability and efficiency</li>\n<li>DevOps principles applied to ML</li>\n</ul>\n<p><strong>What changed dramatically</strong>:</p>\n<ul>\n<li>Scale: From millions to billions of parameters</li>\n<li>Approach: From training from scratch to adapting foundation models</li>\n<li>Infrastructure: From single servers to distributed GPU clusters</li>\n<li>Data layer: From feature stores to vector databases</li>\n<li>Monitoring: From performance metrics to content safety guardrails</li>\n</ul>\n<p>This gave rise to <strong>LLMOps</strong>\u2014a specialization of MLOps for managing the lifecycle of large models. It's not just hype. The differences are tangible in day-to-day workflows:</p>\n<ul>\n<li>How we fine-tune models (LoRA, adapters)</li>\n<li>How we deploy them (distributed serving, serverless GPUs)</li>\n<li>How we monitor them (content filtering, bias detection)</li>\n<li>What infrastructure we need (vector databases, GPU clusters)</li>\n</ul>\n<p><strong>The evolution continues</strong>. As models grow and AI systems become more autonomous, we're already seeing:</p>\n<ul>\n<li><strong>AgentOps</strong> for managing AI agents</li>\n<li><strong>RAGOps</strong> for retrieval-augmented systems</li>\n<li>Even more specialized operational practices</li>\n</ul>\n<p>But the end goal remains: <strong>reliably deliver the benefits of machine learning to end-users and business applications, at scale and with trustworthiness.</strong></p>\n<p>Teams that successfully navigate this evolution harness foundation models to build products faster than ever\u2014while maintaining the reliability and efficiency that good operations provide.</p>\n<h2 id=\"references\">References</h2>\n<p>The insights and examples in this post are supported by recent research and industry sources, including an MDPI review on transitioning from MLOps to LLMOps, NVIDIA's technical blogs on GenAIOps and LLMOps, and various practitioner articles and discussions capturing the state of ML in 2024. Platforms like Modal and Ray have published guides showing new deployment patterns (serverless GPUs, distributed serving) in action.</p>", "image": null, "date_modified": "2025-05-06T00:00:00+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["Infrastructure"]}]}