{"version": "https://jsonfeed.org/version/1", "title": "Shared Intelligence: Tips & Tricks in Machine Learning", "home_page_url": "https://slavadubrov.github.io/", "feed_url": "https://slavadubrov.github.io/feed_json_updated.json", "description": null, "icon": null, "authors": [], "language": "en", "items": [{"id": "https://slavadubrov.github.io/blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/", "url": "https://slavadubrov.github.io/blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/", "title": "Quick-Guide on managing Python on macOS with uv", "content_html": "<h1>Quick-Guide on managing Python like an AI Engineer on macOS with <strong>uv</strong></h1>\n<h2>TL;DR Bash Cheat\u2011sheet</h2>\n<p>```bash\nbrew install uv        # install tool\nuv python install 3.12 # grab interpreter</p>\n<h1>New project workflow (modern)</h1>\n<p>uv init                # create new project with pyproject.toml\nuv add pandas numpy    # add dependencies\nuv run train.py        # run with correct interpreter</p>\n<h1>Classical project workflow (requirements.txt)</h1>\n<p>uv venv                           # create .venv\nuv pip install -r requirements.txt # install from requirements\nuv run train.py                   # run script</p>\n<p>brew upgrade uv         # update uv itself (Homebrew install)\n```</p>\n<hr>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "python", "tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/", "url": "https://slavadubrov.github.io/blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/", "title": "Quick-Guide on setting up a MacBook for AI Engineering", "content_html": "<h1>Quick-Guide on setting up a MacBook for AI Engineering</h1>\n<p>Here's my distilled, 10\u2011step workflow to transform a vanilla macOS install into a ready to-go AI engineering working station.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "macos", "tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/", "url": "https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/", "title": "Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025", "content_html": "<h1>Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</h1>\n<p>The race to build bigger, better language models continues at breakneck speed. Today's state-of-the-art models require massive computing resources that no single GPU can handle. Whether you're training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.</p>\n<p>This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face's <a href=\"https://huggingface.co/spaces/nanotron/ultrascale-playbook\">Ultra-Scale Playbook</a>.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["Deep Learning", "Distributed Training", "GPU", "LLM", "Parallelism"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/", "url": "https://slavadubrov.github.io/blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/", "title": "MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond", "content_html": "<h1>MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</h1>\n<p>The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models. From giant language models like GPT-4 to image diffusion models like Stable Diffusion, these powerful models have fundamentally changed how we build and operate ML systems. </p>\n<p>In this post, I'll explore how ML infrastructure and MLOps practices have evolved to support foundation models. We'll contrast the \"classic\" era of MLOps with modern paradigms, examine what's changed, and look at the new patterns and workflows that have emerged.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["infrastructure", "llmops", "mlops"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/", "url": "https://slavadubrov.github.io/blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/", "title": "Quick-Guide on ~/.zprofile vs ~/.zshrc \ud83d\ude80", "content_html": "<h1>Quick-Guide on ~/.zprofile vs ~/.zshrc \ud83d\ude80</h1>\n<h2>TL;DR \u26a1</h2>\n<ul>\n<li><strong><code>~/.zprofile</code></strong> \u2192 runs once when you start a login shell (think \"environment setup\") \ud83d\udd27</li>\n<li><strong><code>~/.zshrc</code></strong> \u2192 runs every time you open an interactive shell (think \"daily experience\") \ud83c\udfae</li>\n</ul>\n<p>Use both strategically: put your PATH and environment variables in <strong><code>~/.zprofile</code></strong>, and your aliases, functions, and prompt customizations in <strong><code>~/.zshrc</code></strong> \u2728</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "macos", "tooling"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/08/quick-guide-on-pyprojecttoml/", "url": "https://slavadubrov.github.io/blog/2025/05/08/quick-guide-on-pyprojecttoml/", "title": "Quick-Guide on `pyproject.toml`", "content_html": "<h1>Quick-Guide on <code>pyproject.toml</code></h1>\n<h2>TL;DR</h2>\n<p>Think of <code>pyproject.toml</code> as the <strong><code>package.json</code> for Python</strong>. It's a single configuration file that holds your project's metadata, dependencies, and tool settings. Whether you use <code>.venv</code>, <code>pyenv</code>, or <code>uv</code>, this one file simplifies development and makes collaboration smoother.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "python"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/", "url": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/", "title": "Quick-guide on Local Stable-Diffusion Toolkits for macOS", "content_html": "<h1>Quick-guide on Local Stable-Diffusion Toolkits for macOS</h1>\n<p>Running generative-AI models on-device means zero cloud costs, no upload limits, and full control of your checkpoints. Whether you're generating portraits, concept art, or iterating on product designs, keeping everything local gives you privacy and unlimited generations.</p>\n<p>Below is a practical guide to five of the most popular macOS-ready front-ends. Each tool wraps the same underlying Stable Diffusion models but offers different trade-offs between simplicity and power.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["genai", "guide", "macos", "tools"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/", "url": "https://slavadubrov.github.io/blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/", "title": "Quick-guide on Running LLMs Locally on macOS", "content_html": "<h1>Quick-guide on Running LLMs Locally on macOS</h1>\n<p>Running large language models locally on your Mac means faster responses, complete privacy, and no API bills. But which tool should you pick?</p>\n<p>This guide breaks down the five most popular options - from dead-simple menu bar apps to full-control command-line tools. Each comes with download links, what makes it special, and honest trade-offs.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "llm", "macos", "tools"]}, {"id": "https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/", "url": "https://slavadubrov.github.io/blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/", "title": "Choosing the Right Open-Source LLM Variant & File Format", "content_html": "<h1>Choosing the Right Open-Source LLM Variant &amp; File Format</h1>\n<hr>\n<h2>Why do open-source LLMs have so many confusing names?</h2>\n<p>You've probably seen model names like <code>Llama-3.1-8B-Instruct.Q4_K_M.gguf</code> or <code>Mistral-7B-v0.3-A3B.awq</code> and wondered what all those suffixes mean. The short answer: <strong>they tell you two critical things.</strong></p>\n<p>Open-source LLMs vary along <strong>two independent dimensions</strong>:</p>\n<ol>\n<li><strong>Model variant</strong> \u2013 the suffix in the name (<code>-Instruct</code>, <code>-Distill</code>, <code>-A3B</code>, etc.) describes <em>how</em> the model was trained and <em>what</em> it's optimized for.</li>\n<li><strong>File format</strong> \u2013 the extension (<code>.gguf</code>, <code>.gptq</code>, <code>.awq</code>, etc.) describes <em>how</em> the weights are stored and <em>where</em> they run best (CPU, GPU, mobile, etc.).</li>\n</ol>\n<p>Think of it like this: the model variant is the recipe, and the file format is the container. Same recipe, different containers for different kitchens.</p>\n<p><code>mermaid\ngraph LR\n    A[Model Release] --&gt; B[Model Variant&lt;br/&gt;What it does]\n    A --&gt; C[File Format&lt;br/&gt;Where it runs]\n    B --&gt; D[Base&lt;br/&gt;Instruct&lt;br/&gt;Distill&lt;br/&gt;QAT&lt;br/&gt;MoE]\n    C --&gt; E[GGUF&lt;br/&gt;GPTQ&lt;br/&gt;AWQ&lt;br/&gt;EXL2&lt;br/&gt;Safetensors]\n    D --&gt; F[Pick based on&lt;br/&gt;your use case]\n    E --&gt; G[Pick based on&lt;br/&gt;your hardware]</code></p>\n<p>Understanding both dimensions helps you avoid downloading 20 GB of the wrong model at midnight and then spending hours debugging CUDA errors.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "llm"]}, {"id": "https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/", "url": "https://slavadubrov.github.io/blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/", "title": "Building a Custom FeatureStoreLite MCP Server Using uv", "content_html": "<h1>Building a Custom FeatureStoreLite MCP Server Using uv</h1>\n<p><em>A step-by-step guide that shows how to create your own lightweight feature store MCP server from scratch using FastMCP, run it through </em><em>uv</em><em>, and integrate it with Claude Desktop. This is a practical example of building a useful MCP server that ML engineers can actually use.</em></p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["guide", "mcp"]}, {"id": "https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/", "url": "https://slavadubrov.github.io/blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/", "title": "Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It", "content_html": "A practical guide to designing, evaluating, and shipping the context layer (a.k.a. context engineering) for agentic AI systems \u2014 with diagrams, patterns, and a starter config.", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["agents", "ai-engineering", "context-layer", "guardrails", "memory", "rag", "retrieval"]}, {"id": "https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/", "url": "https://slavadubrov.github.io/blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/", "title": "Domain-driven design for AI agents: a beginner-friendly guide", "content_html": "Learn how domain-driven design (DDD) keeps AI agents aligned with the business domain, with practical patterns, code snippets, and tooling tips.", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [{"name": "Viacheslav Dubrov"}], "tags": ["agents", "ai-engineering", "architecture", "domain-driven-design"]}, {"id": "https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/", "url": "https://slavadubrov.github.io/blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/", "title": "LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes", "content_html": "<h1>LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes</h1>\n<p>Serving dozens of fine-tuned large language models used to mean provisioning one GPU per model. LoRAX (LoRA eXchange) flips that math on its head: keep a single base model in memory and hot-swap lightweight LoRA adapters per request.</p>\n<p>This guide shows you how LoRAX achieves near-constant cost per token regardless of how many fine-tunes you're serving, when it makes sense to use it, how to deploy it on Kubernetes with Helm, and how to call it through REST, Python, and OpenAI-compatible APIs.</p>", "image": null, "date_modified": "2025-10-22T20:43:39+00:00", "authors": [], "tags": ["Deployment", "Inference", "Kubernetes", "LLM", "LoRA"]}]}