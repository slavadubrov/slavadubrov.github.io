
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A practical guide to Schema-Guided Reasoning (SGR) on vLLM with xgrammar — what it is, how it differs from CoT and prompt engineering, the core patterns (Cascade, Routing, Cycle), and a working implementation example.">
      
      
        <meta name="author" content="Viacheslav Dubrov">
      
      
        <link rel="canonical" href="https://slavadubrov.github.io/blog/2025/12/25/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/">
      
      
        <link rel="prev" href="../../../10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/">
      
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../../assets/favicon-eoc.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Schema-Guided Reasoning on vLLM — Turning LLMs into Reliable Business Logic Engines - Edge of Context: Tips & Tricks in Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/brand.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#schema-guided-reasoning-on-vllm-turning-llms-into-reliable-business-logic-engines" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Edge of Context: Tips &amp; Tricks in Machine Learning" class="md-header__button md-logo" aria-label="Edge of Context: Tips & Tricks in Machine Learning" data-md-component="logo">
      
  <img src="../../../../../assets/logo-eoc.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Edge of Context: Tips & Tricks in Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Schema-Guided Reasoning on vLLM — Turning LLMs into Reliable Business Logic Engines
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../topics/" class="md-tabs__link">
        
  
  
    
  
  Topics

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="../../../../" class="md-tabs__link">
        
  
  
    
  
  Blog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Edge of Context: Tips &amp; Tricks in Machine Learning" class="md-nav__button md-logo" aria-label="Edge of Context: Tips & Tricks in Machine Learning" data-md-component="logo">
      
  <img src="../../../../../assets/logo-eoc.svg" alt="logo">

    </a>
    Edge of Context: Tips & Tricks in Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../topics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Topics
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tldr" class="md-nav__link">
    <span class="md-ellipsis">
      TL;DR
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-schema-guided-reasoning" class="md-nav__link">
    <span class="md-ellipsis">
      What is Schema-Guided Reasoning?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="What is Schema-Guided Reasoning?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-sgr-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why SGR Matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sgr-vs-chain-of-thought-vs-prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      SGR vs Chain of Thought vs Prompt Engineering
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SGR vs Chain of Thought vs Prompt Engineering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      The Comparison
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-engineering-semantic-persuasion" class="md-nav__link">
    <span class="md-ellipsis">
      Prompt Engineering: Semantic Persuasion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chain-of-thought-better-reasoning-same-structure-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Chain of Thought: Better Reasoning, Same Structure Problems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sgr-structured-chain-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      SGR: Structured Chain of Thought
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sgr-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      SGR Patterns
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SGR Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-cascade-sequential-reasoning-steps" class="md-nav__link">
    <span class="md-ellipsis">
      1. Cascade: Sequential Reasoning Steps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-routing-semantic-switch-statement" class="md-nav__link">
    <span class="md-ellipsis">
      2. Routing: Semantic Switch Statement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-cycle-repeated-reasoning-with-lists" class="md-nav__link">
    <span class="md-ellipsis">
      3. Cycle: Repeated Reasoning with Lists
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#making-sgr-work-constrained-decoding" class="md-nav__link">
    <span class="md-ellipsis">
      Making SGR Work: Constrained Decoding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Making SGR Work: Constrained Decoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-cloud-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Cloud Providers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supported-inference-engines" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Inference Engines
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-this-article-focuses-on-vllm-xgrammar" class="md-nav__link">
    <span class="md-ellipsis">
      Why This Article Focuses on vLLM + xgrammar
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-xgrammar-enforces-schemas" class="md-nav__link">
    <span class="md-ellipsis">
      How xgrammar Enforces Schemas
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How xgrammar Enforces Schemas">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#where-does-masking-happen" class="md-nav__link">
    <span class="md-ellipsis">
      Where Does Masking Happen?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-two-phase-process" class="md-nav__link">
    <span class="md-ellipsis">
      The Two-Phase Process
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Two-Phase Process">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-grammar-compilation-one-time-before-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 1: Grammar Compilation (one-time, before inference)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-runtime-mask-generation-every-token" class="md-nav__link">
    <span class="md-ellipsis">
      Phase 2: Runtime Mask Generation (every token)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-pushdown-automata-matter" class="md-nav__link">
    <span class="md-ellipsis">
      Why Pushdown Automata Matter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concrete-example-generating-a-float-field" class="md-nav__link">
    <span class="md-ellipsis">
      Concrete Example: Generating a Float Field
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-why-near-zero-overhead" class="md-nav__link">
    <span class="md-ellipsis">
      Performance: Why "Near-Zero Overhead"?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practical-implementation-with-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Implementation with vLLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Practical Implementation with vLLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Project Structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-1-define-your-schemas" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Define Your Schemas
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-create-the-llm-client-with-xgrammar" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Create the LLM Client with xgrammar
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-orchestrate-the-agent" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Orchestrate the Agent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-run-vllm-with-xgrammar" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Run vLLM with xgrammar
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-output" class="md-nav__link">
    <span class="md-ellipsis">
      Example Output
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#schema-design" class="md-nav__link">
    <span class="md-ellipsis">
      Schema Design
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vllm-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      vLLM Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#production-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Production Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
    <nav class="md-nav" aria-label="References">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sgr-framework" class="md-nav__link">
    <span class="md-ellipsis">
      SGR Framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xgrammar" class="md-nav__link">
    <span class="md-ellipsis">
      xgrammar
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vllm" class="md-nav__link">
    <span class="md-ellipsis">
      vLLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-project" class="md-nav__link">
    <span class="md-ellipsis">
      Demo Project
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-12-25 00:00:00+00:00" class="md-ellipsis">December 25, 2025</time>
                      </div>
                    </li>
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M15 13h1.5v2.82l2.44 1.41-.75 1.3L15 16.69zm4-5H5v11h4.67c-.43-.91-.67-1.93-.67-3a7 7 0 0 1 7-7c1.07 0 2.09.24 3 .67zM5 21a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1V1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v6.1c1.24 1.26 2 2.99 2 4.9a7 7 0 0 1-7 7c-1.91 0-3.64-.76-4.9-2zm11-9.85A4.85 4.85 0 0 0 11.15 16c0 2.68 2.17 4.85 4.85 4.85A4.85 4.85 0 0 0 20.85 16c0-2.68-2.17-4.85-4.85-4.85"/></svg>
                          <time datetime="2025-12-25 00:00:00+00:00" class="md-ellipsis">December 25, 2025</time>
                        </div>
                      </li>
                    
                    
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  


  <nav class="md-tags" >
    
      
      
      
        <a href="../../../../../topics/#tag:agents" class="md-tag">agents</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:ai-engineering" class="md-tag">ai-engineering</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:llm" class="md-tag">llm</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:sgr" class="md-tag">sgr</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:structured-output" class="md-tag">structured-output</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:vllm" class="md-tag">vllm</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:xgrammar" class="md-tag">xgrammar</a>
      
    
  </nav>



<h1 id="schema-guided-reasoning-on-vllm-turning-llms-into-reliable-business-logic-engines">Schema-Guided Reasoning on vLLM — Turning LLMs into Reliable Business Logic Engines</h1>
<h2 id="tldr">TL;DR</h2>
<blockquote>
<p><strong>Schema-Guided Reasoning (SGR)</strong> is a technique that forces LLMs to reason through predefined steps by enforcing structured output schemas. Instead of hoping the model follows your formatting instructions, you <strong>guarantee</strong> it with constrained decoding. Combined with vLLM's xgrammar backend, you get 100% valid JSON output with near-zero latency overhead.</p>
</blockquote>
<p><strong>The problem</strong>: You build an LLM-powered agent. It works in demos. In production, it outputs malformed JSON, skips reasoning steps, and gives inconsistent responses. You add retry loops, validation layers, larger models. Costs explode.</p>
<p><strong>The fix</strong>: Define your reasoning topology as a Pydantic schema. Let xgrammar enforce it at the token generation level. The LLM physically cannot produce invalid output.</p>
<!-- more -->

<hr />
<h2 id="what-is-schema-guided-reasoning">What is Schema-Guided Reasoning?</h2>
<p>Schema-Guided Reasoning (SGR) is a technique pioneered by <a href="https://abdullin.com/schema-guided-reasoning/">Rinat Abdullin</a> that guides LLMs to produce structured, clear, and predictable outputs by enforcing reasoning through predefined steps.</p>
<p>Instead of allowing free-form text completion (which can be inconsistent or ambiguous), the schema acts as a strict guideline. By creating a specific schema (or structured template), you explicitly define:</p>
<ul>
<li><strong>What steps</strong> the model must go through (preventing skipped reasoning)</li>
<li><strong>In which order</strong> it must reason (ensuring logical flow)</li>
<li><strong>Where it should focus</strong> attention (improving depth and accuracy)</li>
</ul>
<p>Think of it as giving the model a "cognitive checklist" that it <strong>must</strong> follow.</p>
<p><img alt="SGR Overview" src="../../../../assets/2025-12-25-sgr-vllm/sgr_overview.svg" /></p>
<h3 id="why-sgr-matters">Why SGR Matters</h3>
<p>The core insight is simple but powerful:</p>
<blockquote>
<p><strong>Schema = Cognitive Scaffold</strong></p>
</blockquote>
<p>When you define a schema with fields like <code>churn_analysis</code>, <code>margin_math</code>, and then <code>max_discount_percent</code>, the model is <strong>forced</strong> to populate these fields in order. It cannot jump to the discount decision without first analyzing the data.</p>
<p>This translates to:</p>
<ul>
<li><strong>Reproducible reasoning</strong> — consistent inference across repeated runs</li>
<li><strong>Auditable outputs</strong> — every reasoning step is explicit and inspectable</li>
<li><strong>Debuggable &amp; testable</strong> — intermediate outputs can be evaluated against test datasets</li>
<li><strong>Works with smaller models</strong> — the schema "holds the hand" of weaker models</li>
<li><strong>5-10% accuracy boost</strong> — common in production deployments</li>
</ul>
<hr />
<h2 id="sgr-vs-chain-of-thought-vs-prompt-engineering">SGR vs Chain of Thought vs Prompt Engineering</h2>
<p>Let's be precise about what makes SGR different from the approaches you're probably already using.</p>
<p><img alt="SGR Comparison" src="../../../../assets/2025-12-25-sgr-vllm/sgr_comparison.svg" /></p>
<h3 id="the-comparison">The Comparison</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Prompt Engineering</th>
<th>Chain of Thought</th>
<th>Schema-Guided Reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Output Structure</strong></td>
<td>Variable text</td>
<td>Free-form prose</td>
<td>Rigid JSON/Pydantic</td>
</tr>
<tr>
<td><strong>Control Mechanism</strong></td>
<td>Semantic persuasion ("Please output JSON")</td>
<td>Heuristic prompting ("Let's think step by step")</td>
<td>Constrained decoding (grammar-based)</td>
</tr>
<tr>
<td><strong>Reasoning Flow</strong></td>
<td>Model determines</td>
<td>Model determines</td>
<td>Developer determines (schema topology)</td>
</tr>
<tr>
<td><strong>Auditability</strong></td>
<td>Low (requires parsing)</td>
<td>Low (requires reading prose)</td>
<td>High (field-level inspection)</td>
</tr>
<tr>
<td><strong>Integration</strong></td>
<td>Difficult (regex parsing)</td>
<td>Difficult (variable format)</td>
<td>Trivial (native object deserialization)</td>
</tr>
<tr>
<td><strong>Error Rate</strong></td>
<td>High (format variability)</td>
<td>Moderate (hallucination of format)</td>
<td>Near-zero (syntax enforced by engine)</td>
</tr>
<tr>
<td><strong>Model Requirement</strong></td>
<td>Strong instruction following</td>
<td>Strong reasoning capability</td>
<td>Works with smaller models too</td>
</tr>
</tbody>
</table>
<h3 id="prompt-engineering-semantic-persuasion">Prompt Engineering: Semantic Persuasion</h3>
<div class="highlight"><pre><span></span><code>Please analyze the customer data and output your response as valid JSON
with the following structure: {&quot;discount&quot;: &lt;number&gt;, &quot;reason&quot;: &lt;string&gt;}
Be careful with the formatting!
</code></pre></div>
<p><strong>The problem</strong>: You're <em>hoping</em> the model's semantic understanding of "output JSON" outweighs its tendency to be conversational. A model update, temperature change, or different few-shot examples can break your parser.</p>
<h3 id="chain-of-thought-better-reasoning-same-structure-problems">Chain of Thought: Better Reasoning, Same Structure Problems</h3>
<div class="highlight"><pre><span></span><code>Let&#39;s think step by step:
1. First, I&#39;ll analyze the customer&#39;s churn risk...
2. Then I&#39;ll calculate the margin...
3. Therefore, I recommend a 15% discount.
</code></pre></div>
<p>CoT improves reasoning <strong>accuracy</strong> but makes structure <strong>worse</strong>. The output is unpredictable prose that's nearly impossible to parse reliably. You end up needing a second LLM call to extract structured data from the reasoning.</p>
<h3 id="sgr-structured-chain-of-thought">SGR: Structured Chain of Thought</h3>
<p>SGR doesn't abandon CoT's insight that intermediate reasoning improves accuracy. It <strong>formalizes</strong> it:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PricingLogic</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="c1"># 1. Data Analysis (must complete before decision)</span>
    <span class="n">churn_analysis</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Analyze churn_probability&quot;</span><span class="p">)</span>
    <span class="n">financial_analysis</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Analyze cart_value and margin&quot;</span><span class="p">)</span>

    <span class="c1"># 2. Math Enforcement (explicit calculation)</span>
    <span class="n">margin_math</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Calculate: &#39;Cart $X * Y% = $Z&#39;&quot;</span><span class="p">)</span>

    <span class="c1"># 3. Decision Constraint (bounded by prior analysis)</span>
    <span class="n">max_discount_percent</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Max allowed discount&quot;</span><span class="p">)</span>

    <span class="c1"># 4. Final Output</span>
    <span class="n">offer_code</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">customer_message</span><span class="p">:</span> <span class="nb">str</span>
</code></pre></div>
<p>The model <strong>cannot</strong> output <code>max_discount_percent</code> without first populating <code>churn_analysis</code>, <code>financial_analysis</code>, and <code>margin_math</code>. The schema enforces the reasoning order.</p>
<hr />
<h2 id="sgr-patterns">SGR Patterns</h2>
<p>SGR enables three foundational patterns for controlling LLM reasoning. These can be combined for complex workflows.</p>
<p><img alt="SGR Patterns" src="../../../../assets/2025-12-25-sgr-vllm/sgr_patterns.svg" /></p>
<h3 id="1-cascade-sequential-reasoning-steps">1. Cascade: Sequential Reasoning Steps</h3>
<p>Cascade ensures the model follows predefined reasoning steps in order. Each field must be completed before the next.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">annotated_types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ge</span><span class="p">,</span> <span class="n">Le</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CandidateEvaluation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Evaluate a job candidate with enforced reasoning order.&quot;&quot;&quot;</span>

    <span class="c1"># Step 1: Summarize (forces context awareness)</span>
    <span class="n">brief_candidate_summary</span><span class="p">:</span> <span class="nb">str</span>

    <span class="c1"># Step 2: Rate (bounded integer)</span>
    <span class="n">rate_skill_match</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Ge</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">Le</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

    <span class="c1"># Step 3: Decide (constrained choices)</span>
    <span class="n">final_recommendation</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;hire&quot;</span><span class="p">,</span> <span class="s2">&quot;reject&quot;</span><span class="p">,</span> <span class="s2">&quot;hold&quot;</span><span class="p">]</span>
</code></pre></div>
<p><strong>Use cases</strong>: Candidate evaluation, document classification, compliance analysis, medical diagnosis</p>
<p><strong>Key insight</strong>: The model must complete <code>brief_candidate_summary</code> before it can rate, and must rate before it can recommend. No shortcuts allowed.</p>
<hr />
<h3 id="2-routing-semantic-switch-statement">2. Routing: Semantic Switch Statement</h3>
<p>Routing forces the model to explicitly choose one path from multiple options. This is implemented using <code>Union</code> types.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Union</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FeatureLookup</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Route to database lookup.&quot;&quot;&quot;</span>
    <span class="n">rationale</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">tool_name</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;fetch_user_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;fetch_user_features&quot;</span>
    <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GeneralResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Standard response for non-pricing queries.&quot;&quot;&quot;</span>
    <span class="n">tool_name</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;respond&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;respond&quot;</span>
    <span class="n">content</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RouterSchema</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The model must pick exactly ONE branch.&quot;&quot;&quot;</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">FeatureLookup</span><span class="p">,</span> <span class="n">GeneralResponse</span><span class="p">]</span>
</code></pre></div>
<p><strong>Use cases</strong>: Intent classification, tool selection, support triage, multi-agent dispatch</p>
<p><strong>Key insight</strong>: The <code>Literal</code> type with a discriminator field (like <code>tool_name</code>) ensures the model commits to one branch and fills in the required fields for that specific path.</p>
<hr />
<h3 id="3-cycle-repeated-reasoning-with-lists">3. Cycle: Repeated Reasoning with Lists</h3>
<p>Cycle forces the model to produce multiple items, with constraints on minimum and maximum count.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Annotated</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">annotated_types</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinLen</span><span class="p">,</span> <span class="n">MaxLen</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RiskFactor</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">explanation</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">severity</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">,</span> <span class="s2">&quot;medium&quot;</span><span class="p">,</span> <span class="s2">&quot;high&quot;</span><span class="p">]</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RiskAssessment</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate 2-4 risk factors.&quot;&quot;&quot;</span>
    <span class="n">factors</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">RiskFactor</span><span class="p">],</span> <span class="n">MinLen</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">MaxLen</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
</code></pre></div>
<p><strong>Use cases</strong>: Risk assessment, issue extraction, parallel tool calls, multi-step planning</p>
<p><strong>Key insight</strong>: The <code>MinLen</code> and <code>MaxLen</code> annotations force the model to generate at least 2 but no more than 4 items. Combined with Routing, this enables parallel tool dispatch.</p>
<hr />
<h2 id="making-sgr-work-constrained-decoding">Making SGR Work: Constrained Decoding</h2>
<p>The patterns above are powerful, but they're just Pydantic schemas — how do we actually <strong>enforce</strong> them? The answer is <strong>Constrained Decoding</strong> (also called Structured Output).</p>
<p>Constrained Decoding works by modifying the token generation process. Instead of allowing the model to freely sample from its vocabulary, the decoding engine applies a <strong>grammar mask</strong> that blocks tokens that would violate the schema. This happens at the inference engine level, not in your application code.</p>
<blockquote>
<p>[!TIP]
SGR doesn't require "reasoning models" (like o1 or DeepSeek-R1). It works well with instruction-tuned models, and especially well with models distilled from reasoning models.</p>
</blockquote>
<h3 id="supported-cloud-providers">Supported Cloud Providers</h3>
<p>Most modern LLM providers now support Structured Outputs via constrained decoding:</p>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OpenAI</strong></td>
<td><a href="https://platform.openai.com/docs/guides/structured-outputs">Structured Outputs</a> (including Azure). GPT-5 uses JSON Schema via llguidance</td>
</tr>
<tr>
<td><strong>Google/Gemini</strong></td>
<td><a href="https://ai.google.dev/gemini-api/docs/structured-output">JSON Schema</a> support since Nov 2025 (Pydantic and Zod)</td>
</tr>
<tr>
<td><strong>Mistral</strong></td>
<td><a href="https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/">Custom Structured Output</a></td>
</tr>
<tr>
<td><strong>Grok</strong></td>
<td><a href="https://docs.x.ai/docs/guides/structured-outputs">Structured Outputs</a> for multiple models</td>
</tr>
<tr>
<td><strong>Fireworks AI</strong></td>
<td><a href="https://docs.fireworks.ai/structured-responses/structured-response-formatting">JSON Schema</a></td>
</tr>
<tr>
<td><strong>Cerebras</strong></td>
<td><a href="https://inference-docs.cerebras.ai/capabilities/structured-outputs">Structured Outputs</a></td>
</tr>
<tr>
<td><strong>OpenRouter</strong></td>
<td>Depends on downstream provider, maps to JSON Schema</td>
</tr>
</tbody>
</table>
<h3 id="supported-inference-engines">Supported Inference Engines</h3>
<p>For self-hosted models, most modern inference engines support constrained decoding:</p>
<table>
<thead>
<tr>
<th>Engine</th>
<th>Backend</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>vLLM</strong></td>
<td><a href="https://github.com/mlc-ai/xgrammar">xgrammar</a> or <a href="https://github.com/guidance-ai/llguidance">guidance</a></td>
</tr>
<tr>
<td><strong>SGLang</strong></td>
<td><a href="https://github.com/dottxt-ai/outlines">Outlines</a>, <a href="https://github.com/mlc-ai/xgrammar">XGrammar</a>, or <a href="https://github.com/guidance-ai/llguidance">llguidance</a></td>
</tr>
<tr>
<td><strong>TensorRT-LLM</strong></td>
<td><a href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llm-api/llm_guided_decoding.py">GuidedDecoding</a></td>
</tr>
<tr>
<td><strong>Ollama</strong></td>
<td><a href="https://ollama.com/blog/structured-outputs">Structured Outputs</a></td>
</tr>
</tbody>
</table>
<h3 id="why-this-article-focuses-on-vllm-xgrammar">Why This Article Focuses on vLLM + xgrammar</h3>
<p>For this article, we'll dive deep into <strong>vLLM with the xgrammar backend</strong> because:</p>
<ol>
<li><strong>Production-grade</strong>: vLLM is the most widely deployed open-source LLM inference engine</li>
<li><strong>Zero-overhead</strong>: xgrammar is implemented in C++ with near-zero latency impact</li>
<li><strong>OpenAI-compatible API</strong>: Easy migration from cloud to self-hosted</li>
<li><strong>Full schema support</strong>: Handles complex nested schemas, unions, and recursive structures</li>
</ol>
<p>Let's look at how xgrammar actually enforces these schemas at the token level.</p>
<hr />
<h2 id="how-xgrammar-enforces-schemas">How xgrammar Enforces Schemas</h2>
<p>Now let's get precise about <strong>when</strong> and <strong>how</strong> xgrammar enforces your schema. Understanding this helps you debug and tune your SGR workflows.</p>
<p><img alt="xgrammar Enforcement" src="../../../../assets/2025-12-25-sgr-vllm/xgrammar_enforcement.svg" /></p>
<h3 id="where-does-masking-happen">Where Does Masking Happen?</h3>
<p>Here's the key insight: <strong>xgrammar modifies the output logits AFTER the model's forward pass, BEFORE sampling</strong>. It does not change the model itself — it filters what tokens can be selected.</p>
<p>The standard LLM inference loop looks like this:</p>
<div class="highlight"><pre><span></span><code>1. Input tokens → GPU Forward Pass → Logits (probability scores for all ~128K tokens)
2. Logits → Sampling (temperature, top-p, etc.) → Next Token
3. Repeat until done
</code></pre></div>
<p>xgrammar inserts itself between steps 1 and 2:</p>
<div class="highlight"><pre><span></span><code>1. Input tokens → GPU Forward Pass → Raw Logits
2. Raw Logits → xgrammar Logits Processor → Masked Logits
3. Masked Logits → Sampling → Next Token (guaranteed valid)
4. Repeat until done
</code></pre></div>
<p>The critical point: <strong>the model computes its full probability distribution on the GPU first</strong>. Then xgrammar, running on CPU, applies a bitmask to the logits before sampling. Invalid tokens get their logits set to <code>-∞</code>, which makes their probability exactly 0 after softmax.</p>
<h3 id="the-two-phase-process">The Two-Phase Process</h3>
<p>xgrammar's efficiency comes from splitting the work into two phases:</p>
<h4 id="phase-1-grammar-compilation-one-time-before-inference">Phase 1: Grammar Compilation (one-time, before inference)</h4>
<div class="highlight"><pre><span></span><code><span class="c1"># This happens once per schema</span>
<span class="n">tokenizer_info</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">grammar_compiler</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarCompiler</span><span class="p">(</span><span class="n">tokenizer_info</span><span class="p">)</span>
<span class="n">compiled_grammar</span> <span class="o">=</span> <span class="n">grammar_compiler</span><span class="o">.</span><span class="n">compile_json_schema</span><span class="p">(</span><span class="n">schema_json</span><span class="p">)</span>
</code></pre></div>
<p>During compilation, xgrammar:</p>
<ol>
<li>Converts your JSON Schema to a Context-Free Grammar (CFG)</li>
<li>Builds a Pushdown Automaton (PDA) — like a state machine with a stack for handling nested structures like <code>{"a": {"b": {"c": ...}}}</code></li>
<li>Pre-computes which tokens are valid at each grammar position (the "adaptive token mask cache")</li>
<li>Categorizes tokens as "context-independent" (can be pre-checked) or "context-dependent" (must be checked at runtime based on stack state)</li>
</ol>
<blockquote>
<p>[!NOTE]
About 99% of tokens are context-independent and can be cached. This is why xgrammar is so fast — most validity checks are just cache lookups.</p>
</blockquote>
<h4 id="phase-2-runtime-mask-generation-every-token">Phase 2: Runtime Mask Generation (every token)</h4>
<p>At each generation step:</p>
<ol>
<li>The <code>GrammarMatcher</code> tracks the current position in the grammar</li>
<li>It retrieves the pre-computed mask for context-independent tokens (cache lookup)</li>
<li>It runs the PDA to check the remaining context-dependent tokens</li>
<li>It combines these into a final bitmask and applies it to the logits</li>
</ol>
<h3 id="why-pushdown-automata-matter">Why Pushdown Automata Matter</h3>
<p>You might wonder: why not just use regex? The answer is <strong>nesting</strong>.</p>
<p>A regular expression (which is a Finite State Machine) cannot reliably match structures like:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span><span class="nt">&quot;user&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;profile&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;settings&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nt">&quot;theme&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;dark&quot;</span><span class="p">}}}}</span>
</code></pre></div>
<p>The problem is matching the closing braces <code>}}}</code> — you need to "remember" how many you opened. A Pushdown Automaton has a <strong>stack</strong> that tracks this context, enabling it to handle arbitrary nesting depth.</p>
<p>This is why xgrammar can enforce complex schemas with Union types, nested objects, and recursive structures — capabilities that simpler regex-based approaches cannot match.</p>
<h3 id="concrete-example-generating-a-float-field">Concrete Example: Generating a Float Field</h3>
<p>When the model is generating <code>"max_discount_percent":</code>, xgrammar knows from the schema that a <code>float</code> comes next. The mask:</p>
<ul>
<li><strong>Allows</strong> (probability unchanged): <code>0</code>, <code>1</code>, <code>2</code>, ..., <code>9</code>, <code>.</code>, <code>-</code></li>
<li><strong>Blocks</strong> (probability → 0): <code>"</code>, <code>{</code>, <code>[</code>, <code>true</code>, <code>false</code>, <code>null</code>, and all 128K+ other tokens</li>
</ul>
<p>The model's forward pass might have assigned high probability to the word <code>"fifteen"</code>. But after xgrammar's mask, that token has probability 0. The model <strong>must</strong> output digits.</p>
<h3 id="performance-why-near-zero-overhead">Performance: Why "Near-Zero Overhead"?</h3>
<p>Three factors make xgrammar fast:</p>
<ol>
<li>
<p><strong>Parallel execution</strong>: Mask computation (CPU) overlaps with the next forward pass (GPU). While the GPU computes logits for token N+1, the CPU computes the mask for token N.</p>
</li>
<li>
<p><strong>Caching</strong>: 99%+ of token validity is pre-computed during grammar compilation. Runtime checks are mostly cache lookups.</p>
</li>
<li>
<p><strong>C++ implementation</strong>: The hot path is optimized C++, not Python. The mask is applied directly to logits in-place.</p>
</li>
</ol>
<p>In benchmarks, xgrammar often shows <strong>negligible overhead</strong> — and sometimes structured generation is <em>faster</em> than unconstrained generation because the constrained vocabulary reduces sampling complexity.</p>
<hr />
<h2 id="practical-implementation-with-vllm">Practical Implementation with vLLM</h2>
<p>Let's look at a complete implementation using the <a href="https://github.com/slavadubrov/sgr-discount-manager">sgr-discount-manager</a> project — a demo that shows SGR patterns for dynamic pricing.</p>
<p><img alt="Agent Workflow" src="../../../../assets/2025-12-25-sgr-vllm/agent_workflow.svg" /></p>
<h3 id="project-structure">Project Structure</h3>
<div class="highlight"><pre><span></span><code>sgr/
├── agent.py            # Main orchestration
├── models/
│   └── schemas.py      # Pydantic SGR schemas
├── prompts/
│   ├── routing.py      # Phase 1 prompts
│   └── pricing.py      # Phase 3 prompts
├── store/
│   └── hybrid_store.py # Hot/Cold data retrieval
└── utils/
    └── llm_client.py   # LLM client wrapper with xgrammar
</code></pre></div>
<h3 id="step-1-define-your-schemas">Step 1: Define Your Schemas</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># sgr/models/schemas.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Union</span>


<span class="c1"># --- Phase 1: Routing (Union for branching) ---</span>
<span class="k">class</span><span class="w"> </span><span class="nc">FeatureLookup</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Route to DB lookup if pricing context is needed.&quot;&quot;&quot;</span>
    <span class="n">rationale</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">tool_name</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;fetch_user_features&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;fetch_user_features&quot;</span>
    <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GeneralResponse</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Standard response for non-pricing queries.&quot;&quot;&quot;</span>
    <span class="n">tool_name</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;respond&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;respond&quot;</span>
    <span class="n">content</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">class</span><span class="w"> </span><span class="nc">RouterSchema</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">action</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">FeatureLookup</span><span class="p">,</span> <span class="n">GeneralResponse</span><span class="p">]</span>


<span class="c1"># --- Phase 2: Pricing Logic (Cascade for sequential reasoning) ---</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PricingLogic</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Strict reasoning topology for dynamic pricing.</span>
<span class="sd">    Fields are ordered to enforce the analysis→decision flow.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 1. Data Analysis (Reflection)</span>
    <span class="n">churn_analysis</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> 
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Analyze churn_probability (High &gt; 0.7).&quot;</span><span class="p">)</span>
    <span class="n">financial_analysis</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> 
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Analyze cart_value and profit_margin.&quot;</span><span class="p">)</span>

    <span class="c1"># 2. Hard Math Enforcement</span>
    <span class="n">margin_math</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> 
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Calculate absolute profit: &#39;Cart $200 * 0.20 Margin = $40&#39;.&quot;</span><span class="p">)</span>

    <span class="c1"># 3. The Decision Constraint</span>
    <span class="n">max_discount_percent</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> 
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Max allowed discount %. NEVER exceed margin.&quot;</span><span class="p">)</span>

    <span class="c1"># 4. Final Output</span>
    <span class="n">offer_code</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Generated code (e.g. SAVE20).&quot;</span><span class="p">)</span>
    <span class="n">customer_message</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;The final polite offer text.&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="step-2-create-the-llm-client-with-xgrammar">Step 2: Create the LLM Client with xgrammar</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># sgr/utils/llm_client.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypeVar</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="n">BaseModel</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LLMClient</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper for vLLM with xgrammar-enforced structured generation.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;EMPTY&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_available_model</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_available_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Auto-detect the model running on vLLM server.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">models</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">models</span><span class="o">.</span><span class="n">data</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">models</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">id</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">return</span> <span class="s2">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_sgr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="n">schema_class</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">T</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run inference with Schema-Guided Response constraints.</span>

<span class="sd">        Uses vLLM&#39;s guided_json with xgrammar backend to enforce</span>
<span class="sd">        strict schema constraints at the token generation level.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schema_dict</span> <span class="o">=</span> <span class="n">schema_class</span><span class="o">.</span><span class="n">model_json_schema</span><span class="p">()</span>

        <span class="c1"># Enhance system message with schema for model guidance</span>
        <span class="n">enhanced_messages</span> <span class="o">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">enhanced_messages</span> <span class="ow">and</span> <span class="n">enhanced_messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;role&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;system&quot;</span><span class="p">:</span>
            <span class="n">schema_json</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">schema_dict</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">enhanced_messages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="n">enhanced_messages</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;content&quot;</span><span class="p">]</span>
                    <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Respond with JSON matching this schema:</span><span class="se">\n</span><span class="si">{</span><span class="n">schema_json</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">),</span>
            <span class="p">}</span>

        <span class="c1"># The magic: vLLM&#39;s guided_json with xgrammar backend</span>
        <span class="n">completion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">enhanced_messages</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># Low temp for deterministic reasoning</span>
            <span class="n">extra_body</span><span class="o">=</span><span class="p">{</span>
                <span class="s2">&quot;guided_json&quot;</span><span class="p">:</span> <span class="n">schema_dict</span><span class="p">,</span>  <span class="c1"># Pydantic schema as dict</span>
                <span class="s2">&quot;guided_decoding_backend&quot;</span><span class="p">:</span> <span class="s2">&quot;xgrammar&quot;</span><span class="p">,</span>  <span class="c1"># Hardware-enforced</span>
            <span class="p">},</span>
        <span class="p">)</span>

        <span class="n">raw_response</span> <span class="o">=</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
        <span class="k">return</span> <span class="n">schema_class</span><span class="o">.</span><span class="n">model_validate_json</span><span class="p">(</span><span class="n">raw_response</span><span class="p">)</span>
</code></pre></div>
<blockquote>
<p>[!NOTE]
The <code>guided_json</code> parameter accepts a JSON Schema dict. Combined with <code>guided_decoding_backend: "xgrammar"</code>, this ensures the LLM can only generate tokens that form valid JSON matching your schema.</p>
</blockquote>
<h3 id="step-3-orchestrate-the-agent">Step 3: Orchestrate the Agent</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># sgr/agent.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.models.schemas</span><span class="w"> </span><span class="kn">import</span> <span class="n">PricingLogic</span><span class="p">,</span> <span class="n">RouterSchema</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.prompts.routing</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_routing_prompt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.prompts.pricing</span><span class="w"> </span><span class="kn">import</span> <span class="n">build_pricing_context_prompt</span><span class="p">,</span> <span class="n">ASSISTANT_FETCH_MESSAGE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.store.hybrid_store</span><span class="w"> </span><span class="kn">import</span> <span class="n">HybridFeatureStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils.llm_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMClient</span>


<span class="k">def</span><span class="w"> </span><span class="nf">pricing_agent</span><span class="p">(</span><span class="n">user_query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Process a pricing query with three-phase SGR workflow.&quot;&quot;&quot;</span>

    <span class="n">llm</span> <span class="o">=</span> <span class="n">LLMClient</span><span class="p">()</span>
    <span class="n">feature_store</span> <span class="o">=</span> <span class="n">HybridFeatureStore</span><span class="p">()</span>

    <span class="c1"># Build conversation history</span>
    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">build_routing_prompt</span><span class="p">(</span><span class="n">user_id</span><span class="p">)},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">user_query</span><span class="p">},</span>
    <span class="p">]</span>

    <span class="c1"># --- Phase 1: Routing (Uses RouterSchema) ---</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;🤖 Processing: &#39;</span><span class="si">{</span><span class="n">user_query</span><span class="si">}</span><span class="s2">&#39; for </span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">decision</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">run_sgr</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">RouterSchema</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;📍 Routing decision: </span><span class="si">{</span><span class="n">decision</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">tool_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">decision</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">tool_name</span> <span class="o">==</span> <span class="s2">&quot;respond&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">decision</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">content</span>

    <span class="c1"># --- Phase 2: Context Retrieval ---</span>
    <span class="k">if</span> <span class="n">decision</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">tool_name</span> <span class="o">==</span> <span class="s2">&quot;fetch_user_features&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;🔍 Fetching features for </span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">feature_store</span><span class="o">.</span><span class="n">get_user_context</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;Error: User profile not found.&quot;</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   [Data] LTV: $</span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;user_ltv&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> | &quot;</span>
              <span class="sa">f</span><span class="s2">&quot;Margin: </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;cart_profit_margin&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

        <span class="c1"># Inject context into conversation</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">ASSISTANT_FETCH_MESSAGE</span><span class="p">})</span>
        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">build_pricing_context_prompt</span><span class="p">(</span>
                <span class="n">churn_prob</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;churn_probability&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
                <span class="n">cart_val</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;current_cart_value&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
                <span class="n">margin</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cart_profit_margin&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
                <span class="n">user_ltv</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;user_ltv&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="p">),</span>
        <span class="p">})</span>

        <span class="c1"># --- Phase 3: SGR Logic Execution (Uses PricingLogic) ---</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;🧠 Calculating Offer (Schema Enforced)...&quot;</span><span class="p">)</span>
        <span class="n">offer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">run_sgr</span><span class="p">(</span><span class="n">history</span><span class="p">,</span> <span class="n">PricingLogic</span><span class="p">)</span>

        <span class="c1"># Audit log — the SGR benefit: explicit reasoning traces</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   [Audit] Math: </span><span class="si">{</span><span class="n">offer</span><span class="o">.</span><span class="n">margin_math</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;   [Audit] Max Allowed: </span><span class="si">{</span><span class="n">offer</span><span class="o">.</span><span class="n">max_discount_percent</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">offer</span><span class="o">.</span><span class="n">customer_message</span>

    <span class="k">return</span> <span class="s2">&quot;I&#39;m sorry, I couldn&#39;t process your request.&quot;</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">pricing_agent</span><span class="p">(</span><span class="s2">&quot;I want a discount or I&#39;m leaving!&quot;</span><span class="p">,</span> <span class="s2">&quot;user_102&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">💬 Final Reply: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="step-4-run-vllm-with-xgrammar">Step 4: Run vLLM with xgrammar</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Start vLLM server with xgrammar backend (default in recent versions)</span>
python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span>Qwen/Qwen2.5-7B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">8000</span>

<span class="c1"># Run the agent</span>
uv<span class="w"> </span>run<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>sgr.agent
</code></pre></div>
<h3 id="example-output">Example Output</h3>
<div class="highlight"><pre><span></span><code>🤖 Processing: &#39;I want a discount or I&#39;m leaving!&#39; for user_102
📍 Routing decision: fetch_user_features
🔍 Fetching features for user_102...
   [Data] LTV: $1,500 | Margin: 20%
🧠 Calculating Offer (Schema Enforced)...
   [Audit] Math: Cart $200 * 0.20 Margin = $40
   [Audit] Max Allowed: 15.0%

💬 Final Reply: We value your loyalty! Here&#39;s a special 15% discount 
   with code SAVE15. This reflects our appreciation for your continued 
   business with us.
</code></pre></div>
<p>The audit log shows exactly how the model reasoned: it calculated the margin ($40 on a $200 cart at 20% margin), and correctly bounded the discount to stay within the profit constraint.</p>
<hr />
<h2 id="best-practices">Best Practices</h2>
<h3 id="schema-design">Schema Design</h3>
<ol>
<li><strong>Order fields by reasoning flow</strong>: Put analysis fields before decision fields</li>
<li><strong>Use descriptive Field descriptions</strong>: They guide the model's attention</li>
<li><strong>Constrain with Literal and Annotated</strong>: Use <code>Literal["a", "b"]</code> for enums, <code>Annotated[int, Ge(1), Le(10)]</code> for bounds</li>
<li><strong>Keep schemas focused</strong>: One schema per reasoning phase, compose with multiple calls</li>
</ol>
<h3 id="vllm-configuration">vLLM Configuration</h3>
<ol>
<li><strong>Use low temperature</strong> (0.1-0.3) for deterministic reasoning</li>
<li><strong>Let xgrammar handle structure</strong>: Don't over-engineer prompts for formatting</li>
<li><strong>Monitor token usage</strong>: SGR typically uses fewer tokens than CoT (no verbose prose)</li>
</ol>
<h3 id="production-considerations">Production Considerations</h3>
<ol>
<li><strong>Schema versioning</strong>: Track schema changes like API versions</li>
<li><strong>Fallback handling</strong>: Even with SGR, network/server errors need graceful handling</li>
<li><strong>Audit logging</strong>: Log raw SGR outputs for compliance and debugging</li>
<li><strong>Test with edge cases</strong>: Ensure schemas handle boundary conditions</li>
</ol>
<hr />
<h2 id="conclusion">Conclusion</h2>
<p>Schema-Guided Reasoning bridges the gap between the flexibility of LLMs and the reliability requirements of production systems. By defining your reasoning topology as a Pydantic schema and letting xgrammar enforce it, you get:</p>
<ul>
<li><strong>Guaranteed valid output</strong> — no retry loops, no parsing failures</li>
<li><strong>Explicit reasoning traces</strong> — every step is auditable</li>
<li><strong>Smaller model viability</strong> — the schema compensates for weaker instruction-following</li>
<li><strong>Lower costs</strong> — fewer tokens, no retries, smaller models work</li>
</ul>
<p>The <a href="https://github.com/slavadubrov/sgr-discount-manager">sgr-discount-manager</a> demo shows how these patterns work in practice. Clone it, run it, and adapt the schemas for your use case.</p>
<hr />
<h2 id="references">References</h2>
<h3 id="sgr-framework">SGR Framework</h3>
<ul>
<li><a href="https://abdullin.com/schema-guided-reasoning/">Schema-Guided Reasoning (SGR)</a> — Rinat Abdullin's original framework</li>
<li><a href="https://abdullin.com/schema-guided-reasoning/patterns">SGR Patterns</a> — Cascade, Routing, Cycle patterns</li>
</ul>
<h3 id="xgrammar">xgrammar</h3>
<ul>
<li><a href="https://github.com/mlc-ai/xgrammar">xgrammar GitHub</a> — Fast, flexible structured generation library</li>
<li><a href="https://xgrammar.mlc.ai/docs/">xgrammar Documentation</a> — Official docs with quick start guide</li>
<li><a href="https://xgrammar.mlc.ai/docs/start/quick_start">xgrammar Quick Start</a> — Getting started with xgrammar</li>
<li><a href="https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar">Achieving Efficient Structured Generation with XGrammar</a> — MLC blog post on xgrammar internals</li>
</ul>
<h3 id="vllm">vLLM</h3>
<ul>
<li><a href="https://docs.vllm.ai/en/latest/features/structured_outputs.html">vLLM Structured Outputs</a> — Official documentation</li>
</ul>
<h3 id="demo-project">Demo Project</h3>
<ul>
<li><a href="https://github.com/slavadubrov/sgr-discount-manager">sgr-discount-manager</a> — Working demo with all code examples from this post</li>
</ul>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="/feed_rss_created.xml" target="_blank" rel="noopener" title="RSS" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19 7.38 20 6.18 20 5 20 4 19 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27zm0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.toc", "navigation.toc.sticky", "navigation.toc.maxdepth", "navigation.toc.title", "navigation.toc.collapse", "navigation.toc.collapse_empty_groups", "navigation.toc.collapse_single_children", "content.code.copy"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
      
        <script src="../../../../../javascripts/mermaid-init.js"></script>
      
    
  </body>
</html>