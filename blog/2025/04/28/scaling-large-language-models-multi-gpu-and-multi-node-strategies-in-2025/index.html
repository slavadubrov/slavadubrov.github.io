
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://slavadubrov.github.io/blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/">
      
      
        <link rel="prev" href="../../19/setting-up-a-macbook-for-ai-engineering/">
      
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025 - Shared Intelligence: Tips & Tricks in Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Shared Intelligence: Tips &amp; Tricks in Machine Learning" class="md-header__button md-logo" aria-label="Shared Intelligence: Tips & Tricks in Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Shared Intelligence: Tips & Tricks in Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../topics/" class="md-tabs__link">
        
  
  
    
  
  Topics

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="../../../../" class="md-tabs__link">
        
  
  
    
  
  Blog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Shared Intelligence: Tips &amp; Tricks in Machine Learning" class="md-nav__button md-logo" aria-label="Shared Intelligence: Tips & Tricks in Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Shared Intelligence: Tips & Tricks in Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../topics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Topics
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-parallelism-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      1. Parallelism Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Parallelism Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-data-parallelism-dp" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 Data Parallelism (DP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.1 Data Parallelism (DP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-fully-sharded-data-parallelism-fsdp" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.1 Fully Sharded Data Parallelism (FSDP)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-tensor-parallelism-tp" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 Tensor Parallelism (TP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-pipeline-parallelism-pp" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 Pipeline Parallelism (PP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-context-parallelism-cp" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 Context Parallelism (CP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-expert-parallelism-or-mixture-of-experts" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 Expert Parallelism (or Mixture of Experts)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-4d-5d-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      1.6 4D-5D Parallelism
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-training-strategies-2025-update" class="md-nav__link">
    <span class="md-ellipsis">
      2. Training Strategies (2025 update)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Training Strategies (2025 update)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-single-node-multi-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Single-Node, Multi-GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-multi-node-multi-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Multi-Node, Multi-GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-4d-5d-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 4D-5D Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-recommended-tools-and-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      3. Recommended Tools and Libraries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-choosing-the-right-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      4. Choosing the Right Strategy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cheatsheet-from-huggingface-folks" class="md-nav__link">
    <span class="md-ellipsis">
      Cheatsheet from HuggingFace Folks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-04-28 00:00:00+00:00" class="md-ellipsis">April 28, 2025</time>
                      </div>
                    </li>
                    
                    
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  


  <nav class="md-tags" >
    
      
      
      
        <a href="../../../../../topics/#tag:deep-learning" class="md-tag">Deep Learning</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:distributed-training" class="md-tag">Distributed Training</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:gpu" class="md-tag">GPU</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:llm" class="md-tag">LLM</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:parallelism" class="md-tag">Parallelism</a>
      
    
  </nav>



<h1 id="scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025">Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025</h1>
<p>As LLMs continue to grow in complexity and size, efficient training and inference require leveraging multiple GPUs and, often, multiple systems. This guide explores prevalent strategies and tools in 2025 that facilitate such scalability, incorporating insights from Hugging Face's <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">Ultra-Scale Playbook</a>.</p>
<!-- more -->

<h2 id="1-parallelism-techniques">1. Parallelism Techniques</h2>
<h3 id="11-data-parallelism-dp">1.1 Data Parallelism (DP)</h3>
<p>In classic data-parallel training <strong>every GPU keeps a full copy of the model</strong>.
A large batch is split into <em>N</em> micro-batches; each rank runs forward + backward on its piece and then gradients are <strong>all-reduced (averaged)</strong> so that all replicas stay in sync before the optimizer step.</p>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Simplicity first</strong> - almost zero code changes; works everywhere.</li>
<li><strong>Redundant memory</strong> - O(total params) on every GPU, so model size is bounded by a single card.</li>
<li><strong>Communication cost</strong> - one gradient all-reduce per step (~2 x parameter size).</li>
<li><strong>Throughput scaling</strong> - global batch = per-GPU batch x <em>N</em>; watch out for generalization when scaling batch too far.</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>flowchart LR
    subgraph DataLoader
        D[Global batch] --&gt; |split| MB1[Micro-batch 1]
        D[Global batch] --&gt; |split| MB2[Micro-batch 2]
        D[Global batch] --&gt; |split| MBN[Micro-batch N]
    end
    subgraph GPU1
        MB1[Micro-batch 1] --&gt; M1[Model copy]
    end
    subgraph GPU2
        MB2[Micro-batch 2] --&gt; M2[Model copy]
    end
    subgraph GPUN
        MBN[Micro-batch N] --&gt; MN[Model copy]
    end
    M1[Model copy] &amp; M2[Model copy] &amp; MN[Model copy] --&gt; G[All-reduce → average gradients]
    G[All-reduce → average gradients] --&gt; U[Synchronised weight update]</code></pre>
<ul>
<li><strong>Tools</strong>: <a href="https://pytorch.org/docs/stable/notes/ddp.html">PyTorch DDP</a>, <a href="https://horovod.ai/">Horovod</a>.</li>
</ul>
<h4 id="111-fully-sharded-data-parallelism-fsdp">1.1.1 Fully Sharded Data Parallelism (FSDP)</h4>
<p>FSDP is a type of data-parallel training, but unlike traditional data-parallel, which maintains a per-GPU copy of a model's parameters, gradients and optimizer states, it shards all of these states across data-parallel workers and can optionally offload the sharded model parameters to CPUs. <a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api">Pytorch</a></p>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Memory scaling</strong>: O(total params / NGPU) - enables multi-billion-parameter models to fit on 24 GB cards.</li>
<li><strong>Zero redundancy</strong>: No GPU ever holds a full copy of the model; identical to DeepSpeed ZeRO-3.</li>
<li><strong>Overlap compute &amp; communication</strong>: PyTorch overlaps the all-gather with computation to hide latency.</li>
<li><strong>Granularity control</strong>: You can wrap the whole model or nest FSDP wrappers on sub-modules for finer control.</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>flowchart TD
    %% GPU-local state
    subgraph "GPU 1"
        direction TB
        P1[Param shard P₁]
        G1[Grad shard G₁]
        O1[Opt shard O₁]
    end
    subgraph "GPU 2"
        direction TB
        P2[Param shard P₂]
        G2[Grad shard G₂]
        O2[Opt shard O₂]
    end
    subgraph "GPU N"
        direction TB
        PN[Param shard Pₙ]
        GN[Grad shard Gₙ]
        ON[Opt shard Oₙ]
    end

    %% Mini-batch pipeline
    start([Start micro-batch]) --&gt; gather[Step 1: All-Gather]
    gather --&gt; fwd[Step 2: Forward compute]
    fwd --&gt; reshard[Step 3: Re-shard P]
    reshard --&gt; bwd[Step 4: Backward compute]
    bwd --&gt; reduce[Step 5: Reduce-Scatter]
    reduce --&gt; update[Step 6: Optimizer update]

    %% Collective edges (dotted to indicate broadcast)
    P1 -.-&gt; gather
    P2 -.-&gt; gather
    PN -.-&gt; gather</code></pre>
<blockquote>
<p><strong>Note</strong>: In the diagram above, P represents Parameters (model weights), G represents Gradients, and O represents Optimizer states. These are the three main components of model state that are sharded across GPUs in FSDP.</p>
</blockquote>
<ul>
<li><strong>Use Case</strong>: Training very large models (&gt; 10 B parameters) that do not fit on a single GPU.</li>
<li><strong>Tools</strong>: <a href="https://pytorch.org/docs/stable/fsdp.html">PyTorch FSDP</a>, <a href="https://www.deepspeed.ai/tutorials/zero/">DeepSpeed ZeRO-3</a>.</li>
</ul>
<h3 id="12-tensor-parallelism-tp">1.2 Tensor Parallelism (TP)</h3>
<p>TP <strong>slices individual weight tensors across GPUs</strong> so each rank stores only a shard (e.g., specific columns or rows). During the forward pass each rank computes its partial matrix multiplication; intermediate activations are <strong>all-gathered or reduced</strong> to produce the layer output.</p>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Shards compute &amp; memory</strong> - enables layers larger than a single GPU.</li>
<li><strong>Orthogonal to DP</strong> - combine TP x DP for higher scale (Megatron uses a 2-D «TP x DP» grid).</li>
<li><strong>Best for dense GEMM(General Matrix Multiplication)-heavy blocks</strong> - attention &amp; FFN matrices.</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>flowchart LR
    A[X activations] --&gt; |broadcast| X1[GPU1]
    A --&gt; |broadcast| X2[GPU2]
    A --&gt; |broadcast| XN[GPUN]
    subgraph ShardedWeights
        W1[W shard₁] --- X1
        W2[W shard₂] --- X2
        WN[W shardₙ] --- XN
    end
    X1 --&gt; P1[Partial Y₁]
    X2 --&gt; P2[Partial Y₂]
    XN --&gt; PN[Partial Yₙ]
    P1 &amp; P2 &amp; PN --&gt; C[Concat / reduce → Y]</code></pre>
<ul>
<li><strong>Tools</strong>: <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>, <a href="https://github.com/hpcaitech/ColossalAI">ColossalAI</a>.</li>
</ul>
<hr />
<h3 id="13-pipeline-parallelism-pp">1.3 Pipeline Parallelism (PP)</h3>
<p>PP <strong>distributes consecutive blocks of layers to different GPUs</strong> (pipeline stages).
Micro-batches flow through stages like an assembly line, so computation and communication overlap.</p>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Memory relief</strong> - each rank stores only its slice of the network depth.</li>
<li><strong>Bubble latency</strong> - first and last few micro-batches see idle time; mitigate with enough micro-batches or sophisticated scheduling.</li>
<li><strong>Composable with DP/TP</strong> - e.g., 2 x TP inside each stage x 4 x PP across depth.</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>sequenceDiagram
    participant S0 as GPU-Stage 0 (Layers 1-4)
    participant S1 as GPU-Stage 1 (Layers 5-8)
    participant S2 as GPU-Stage 2 (Layers 9-12)
    Note over S0,S2: ← time →
    S0-&gt;&gt;S0: Fwd/Bwd µ-batch 0
    S0-&gt;&gt;S1: send activations
    S1-&gt;&gt;S1: Fwd/Bwd µ-batch 0
    S1-&gt;&gt;S2: send activations
    S0-&gt;&gt;S0: Fwd/Bwd µ-batch 1
    S2-&gt;&gt;S2: Fwd/Bwd µ-batch 0</code></pre>
<ul>
<li><strong>Tools</strong>: <a href="https://www.deepspeed.ai/tutorials/pipeline/">DeepSpeed PP</a>, <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a>, <a href="https://arxiv.org/abs/1811.06965">GPipe</a>.</li>
</ul>
<h3 id="14-context-parallelism-cp">1.4 Context Parallelism (CP)</h3>
<p>CP (a.k.a. <strong>sequence parallelism</strong>) splits the <strong>sequence length / token dimension</strong> across GPUs so each rank handles a contiguous block of tokens, enabling context windows far beyond single-GPU memory.</p>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Long-context enabler</strong> - reach 32 k, 64 k+ tokens.</li>
<li><strong>Attention communication</strong> - GPUs exchange keys/values (all-gather) for cross-token attention each layer.</li>
<li><strong>Pairs well with TP &amp; PP</strong> - CP handles tokens while others handle model axes.</li>
<li><strong>Early-stage technique</strong> - currently in research code (Picotron / Nanotron).</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>flowchart LR
    subgraph Input["Input Sequence"]
        S[Sequence 0-8191 tokens]
    end

    subgraph CrossGPU["Cross-GPU Processing"]
        direction LR
        subgraph GPU1["GPU 1"]
            direction TB
            T0[Tokens 0-4095]
            A0[Self-Attention Block]
            T0 --&gt; A0
        end

        subgraph GPU2["GPU 2"]
            direction TB
            T1[Tokens 4096-8191]
            A1[Self-Attention Block]
            T1 --&gt; A1
        end

        GPU1 &lt;--&gt;|Exchange Keys/Values| GPU2
    end

    subgraph Output["Output Processing"]
        M[Merge Logits]
        O[Output Sequence]
        M --&gt; O
    end

    S --&gt; |Split| T0
    S --&gt; |Split| T1

    A0 --&gt; M
    A1 --&gt; M</code></pre>
<ul>
<li><strong>Tools</strong>: <a href="https://github.com/huggingface/picotron">Picotron</a>, <a href="https://github.com/huggingface/nanotron">Nanotron</a>.</li>
</ul>
<h3 id="15-expert-parallelism-or-mixture-of-experts">1.5 Expert Parallelism (or Mixture of Experts)</h3>
<p>MoE layers contain dozens (or even hundreds) of parallel <strong>experts</strong> (small feed-forward sub-networks).
For every token a lightweight <strong>gating network</strong> selects the top-<em>k</em> experts, so only that subset runs.
This decouples <strong>model capacity</strong> (total parameters) from <strong>per-token compute/FLOPs</strong>.</p>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Sparse activation</strong> - With <em>k = 2</em> out of 64 experts each token touches ~3 % of the parameters, yet the model still "sees" the full capacity during training.</li>
<li><strong>Conditional computation</strong> - Tokens route to different experts, letting each specialize (e.g., code vs poetry).</li>
<li><strong>Load-balancing loss</strong> - Extra loss term keeps expert usage uniform to avoid stragglers.</li>
<li><strong>Scale to trillions</strong> - Total parameters scale linearly with #experts, compute stays roughly constant.</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>flowchart LR
    subgraph Input_Tokens["Input Tokens"]
        T1["T₁"]
        T2["T₂"]
        T3["T₃"]
    end
    G["Gating Network"]
    subgraph Experts["Experts"]
        E1["Expert 1"]
        E2["Expert 2"]
        E3["Expert 3"]
        E4["⋯"]
    end
    T1 --&gt; G
    T2 --&gt; G
    T3 --&gt; G
    G --&gt;|top-k routes| E1
    G --&gt;|top-k routes| E2
    G --&gt;|top-k routes| E3
    E1 &amp; E2 &amp; E3 --&gt; O["Concatenate + Mix"]</code></pre>
<ul>
<li><strong>Use Case</strong>: Scaling to 100 B-1 T+ parameters without proportional compute cost.</li>
<li><strong>Tools</strong>: <a href="https://www.deepspeed.ai/tutorials/mixture-of-experts/">DeepSpeed-MoE</a>, <a href="https://arxiv.org/abs/2001.04451">GShard / Switch Transformer</a>.</li>
</ul>
<hr />
<h3 id="16-4d-5d-parallelism">1.6 4D-5D Parallelism</h3>
<ul>
<li><strong>4D</strong> composes <strong>Data (D)</strong>, <strong>Tensor (T)</strong>, <strong>Pipeline (P)</strong>, and <strong>Context (C)</strong> parallelism so every axis of the workload can be distributed.
  Picture the GPUs as a 4-D lattice: <em>N = DxTxPxC</em> ranks.</li>
<li><strong>5D</strong> combines <strong>4D</strong> + <strong>Expert Parallelism</strong>.</li>
</ul>
<p><strong>Key ideas</strong></p>
<ul>
<li><strong>Extreme scale</strong> - Easily maps 10³-10⁴ GPUs for 100 B-parameter, 8 k-context models.</li>
<li><strong>Topology aware</strong> - Tune each dimension to match intra-node (NVLink), inter-node (IB), and rack-level bandwidth.</li>
<li><strong>Memory &amp; compute balance</strong> - TP shards big matrices, CP splits long sequences, PP handles depth, DP feeds throughput.</li>
</ul>
<p><strong>Mermaid Diagram</strong></p>
<pre class="mermaid"><code>graph TD
    %% Example 2x2x2x2 grid (16 GPUs)
    subgraph Stage0["Pipeline Stage 0"]
        subgraph TP0["Tensor Group 0"]
            R0000["GPU D0-C0"]
            R0001["GPU D0-C1"]
        end
        subgraph TP1["Tensor Group 1"]
            R0010["GPU D0-C0"]
            R0011["GPU D0-C1"]
        end
    end
    subgraph Stage1["Pipeline Stage 1"]
        subgraph TP0S1["Tensor Group 0"]
            R0100["GPU D1-C0"]
            R0101["GPU D1-C1"]
        end
        subgraph TP1S1["Tensor Group 1"]
            R0110["GPU D1-C0"]
            R0111["GPU D1-C1"]
        end
    end
    A["Micro-batches (DP)"] --&gt; R0000
    R0000 --&gt;|TP| R0010
    R0010 --&gt;|PP| R0100
    R0100 --&gt;|CP assemble| Z["Output"]</code></pre>
<ul>
<li><strong>Use Case</strong>: Training &gt; 100 B-parameter models with multi-node clusters and long context windows.</li>
<li><strong>Tools</strong>: <a href="https://github.com/huggingface/picotron">Picotron</a>, <a href="https://github.com/huggingface/nanotron">Nanotron</a>.</li>
</ul>
<h2 id="2-training-strategies-2025-update">2. Training Strategies (2025 update)</h2>
<blockquote>
<p><strong>Rule of thumb</strong> - pick the simplest scheme that fits in memory <strong>and</strong> saturates your interconnect.
Start with a shard-aware data-parallel variant (FSDP/ZeRO-3).
Add <strong>Tensor ↔ Pipeline ↔ Context</strong> axes only when the model or the sequence length forces you.</p>
</blockquote>
<table>
<thead>
<tr>
<th>Hardware scope</th>
<th>Fastest link</th>
<th>Go-to recipe</th>
<th>When to switch</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1 node</strong> (2-8 GPUs, NVLink / PCIe Gen5)</td>
<td>200-900 GB/s</td>
<td><strong>FSDP + small TP</strong> via <code>torchrun</code> or DeepSpeed</td>
<td>Model &gt; 1 x GPU</td>
</tr>
<tr>
<td><strong>2-16 nodes</strong> (≤128 GPUs, NVLink + InfiniBand)</td>
<td>25-200 GB/s</td>
<td><strong>"TP inside, DP across" + optional PP</strong></td>
<td>Model &gt; 1 x node</td>
</tr>
<tr>
<td><strong>&gt;16 nodes</strong> (hundreds-thousands GPUs)</td>
<td>≤25 GB/s</td>
<td><strong>4-D grid (DPxTPxPPxCP)</strong></td>
<td>70 B + params <strong>and</strong> 32 k + tokens</td>
</tr>
</tbody>
</table>
<h3 id="21-single-node-multi-gpu">2.1 Single-Node, Multi-GPU</h3>
<p>Combine zeRO-style <strong>Fully-Sharded Data Parallelism (FSDP)</strong> with a low-degree <strong>Tensor Parallelism</strong> group that stays inside the node.</p>
<ul>
<li>FSDP shards parameters, gradients, and optimizer states across all GPUs, so each GPU uses only about 1/n of the total model memory — significantly reducing memory usage per GPU.</li>
<li>TP protects matmul kernels from weight-gather latency; keep <code>tp&lt;=2</code> on PCIe, up to <code>tp&lt;=4</code> on NVLink.</li>
</ul>
<h3 id="22-multi-node-multi-gpu">2.2 Multi-Node, Multi-GPU</h3>
<p>Start with <strong>Tensor Parallelism inside a node</strong> and <strong>Data Parallelism across nodes</strong>; introduce <strong>Pipeline Parallelism</strong> when the model no longer fits on one node.</p>
<ul>
<li>Keep TP collectives inside the node to avoid slow inter-node all-reduces.</li>
<li>Tune <strong>micro-batch = 4 x PP degree</strong> as recommended by the Ultra-Scale Playbook to limit the pipeline bubble.</li>
</ul>
<h3 id="23-4d-5d-parallelism">2.3 4D-5D Parallelism</h3>
<p>When <strong>weights</strong> and <strong>sequence length</strong> both exceed a node, use every axis (DP x TP x PP x CP).</p>
<p>Guidelines:</p>
<ul>
<li><strong>TP</strong> groups stay inside nodes; <strong>PP/CP</strong> may span nodes.</li>
<li>Increase <strong>DP</strong> first when you need a larger global batch; it is the cheapest axis communication-wise.</li>
<li>Expect ~75 % scaling efficiency up to 512 GPUs on InfiniBand clusters <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">(HF benchmarks, Feb 2025)</a>.</li>
</ul>
<hr />
<h3 id="3-recommended-tools-and-libraries">3. Recommended Tools and Libraries</h3>
<table>
<thead>
<tr>
<th>Tool/Library</th>
<th>Description</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepSpeed</td>
<td>Optimizes training and inference for large models</td>
<td><a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a></td>
</tr>
<tr>
<td>Megatron-LM</td>
<td>Framework for training large transformer models with TP and PP</td>
<td><a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a></td>
</tr>
<tr>
<td>ColossalAI</td>
<td>Provides a unified interface for various parallelism strategies</td>
<td><a href="https://github.com/hpcaitech/ColossalAI">ColossalAI</a></td>
</tr>
<tr>
<td>Horovod</td>
<td>Distributed training framework supporting multiple backends</td>
<td><a href="https://github.com/horovod/horovod">Horovod</a></td>
</tr>
<tr>
<td>Hugging Face Accelerate</td>
<td>Simplifies training and inference across devices</td>
<td><a href="https://github.com/huggingface/accelerate">Accelerate</a></td>
</tr>
<tr>
<td>TensorRT-LLM</td>
<td>High-performance inference library by NVIDIA</td>
<td><a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a></td>
</tr>
<tr>
<td>vLLM</td>
<td>Efficient LLM inference engine</td>
<td><a href="https://github.com/vllm-project/vllm">vLLM</a></td>
</tr>
<tr>
<td>Picotron</td>
<td>Minimalistic 4D-parallelism distributed training framework for educational purposes</td>
<td><a href="https://github.com/huggingface/picotron">Picotron</a></td>
</tr>
<tr>
<td>Nanotron</td>
<td>Minimalistic large language model 3D-parallelism training framework</td>
<td><a href="https://github.com/huggingface/nanotron">Nanotron</a></td>
</tr>
</tbody>
</table>
<h3 id="4-choosing-the-right-strategy">4. Choosing the Right Strategy</h3>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Recommended Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training on a single machine with multiple GPUs</td>
<td>Combine DP with TP or PP using DeepSpeed or PyTorch FSDP.</td>
</tr>
<tr>
<td>Training across multiple machines</td>
<td>Utilize DeepSpeed with a combination of DP, TP, and PP.</td>
</tr>
<tr>
<td>Training with very long context windows</td>
<td>Use Picotron or Nanotron with Context Parallelism.</td>
</tr>
<tr>
<td>Training extremely large models</td>
<td>Leverage 4D parallelism with Picotron or Nanotron.</td>
</tr>
<tr>
<td>Inference with latency constraints</td>
<td>Deploy using TensorRT-LLM or vLLM.</td>
</tr>
<tr>
<td>Inference for very large models</td>
<td>Use DeepSpeed Inference with ZeRO-Offload.</td>
</tr>
<tr>
<td>Quick deployment of models</td>
<td>Leverage Hugging Face TGI.</td>
</tr>
</tbody>
</table>
<h3 id="cheatsheet-from-huggingface-folks">Cheatsheet from HuggingFace Folks</h3>
<p><img alt="Ultra-Scale LLM Cheatsheet" src="https://nanotron-ultrascale-playbook.static.hf.space/dist/assets/images/ultra-cheatsheet.svg" /></p>
<p>⸻</p>
<p>By adopting these strategies and tools, you can effectively scale LLM training and inference across multiple GPUs and systems, ensuring optimal performance and resource utilization.</p>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.toc", "navigation.toc.sticky", "navigation.toc.maxdepth", "navigation.toc.title", "navigation.toc.collapse", "navigation.toc.collapse_empty_groups", "navigation.toc.collapse_single_children"], "search": "../../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>