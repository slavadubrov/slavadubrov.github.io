
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://slavadubrov.github.io/blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/">
      
      
        <link rel="prev" href="../../../04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/">
      
      
        <link rel="next" href="../../06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025 - Shared Intelligence: Tips & Tricks in Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#scaling-large-language-models-practical-multi-gpu-and-multi-node-strategies-for-2025" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Shared Intelligence: Tips &amp; Tricks in Machine Learning" class="md-header__button md-logo" aria-label="Shared Intelligence: Tips & Tricks in Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Shared Intelligence: Tips & Tricks in Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../topics/" class="md-tabs__link">
        
  
  
    
  
  Topics

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="../../../../" class="md-tabs__link">
        
  
  
    
  
  Blog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Shared Intelligence: Tips &amp; Tricks in Machine Learning" class="md-nav__button md-logo" aria-label="Shared Intelligence: Tips & Tricks in Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Shared Intelligence: Tips & Tricks in Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../topics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Topics
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#why-scaling-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why Scaling Matters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-parallelism-techniques-explained-simply" class="md-nav__link">
    <span class="md-ellipsis">
      1. Parallelism Techniques Explained Simply
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Parallelism Techniques Explained Simply">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-data-parallelism-dp" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 Data Parallelism (DP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-fully-sharded-data-parallelism-fsdp" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 Fully Sharded Data Parallelism (FSDP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-tensor-parallelism-tp" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 Tensor Parallelism (TP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-pipeline-parallelism-pp" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 Pipeline Parallelism (PP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-context-parallelism-cp" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 Context Parallelism (CP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-expert-parallelism-or-mixture-of-experts" class="md-nav__link">
    <span class="md-ellipsis">
      1.6 Expert Parallelism (or Mixture of Experts)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-practical-training-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      2. Practical Training Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Practical Training Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-single-machine-2-8-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Single Machine (2-8 GPUs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-small-cluster-2-16-nodes-128-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Small Cluster (2-16 nodes, ≤128 GPUs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-large-cluster-hundreds-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Large Cluster (hundreds+ GPUs)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-practical-tools-worth-learning" class="md-nav__link">
    <span class="md-ellipsis">
      3. Practical Tools Worth Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-making-the-right-choice-a-simple-decision-tree" class="md-nav__link">
    <span class="md-ellipsis">
      4. Making the Right Choice: A Simple Decision Tree
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-the-ultra-scale-cheatsheet" class="md-nav__link">
    <span class="md-ellipsis">
      5. The Ultra-Scale Cheatsheet
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-05-04 00:00:00+00:00" class="md-ellipsis">May 4, 2025</time>
                      </div>
                    </li>
                    
                    
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  


  <nav class="md-tags" >
    
      
      
      
        <a href="../../../../../topics/#tag:deep-learning" class="md-tag">Deep Learning</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:distributed-training" class="md-tag">Distributed Training</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:gpu" class="md-tag">GPU</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:llm" class="md-tag">LLM</a>
      
    
      
      
      
        <a href="../../../../../topics/#tag:parallelism" class="md-tag">Parallelism</a>
      
    
  </nav>



<h1 id="scaling-large-language-models-practical-multi-gpu-and-multi-node-strategies-for-2025">Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</h1>
<p>The race to build bigger, better language models continues at breakneck speed. Today's state-of-the-art models require massive computing resources that no single GPU can handle. Whether you're training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.</p>
<p>This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face's <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">Ultra-Scale Playbook</a>.</p>
<!-- more -->

<h2 id="why-scaling-matters">Why Scaling Matters</h2>
<p>Before diving into techniques, let's understand why this matters:</p>
<ul>
<li><strong>Model size</strong>: A 70B parameter model requires ~140GB just to store in FP16 - far beyond any single GPU</li>
<li><strong>Training time</strong>: Even with 8 A100s, training a 13B model from scratch takes weeks</li>
<li><strong>Context length</strong>: Processing long contexts (32k+ tokens) often exceeds single-GPU memory</li>
<li><strong>Inference speed</strong>: Distributing inference can reduce latency for demanding applications</li>
</ul>
<p>Let's explore how to overcome these challenges.</p>
<h2 id="1-parallelism-techniques-explained-simply">1. Parallelism Techniques Explained Simply</h2>
<h3 id="11-data-parallelism-dp">1.1 Data Parallelism (DP)</h3>
<p>Think of this as multiple workers all having the same instruction manual (model), but each working on different examples.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Each GPU gets an identical copy of the model</li>
<li>Each processes different data samples</li>
<li>Results are combined by averaging gradients</li>
</ul>
<p><strong>When to use it:</strong></p>
<ul>
<li>Your model fits on a single GPU</li>
<li>You want to process more data in parallel</li>
<li>You need a simple solution with minimal code changes</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    subgraph DataLoader
        D[Global batch] --&gt; |split| MB1[Micro-batch 1]
        D[Global batch] --&gt; |split| MB2[Micro-batch 2]
        D[Global batch] --&gt; |split| MBN[Micro-batch N]
    end
    subgraph GPU1
        MB1[Micro-batch 1] --&gt; M1[Model copy]
    end
    subgraph GPU2
        MB2[Micro-batch 2] --&gt; M2[Model copy]
    end
    subgraph GPUN
        MBN[Micro-batch N] --&gt; MN[Model copy]
    end
    M1[Model copy] &amp; M2[Model copy] &amp; MN[Model copy] --&gt; G[All-reduce -&gt; average gradients]
    G[All-reduce -&gt; average gradients] --&gt; U[Synchronised weight update]</code></pre>
<p><strong>Tools</strong>: <a href="https://pytorch.org/docs/stable/notes/ddp.html">PyTorch DDP</a>, <a href="https://horovod.ai/">Horovod</a>.</p>
<h3 id="12-fully-sharded-data-parallelism-fsdp">1.2 Fully Sharded Data Parallelism (FSDP)</h3>
<p>FSDP is like DP but more memory-efficient - imagine each worker only keeping part of the instruction manual and borrowing pages from colleagues when needed.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Model parameters, gradients, and optimizer states are split across GPUs</li>
<li>During computation, GPUs gather needed parameters from others</li>
<li>After backward pass, each GPU only updates its part</li>
</ul>
<p><strong>Real-world impact:</strong></p>
<ul>
<li>Training very large models (&gt; 10 B parameters) that do not fit on a single GPU.</li>
</ul>
<pre class="mermaid"><code>flowchart TD
    %% GPU-local state
    subgraph "GPU 1"
        direction TB
        P1[Param shard P₁]
        G1[Grad shard G₁]
        O1[Opt shard O₁]
    end
    subgraph "GPU 2"
        direction TB
        P2[Param shard P₂]
        G2[Grad shard G₂]
        O2[Opt shard O₂]
    end
    subgraph "GPU N"
        direction TB
        PN[Param shard Pₙ]
        GN[Grad shard Gₙ]
        ON[Opt shard Oₙ]
    end

    %% Mini-batch pipeline
    start([Start micro-batch]) --&gt; gather[Step 1: All-Gather]
    gather --&gt; fwd[Step 2: Forward compute]
    fwd --&gt; reshard[Step 3: Re-shard P]
    reshard --&gt; bwd[Step 4: Backward compute]
    bwd --&gt; reduce[Step 5: Reduce-Scatter]
    reduce --&gt; update[Step 6: Optimizer update]

    %% Collective edges (dotted to indicate broadcast)
    P1 -.-&gt; gather
    P2 -.-&gt; gather
    PN -.-&gt; gather</code></pre>
<p><strong>Tools</strong>: <a href="https://pytorch.org/docs/stable/fsdp.html">PyTorch FSDP</a>, <a href="https://www.deepspeed.ai/tutorials/zero/">DeepSpeed ZeRO-3</a>.</p>
<h3 id="13-tensor-parallelism-tp">1.3 Tensor Parallelism (TP)</h3>
<p>Tensor parallelism splits individual layers across GPUs - like dividing a massive spreadsheet calculation across multiple people.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Individual weight matrices are split across GPUs</li>
<li>Each GPU computes part of each layer's output</li>
<li>Results are combined before moving to the next layer</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li>Massive attention layers and FFNs</li>
<li>When FSDP alone isn't enough</li>
<li>Works well inside a node with fast GPU-GPU connections</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    A[X activations] --&gt; |broadcast| X1[GPU1]
    A --&gt; |broadcast| X2[GPU2]
    A --&gt; |broadcast| XN[GPUN]
    subgraph ShardedWeights
        W1[W shard₁] --- X1
        W2[W shard₂] --- X2
        WN[W shardₙ] --- XN
    end
    X1 --&gt; P1[Partial Y₁]
    X2 --&gt; P2[Partial Y₂]
    XN --&gt; PN[Partial Yₙ]
    P1 &amp; P2 &amp; PN --&gt; C[Concat / reduce -&gt; Y]</code></pre>
<p><strong>Tools</strong>: <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>, <a href="https://github.com/hpcaitech/ColossalAI">ColossalAI</a>.</p>
<h3 id="14-pipeline-parallelism-pp">1.4 Pipeline Parallelism (PP)</h3>
<p>Pipeline parallelism splits the model across its depth - like an assembly line where each station handles different layers.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Different layers run on different GPUs</li>
<li>Data flows through the pipeline in micro-batches</li>
<li>Each GPU only stores its assigned layers</li>
</ul>
<p><strong>When to use:</strong></p>
<ul>
<li>Very deep models</li>
<li>When you need to scale beyond a single node</li>
<li>Combined with other techniques for maximum scaling</li>
</ul>
<pre class="mermaid"><code>sequenceDiagram
    participant S0 as GPU-Stage 0 (Layers 1-4)
    participant S1 as GPU-Stage 1 (Layers 5-8)
    participant S2 as GPU-Stage 2 (Layers 9-12)
    Note over S0,S2: ← time -&gt;
    S0-&gt;&gt;S0: Fwd/Bwd µ-batch 0
    S0-&gt;&gt;S1: send activations
    S1-&gt;&gt;S1: Fwd/Bwd µ-batch 0
    S1-&gt;&gt;S2: send activations
    S0-&gt;&gt;S0: Fwd/Bwd µ-batch 1
    S2-&gt;&gt;S2: Fwd/Bwd µ-batch 0</code></pre>
<p><strong>Tools</strong>: <a href="https://www.deepspeed.ai/tutorials/pipeline/">DeepSpeed PP</a>, <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a>, <a href="https://arxiv.org/abs/1811.06965">GPipe</a>.</p>
<h3 id="15-context-parallelism-cp">1.5 Context Parallelism (CP)</h3>
<p>For handling extremely long sequences - imagine different people each reading different paragraphs of a book, then sharing key information.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Sequence/context length is split across GPUs</li>
<li>Each GPU processes a chunk of the input sequence</li>
<li>GPUs exchange information for cross-attention</li>
</ul>
<p><strong>Real-world use case:</strong></p>
<ul>
<li>Enables processing 100K+ tokens on consumer hardware</li>
<li>Critical for document QA, code generation, and long-context reasoning</li>
<li>Becoming essential as context windows expand</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    subgraph Input["Input Sequence"]
        S[Sequence 0-8191 tokens]
    end

    subgraph CrossGPU["Cross-GPU Processing"]
        direction LR
        subgraph GPU1["GPU 1"]
            direction TB
            T0[Tokens 0-4095]
            A0[Self-Attention Block]
            T0 --&gt; A0
        end

        subgraph GPU2["GPU 2"]
            direction TB
            T1[Tokens 4096-8191]
            A1[Self-Attention Block]
            T1 --&gt; A1
        end

        GPU1 &lt;--&gt;|Exchange Keys/Values| GPU2
    end

    subgraph Output["Output Processing"]
        M[Merge Logits]
        O[Output Sequence]
        M --&gt; O
    end

    S --&gt; |Split| T0
    S --&gt; |Split| T1

    A0 --&gt; M
    A1 --&gt; M</code></pre>
<p><strong>Tools</strong>: <a href="https://github.com/huggingface/picotron">Picotron</a>, <a href="https://github.com/huggingface/nanotron">Nanotron</a>.</p>
<h3 id="16-expert-parallelism-or-mixture-of-experts">1.6 Expert Parallelism (or Mixture of Experts)</h3>
<p>MoE models act like specialized consultants - rather than activating the entire model for every input, each token gets routed to only the "experts" it needs.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Multiple specialized sub-networks (experts) exist within the model</li>
<li>A routing mechanism determines which experts to use for each token</li>
<li>Only a small subset of parameters is used for any given token</li>
</ul>
<p><strong>Why it matters:</strong></p>
<ul>
<li>Scales to massive models (1T+ parameters) without proportional compute costs</li>
<li>Allows more efficient use of parameters</li>
<li>Models like Mixtral and Grok use this approach</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    subgraph Input_Tokens["Input Tokens"]
        T1["T₁"]
        T2["T₂"]
        T3["T₃"]
    end
    G["Gating Network"]
    subgraph Experts["Experts"]
        E1["Expert 1"]
        E2["Expert 2"]
        E3["Expert 3"]
        E4["⋯"]
    end
    T1 --&gt; G
    T2 --&gt; G
    T3 --&gt; G
    G --&gt;|top-k routes| E1
    G --&gt;|top-k routes| E2
    G --&gt;|top-k routes| E3
    E1 &amp; E2 &amp; E3 --&gt; O["Concatenate + Mix"]</code></pre>
<p><strong>Tools</strong>: <a href="https://github.com/huggingface/picotron">Picotron</a>, <a href="https://github.com/huggingface/nanotron">Nanotron</a>.</p>
<h2 id="2-practical-training-strategies">2. Practical Training Strategies</h2>
<p>For most practitioners, here's what I recommend based on your hardware:</p>
<h3 id="21-single-machine-2-8-gpus">2.1 Single Machine (2-8 GPUs)</h3>
<p><strong>Best approach:</strong> FSDP + small TP</p>
<ul>
<li>Start with pure FSDP using PyTorch or DeepSpeed</li>
<li>If your model has huge layers, add TP=2 inside the node</li>
<li>Use <code>accelerate</code> or <code>torchrun</code> for the simplest setup</li>
</ul>
<p><strong>Real-world tip:</strong> On consumer hardware with PCIe connections, stick to TP≤2. On servers with NVLink, you can go up to TP=4 efficiently.</p>
<h3 id="22-small-cluster-2-16-nodes-128-gpus">2.2 Small Cluster (2-16 nodes, ≤128 GPUs)</h3>
<p><strong>Best approach:</strong> "TP inside, DP across" + optional PP</p>
<ul>
<li>Keep TP groups within each node (utilizing fast NVLink)</li>
<li>Use DP or FSDP across nodes</li>
<li>Add PP when model depth exceeds single-node capacity</li>
</ul>
<p><strong>Pro tip:</strong> When using PP, set micro-batch size to 4x your PP degree to minimize pipeline bubbles.</p>
<h3 id="23-large-cluster-hundreds-gpus">2.3 Large Cluster (hundreds+ GPUs)</h3>
<p><strong>Best approach:</strong> 4D parallelism (DPxTPxPPxCP)</p>
<ul>
<li>Necessary for 70B+ models with 32k+ context windows</li>
<li>Requires careful mapping to hardware topology</li>
<li>Expect ~75% scaling efficiency with good InfiniBand networking</li>
</ul>
<p><strong>Real-world example:</strong> Training a 70B model with 32k context might use:</p>
<ul>
<li>TP=8 (within each node)</li>
<li>PP=4 (across nodes)</li>
<li>CP=4 (for long sequence handling)</li>
<li>DP=4 (for throughput)</li>
<li>Total: 512 GPUs organized in a 4D grid</li>
</ul>
<h2 id="3-practical-tools-worth-learning">3. Practical Tools Worth Learning</h2>
<table>
<thead>
<tr>
<th>Tool</th>
<th>When to Use It</th>
<th>Learning Curve</th>
</tr>
</thead>
<tbody>
<tr>
<td>PyTorch FSDP</td>
<td>Training medium-large models (1-20B) on a single node</td>
<td>★★☆☆☆</td>
</tr>
<tr>
<td>DeepSpeed ZeRO</td>
<td>Multi-node training with simple setup</td>
<td>★★★☆☆</td>
</tr>
<tr>
<td>Megatron-LM</td>
<td>Production training of very large models</td>
<td>★★★★☆</td>
</tr>
<tr>
<td>vLLM</td>
<td>Fast inference with attention caching</td>
<td>★★☆☆☆</td>
</tr>
<tr>
<td>TensorRT-LLM</td>
<td>Production inference with NVIDIA GPUs</td>
<td>★★★★☆</td>
</tr>
<tr>
<td>Hugging Face Accelerate</td>
<td>Simple distributed training with minimal code changes</td>
<td>★☆☆☆☆</td>
</tr>
<tr>
<td>Nanotron</td>
<td>Research and education on 3D parallelism</td>
<td>★★★☆☆</td>
</tr>
</tbody>
</table>
<h2 id="4-making-the-right-choice-a-simple-decision-tree">4. Making the Right Choice: A Simple Decision Tree</h2>
<ol>
<li>
<p><strong>Does your model fit on a single GPU?</strong></p>
<ul>
<li>Yes -&gt; Use standard training</li>
<li>No -&gt; Continue to question 2</li>
</ul>
</li>
<li>
<p><strong>Do you have multiple GPUs in one machine?</strong></p>
<ul>
<li>Yes -&gt; Try FSDP first</li>
<li>No -&gt; Skip to question 4</li>
</ul>
</li>
<li>
<p><strong>Is FSDP still not enough?</strong></p>
<ul>
<li>Add TP=2 inside the node</li>
<li>If still insufficient, add CP for long contexts</li>
</ul>
</li>
<li>
<p><strong>Training across multiple nodes?</strong></p>
<ul>
<li>Start with "TP inside nodes, DP across nodes"</li>
<li>Add PP when model depth exceeds a single node</li>
<li>For very large models with long contexts, consider 4D parallelism</li>
</ul>
</li>
</ol>
<p><strong>Diagram for visualization</strong></p>
<pre class="mermaid"><code>flowchart TD
    start([LLM Scaling Decision]) --&gt; fit{Does model fit on single GPU?}
    fit --&gt;|Yes| dp[Use standard training]
    fit --&gt;|No| multiple{Multiple GPUs in one machine?}
    multiple --&gt;|Yes| fsdp[Try FSDP first]
    multiple --&gt;|No| multinode[Training across multiple nodes]
    fsdp --&gt; enough{Is FSDP still not enough?}
    enough --&gt;|Yes| tp[Add TP=2 inside the node]
    tp --&gt; context{Need longer contexts?}
    context --&gt;|Yes| cp[Add CP for long contexts]
    multinode --&gt; hybrid[Start with TP inside nodes, DP across nodes]
    hybrid --&gt; depth{Model depth exceeds single node?}
    depth --&gt;|Yes| pp[Add PP when model depth exceeds a single node]
    pp --&gt; large{Very large model with long contexts?}
    large --&gt;|Yes| d4[Consider 4D parallelism]</code></pre>
<h2 id="5-the-ultra-scale-cheatsheet">5. The Ultra-Scale Cheatsheet</h2>
<p>This visual guide from HuggingFace's team summarizes the key decision points:</p>
<p><img alt="Ultra-Scale LLM Cheatsheet" src="https://nanotron-ultrascale-playbook.static.hf.space/dist/assets/images/ultra-cheatsheet.svg" /></p>
<h2 id="conclusion">Conclusion</h2>
<p>Scaling LLMs is both an art and a science. While these techniques may seem complex at first, they follow logical patterns based on hardware constraints and model structure. Start simple with FSDP and add more dimensions of parallelism only as needed.</p>







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.toc", "navigation.toc.sticky", "navigation.toc.maxdepth", "navigation.toc.title", "navigation.toc.collapse", "navigation.toc.collapse_empty_groups", "navigation.toc.collapse_single_children"], "search": "../../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>