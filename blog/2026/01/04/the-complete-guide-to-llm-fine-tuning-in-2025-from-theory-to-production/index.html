
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A comprehensive, beginner-friendly guide to LLM fine-tuning covering PEFT methods (LoRA, QLoRA, DoRA), frameworks (Unsloth, Axolotl), practical code examples, output formats, and deployment strategies.">
      
      
        <meta name="author" content="Viacheslav Dubrov">
      
      
        <link rel="canonical" href="https://slavadubrov.github.io/blog/2026/01/04/the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production/">
      
      
        <link rel="prev" href="../../../../2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/">
      
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../../assets/favicon-eoc.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production - Edge of Context: Practical AI Engineering</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/brand.css">
    
      <link rel="stylesheet" href="../../../../../assets/blog-sidebar.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
     
<style>
  /* Header social icons */
  .md-header__social {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    margin-left: 0.5rem;
  }
  .md-header__social a {
    color: var(--md-primary-bg-color);
    opacity: 0.7;
    transition: opacity 0.2s;
  }
  .md-header__social a:hover {
    opacity: 1;
  }
  .md-header__social svg {
    width: 1.2rem;
    height: 1.2rem;
    fill: currentColor;
  }
  /* Adjust search to make room for social icons */
  .md-search {
    margin-right: 0;
  }
</style>

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
     
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Edge of Context: Practical AI Engineering" class="md-header__button md-logo" aria-label="Edge of Context: Practical AI Engineering" data-md-component="logo">
      
  <img src="../../../../../assets/logo-eoc.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Edge of Context: Practical AI Engineering
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
    
  
  Blog

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
<script>
  // Inject social icons into header after DOM loads
  document.addEventListener("DOMContentLoaded", function () {
    const header = document.querySelector(".md-header__inner");
    if (header) {
      const social = document.createElement("div");
      social.className = "md-header__social";
      social.innerHTML = `
      <a href="https://www.linkedin.com/in/slavadubrov" title="LinkedIn" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
      <a href="https://slavadubrov.substack.com/" title="Substack" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"/></svg>
      </a>
      <a href="https://github.com/slavadubrov" title="GitHub" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    `;
      header.appendChild(social);
    }
  });
</script>

    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Edge of Context: Practical AI Engineering" class="md-nav__button md-logo" aria-label="Edge of Context: Practical AI Engineering" data-md-component="logo">
      
  <img src="../../../../../assets/logo-eoc.svg" alt="logo">

    </a>
    Edge of Context: Practical AI Engineering
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Archive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2026/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2026
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2025/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2025
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ai-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/agentic-ai/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agentic AI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/infrastructure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Infrastructure
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/tooling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tooling
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#strategic-decision-fine-tuning-vs-alternatives" class="md-nav__link">
    <span class="md-ellipsis">
      Strategic Decision: Fine-Tuning vs Alternatives
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Strategic Decision: Fine-Tuning vs Alternatives">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning-vs-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-Tuning vs RAG
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fine-tuning-vs-prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-Tuning vs Prompt Engineering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fine-tuning-vs-schema-guided-reasoning-sgr" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-Tuning vs Schema-Guided Reasoning (SGR)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quick-reference-matching-problems-to-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Quick Reference: Matching Problems to Solutions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-economic-case" class="md-nav__link">
    <span class="md-ellipsis">
      The Economic Case
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#types-of-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      Types of Fine-Tuning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Types of Fine-Tuning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-continued-pre-training-unsupervised" class="md-nav__link">
    <span class="md-ellipsis">
      1. Continued Pre-training (Unsupervised)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-supervised-fine-tuning-sft" class="md-nav__link">
    <span class="md-ellipsis">
      2. Supervised Fine-Tuning (SFT)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      3. Instruction Tuning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-7-stage-fine-tuning-lifecycle" class="md-nav__link">
    <span class="md-ellipsis">
      The 7-Stage Fine-Tuning Lifecycle
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-1-data-preparation-data-is-the-foundation" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 1: Data Preparation ‚Äî "Data is the Foundation"
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 1: Data Preparation ‚Äî &#34;Data is the Foundation&#34;">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-5-stage-data-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      The 5-Stage Data Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The 5-Stage Data Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-ingestion-filtering" class="md-nav__link">
    <span class="md-ellipsis">
      1. Ingestion &amp; Filtering
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-pii-scrubbing-enterprise-critical" class="md-nav__link">
    <span class="md-ellipsis">
      2. PII Scrubbing (Enterprise Critical)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-deduplication-minhash-lsh" class="md-nav__link">
    <span class="md-ellipsis">
      3. Deduplication (MinHash LSH)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-synthetic-augmentation-the-2025-secret" class="md-nav__link">
    <span class="md-ellipsis">
      4. Synthetic Augmentation (The 2025 Secret)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-formatting" class="md-nav__link">
    <span class="md-ellipsis">
      5. Formatting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-format-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Data Format Examples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-principles" class="md-nav__link">
    <span class="md-ellipsis">
      Key Principles
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-2-model-selection-hardware-requirements" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 2: Model Selection &amp; Hardware Requirements
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 2: Model Selection &amp; Hardware Requirements">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hardware-requirements-by-model-size" class="md-nav__link">
    <span class="md-ellipsis">
      Hardware Requirements by Model Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-calculations" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Calculations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-3-training-methods-peft-lora" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 3: Training Methods (PEFT &amp; LoRA)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 3: Training Methods (PEFT &amp; LoRA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#full-fine-tuning-vs-peft" class="md-nav__link">
    <span class="md-ellipsis">
      Full Fine-Tuning vs PEFT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lora-the-industry-standard" class="md-nav__link">
    <span class="md-ellipsis">
      LoRA: The Industry Standard
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#peft-methods-compared" class="md-nav__link">
    <span class="md-ellipsis">
      PEFT Methods Compared
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PEFT Methods Compared">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-to-use-which" class="md-nav__link">
    <span class="md-ellipsis">
      When to Use Which?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dora-weight-decomposed-lora" class="md-nav__link">
    <span class="md-ellipsis">
      DoRA: Weight-Decomposed LoRA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#half-fine-tuning-hft" class="md-nav__link">
    <span class="md-ellipsis">
      Half Fine-Tuning (HFT)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adapter-merging-for-multi-task-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Adapter Merging for Multi-Task Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-4-fine-tuning-preference-alignment" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 4: Fine-Tuning &amp; Preference Alignment
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 4: Fine-Tuning &amp; Preference Alignment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#traditional-approach-rlhf-with-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      Traditional Approach: RLHF with PPO
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modern-streamlined-dpo" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Streamlined: DPO
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#newest-single-stage-orpo" class="md-nav__link">
    <span class="md-ellipsis">
      Newest Single-Stage: ORPO
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fine-tuning-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-Tuning Frameworks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Fine-Tuning Frameworks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unsloth-speed-efficiency-champion" class="md-nav__link">
    <span class="md-ellipsis">
      Unsloth ‚Äî Speed &amp; Efficiency Champion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#axolotl-config-driven-production" class="md-nav__link">
    <span class="md-ellipsis">
      Axolotl ‚Äî Config-Driven Production
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#framework-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Framework Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practical-demo-fine-tuning-with-unsloth" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Demo: Fine-Tuning with Unsloth
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Practical Demo: Fine-Tuning with Unsloth">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quick-start" class="md-nav__link">
    <span class="md-ellipsis">
      Quick Start
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-deep-dive" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Deep Dive
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#core-training-code" class="md-nav__link">
    <span class="md-ellipsis">
      Core Training Code
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fine-tuning-with-axolotl" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-Tuning with Axolotl
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-5-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 5: Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 5: Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automated-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      Automated Benchmarks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-as-judge" class="md-nav__link">
    <span class="md-ellipsis">
      LLM-as-Judge
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#domain-specific-eval" class="md-nav__link">
    <span class="md-ellipsis">
      Domain-Specific Eval
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-6-deployment-output-formats" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 6: Deployment &amp; Output Formats
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 6: Deployment &amp; Output Formats">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-lora-adapter-default" class="md-nav__link">
    <span class="md-ellipsis">
      1. LoRA Adapter (Default)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-merged-model" class="md-nav__link">
    <span class="md-ellipsis">
      2. Merged Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-gguf-format" class="md-nav__link">
    <span class="md-ellipsis">
      3. GGUF Format
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stage-7-serving-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Stage 7: Serving &amp; Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stage 7: Serving &amp; Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#with-vllm-production" class="md-nav__link">
    <span class="md-ellipsis">
      With vLLM (Production)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-ollama-local" class="md-nav__link">
    <span class="md-ellipsis">
      With Ollama (Local)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#with-llamacpp-cpu" class="md-nav__link">
    <span class="md-ellipsis">
      With llama.cpp (CPU)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
    <nav class="md-nav" aria-label="References">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#papers-research" class="md-nav__link">
    <span class="md-ellipsis">
      Papers &amp; Research
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-processing-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Data Processing Tools
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#schema-guided-reasoning-sgr" class="md-nav__link">
    <span class="md-ellipsis">
      Schema-Guided Reasoning (SGR)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Training Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference-deployment" class="md-nav__link">
    <span class="md-ellipsis">
      Inference &amp; Deployment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#guides-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Guides &amp; Resources
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
<div class="md-content md-content--post" data-md-component="content">
  <div
    class="md-sidebar md-sidebar--post"
    data-md-component="sidebar"
    data-md-type="navigation"
  >
    <div class="md-sidebar__scrollwrap">
      <div class="md-sidebar__inner md-post">
        <nav class="md-nav md-nav--primary">
          <div class="md-post__back">
            <div class="md-nav__title md-nav__container">
              <a href="../../../../" class="md-nav__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                <span class="md-ellipsis"> Back to index </span>
              </a>
            </div>
          </div>

          
          <ul class="md-post__meta md-nav__list">
            <li class="md-nav__item md-nav__item--section">
              <div class="md-post__title">
                <span class="md-ellipsis">Topics</span>
              </div>
              <nav class="md-nav">
                        
                 
                <div
                  class="blog-category-section blog-category-section--active"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >ü§ñ</span
                    >
                    <span class="blog-category-name">AI Engineering</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post blog-category-post--active"
                    >
                      <a href="./">The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/">LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/11/choosing-the-right-open-source-llm-variant--file-format/">Choosing the Right Open-Source LLM Variant & File Format</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/10/quick-guide-on-running-llms-locally-on-macos/">Quick-guide on Running LLMs Locally on macOS</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/">Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >ü¶æ</span
                    >
                    <span class="blog-category-name">Agentic AI</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/">Schema-Guided Reasoning on vLLM ‚Äî Turning LLMs into Reliable Business Logic Engines</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/">Domain-driven design for AI agents: a beginner-friendly guide</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/">Context Engineering in the Agentic‚ÄëAI Era ‚Äî and How to Cook It</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üîß</span
                    >
                    <span class="blog-category-name">Infrastructure</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/">MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üõ†Ô∏è</span
                    >
                    <span class="blog-category-name">Tooling</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/">Building a Custom FeatureStoreLite MCP Server Using uv</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/">Quick-guide on Local Stable-Diffusion Toolkits for macOS</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/">Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc üöÄ</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/">Quick-Guide on setting up a MacBook for AI Engineering</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üêç</span
                    >
                    <span class="blog-category-name">Python</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/08/the-ultimate-guide-to-pyprojecttoml/">The Ultimate Guide to `pyproject.toml`</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/04/17/quick-guide-managing-python-on-macos-with-uv/">Quick Guide: Managing Python on macOS with uv</a>
                    </li>
                     
                  </ul>
                </div>
                
              </nav>
            </li>
          </ul>

          
        </nav>
        
      </div>
    </div>
  </div>
  <article class="md-content__inner md-typeset">
     
  




<h1 id="the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production">The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production</h1>
<p>Fine-tuning has become the secret weapon for building specialized AI applications. While general-purpose models like GPT-4 and Claude excel at broad tasks, fine-tuning transforms them into laser-focused experts for your specific domain. This guide walks you through everything you need to know‚Äîfrom understanding when to fine-tune to deploying your custom model.</p>
<!-- more -->

<h2 id="strategic-decision-fine-tuning-vs-alternatives">Strategic Decision: Fine-Tuning vs Alternatives</h2>
<p>Before investing GPU hours and engineering time, you need to answer a fundamental question: <strong>is fine-tuning the right solution for your problem?</strong></p>
<p><img alt="Decision Flowchart" src="../../../../assets/2026-01-04-finetuning-guide/decision_flowchart.svg" /></p>
<h3 id="fine-tuning-vs-rag">Fine-Tuning vs RAG</h3>
<p>Do not fine-tune just to add "knowledge." Here's when to use each approach:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Fine-Tuning</th>
<th>RAG (Retrieval-Augmented Generation)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Core Function</strong></td>
<td>Alters internal weights to teach skills, styles, or behaviors</td>
<td>Provides external, up-to-date context at inference time</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>‚Ä¢ Specific conversational styles<br>‚Ä¢ Complex instruction following<br>‚Ä¢ Domain-specific reasoning</td>
<td>‚Ä¢ Rapidly changing data (news, stock prices)<br>‚Ä¢ Reducing hallucinations (grounding)<br>‚Ä¢ Citing sources</td>
</tr>
<tr>
<td><strong>Knowledge Handling</strong></td>
<td>Internalizes patterns, not facts</td>
<td>Retrieves facts from external knowledge base</td>
</tr>
<tr>
<td><strong>Update Frequency</strong></td>
<td>Requires retraining for updates</td>
<td>Updates immediately with new documents</td>
</tr>
</tbody>
</table>
<h3 id="fine-tuning-vs-prompt-engineering">Fine-Tuning vs Prompt Engineering</h3>
<p>Modern LLMs are remarkably responsive to well-crafted prompts. Before fine-tuning, exhaust prompt engineering options:</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Fine-Tuning</th>
<th>Prompt Engineering</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Setup Cost</strong></td>
<td>High (data curation, GPU compute, iteration)</td>
<td>Low (iterative prompt refinement)</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Locked after training</td>
<td>Change anytime without retraining</td>
</tr>
<tr>
<td><strong>Format/Style</strong></td>
<td>Best for complex, consistent output formats</td>
<td>Good for simple formatting with few-shot examples</td>
</tr>
<tr>
<td><strong>Latency</strong></td>
<td>Lower (no long system prompts)</td>
<td>Higher (context tax on every request)</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Complex behaviors, distillation, cost at scale</td>
<td>Rapid iteration, changing requirements</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!TIP] <strong>Try Prompting First</strong>
Start with few-shot examples in your prompt. If you don't get consistent behavior, try SGR before jumping to fine-tuning.</p>
</blockquote>
<h3 id="fine-tuning-vs-schema-guided-reasoning-sgr">Fine-Tuning vs Schema-Guided Reasoning (SGR)</h3>
<p>Libraries like <code>xgrammar</code> and <code>outlines</code> constrain model outputs at inference time using Finite State Machines. They work with base models out of the box‚Äî<strong>no training required</strong>.</p>
<p><strong>SGR isn't just about structured outputs.</strong> Its primary value is <strong>consistency and reliability</strong>. When prompting alone produces inconsistent results, SGR enforces deterministic output patterns without the cost and complexity of fine-tuning.</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>SGR (No Fine-Tuning)</th>
<th>Fine-Tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Setup</strong></td>
<td>Immediate‚Äîdefine schema, deploy</td>
<td>Requires data curation, GPU compute, iteration</td>
</tr>
<tr>
<td><strong>Consistency</strong></td>
<td>Guaranteed structure, reliable patterns</td>
<td>Learned behavior (may still vary)</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Change schema anytime without retraining</td>
<td>Locked after training</td>
</tr>
<tr>
<td><strong>Latency</strong></td>
<td>Slight overhead (model may "fight" schema)</td>
<td>Lower (model naturally outputs format)</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Structured outputs, consistent behavior</td>
<td>Complex reasoning, deep behavioral changes</td>
</tr>
</tbody>
</table>
<p><strong>Recommendation:</strong></p>
<ol>
<li><strong>Start with prompting</strong> ‚Äî Simple few-shot examples for basic formatting</li>
<li><strong>Add SGR if inconsistent</strong> ‚Äî Use <code>xgrammar</code> or <code>outlines</code> to guarantee output structure and reliability</li>
<li><strong>Fine-tune as last resort</strong> ‚Äî Only when you need deep behavioral changes that schema constraints can't achieve</li>
</ol>
<h3 id="quick-reference-matching-problems-to-solutions">Quick Reference: Matching Problems to Solutions</h3>
<table>
<thead>
<tr>
<th>Challenge</th>
<th>Best Solution</th>
<th>Why?</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Missing knowledge</strong></td>
<td>RAG</td>
<td>Models hallucinate facts. Retrieval provides grounded, up-to-date context</td>
</tr>
<tr>
<td><strong>Wrong format/tone</strong></td>
<td>Prompt Engineering</td>
<td>Modern models follow style instructions well via few-shot examples</td>
</tr>
<tr>
<td><strong>Inconsistent outputs</strong></td>
<td>SGR (xgrammar)</td>
<td>Guaranteed structure and reliability without training</td>
</tr>
<tr>
<td><strong>Complex behavioral changes</strong></td>
<td>Fine-Tuning (SFT)</td>
<td>Deep persona, reasoning patterns, or multi-step workflows</td>
</tr>
<tr>
<td><strong>Safety/preference</strong></td>
<td>Alignment (DPO)</td>
<td>When outputs are correct but don't match preferences</td>
</tr>
<tr>
<td><strong>Latency/cost at scale</strong></td>
<td>Distillation (SFT)</td>
<td>Train smaller student model on larger teacher's outputs</td>
</tr>
<tr>
<td><strong>Reduce model size</strong></td>
<td>Quantization</td>
<td>No training‚Äîcompress weights (FP16‚ÜíINT4) for faster inference</td>
</tr>
</tbody>
</table>
<h3 id="the-economic-case">The Economic Case</h3>
<p>Fine-tuning shines in <strong>high-volume, stable-requirement scenarios</strong>. Consider this: a robust RAG system might require 2,000 tokens of context on every call (system prompt + retrieved docs + few-shot examples). That's your "context tax" on every request.</p>
<p>A fine-tuned model can internalize those instructions, reducing your prompt from 2,000 tokens to 50. At scale, this pays for the training compute within weeks.</p>
<blockquote>
<p>[!TIP] <strong>The Hybrid Approach</strong>
The industry sweet spot is often a fine-tuned smaller model (8B params) combined with lightweight RAG for facts. This often outperforms prompting a massive model (70B+) in both accuracy and cost.</p>
</blockquote>
<hr />
<h2 id="types-of-fine-tuning">Types of Fine-Tuning</h2>
<p>Before diving into the lifecycle, understand the three main approaches to fine-tuning:</p>
<p><img alt="Fine-Tuning Types" src="../../../../assets/2026-01-04-finetuning-guide/finetuning_types.svg" /></p>
<h3 id="1-continued-pre-training-unsupervised">1. Continued Pre-training (Unsupervised)</h3>
<p>Continued pre-training extends the base model's knowledge by training on additional raw text <strong>without labels</strong>. The model simply learns to predict the next token, just like during original pre-training.</p>
<p><strong>When to use:</strong></p>
<ul>
<li>Your domain has specialized vocabulary the base model doesn't know (medical, legal, financial)</li>
<li>You have large amounts of domain text but no labeled examples</li>
<li>The base model struggles with domain-specific terminology</li>
</ul>
<p><strong>Example:</strong> Training on millions of clinical notes so the model understands medical abbreviations, drug names, and clinical workflows.</p>
<h3 id="2-supervised-fine-tuning-sft">2. Supervised Fine-Tuning (SFT)</h3>
<p>SFT trains on labeled <strong>(input, output) pairs</strong>. You show the model exactly what output you expect for each input.</p>
<p><strong>When to use:</strong></p>
<ul>
<li>You have a specific task with clear input/output format</li>
<li>Quality labeled data is available (even small amounts)</li>
<li>You need consistent, predictable behavior</li>
</ul>
<p><strong>Example:</strong> Training on (SQL query description, SQL code) pairs for Text-to-SQL conversion.</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;input&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Get all users who signed up last month&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;output&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;SELECT * FROM users WHERE signup_date &gt;= DATE_SUB(NOW(), INTERVAL 1 MONTH)&quot;</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="3-instruction-tuning">3. Instruction Tuning</h3>
<p>Instruction tuning is a <strong>special case of SFT</strong> designed to make models follow diverse natural language instructions. The training data consists of (instruction, response) pairs across many different tasks.</p>
<p><strong>When to use:</strong></p>
<ul>
<li>You want a general-purpose assistant (like ChatGPT or Claude)</li>
<li>The model needs to handle varied, open-ended requests</li>
<li>You're building a chat interface</li>
</ul>
<p><strong>Example:</strong> Training on thousands of diverse instructions like "Summarize this article," "Write a poem about X," "Explain Y in simple terms."</p>
<h3 id="comparison">Comparison</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Continued Pre-training</th>
<th>SFT</th>
<th>Instruction Tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data</strong></td>
<td>Raw text</td>
<td>(input, output) pairs</td>
<td>(instruction, response) pairs</td>
</tr>
<tr>
<td><strong>Labels</strong></td>
<td>None (unsupervised)</td>
<td>Task-specific</td>
<td>Diverse tasks</td>
</tr>
<tr>
<td><strong>Goal</strong></td>
<td>Domain knowledge</td>
<td>Specific task behavior</td>
<td>Follow any instruction</td>
</tr>
<tr>
<td><strong>Data Volume</strong></td>
<td>Large (millions of tokens)</td>
<td>Small-Medium (500-10k examples)</td>
<td>Medium-Large (10k-100k examples)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!NOTE] <strong>Most Common Approach</strong>
In practice, most practitioners use <strong>SFT</strong> for specific tasks or <strong>Instruction Tuning</strong> for chat applications. Continued pre-training is rarer because it requires massive amounts of domain text and is computationally expensive.</p>
</blockquote>
<hr />
<h2 id="the-7-stage-fine-tuning-lifecycle">The 7-Stage Fine-Tuning Lifecycle</h2>
<p>Fine-tuning isn't a single action‚Äîit's a structured lifecycle. Understanding this pipeline is critical for success.</p>
<p><img alt="7-Stage Pipeline" src="../../../../assets/2026-01-04-finetuning-guide/pipeline_7_stages.svg" /></p>
<p>Each stage builds on the previous one:</p>
<ol>
<li><strong>Data Preparation</strong> ‚Äî Clean, deduplicate, and format your data (highest leverage step)</li>
<li><strong>Model Selection</strong> ‚Äî Choose the right base model and load weights</li>
<li><strong>Training Setup</strong> ‚Äî Configure hardware, hyperparameters, and optimization strategy</li>
<li><strong>Fine-Tuning</strong> ‚Äî Run SFT, DPO, or ORPO training</li>
<li><strong>Evaluation</strong> ‚Äî Benchmark performance and validate quality</li>
<li><strong>Deployment</strong> ‚Äî Export and serve your model</li>
<li><strong>Monitoring</strong> ‚Äî Track performance, maintain, and iterate</li>
</ol>
<blockquote>
<p>[!WARNING] <strong>Data is the Foundation</strong>
Stage 1 (Dataset Preparation) is the highest leverage step. Flaws in your data cannot be fixed by algorithms later. <strong>Quality over quantity</strong> ‚Äî 500-1,000 carefully curated examples often outperform 50,000 noisy ones.</p>
</blockquote>
<hr />
<h2 id="stage-1-data-preparation-data-is-the-foundation">Stage 1: Data Preparation ‚Äî "Data is the Foundation"</h2>
<p>This is where most fine-tuning projects succeed or fail. The industry has moved far beyond simple "clean and format" scripts.</p>
<p><img alt="Data Pipeline" src="../../../../assets/2026-01-04-finetuning-guide/data_pipeline.svg" /></p>
<h3 id="the-5-stage-data-pipeline">The 5-Stage Data Pipeline</h3>
<p>Modern production pipelines use tools like <strong>DataTrove</strong> (Hugging Face) and <strong>Distilabel</strong> (Argilla) rather than custom scripts:</p>
<h4 id="1-ingestion-filtering">1. Ingestion &amp; Filtering</h4>
<ul>
<li><strong>Action</strong>: Remove "refusals" (e.g., "I cannot answer that"), broken UTF-8, non-target languages</li>
<li><strong>Tools</strong>: Trafilatura (extraction) + FastText (language ID)</li>
</ul>
<h4 id="2-pii-scrubbing-enterprise-critical">2. PII Scrubbing (Enterprise Critical)</h4>
<ul>
<li><strong>Action</strong>: Detect and redact emails, IP addresses, phone numbers before training</li>
<li><strong>Tools</strong>: Microsoft Presidio or scrubadub</li>
<li><strong>Why</strong>: Training on customer PII is a critical security failure</li>
</ul>
<h4 id="3-deduplication-minhash-lsh">3. Deduplication (MinHash LSH)</h4>
<ul>
<li><strong>Action</strong>: Remove near-duplicates to prevent memorization</li>
<li><strong>Tools</strong>: DataTrove (industry standard for terabyte-scale processing)</li>
</ul>
<h4 id="4-synthetic-augmentation-the-2025-secret">4. Synthetic Augmentation (The 2025 Secret)</h4>
<ul>
<li><strong>Action</strong>: Use a stronger "teacher" model (GPT-4o, DeepSeek-V3) to rewrite raw data into high-quality instruction-response pairs</li>
<li><strong>Tools</strong>: Distilabel</li>
<li><strong>Impact</strong>: This step often provides the biggest quality boost</li>
</ul>
<h4 id="5-formatting">5. Formatting</h4>
<ul>
<li><strong>Action</strong>: Convert to standard formats (Alpaca or ShareGPT)</li>
</ul>
<h3 id="data-format-examples">Data Format Examples</h3>
<p><strong>Alpaca Format</strong> (Instruction-Following):</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;instruction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Summarize the following text.&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;input&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;The text to be summarized...&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;output&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;This is the summary.&quot;</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>ShareGPT/ChatML Format</strong> (Conversational):</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;conversations&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;from&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Hello, who are you?&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="nt">&quot;from&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;I am a helpful AI assistant.&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="key-principles">Key Principles</h3>
<ul>
<li><strong>Quality over Quantity</strong>: 500-1,000 carefully curated examples often outperform 50,000 noisy ones</li>
<li><strong>Cleanliness</strong>: Remove irrelevant information, normalize text, ensure consistent formatting</li>
<li><strong>Balance</strong>: Ensure representation across different topics to prevent bias</li>
<li><strong>Enterprise Critical</strong>: PII scrubbing (Stage 2) is mandatory for production systems</li>
</ul>
<hr />
<h2 id="stage-2-model-selection-hardware-requirements">Stage 2: Model Selection &amp; Hardware Requirements</h2>
<p>Choosing the right base model and understanding hardware constraints is critical for project success.</p>
<h3 id="hardware-requirements-by-model-size">Hardware Requirements by Model Size</h3>
<p>The physics of fine-tuning impose strict memory constraints:</p>
<table>
<thead>
<tr>
<th>Tier</th>
<th>Hardware Config</th>
<th>Capability</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Enterprise Standard</strong></td>
<td>8x NVIDIA H100 (80GB)</td>
<td>Full Fine-Tuning</td>
<td>Training 70B models with long context (32k+) at max speed</td>
</tr>
<tr>
<td><strong>Minimum Viable (Pro)</strong></td>
<td>4x NVIDIA A100 (80GB)</td>
<td>QLoRA / LoRA</td>
<td>Fine-tuning Qwen 72B or Llama 70B in 4-bit</td>
</tr>
<tr>
<td><strong>Local R&amp;D</strong></td>
<td>4x RTX 6000 Ada (48GB)</td>
<td>QLoRA</td>
<td>On-prem workstation for data privacy requirements</td>
</tr>
<tr>
<td><strong>Hobbyist/Indie</strong></td>
<td>1-2x RTX 3090/4090 (24GB)</td>
<td>QLoRA</td>
<td>Fine-tune 7B-32B models with parameter-efficient methods</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!TIP] <strong>Consumer GPUs Are More Capable Than You Think</strong>
With QLoRA and optimized frameworks like Unsloth, a single RTX 4090 (24GB) can fine-tune models up to 32B parameters. RTX 3090s remain excellent value for 7B-13B model training. The 24GB VRAM sweet spot makes these cards highly capable for serious fine-tuning work.</p>
<p>[!NOTE] <strong>Why H100s?</strong>
It's not just VRAM‚Äîit's <strong>FP8 precision</strong>. H100s support native FP8 training, which effectively doubles memory capacity and throughput compared to A100s. For long-context models (128k tokens), FP8 on H100s is often the only way to fit reasonable batch sizes.</p>
</blockquote>
<h3 id="memory-calculations">Memory Calculations</h3>
<p>To fine-tune a 72B model, you need to store:</p>
<ul>
<li><strong>Model Weights</strong> (16-bit): ~144 GB</li>
<li><strong>Gradients &amp; Optimizer States</strong>: ~2-3x the model size (depending on optimizer, e.g., AdamW)</li>
<li><strong>Activations</strong>: Scales with context length (e.g., 32k tokens)</li>
</ul>
<p>This is why QLoRA (4-bit quantization + LoRA adapters) is essential for most teams.</p>
<hr />
<h2 id="stage-3-training-methods-peft-lora">Stage 3: Training Methods (PEFT &amp; LoRA)</h2>
<h3 id="full-fine-tuning-vs-peft">Full Fine-Tuning vs PEFT</h3>
<p><strong>Full Fine-Tuning (FFT)</strong> updates all model weights. For a 7B model at 16-bit precision, you need roughly 112GB of VRAM just for training. This is prohibitive for most teams.</p>
<p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> changes the game by updating only a small subset of parameters while freezing the rest.</p>
<h3 id="lora-the-industry-standard">LoRA: The Industry Standard</h3>
<p>LoRA (Low-Rank Adaptation) is the foundational PEFT technique. The key insight: weight changes during fine-tuning have low "intrinsic rank."</p>
<p><img alt="LoRA Architecture" src="../../../../assets/2026-01-04-finetuning-guide/lora_architecture.svg" /></p>
<p>Instead of updating a massive weight matrix <strong>W</strong> (dimension d√ód), LoRA learns two smaller matrices:</p>
<ul>
<li><strong>A</strong> (d √ó r) ‚Äî down-projection</li>
<li><strong>B</strong> (r √ó d) ‚Äî up-projection</li>
</ul>
<p>The update becomes: <strong>ŒîW = B √ó A</strong></p>
<p>With rank <code>r=16</code>, this reduces trainable parameters by <strong>~10,000x</strong>, dropping VRAM from 120GB to 16GB.</p>
<h3 id="peft-methods-compared">PEFT Methods Compared</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>How It Works</th>
<th>Memory Savings</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LoRA</strong></td>
<td>Low-rank matrices injected into frozen weights</td>
<td>~10x</td>
<td>General fine-tuning</td>
</tr>
<tr>
<td><strong>QLoRA</strong></td>
<td>LoRA + 4-bit base model quantization</td>
<td>~20x</td>
<td>Consumer GPUs (16-24GB)</td>
</tr>
<tr>
<td><strong>DoRA</strong></td>
<td>LoRA with magnitude/direction decomposition</td>
<td>~10x</td>
<td>When LoRA hits performance ceiling</td>
</tr>
<tr>
<td><strong>HFT</strong></td>
<td>Freezes half parameters per training round</td>
<td>~2x</td>
<td>Balance between FFT and PEFT</td>
</tr>
</tbody>
</table>
<h4 id="when-to-use-which">When to Use Which?</h4>
<ul>
<li><strong>LoRA</strong>: Start here. It's fast, memory-efficient, and widely supported</li>
<li><strong>QLoRA</strong>: When you need to fine-tune 70B models on consumer hardware</li>
<li><strong>DoRA</strong>: When you need to match full fine-tuning quality on complex reasoning tasks</li>
<li><strong>HFT</strong>: When you need better performance than LoRA but can't afford full fine-tuning</li>
</ul>
<h3 id="dora-weight-decomposed-lora">DoRA: Weight-Decomposed LoRA</h3>
<p>DoRA (Weight-Decomposed Low-Rank Adaptation) is a novel technique that bridges the performance gap between standard LoRA and full fine-tuning.</p>
<p><img alt="DoRA Architecture" src="../../../../assets/2026-01-04-finetuning-guide/dora_architecture.svg" /></p>
<p><strong>How it works:</strong></p>
<p>Instead of treating weights as a single entity, DoRA decomposes pre-trained weights into two components:</p>
<ol>
<li><strong>Magnitude</strong> ‚Äî Trainable scalar per column (controls "strength")</li>
<li><strong>Direction</strong> ‚Äî Updated with LoRA matrices (controls "what")</li>
</ol>
<p>The update becomes: <strong>W' = m √ó (V + B √ó A)</strong></p>
<p>Where:</p>
<ul>
<li><code>m</code> = magnitude (trainable)</li>
<li><code>V</code> = direction (W / ||W||)</li>
<li><code>B √ó A</code> = LoRA update to direction</li>
</ul>
<p><strong>Why it outperforms standard LoRA:</strong></p>
<ul>
<li>Richer parameter updates while maintaining efficiency</li>
<li>Achieves learning outcomes closer to full fine-tuning</li>
<li>Same memory efficiency (~10x savings)</li>
<li>Particularly effective on complex reasoning tasks</li>
</ul>
<h3 id="half-fine-tuning-hft">Half Fine-Tuning (HFT)</h3>
<p>HFT offers a unique balance between full fine-tuning and PEFT methods:</p>
<ul>
<li><strong>Methodology</strong>: Freezes half of the model's parameters during each fine-tuning round while updating the other half</li>
<li><strong>Strategy</strong>: The frozen and active halves vary across rounds</li>
<li><strong>Benefit</strong>: Retains foundational knowledge (frozen params) while acquiring new skills (active params)</li>
<li><strong>Use case</strong>: When LoRA is insufficient but full fine-tuning is too expensive</li>
</ul>
<h3 id="adapter-merging-for-multi-task-learning">Adapter Merging for Multi-Task Learning</h3>
<p>Instead of fine-tuning a monolithic model for multiple tasks, train separate small adapter modules for each function while keeping the base LLM frozen.</p>
<p><strong>Merging Methods:</strong></p>
<ol>
<li><strong>Concatenation</strong> ‚Äî Combines adapter parameters, increasing rank (fast, simple)</li>
<li><strong>Linear Combination</strong> ‚Äî Weighted sum of adapters (more control)</li>
<li><strong>SVD</strong> ‚Äî Matrix decomposition for merging (versatile but slower)</li>
</ol>
<p><strong>Example use case:</strong> One adapter for summarization, another for translation, merged into a single multi-task model.</p>
<hr />
<h2 id="stage-4-fine-tuning-preference-alignment">Stage 4: Fine-Tuning &amp; Preference Alignment</h2>
<p>When SFT isn't enough‚Äîthe model technically answers correctly but in "wrong" ways (too verbose, unsafe, wrong tone)‚Äîyou need preference alignment.</p>
<p><img alt="Alignment Methods" src="../../../../assets/2026-01-04-finetuning-guide/alignment_methods.svg" /></p>
<h4 id="traditional-approach-rlhf-with-ppo">Traditional Approach: RLHF with PPO</h4>
<p>The old standard was a complex 3-stage pipeline:</p>
<ol>
<li><strong>SFT</strong> ‚Äî Learn the task</li>
<li><strong>Reward Model</strong> ‚Äî Train on human preferences (chosen vs rejected)</li>
<li><strong>PPO (Proximal Policy Optimization)</strong> ‚Äî Reinforcement learning to optimize policy</li>
</ol>
<p><strong>Problems:</strong></p>
<ul>
<li>Complex to implement and manage</li>
<li>Computationally expensive (training multiple models)</li>
<li>Unstable training (hyperparameter sensitive)</li>
</ul>
<h4 id="modern-streamlined-dpo">Modern Streamlined: DPO</h4>
<p><strong>DPO (Direct Preference Optimization)</strong> simplifies preference alignment by eliminating the explicit reward model and RL training loop. While DPO optimizes the same objective as RLHF (reward maximization with KL-divergence constraint), it achieves this through a reparameterized supervised learning objective rather than explicit reinforcement learning:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
    <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;Explain quantum computing&quot;</span><span class="p">,</span>
    <span class="s2">&quot;chosen&quot;</span><span class="p">:</span> <span class="s2">&quot;Quantum computing uses qubits...&quot;</span><span class="p">,</span>   <span class="c1"># Preferred response</span>
    <span class="s2">&quot;rejected&quot;</span><span class="p">:</span> <span class="s2">&quot;Well, it&#39;s complicated...&quot;</span>        <span class="c1"># Non-preferred response</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>Benefits:</strong></p>
<ul>
<li>Simpler implementation (no explicit reward model or RL training loop)</li>
<li>More stable training (supervised learning approach)</li>
<li>Less compute required</li>
</ul>
<p><strong>The PPO vs DPO Debate:</strong></p>
<p>Recent research suggests the debate isn't settled:</p>
<ul>
<li>DPO may yield biased solutions in some scenarios</li>
<li>Well-tuned PPO can still achieve state-of-the-art results, particularly in complex tasks like code generation</li>
<li>PPO's explicit reward signal provides more granular guidance for specialized tasks</li>
</ul>
<h4 id="newest-single-stage-orpo">Newest Single-Stage: ORPO</h4>
<p><strong>ORPO (Odds-Ratio Preference Optimization)</strong> is the 2025 recommendation for most use cases. It combines SFT and preference alignment into a <strong>single training stage</strong>.</p>
<p><strong>How it works:</strong></p>
<p>ORPO uses a combined loss function that simultaneously:</p>
<ol>
<li><strong>Maximizes likelihood</strong> of the chosen response (learning the task)</li>
<li><strong>Penalizes</strong> the rejected response using an odds-ratio term (learning preferences)</li>
</ol>
<p><strong>Key Hyperparameters:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORPOConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">ORPOConfig</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">8e-6</span><span class="p">,</span>  <span class="c1"># Very low, as recommended by the ORPO paper</span>
    <span class="n">beta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>            <span class="c1"># Controls strength of preference penalty</span>
    <span class="c1"># ... other params</span>
<span class="p">)</span>
</code></pre></div>
<ul>
<li><strong>Learning Rate</strong>: Use very low values (8e-6) as recommended by the ORPO paper</li>
<li><strong>Beta</strong>: Controls the strength of the preference penalty (typically 0.1)</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>‚úÖ Single training stage (no separate SFT needed)</li>
<li>‚úÖ No reward model required</li>
<li>‚úÖ Fastest path to production</li>
<li>‚úÖ Simpler than DPO (fewer hyperparameters)</li>
</ul>
<p><strong>When to use what:</strong></p>
<ul>
<li><strong>ORPO</strong>: Start here for most use cases (fastest, simplest)</li>
<li><strong>DPO</strong>: When you need more control over the alignment process</li>
<li><strong>PPO</strong>: Only for specialized tasks requiring explicit reward signals (e.g., code generation)</li>
</ul>
<hr />
<h2 id="fine-tuning-frameworks">Fine-Tuning Frameworks</h2>
<p>The ecosystem has consolidated around four major tools:</p>
<h3 id="unsloth-speed-efficiency-champion">Unsloth ‚Äî Speed &amp; Efficiency Champion</h3>
<p>Unsloth uses HuggingFace packages (<code>trl</code> and <code>transformers</code>) under the hood but adds additional optimizations:</p>
<ul>
<li><strong>Custom Triton kernels</strong> ‚Äî Hand-written GPU kernels for attention, RoPE, and cross-entropy that bypass PyTorch overhead</li>
<li><strong>Memory-efficient backpropagation</strong> ‚Äî Recomputes activations during backward pass instead of storing them</li>
<li><strong>Fused operations</strong> ‚Äî Combines multiple operations (layer norm + linear, etc.) into single GPU calls</li>
<li><strong>4-bit quantization integration</strong> ‚Äî Seamless QLoRA with optimized dequantization</li>
</ul>
<blockquote>
<p>[!IMPORTANT] <strong>Import Order Matters</strong>
Because Unsloth patches HuggingFace packages, you <strong>must</strong> import and initialize Unsloth's <code>FastLanguageModel</code> <strong>before</strong> importing <code>trl</code> or <code>transformers</code>. Incorrect import order will cause failures.</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="c1"># ‚úÖ Correct order</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unsloth</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastLanguageModel</span>  <span class="c1"># Must be first!</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="c1"># ‚ùå Wrong order - will fail</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unsloth</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastLanguageModel</span>  <span class="c1"># Too late!</span>
</code></pre></div>
<p><strong>Best for:</strong> Single-GPU training, prototyping, Colab notebooks, anyone paying for GPU hours.</p>
<p><strong>Key advantage:</strong> Custom Triton kernels make it 2-5x faster than standard HuggingFace implementations.</p>
<h3 id="axolotl-config-driven-production">Axolotl ‚Äî Config-Driven Production</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># config.yaml - no code required</span>
<span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">meta-llama/Meta-Llama-3-8B</span>
<span class="nt">adapter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qlora</span>
<span class="nt">lora_r</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="nt">lora_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">datasets</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/my_data.jsonl</span>
<span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">alpaca</span>
<span class="nt">sample_packing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<p>Run with: <code>accelerate launch -m axolotl.cli.train config.yaml</code></p>
<p><strong>Best for:</strong> Production pipelines, multi-GPU clusters, reproducible experiments.</p>
<p><strong>Key advantage:</strong> YAML configs are version-controllable and shareable.</p>
<h3 id="framework-comparison">Framework Comparison</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Unsloth</th>
<th>Axolotl</th>
<th>TRL</th>
<th>Torchtune</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Strength</strong></td>
<td>Speed &amp; efficiency</td>
<td>Multi-GPU scale</td>
<td>Ecosystem</td>
<td>PyTorch native</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Fastest (2-5x)</td>
<td>High</td>
<td>Moderate</td>
<td>High</td>
</tr>
<tr>
<td><strong>Multi-GPU</strong></td>
<td>Growing</td>
<td>Excellent</td>
<td>Good</td>
<td>Excellent</td>
</tr>
<tr>
<td><strong>Config</strong></td>
<td>Python</td>
<td>YAML</td>
<td>Python</td>
<td>Python</td>
</tr>
<tr>
<td><strong>Best for</strong></td>
<td>Local/Colab</td>
<td>Clusters</td>
<td>Research</td>
<td>PyTorch purists</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="practical-demo-fine-tuning-with-unsloth">Practical Demo: Fine-Tuning with Unsloth</h2>
<p>Let's walk through a complete example using my <a href="https://github.com/slavadubrov/unsloth-finetune-demo">unsloth-finetune-demo</a> repository. This demo fine-tunes Nemotron-Nano for function calling.</p>
<p><img alt="Training Pipeline" src="../../../../assets/2026-01-04-finetuning-guide/training_pipeline.svg" /></p>
<h3 id="quick-start">Quick Start</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Clone and setup</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/slavadubrov/unsloth-finetune-demo.git
<span class="nb">cd</span><span class="w"> </span>unsloth-finetune-demo

<span class="c1"># Install with uv (recommended)</span>
uv<span class="w"> </span>sync

<span class="c1"># Run fine-tuning (quick test)</span>
uv<span class="w"> </span>run<span class="w"> </span>finetune<span class="w"> </span>--max-samples<span class="w"> </span><span class="m">1000</span>
</code></pre></div>
<h3 id="configuration-deep-dive">Configuration Deep Dive</h3>
<p>The key configuration lives in <code>config.py</code>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Model &amp; Dataset</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&quot;</span>  <span class="c1"># 4B params, 128K context</span>
<span class="n">DATASET_NAME</span> <span class="o">=</span> <span class="s2">&quot;glaiveai/glaive-function-calling-v2&quot;</span>   <span class="c1"># 113K examples</span>

<span class="c1"># LoRA Configuration</span>
<span class="n">LORA_R</span> <span class="o">=</span> <span class="mi">16</span>        <span class="c1"># Rank - higher = smarter but more VRAM</span>
<span class="n">LORA_ALPHA</span> <span class="o">=</span> <span class="mi">32</span>    <span class="c1"># Scaling factor - usually 2x LORA_R</span>
<span class="n">MAX_SEQ_LENGTH</span> <span class="o">=</span> <span class="mi">4096</span>

<span class="c1"># Target all linear layers for best quality</span>
<span class="n">LORA_TARGET_MODULES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
    <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;down_proj&quot;</span><span class="p">,</span>
<span class="p">]</span>
</code></pre></div>
<blockquote>
<p>[!NOTE] <strong>The Alpha/Rank Ratio</strong>
Industry best practice in 2025: set <strong>alpha = 2 √ó rank</strong> (e.g., rank=16, alpha=32). This provides stronger weight updates without destabilizing training.</p>
</blockquote>
<h3 id="core-training-code">Core Training Code</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">unsloth</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastLanguageModel</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">trl</span><span class="w"> </span><span class="kn">import</span> <span class="n">SFTTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="c1"># Load model with 4-bit quantization</span>
<span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1&quot;</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Add LoRA adapters</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FastLanguageModel</span><span class="o">.</span><span class="n">get_peft_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;down_proj&quot;</span><span class="p">],</span>
    <span class="n">use_gradient_checkpointing</span><span class="o">=</span><span class="s2">&quot;unsloth&quot;</span><span class="p">,</span>  <span class="c1"># Magic sauce for memory</span>
<span class="p">)</span>

<span class="c1"># Train with SFTTrainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Crucial for efficiency!</span>
    <span class="n">args</span><span class="o">=</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="fine-tuning-with-axolotl">Fine-Tuning with Axolotl</h2>
<blockquote>
<p>[!NOTE] <strong>Demo Coming Soon</strong>
I'm currently working on a hands-on Axolotl demo‚Äîstay tuned! In the meantime, check out the <a href="https://huggingface.co/blog/accelerate-nd-parallel">Accelerate n-D Parallelism Guide</a> from Hugging Face for multi-GPU training strategies.</p>
</blockquote>
<p>For production and multi-GPU setups, Axolotl's config-first approach excels:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># axolotl_config.yaml</span>
<span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">meta-llama/Meta-Llama-3-8B</span>
<span class="nt">model_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LlamaForCausalLM</span>

<span class="c1"># QLoRA configuration</span>
<span class="nt">load_in_4bit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">adapter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qlora</span>
<span class="nt">lora_r</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="nt">lora_alpha</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">lora_dropout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.05</span>
<span class="nt">lora_target_modules</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">q_proj</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k_proj</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v_proj</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">o_proj</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gate_proj</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">up_proj</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">down_proj</span>

<span class="c1"># Dataset</span>
<span class="nt">datasets</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">data/training_data.jsonl</span>
<span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">alpaca</span>

<span class="c1"># Training settings</span>
<span class="nt">sequence_len</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4096</span>
<span class="nt">sample_packing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"> </span><span class="c1"># Critical for speed!</span>
<span class="nt">micro_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0002</span>
<span class="nt">num_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>

<span class="c1"># Hardware</span>
<span class="nt">bf16</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">flash_attention</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>
<p>Run training:</p>
<div class="highlight"><pre><span></span><code>accelerate<span class="w"> </span>launch<span class="w"> </span>-m<span class="w"> </span>axolotl.cli.train<span class="w"> </span>axolotl_config.yaml
</code></pre></div>
<hr />
<h2 id="stage-5-evaluation">Stage 5: Evaluation</h2>
<p>Training is easy; knowing if it worked is hard. Evaluation should happen before deployment.</p>
<h3 id="automated-benchmarks">Automated Benchmarks</h3>
<p>Use <code>lm-evaluation-harness</code> for standardized testing:</p>
<div class="highlight"><pre><span></span><code>lm_eval<span class="w"> </span>--model<span class="w"> </span>hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_args<span class="w"> </span><span class="nv">pretrained</span><span class="o">=</span>./outputs/merged-model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tasks<span class="w"> </span>hellaswag,arc_easy,mmlu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--batch_size<span class="w"> </span><span class="m">8</span>
</code></pre></div>
<h3 id="llm-as-judge">LLM-as-Judge</h3>
<p>For subjective quality, use a larger model to evaluate:</p>
<div class="highlight"><pre><span></span><code><span class="n">judge_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Rate this response from 1-5 on:</span>
<span class="s2">- Relevance</span>
<span class="s2">- Accuracy</span>
<span class="s2">- Formatting</span>

<span class="s2">Response: </span><span class="si">{model_output}</span>
<span class="s2">Expected: </span><span class="si">{ground_truth}</span>
<span class="s2">&quot;&quot;&quot;</span>
</code></pre></div>
<h3 id="domain-specific-eval">Domain-Specific Eval</h3>
<p>Create a held-out test set of real examples from your use case. This is the most important evaluation‚Äîgeneric benchmarks won't tell you if your function-calling model actually works.</p>
<hr />
<h2 id="stage-6-deployment-output-formats">Stage 6: Deployment &amp; Output Formats</h2>
<p>After training, you have three export options:</p>
<p><img alt="Output Formats" src="../../../../assets/2026-01-04-finetuning-guide/output_formats.svg" /></p>
<h3 id="1-lora-adapter-default">1. LoRA Adapter (Default)</h3>
<div class="highlight"><pre><span></span><code>uv<span class="w"> </span>run<span class="w"> </span>finetune<span class="w">  </span><span class="c1"># Saves ~100-500MB adapter</span>
</code></pre></div>
<ul>
<li><strong>Size:</strong> ~100-500 MB</li>
<li><strong>Best for:</strong> Development, testing, multiple adapters on one base model</li>
<li><strong>Flexibility:</strong> Swap adapters without re-downloading the base model</li>
</ul>
<h3 id="2-merged-model">2. Merged Model</h3>
<div class="highlight"><pre><span></span><code>uv<span class="w"> </span>run<span class="w"> </span>finetune<span class="w"> </span>--merge<span class="w">  </span><span class="c1"># Creates standalone ~8-16GB model</span>
</code></pre></div>
<ul>
<li><strong>Size:</strong> ~8-16 GB (full 16-bit weights)</li>
<li><strong>Best for:</strong> Sharing on HuggingFace, vLLM serving, simple deployment</li>
<li><strong>Trade-off:</strong> Larger storage, but no separate base model needed</li>
</ul>
<h3 id="3-gguf-format">3. GGUF Format</h3>
<div class="highlight"><pre><span></span><code>uv<span class="w"> </span>run<span class="w"> </span>finetune<span class="w"> </span>--gguf<span class="w"> </span>q4_k_m<span class="w">  </span><span class="c1"># Creates ~2-4GB quantized model</span>
</code></pre></div>
<ul>
<li><strong>Size:</strong> ~2-4 GB (Q4_K_M quantization)</li>
<li><strong>Best for:</strong> CPU inference, Ollama, llama.cpp, edge deployment</li>
<li><strong>Options:</strong> <code>q4_k_m</code> (balanced), <code>q5_k_m</code> (higher quality), <code>q8_0</code> (near-lossless)</li>
</ul>
<hr />
<h2 id="stage-7-serving-monitoring">Stage 7: Serving &amp; Monitoring</h2>
<h3 id="with-vllm-production">With vLLM (Production)</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Requires merged model format</span>
vllm<span class="w"> </span>serve<span class="w"> </span>./outputs/unsloth-nemotron-function-calling-merged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--host<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">8000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-model-len<span class="w"> </span><span class="m">4096</span>
</code></pre></div>
<p>Query via OpenAI-compatible API:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;dummy&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;unsloth-nemotron-function-calling-merged&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Book a flight to Tokyo&quot;</span><span class="p">}]</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="with-ollama-local">With Ollama (Local)</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Create Modelfile</span>
<span class="nb">echo</span><span class="w"> </span><span class="s1">&#39;FROM ./outputs/unsloth-nemotron-function-calling-gguf/model-q4_k_m.gguf&#39;</span><span class="w"> </span>&gt;<span class="w"> </span>Modelfile

<span class="c1"># Import to Ollama</span>
ollama<span class="w"> </span>create<span class="w"> </span>my-function-model<span class="w"> </span>-f<span class="w"> </span>Modelfile

<span class="c1"># Run</span>
ollama<span class="w"> </span>run<span class="w"> </span>my-function-model
</code></pre></div>
<h3 id="with-llamacpp-cpu">With llama.cpp (CPU)</h3>
<div class="highlight"><pre><span></span><code>./main<span class="w"> </span>-m<span class="w"> </span>./outputs/model-q4_k_m.gguf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="s2">&quot;What&#39;s the weather in Tokyo?&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ctx-size<span class="w"> </span><span class="m">4096</span>
</code></pre></div>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<ol>
<li><strong>Follow the 7-stage lifecycle</strong> ‚Äî Data Preparation is the highest leverage step</li>
<li><strong>Don't default to fine-tuning</strong> ‚Äî Try RAG and prompting first; use the decision framework</li>
<li><strong>Data is the foundation</strong> ‚Äî Use modern pipelines (DataTrove, Distilabel) with PII scrubbing for enterprise</li>
<li><strong>Start with QLoRA</strong> ‚Äî Fine-tune 70B models on consumer GPUs (4x A100 minimum for production)</li>
<li><strong>Use ORPO for alignment</strong> ‚Äî Single-stage training is faster and simpler than DPO or PPO</li>
<li><strong>Consider DoRA</strong> ‚Äî When LoRA hits performance ceiling on complex reasoning tasks</li>
<li><strong>Sample packing</strong> is the single biggest training speedup</li>
<li><strong>Start with Unsloth</strong> for prototyping, <strong>Axolotl</strong> for production</li>
<li><strong>Export to GGUF</strong> for local/edge deployment</li>
</ol>
<hr />
<h2 id="references">References</h2>
<h3 id="papers-research">Papers &amp; Research</h3>
<ul>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></li>
<li><a href="https://arxiv.org/abs/2402.09353">DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>
<li><a href="https://arxiv.org/abs/2305.18290">DPO: Direct Preference Optimization</a></li>
<li><a href="https://arxiv.org/abs/2403.07691">ORPO: Odds Ratio Preference Optimization</a></li>
<li><a href="https://arxiv.org/abs/1707.06347">PPO: Proximal Policy Optimization Algorithms</a> ‚Äî OpenAI, 2017</li>
<li><a href="https://arxiv.org/abs/2404.18466">HFT: Half Fine-Tuning for Large Language Models</a> ‚Äî Mitigating catastrophic forgetting</li>
</ul>
<h3 id="data-processing-tools">Data Processing Tools</h3>
<ul>
<li><a href="https://github.com/huggingface/datatrove">DataTrove</a> ‚Äî Hugging Face data processing at scale</li>
<li><a href="https://github.com/argilla-io/distilabel">Distilabel</a> ‚Äî Synthetic data generation (Argilla)</li>
<li><a href="https://github.com/adbar/trafilatura">Trafilatura</a> ‚Äî Web text extraction and crawling</li>
<li><a href="https://fasttext.cc/">FastText</a> ‚Äî Facebook AI language identification (supports 217 languages)</li>
<li><a href="https://github.com/microsoft/presidio">Microsoft Presidio</a> ‚Äî PII detection and anonymization</li>
<li><a href="https://github.com/LeapBeyond/scrubadub">scrubadub</a> ‚Äî Python library for PII removal</li>
</ul>
<h3 id="schema-guided-reasoning-sgr">Schema-Guided Reasoning (SGR)</h3>
<ul>
<li><a href="https://github.com/mlc-ai/xgrammar">xgrammar</a> ‚Äî Constrained decoding with FSMs</li>
<li><a href="https://github.com/dottxt-ai/outlines">outlines</a> ‚Äî Structured generation for LLMs</li>
</ul>
<h3 id="training-frameworks">Training Frameworks</h3>
<ul>
<li><a href="https://docs.unsloth.ai/">Unsloth</a> ‚Äî Speed &amp; efficiency champion (2-5x faster)</li>
<li><a href="https://github.com/axolotl-ai-cloud/axolotl">Axolotl</a> ‚Äî Config-driven multi-GPU training</li>
<li><a href="https://huggingface.co/docs/trl">TRL (Transformer Reinforcement Learning)</a> ‚Äî Hugging Face RL training</li>
<li><a href="https://github.com/pytorch/torchtune">Torchtune</a> ‚Äî PyTorch-native fine-tuning library</li>
</ul>
<h3 id="inference-deployment">Inference &amp; Deployment</h3>
<ul>
<li><a href="https://docs.vllm.ai/">vLLM</a> ‚Äî High-throughput LLM serving engine</li>
<li><a href="https://ollama.ai/">Ollama</a> ‚Äî Local LLM runner for Mac/Windows/Linux</li>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> ‚Äî CPU/GPU inference with GGUF format</li>
</ul>
<h3 id="evaluation">Evaluation</h3>
<ul>
<li><a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a> ‚Äî EleutherAI standardized LLM benchmarking</li>
</ul>
<h3 id="guides-resources">Guides &amp; Resources</h3>
<ul>
<li><a href="https://github.com/slavadubrov/unsloth-finetune-demo">Demo Repository</a> ‚Äî Practical fine-tuning example</li>
<li><a href="https://notebooklm.google.com/notebook/f6bfdb56-8949-4929-87e4-ab6dee31a4a8">LLM Fine-Tuning. Theoretical Intuition and Practical Implementation</a> ‚Äî NotebookLM research notebook</li>
<li><a href="https://huggingface.co/blog/accelerate-nd-parallel">Accelerate n-D Parallelism Guide</a> ‚Äî Hugging Face multi-GPU training strategies</li>
</ul>







  
  




  


 
  </article>
</div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="https://www.linkedin.com/in/slavadubrov" target="_blank" rel="noopener" title="LinkedIn" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://slavadubrov.substack.com/" target="_blank" rel="noopener" title="Substack" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22.539 8.242H1.46V5.406h21.08zM1.46 10.812V24L12 18.11 22.54 24V10.812zM22.54 0H1.46v2.836h21.08z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/slavadubrov" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
    <a href="/feed_rss_created.xml" target="_blank" rel="noopener" title="RSS" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19 7.38 20 6.18 20 5 20 4 19 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27zm0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.toc", "navigation.toc.sticky", "navigation.toc.maxdepth", "navigation.toc.title", "navigation.toc.collapse", "navigation.toc.collapse_empty_groups", "navigation.toc.collapse_single_children", "content.code.copy"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
      
        <script src="../../../../../javascripts/mermaid-init.js"></script>
      
    
  </body>
</html>