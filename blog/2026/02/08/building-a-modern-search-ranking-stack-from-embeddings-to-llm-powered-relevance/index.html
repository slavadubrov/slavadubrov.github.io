
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A practical guide to building a multi-stage search ranking pipeline ‚Äî hybrid retrieval, cross-encoder reranking, and LLM listwise ranking ‚Äî with a working demo on the Amazon ESCI benchmark.">
      
      
        <meta name="author" content="Viacheslav Dubrov">
      
      
        <link rel="canonical" href="https://slavadubrov.github.io/blog/2026/02/08/building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance/">
      
      
        <link rel="prev" href="../../../01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/">
      
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../../assets/favicon-eoc.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Building a Modern Search Ranking Stack: From Embeddings to LLM-Powered Relevance - Edge of Context: Practical AI Engineering</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/brand.css">
    
      <link rel="stylesheet" href="../../../../../assets/blog-sidebar.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
     
<style>
  /* Header social icons */
  .md-header__social {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    margin-left: 0.5rem;
  }
  .md-header__social a {
    color: var(--md-primary-bg-color);
    opacity: 0.7;
    transition: opacity 0.2s;
  }
  .md-header__social a:hover {
    opacity: 1;
  }
  .md-header__social svg {
    width: 1.2rem;
    height: 1.2rem;
    fill: currentColor;
  }
  /* Adjust search to make room for social icons */
  .md-search {
    margin-right: 0;
  }
</style>

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
     
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="Edge of Context: Practical AI Engineering" class="md-header__button md-logo" aria-label="Edge of Context: Practical AI Engineering" data-md-component="logo">
      
  <img src="../../../../../assets/logo-eoc.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Edge of Context: Practical AI Engineering
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Building a Modern Search Ranking Stack: From Embeddings to LLM-Powered Relevance
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../../" class="md-tabs__link">
          
  
  
    
  
  Blog

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
<script>
  // Inject social icons into header after DOM loads
  document.addEventListener("DOMContentLoaded", function () {
    const header = document.querySelector(".md-header__inner");
    if (header) {
      const social = document.createElement("div");
      social.className = "md-header__social";
      social.innerHTML = `
      <a href="https://www.linkedin.com/in/slavadubrov" title="LinkedIn" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
      <a href="https://slavadubrov.substack.com/" title="Substack" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"/></svg>
      </a>
      <a href="https://github.com/slavadubrov" title="GitHub" target="_blank" rel="noopener">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    `;
      header.appendChild(social);
    }
  });
</script>

    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="Edge of Context: Practical AI Engineering" class="md-nav__button md-logo" aria-label="Edge of Context: Practical AI Engineering" data-md-component="logo">
      
  <img src="../../../../../assets/logo-eoc.svg" alt="logo">

    </a>
    Edge of Context: Practical AI Engineering
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Blog
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Archive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2026/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2026
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2025/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2025
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/ai-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/agentic-ai/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agentic AI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/agents-101/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agents 101
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/infrastructure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Infrastructure
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/paper-review/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Paper Review
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/python/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/search-and-recs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Search and Recs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/tooling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tooling
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-evolution-of-search-ranking" class="md-nav__link">
    <span class="md-ellipsis">
      The Evolution of Search Ranking
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Evolution of Search Ranking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#era-1-the-lexical-age-bm25" class="md-nav__link">
    <span class="md-ellipsis">
      Era 1: The Lexical Age (BM25)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#era-2-the-dense-retrieval-age-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Era 2: The Dense Retrieval Age (Embeddings)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#era-3-the-interactive-age-cross-encoders-and-llms" class="md-nav__link">
    <span class="md-ellipsis">
      Era 3: The Interactive Age (Cross-Encoders and LLMs)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-the-multi-stage-funnel" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture: The Multi-Stage Funnel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-demo-a-five-stage-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      The Demo: A Five-Stage Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Demo: A Five-Stage Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quick-start" class="md-nav__link">
    <span class="md-ellipsis">
      Quick Start
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-dataset-amazon-esci" class="md-nav__link">
    <span class="md-ellipsis">
      The Dataset: Amazon ESCI
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#retrieval-hybrid-search" class="md-nav__link">
    <span class="md-ellipsis">
      Retrieval: Hybrid Search
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Retrieval: Hybrid Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bm25-the-lexical-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      BM25: The Lexical Baseline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dense-bi-encoder-semantic-retrieval" class="md-nav__link">
    <span class="md-ellipsis">
      Dense Bi-Encoder: Semantic Retrieval
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dense Bi-Encoder: Semantic Retrieval">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-bi-encoders-learn-good-representations" class="md-nav__link">
    <span class="md-ellipsis">
      How Bi-Encoders Learn Good Representations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving-bi-encoder-embeddings-at-scale" class="md-nav__link">
    <span class="md-ellipsis">
      Serving Bi-Encoder Embeddings at Scale
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-hybrid-bm25-and-dense-have-complementary-failures" class="md-nav__link">
    <span class="md-ellipsis">
      Why Hybrid: BM25 and Dense Have Complementary Failures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reciprocal-rank-fusion-rrf" class="md-nav__link">
    <span class="md-ellipsis">
      Reciprocal Rank Fusion (RRF)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cross-encoder-reranking" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Encoder Reranking
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cross-Encoder Reranking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-cross-attention-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why Cross-Attention Matters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-cross-encoders-are-trained" class="md-nav__link">
    <span class="md-ellipsis">
      How Cross-Encoders Are Trained
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-speed-quality-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      The Speed-Quality Tradeoff
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llm-listwise-reranking" class="md-nav__link">
    <span class="md-ellipsis">
      LLM Listwise Reranking
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Listwise Reranking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-listwise-prompt" class="md-nav__link">
    <span class="md-ellipsis">
      The Listwise Prompt
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#three-execution-modes" class="md-nav__link">
    <span class="md-ellipsis">
      Three Execution Modes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parsing-and-fallback" class="md-nav__link">
    <span class="md-ellipsis">
      Parsing and Fallback
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-every-stage-earns-its-keep" class="md-nav__link">
    <span class="md-ellipsis">
      Results: Every Stage Earns Its Keep
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Results: Every Stage Earns Its Keep">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-observations" class="md-nav__link">
    <span class="md-ellipsis">
      Key Observations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-measuring-what-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation: Measuring What Matters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Evaluation: Measuring What Matters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ndcg10-primary-metric" class="md-nav__link">
    <span class="md-ellipsis">
      NDCG@10 (Primary Metric)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mrr10-user-experience" class="md-nav__link">
    <span class="md-ellipsis">
      MRR@10 (User Experience)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recall100-retrieval-coverage" class="md-nav__link">
    <span class="md-ellipsis">
      Recall@100 (Retrieval Coverage)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-indexing-hnsw-vs-ivf" class="md-nav__link">
    <span class="md-ellipsis">
      Vector Indexing: HNSW vs. IVF
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vector Indexing: HNSW vs. IVF">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hnsw-hierarchical-navigable-small-world" class="md-nav__link">
    <span class="md-ellipsis">
      HNSW (Hierarchical Navigable Small World)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ivf-inverted-file" class="md-nav__link">
    <span class="md-ellipsis">
      IVF (Inverted File)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-llm-layer-beyond-reranking" class="md-nav__link">
    <span class="md-ellipsis">
      The LLM Layer: Beyond Reranking
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The LLM Layer: Beyond Reranking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#query-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      Query Understanding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm-as-a-judge-for-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      LLM-as-a-Judge for Evaluation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#knowledge-distillation" class="md-nav__link">
    <span class="md-ellipsis">
      Knowledge Distillation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#personalization-who-is-searching-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Personalization: Who Is Searching Matters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Personalization: Who Is Searching Matters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#two-tower-models-for-personalization" class="md-nav__link">
    <span class="md-ellipsis">
      Two-Tower Models for Personalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-bias-the-silent-distortion" class="md-nav__link">
    <span class="md-ellipsis">
      Position Bias: The Silent Distortion
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bridging-the-domain-gap" class="md-nav__link">
    <span class="md-ellipsis">
      Bridging the Domain Gap
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bridging the Domain Gap">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#synthetic-data-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Synthetic Data Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsc-soft-tokens-for-domain-adaptation" class="md-nav__link">
    <span class="md-ellipsis">
      RMSC: Soft Tokens for Domain Adaptation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-practical-maturity-path" class="md-nav__link">
    <span class="md-ellipsis">
      The Practical Maturity Path
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-frontier-reasoning-rerankers-and-agentic-search" class="md-nav__link">
    <span class="md-ellipsis">
      The Frontier: Reasoning Rerankers and Agentic Search
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Frontier: Reasoning Rerankers and Agentic Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reasoning-based-rerankers" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning-Based Rerankers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#agentic-search" class="md-nav__link">
    <span class="md-ellipsis">
      Agentic Search
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
    <nav class="md-nav" aria-label="References">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#papers" class="md-nav__link">
    <span class="md-ellipsis">
      Papers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datasets-and-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      Datasets and Benchmarks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#models-used-in-the-demo" class="md-nav__link">
    <span class="md-ellipsis">
      Models Used in the Demo
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tools-and-platforms" class="md-nav__link">
    <span class="md-ellipsis">
      Tools and Platforms
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#industry-references" class="md-nav__link">
    <span class="md-ellipsis">
      Industry References
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-project" class="md-nav__link">
    <span class="md-ellipsis">
      Demo Project
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
<div class="md-content md-content--post" data-md-component="content">
  <div
    class="md-sidebar md-sidebar--post"
    data-md-component="sidebar"
    data-md-type="navigation"
  >
    <div class="md-sidebar__scrollwrap">
      <div class="md-sidebar__inner md-post">
        <nav class="md-nav md-nav--primary">
          <div class="md-post__back">
            <div class="md-nav__title md-nav__container">
              <a href="../../../../" class="md-nav__link">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                <span class="md-ellipsis"> Back to index </span>
              </a>
            </div>
          </div>

          
          <ul class="md-post__meta md-nav__list">
            <li class="md-nav__item md-nav__item--section">
              <div class="md-post__title">
                <span class="md-ellipsis">Topics</span>
              </div>
              <nav class="md-nav">
                        
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >ü§ñ</span
                    >
                    <span class="blog-category-name">AI Engineering</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../01/04/the-complete-guide-to-llm-fine-tuning-in-2025-from-theory-to-production/">The Complete Guide to LLM Fine-Tuning in 2025: From Theory to Production</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/">LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/11/choosing-the-right-open-source-llm-variant--file-format/">Choosing the Right Open-Source LLM Variant & File Format</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/10/quick-guide-on-running-llms-locally-on-macos/">Quick-guide on Running LLMs Locally on macOS</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/">Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üìö</span
                    >
                    <span class="blog-category-name">Agents 101</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../01/31/the-cognitive-engine-choosing-the-right-reasoning-loop/">The Cognitive Engine: Choosing the Right Reasoning Loop</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >ü¶æ</span
                    >
                    <span class="blog-category-name">Agentic AI</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../01/11/enterprise-rag-challenge-3-winning-approaches-for-autonomous-ai-agents/">Enterprise RAG Challenge 3: Winning Approaches for Autonomous AI Agents</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/">Schema-Guided Reasoning on vLLM ‚Äî Turning LLMs into Reliable Business Logic Engines</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/">Domain-driven design for AI agents: a beginner-friendly guide</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/">Context Engineering in the Agentic‚ÄëAI Era ‚Äî and How to Cook It</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üîß</span
                    >
                    <span class="blog-category-name">Infrastructure</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/">MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      ></span
                    >
                    <span class="blog-category-name">Paper Review</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../01/21/mhc-how-deepseek-scaled-residual-connections-without-breaking-training/">mHC: How DeepSeek Scaled Residual Connections Without Breaking Training</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üêç</span
                    >
                    <span class="blog-category-name">Python</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/08/the-ultimate-guide-to-pyprojecttoml/">The Ultimate Guide to `pyproject.toml`</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/04/17/quick-guide-managing-python-on-macos-with-uv/">Quick Guide: Managing Python on macOS with uv</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section blog-category-section--active"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üîç</span
                    >
                    <span class="blog-category-name">Search and Recs</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post blog-category-post--active"
                    >
                      <a href="./">Building a Modern Search Ranking Stack: From Embeddings to LLM-Powered Relevance</a>
                    </li>
                     
                  </ul>
                </div>
                 
                <div
                  class="blog-category-section"
                >
                  <div class="blog-category-header">
                    <span class="blog-category-icon"
                      >üõ†Ô∏è</span
                    >
                    <span class="blog-category-name">Tooling</span>
                  </div>
                  <ul class="blog-category-posts">
                      
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/">Building a Custom FeatureStoreLite MCP Server Using uv</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/">Quick-guide on Local Stable-Diffusion Toolkits for macOS</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/">Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc üöÄ</a>
                    </li>
                    
                    <li
                      class="blog-category-post"
                    >
                      <a href="../../../../2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/">Quick-Guide on setting up a MacBook for AI Engineering</a>
                    </li>
                     
                  </ul>
                </div>
                
              </nav>
            </li>
          </ul>

          
        </nav>
        
      </div>
    </div>
  </div>
  <article class="md-content__inner md-typeset">
     
  




<h1 id="building-a-modern-search-ranking-stack-from-embeddings-to-llm-powered-relevance">Building a Modern Search Ranking Stack: From Embeddings to LLM-Powered Relevance</h1>
<p>Search is no longer a string-matching problem. A query for "wireless headphones" on a product search engine is not just about finding items containing those two words ‚Äî it is about surfacing the best result based on semantic relevance, product quality, user preferences, and real-time availability. The gap between BM25 keyword matching and what users actually expect has forced a complete rethinking of search architecture.</p>
<p>This post walks through the anatomy of a modern search ranking stack: a multi-stage pipeline that combines sparse lexical retrieval, dense semantic embeddings, reciprocal rank fusion, cross-encoder reranking, and LLM-powered listwise ranking. I built a <a href="https://github.com/slavadubrov/search-ranking-stack">working demo</a> that benchmarks each stage on the Amazon ESCI product search dataset ‚Äî proving the value of every layer with real numbers.</p>
<!-- more -->

<p><strong>TL;DR</strong>: The optimal production architecture is a multi-stage hybrid pipeline ‚Äî parallel BM25 and dense retrieval fused via Reciprocal Rank Fusion, followed by cross-encoder reranking, with LLMs handling the final precision layer. On the Amazon ESCI benchmark, this pipeline delivers a <strong>22.5% NDCG@10 improvement</strong> over BM25 alone (0.585 to 0.717), with the LLM reranker providing the single largest jump (+0.072).</p>
<hr />
<h2 id="the-evolution-of-search-ranking">The Evolution of Search Ranking</h2>
<p>To understand why modern search stacks look the way they do, we need to trace the trajectory. Search ranking has evolved through three distinct eras, each solving a specific limitation of its predecessor.</p>
<h3 id="era-1-the-lexical-age-bm25">Era 1: The Lexical Age (BM25)</h3>
<p>For decades, <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> was the undisputed king of retrieval. It is a probabilistic model that ranks documents based on the frequency of query terms appearing in the document, normalized by document length and inverse document frequency (IDF).</p>
<p>BM25 excels at exact keyword matching ‚Äî searching for a specific error code, product SKU, or HTTP status code works perfectly because the system needs literal token overlap. However, it suffers from the <strong>vocabulary mismatch problem</strong>: a query for "cheap laptop" will miss documents about "budget notebook computer" if those exact words are not present. BM25 has zero understanding of semantic intent.</p>
<p>Despite this limitation, BM25 remains a robust baseline. It scores <strong>0.429 average nDCG@10 across the <a href="https://arxiv.org/abs/2104.08663">BEIR benchmark's</a> 18 datasets</strong> and <a href="https://arxiv.org/abs/2407.07790">still outperforms some neural models</a> on argumentative retrieval tasks like <a href="https://webis.de/events/touche-20/">Touche-2020</a>.</p>
<h3 id="era-2-the-dense-retrieval-age-embeddings">Era 2: The Dense Retrieval Age (Embeddings)</h3>
<p>The introduction of BERT and Transformer models brought <strong>Dense Retrieval</strong>. Instead of matching keywords, we map both queries and documents into a shared high-dimensional vector space (typically 768 or 1024 dimensions). Relevance becomes a calculation of cosine similarity between vectors.</p>
<p>The <strong>Bi-Encoder</strong> (or "Two-Tower") architecture processes the query and document independently through separate encoder towers, producing fixed-length embeddings. Document vectors can be pre-computed and indexed offline, enabling fast retrieval via Approximate Nearest Neighbor (ANN) algorithms. Now "cheap laptop" and "budget notebook" land close together in vector space.</p>
<p>Under the hood, bi-encoders use a <strong>Siamese architecture</strong> (<a href="https://arxiv.org/abs/1908.10084">Sentence-BERT</a>, Reimers &amp; Gurevych, EMNLP 2019): both towers share the same Transformer weights. Each tower independently processes its input text, then typically mean-pools the token-level hidden states into a single fixed-length vector (typically 384 or 768 dimensions). Weight sharing ensures that queries and documents are projected into the same semantic space ‚Äî a critical requirement for cosine similarity to be meaningful.</p>
<p>These models are trained with <strong>contrastive learning</strong>, typically using the <a href="https://arxiv.org/abs/1807.03748">InfoNCE</a> loss. Given a batch of (query, positive_document) pairs, the objective is to maximize <code>sim(query, positive_doc)</code> while minimizing <code>sim(query, negative_docs)</code> ‚Äî where negatives come from other queries' positives within the same batch (in-batch negatives). A temperature parameter <span class="arithmatex">\(\tau\)</span> controls the sharpness of the distribution: lower values push the model to make harder distinctions between positives and negatives.</p>
<p>The quality of training data is the single biggest lever for bi-encoder performance. Models start from (query, positive_document) pairs from datasets like <a href="https://arxiv.org/abs/1611.09268">MS MARCO</a>, then augment with <strong>hard negatives</strong> ‚Äî documents that the current model ranks highly but are actually not relevant. Random negatives are too easy and provide little learning signal; hard negatives force the model to learn subtle distinctions. The <a href="https://arxiv.org/abs/2210.11773">SimANS</a> framework (Zhou et al., EMNLP 2022) formalizes this: exclude both easy negatives (too low rank) and potential false negatives (too high rank), and train on the "hard middle ground."</p>
<p>The trade-off? Bi-encoders compress all semantic nuance into a single fixed-size vector. This "representation bottleneck" means they often miss fine-grained interactions between specific query terms and document content.</p>
<h3 id="era-3-the-interactive-age-cross-encoders-and-llms">Era 3: The Interactive Age (Cross-Encoders and LLMs)</h3>
<p><strong>Cross-encoders</strong> (<a href="https://arxiv.org/abs/1901.04085">Nogueira &amp; Cho, 2019</a>) feed the query and document into a Transformer simultaneously as a concatenated sequence (<code>[CLS] Query [SEP] Document</code>), allowing every query token to attend to every document token through the full self-attention mechanism. This deep interaction captures nuances that independent encoding cannot.</p>
<p><strong>LLM Reranking</strong> takes this further: large language models now act as zero-shot listwise rankers, effectively serving as a "human judge" that can reason about <em>why</em> one document is better than another. <a href="https://arxiv.org/abs/2304.09542">RankGPT</a> (Sun et al., EMNLP 2023 Outstanding Paper) demonstrated that GPT-4 operating as a zero-shot listwise reranker matches or exceeds supervised methods.</p>
<p>The precision is unmatched ‚Äî but so is the computational cost. You cannot pre-compute scores, and inference is 100x slower than bi-encoder retrieval. This constraint gives rise to the defining architectural pattern of modern search: <strong>the multi-stage funnel</strong>.</p>
<hr />
<h2 id="architecture-the-multi-stage-funnel">Architecture: The Multi-Stage Funnel</h2>
<p>Because we cannot run an expensive Cross-Encoder or LLM on millions of documents, modern search stacks use a hierarchical funnel architecture. Each stage acts as a filter, progressively reducing the candidate pool while increasing the model complexity.</p>
<p>A single-stage search is either too slow (complex models on everything) or too imprecise (simple models everywhere). <strong>The funnel is the dominant production pattern for search at scale.</strong></p>
<p><img alt="Multi-Stage Ranking Funnel" src="../../../../assets/2026-02-08-search-ranking-stack/multi_stage_funnel.svg" /></p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Candidate Pool</th>
<th>Primary Objective</th>
<th>Model Complexity</th>
<th>Latency Budget</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Retrieval</strong></td>
<td>10^9 - 10^12</td>
<td>Maximum Recall</td>
<td>Low (BM25, Bi-Encoders)</td>
<td>&lt; 50ms</td>
</tr>
<tr>
<td><strong>Pre-Ranking</strong></td>
<td>10^4 - 10^5</td>
<td>Efficient Filtering</td>
<td>Medium (Two-Tower, GBDT)</td>
<td>&lt; 100ms</td>
</tr>
<tr>
<td><strong>Full Ranking</strong></td>
<td>10^2 - 10^3</td>
<td>Maximum Precision</td>
<td>High (Cross-Encoders, LLMs)</td>
<td>&lt; 500ms</td>
</tr>
<tr>
<td><strong>Blending</strong></td>
<td>10^1 - 10^2</td>
<td>Diversity and Safety</td>
<td>Rules and Multi-Objective</td>
<td>&lt; 20ms</td>
</tr>
</tbody>
</table>
<p>The critical insight: <strong>retrieval sets the ceiling, reranking optimizes within it</strong>. If a relevant document does not survive the retrieval stage, no downstream model can recover it.</p>
<hr />
<h2 id="the-demo-a-five-stage-pipeline">The Demo: A Five-Stage Pipeline</h2>
<p>To make all of this concrete, I built a <a href="https://github.com/slavadubrov/search-ranking-stack">search-ranking-stack</a> demo that implements a five-stage pipeline on the <a href="https://github.com/amazon-science/esci-data">Amazon ESCI</a> product search benchmark. Each stage is measured independently so you can see exactly where the gains come from.</p>
<p><img alt="Demo Pipeline Architecture" src="../../../../assets/2026-02-08-search-ranking-stack/demo_architecture.svg" /></p>
<p>The pipeline:</p>
<ol>
<li><strong>BM25 Sparse Retrieval</strong> ‚Äî lexical baseline (rank_bm25)</li>
<li><strong>Dense Bi-Encoder Retrieval</strong> ‚Äî semantic search (all-MiniLM-L6-v2)</li>
<li><strong>Hybrid RRF Fusion</strong> ‚Äî combines sparse and dense results</li>
<li><strong>Cross-Encoder Reranking</strong> ‚Äî fine-grained relevance scoring (ms-marco-MiniLM-L-12-v2)</li>
<li><strong>LLM Listwise Reranking</strong> ‚Äî reasoning-powered final ranking (Ollama / Claude / local)</li>
</ol>
<p>Steps 1--3 correspond to the <strong>Retrieval</strong> stage of the funnel (maximize recall), while steps 4--5 correspond to the <strong>Full Ranking</strong> stage (maximize precision). The demo skips Pre-Ranking and Blending ‚Äî at ~8,500 documents, we can afford to send all hybrid results directly to reranking.</p>
<h3 id="quick-start">Quick Start</h3>
<div class="highlight"><pre><span></span><code>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/slavadubrov/search-ranking-stack.git
<span class="nb">cd</span><span class="w"> </span>search-ranking-stack
uv<span class="w"> </span>sync

<span class="c1"># Download and sample ESCI dataset (~2.5GB download, ~5MB sample)</span>
uv<span class="w"> </span>run<span class="w"> </span>download-data

<span class="c1"># Run the full pipeline (without LLM reranking)</span>
uv<span class="w"> </span>run<span class="w"> </span>run-all

<span class="c1"># Run with LLM reranking via Ollama</span>
uv<span class="w"> </span>run<span class="w"> </span>run-all<span class="w"> </span>--llm-mode<span class="w"> </span>ollama
</code></pre></div>
<h3 id="the-dataset-amazon-esci">The Dataset: Amazon ESCI</h3>
<p>The demo uses the <strong>Amazon Shopping Queries Dataset</strong> (ESCI) from <a href="https://github.com/amazon-science/esci-data">KDD Cup 2022</a> ‚Äî a real product search benchmark with four-level graded relevance labels:</p>
<table>
<thead>
<tr>
<th>Label</th>
<th>Gain</th>
<th>Meaning</th>
<th>Example (Query: "wireless headphones")</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Exact (E)</strong></td>
<td>3</td>
<td>Satisfies all query requirements</td>
<td>Sony WH-1000XM5 Wireless Headphones</td>
</tr>
<tr>
<td><strong>Substitute (S)</strong></td>
<td>2</td>
<td>Functional alternative</td>
<td>Wired headphones with Bluetooth adapter</td>
</tr>
<tr>
<td><strong>Complement (C)</strong></td>
<td>1</td>
<td>Related useful item</td>
<td>Headphone carrying case</td>
</tr>
<tr>
<td><strong>Irrelevant (I)</strong></td>
<td>0</td>
<td>No meaningful relationship</td>
<td>USB charging cable</td>
</tr>
</tbody>
</table>
<p>Graded relevance matters because it lets us use <strong>NDCG</strong> (Normalized Discounted Cumulative Gain), which distinguishes between a "perfect" ranking and a "merely adequate" one. Binary metrics treat both as equally relevant and cannot distinguish different levels of relevance at the same position.</p>
<p>We sample ~500 "hard" queries (the <code>small_version</code> flag in ESCI) with ~8,500 products and ~12,000 judgments ‚Äî small enough to run on a laptop in minutes, large enough for statistically meaningful results.</p>
<hr />
<h2 id="retrieval-hybrid-search">Retrieval: Hybrid Search</h2>
<p>The retrieval layer is where the search session begins. Its job is singular: <strong>maximize recall</strong> ‚Äî cast the widest net possible so that nothing relevant is missed.</p>
<h3 id="bm25-the-lexical-baseline">BM25: The Lexical Baseline</h3>
<p>BM25 scores documents by term overlap with the query, with term frequency saturation and document length normalization:</p>
<div class="arithmatex">\[
\text{BM25}(q, d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{tf(t,d) \cdot (k_1 + 1)}{tf(t,d) + k_1 \cdot (1 - b + b \cdot |d|/\text{avgdl})}
\]</div>
<p>Where <a href="https://en.wikipedia.org/wiki/Okapi_BM25"><span class="arithmatex">\(\text{IDF}(t)\)</span></a> is the inverse document frequency of term <span class="arithmatex">\(t\)</span>, <span class="arithmatex">\(tf(t,d)\)</span> is the term frequency in document <span class="arithmatex">\(d\)</span>, <span class="arithmatex">\(|d|\)</span> is document length, and <span class="arithmatex">\(\text{avgdl}\)</span> is the average document length across the corpus. The two key parameters are <span class="arithmatex">\(k_1\)</span> (typically 1.2--2.0), which controls TF saturation ‚Äî how quickly repeated terms stop adding value ‚Äî and <span class="arithmatex">\(b\)</span> (typically 0.75), which controls document length normalization.</p>
<p>The implementation is straightforward ‚Äî simple whitespace tokenization with <code>rank_bm25</code>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># src/search_ranking_stack/stages/s01_bm25.py</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">rank_bm25</span><span class="w"> </span><span class="kn">import</span> <span class="n">BM25Okapi</span>

<span class="k">def</span><span class="w"> </span><span class="nf">run_bm25</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">ESCIData</span><span class="p">,</span> <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">doc_ids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">tokenized_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

    <span class="n">bm25</span> <span class="o">=</span> <span class="n">BM25Okapi</span><span class="p">(</span><span class="n">tokenized_corpus</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">query_id</span><span class="p">,</span> <span class="n">query_text</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">queries</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">bm25</span><span class="o">.</span><span class="n">get_scores</span><span class="p">(</span><span class="n">query_text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
        <span class="n">top_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">scores</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">top_k</span><span class="p">]</span>
        <span class="n">results</span><span class="p">[</span><span class="n">query_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">doc_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]:</span> <span class="nb">float</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">top_indices</span><span class="p">}</span>

    <span class="k">return</span> <span class="n">results</span>
</code></pre></div>
<p>BM25 gives us a Recall@100 of <strong>0.741</strong> ‚Äî meaning 74% of all relevant products appear somewhere in the top 100 results. Not bad for a purely lexical method, but 26% of relevant items are invisible to downstream stages.</p>
<h3 id="dense-bi-encoder-semantic-retrieval">Dense Bi-Encoder: Semantic Retrieval</h3>
<p>The bi-encoder maps queries and documents independently into a shared embedding space:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># src/search_ranking_stack/stages/s02_dense.py</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">)</span>

<span class="c1"># Encode corpus once, cache to disk</span>
<span class="n">corpus_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">doc_texts</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">normalize_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Cosine sim = dot product</span>
    <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># At query time: encode query, compute dot product</span>
<span class="n">query_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">query_texts</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">query_embeddings</span><span class="p">,</span> <span class="n">corpus_embeddings</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</code></pre></div>
<p>With normalized embeddings, cosine similarity reduces to a dot product ‚Äî a single matrix multiplication retrieves all queries at once. The 22M-parameter <code>all-MiniLM-L6-v2</code> model runs comfortably on CPU and pushes Recall@100 to <strong>0.825</strong>, an 11% improvement over BM25.</p>
<h4 id="how-bi-encoders-learn-good-representations">How Bi-Encoders Learn Good Representations</h4>
<p>Bi-encoder training typically proceeds in <strong>two phases</strong>. First, the model is pre-trained on Natural Language Inference (NLI) and Semantic Textual Similarity (STS) datasets, which teach general-purpose semantic understanding ‚Äî the model learns that "a cat sits on a mat" and "a feline rests on a rug" should have similar embeddings. Second, the model is fine-tuned on retrieval-specific data like MS MARCO, where it learns that a search query and its relevant passage should be closer together than the query and irrelevant passages.</p>
<p>The critical ingredient in the second phase is <strong>hard negative mining</strong>. Random negatives (e.g., a document about cooking paired with a query about headphones) are trivially easy to distinguish ‚Äî the model learns nothing from them. Instead, we use the current model itself to find documents it ranks highly but that are not actually relevant. The <strong><a href="https://arxiv.org/abs/2210.11773">SimANS</a></strong> (Simple Ambiguous Negatives Sampling) approach formalizes this: given a query, rank all documents with the current bi-encoder, then exclude both easy negatives (ranked too low ‚Äî the model already handles them) and potential false negatives (ranked too high ‚Äî they might actually be relevant but unlabeled). The "hard middle ground" produces the maximum learning signal.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># What a training triplet looks like after hard negative mining</span>
<span class="n">training_triplet</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;wireless noise canceling headphones&quot;</span><span class="p">,</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="s2">&quot;Sony WH-1000XM5 Wireless Noise Cancelling Headphones&quot;</span><span class="p">,</span>
    <span class="s2">&quot;negative&quot;</span><span class="p">:</span> <span class="s2">&quot;Sony headphone replacement ear pads&quot;</span><span class="p">,</span>  <span class="c1"># Hard negative: same brand, related product, but wrong intent</span>
<span class="p">}</span>
<span class="c1"># The bi-encoder must learn that &quot;ear pads&quot; is NOT what the user wants,</span>
<span class="c1"># even though it shares many tokens with the positive document.</span>
</code></pre></div>
<p>The contrastive loss function (InfoNCE) ties this together. For each query <span class="arithmatex">\(q\)</span> with positive document <span class="arithmatex">\(d^+\)</span> and a set of negative documents <span class="arithmatex">\(\{d^-_1, \ldots, d^-_n\}\)</span>:</p>
<div class="arithmatex">\[
\mathcal{L} = -\log \frac{e^{\text{sim}(q, d^+) / \tau}}{e^{\text{sim}(q, d^+) / \tau} + \sum_{i=1}^{n} e^{\text{sim}(q, d^-_i) / \tau}}
\]</div>
<p>Where <span class="arithmatex">\(\text{sim}(q, d)\)</span> is cosine similarity between query and document embeddings, and <span class="arithmatex">\(\tau\)</span> is the temperature parameter (typically 0.05--0.1) that controls the sharpness of the distribution ‚Äî lower values make the loss more sensitive to hard negatives. This is essentially a <a href="https://arxiv.org/abs/1807.03748">softmax cross-entropy</a>: push the positive pair's similarity up relative to all negatives. When <span class="arithmatex">\(\tau\)</span> is small, even slight differences in similarity produce large gradients, forcing the model to make fine-grained distinctions.</p>
<p><img alt="Bi-Encoder Training Pipeline" src="../../../../assets/2026-02-08-search-ranking-stack/bi_encoder_training_pipeline.svg" /></p>
<h4 id="serving-bi-encoder-embeddings-at-scale">Serving Bi-Encoder Embeddings at Scale</h4>
<p>The key architectural advantage of bi-encoders is the clean <strong>offline/online split</strong>. All document embeddings are computed once at index time and stored in a vector index. At query time, only the query needs a single forward pass through the encoder (~5ms), followed by an ANN search over pre-computed embeddings (~10ms). This asymmetry is what makes dense retrieval practical at scale.</p>
<p>In our demo, the math is modest: 8,500 documents <span class="arithmatex">\(\times\)</span> 384 dimensions <span class="arithmatex">\(\times\)</span> 4 bytes per float = ~13 MB of embeddings. But at production scale, the numbers become serious: 1 billion documents with 768-dimensional embeddings require ~3 TiB of storage. This is where quantization (compressing 32-bit floats to 8-bit integers), <a href="https://ieeexplore.ieee.org/document/5432202">product quantization</a> (decomposing vectors into subspaces), and SSD-backed indexes like <a href="https://proceedings.neurips.cc/paper/2019/hash/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Abstract.html">DiskANN</a> become essential. See the <a href="#vector-indexing-hnsw-vs-ivf">Vector Indexing: HNSW vs. IVF</a> section for details on index algorithms.</p>
<p><img alt="Bi-Encoder Serving Pipeline" src="../../../../assets/2026-02-08-search-ranking-stack/bi_encoder_serving_pipeline.svg" /></p>
<h3 id="why-hybrid-bm25-and-dense-have-complementary-failures">Why Hybrid: BM25 and Dense Have Complementary Failures</h3>
<p>Neither method alone is sufficient. <strong>BM25 excels</strong> when queries contain proper nouns, specific product SKUs, or error codes ‚Äî "iPhone 15 Pro Max 256GB" needs exact token matching. <strong>Dense retrieval excels</strong> when there is vocabulary mismatch ‚Äî "cheap laptop" matching "budget notebook computer" requires semantic understanding.</p>
<p>The industry solution is <strong>Hybrid Search</strong>: run both retrieval methods in parallel, then fuse the results.</p>
<h3 id="reciprocal-rank-fusion-rrf">Reciprocal Rank Fusion (RRF)</h3>
<p>The challenge with hybrid search is that BM25 and dense retrieval produce scores on entirely different scales. BM25 scores are unbounded (0 to 100+), while cosine similarity is bounded between -1 and 1. Attempting a simple linear combination requires continuous tuning.</p>
<p><img alt="Hybrid Search with RRF" src="../../../../assets/2026-02-08-search-ranking-stack/hybrid_search_rrf.svg" /></p>
<p><a href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf">Reciprocal Rank Fusion</a> (Cormack et al., 2009) solves this by discarding raw scores entirely and relying purely on rank position:</p>
<div class="arithmatex">\[
\text{RRF}(d) = \sum_{r \in \text{Rankings}} \frac{1}{k + \text{rank}(d, r)}
\]</div>
<p>Where <span class="arithmatex">\(k\)</span> is a smoothing constant (typically 60) that mitigates outlier dominance. RRF gives higher priority to items consistently ranked near the top across both methods, even if one system scores it much higher than the other. This rank-based approach eliminates the scale mismatch entirely.</p>
<p>The implementation:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># src/search_ranking_stack/stages/s03_hybrid_rrf.py</span>

<span class="k">def</span><span class="w"> </span><span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">ranked_lists</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">fused_results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">query_id</span> <span class="ow">in</span> <span class="n">all_query_ids</span><span class="p">:</span>
        <span class="n">rrf_scores</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">results</span> <span class="ow">in</span> <span class="n">ranked_lists</span><span class="p">:</span>
            <span class="n">sorted_docs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">query_id</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
                                 <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="p">(</span><span class="n">doc_id</span><span class="p">,</span> <span class="n">_score</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_docs</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">rrf_scores</span><span class="p">[</span><span class="n">doc_id</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="n">rank</span><span class="p">)</span>

        <span class="n">sorted_rrf</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">rrf_scores</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span>
                            <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="n">top_k</span><span class="p">]</span>
        <span class="n">fused_results</span><span class="p">[</span><span class="n">query_id</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">sorted_rrf</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused_results</span>
</code></pre></div>
<p>Hybrid RRF achieves <strong>Recall@100 of 0.842</strong> and <strong>NDCG@10 of 0.628</strong> ‚Äî outperforming both BM25 (0.585) and Dense (0.611) individually. Documents only need to rank well in <em>one</em> method to survive fusion.</p>
<hr />
<h2 id="cross-encoder-reranking">Cross-Encoder Reranking</h2>
<p>With 100 hybrid candidates per query, we can afford to apply a more expensive model. The cross-encoder processes the query and document <strong>together</strong> through a single Transformer, with full cross-attention between all tokens.</p>
<p><img alt="Bi-Encoder vs. Cross-Encoder" src="../../../../assets/2026-02-08-search-ranking-stack/bi_encoder_vs_cross_encoder.svg" /></p>
<h3 id="why-cross-attention-matters">Why Cross-Attention Matters</h3>
<p>The fundamental difference is in the <strong>attention matrix</strong>. In a bi-encoder, attention is block-diagonal: query tokens only attend to other query tokens, and document tokens only attend to other document tokens. The two representations never interact at the token level ‚Äî they meet only at the very end through a dot product. A cross-encoder computes the <strong>full attention matrix</strong>, where every query token attends to every document token (and vice versa). This cross-attention is what enables deep token-level interaction.</p>
<p><img alt="Cross-Encoder Attention Architecture" src="../../../../assets/2026-02-08-search-ranking-stack/cross_encoder_architecture.svg" /></p>
<p>Consider why this matters in practice. In a bi-encoder, the query "apple" produces the same embedding every time ‚Äî it is encoded independently, before any document is seen. A cross-encoder sees query and document simultaneously, resolving ambiguity in context. But the advantages go well beyond polysemy:</p>
<ul>
<li><strong>Negation</strong>: A query for "headphones that are <em>not</em> wireless" ‚Äî bi-encoder embeddings for "not wireless" are nearly identical to "wireless" because the negation barely shifts the mean-pooled vector. A cross-encoder sees the "not" token directly attending to "wireless" and correctly scores wired headphones higher.</li>
<li><strong>Qualification</strong>: A query for "laptop under $500" ‚Äî the price constraint modifies relevance. A cross-encoder can attend from "$500" to the price mentioned in the product description and assess whether the constraint is satisfied.</li>
</ul>
<p>The cross-encoder input is formatted as <code>[CLS] query tokens [SEP] document tokens [SEP]</code>, where <code>[CLS]</code> is a special classification token whose final hidden state is fed through a linear classification head to produce a single relevance score. Segment embeddings distinguish which tokens belong to the query vs. the document, and <code>[SEP]</code> marks the boundary between segments.</p>
<h3 id="how-cross-encoders-are-trained">How Cross-Encoders Are Trained</h3>
<p>Cross-encoder training is conceptually simpler than bi-encoder training. The model receives (query, document, relevance_label) triples and learns to predict the label through straightforward supervised learning ‚Äî no contrastive loss needed.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Cross-encoder training data format</span>
<span class="n">training_example</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;wireless headphones&quot;</span><span class="p">,</span>
    <span class="s2">&quot;document&quot;</span><span class="p">:</span> <span class="s2">&quot;Sony WH-1000XM5 Wireless Headphones&quot;</span><span class="p">,</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># Relevant</span>
<span class="p">}</span>
<span class="c1"># Forward pass: [CLS] hidden state ‚Üí Linear layer ‚Üí sigmoid ‚Üí score</span>
<span class="c1"># Loss: binary cross-entropy between predicted score and label</span>
</code></pre></div>
<p>The classification head sits on top of the <code>[CLS]</code> token's final hidden state: a single linear layer maps the hidden dimension to a scalar, followed by sigmoid activation. For binary relevance labels, binary cross-entropy loss is used; for graded labels (like ESCI's four-level scale), MSE loss works better because it preserves the ordinal relationship between grades.</p>
<p><strong><a href="https://link.springer.com/chapter/10.1007/978-3-030-99736-6_44">Hard negative mining is even more critical for cross-encoders</a> than for bi-encoders.</strong> Because cross-encoders are expensive to train ‚Äî each training example requires a full forward pass through the concatenated sequence ‚Äî you cannot afford to waste compute on trivially easy negatives. The practical approach: use a bi-encoder to retrieve the top-K candidates for each training query, then select hard negatives from specific rank ranges (e.g., ranks 10-100). This gives the cross-encoder training examples where the distinction between relevant and irrelevant actually requires deep token interaction to resolve.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># src/search_ranking_stack/stages/s04_cross_encoder.py</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">CrossEncoder</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CrossEncoder</span><span class="p">(</span><span class="s2">&quot;cross-encoder/ms-marco-MiniLM-L-12-v2&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">run_cross_encoder</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hybrid_results</span><span class="p">,</span> <span class="n">top_k_rerank</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">query_id</span><span class="p">,</span> <span class="n">query_text</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">queries</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hybrid_results</span><span class="p">[</span><span class="n">query_id</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">())[:</span><span class="n">top_k_rerank</span><span class="p">]</span>

        <span class="c1"># Form (query, document) pairs for joint encoding</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">doc_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">doc_id</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
            <span class="n">doc_text</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">doc_id</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)[:</span><span class="mi">2048</span><span class="p">]</span>
            <span class="n">pairs</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">query_text</span><span class="p">,</span> <span class="n">doc_text</span><span class="p">])</span>
            <span class="n">doc_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span>

        <span class="c1"># Score all pairs with full cross-attention</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pairs</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

        <span class="c1"># Rerank by cross-encoder score</span>
        <span class="n">scored_docs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">doc_ids</span><span class="p">,</span> <span class="n">scores</span><span class="p">),</span>
                             <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">reranked_results</span><span class="p">[</span><span class="n">query_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">doc_id</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">score</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc_id</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scored_docs</span>
        <span class="p">}</span>
</code></pre></div>
<p>The 33M-parameter <code>ms-marco-MiniLM-L-12-v2</code> model averages about 100ms per query on CPU for 50 candidates. NDCG@10 jumps to <strong>0.645</strong> ‚Äî a solid +0.017 improvement over hybrid retrieval.</p>
<h3 id="the-speed-quality-tradeoff">The Speed-Quality Tradeoff</h3>
<p>Why can't we just use cross-encoders for everything? The fundamental issue is that <strong>pre-computation is impossible</strong>. Unlike bi-encoders, where each document embedding is query-independent and can be computed once and stored, a cross-encoder's output depends on both the query <em>and</em> the document together. The relevance score for "wireless headphones" paired with a Sony product is computed from the full cross-attention between those specific tokens ‚Äî it cannot be cached or reused for a different query.</p>
<p>The computational cost difference is stark. A bi-encoder retrieval requires 1 forward pass (to encode the query) plus N dot products (to compare against pre-computed document embeddings) ‚Äî the dot products are trivially cheap. A cross-encoder requires N full Transformer forward passes, each processing the concatenated query + document sequence at <span class="arithmatex">\(O(L^2)\)</span> cost for combined sequence length <span class="arithmatex">\(L\)</span>. For 50 candidates with an average combined length of 128 tokens, that is 50 separate forward passes through 12 Transformer layers. For 100,000 candidates, it becomes computationally catastrophic (on the order of minutes on a modern GPU vs. ~17ms for a bi-encoder).</p>
<p>Note the fundamental rule confirmed by our results: <strong>Recall@100 stays flat at 0.842 through both reranking stages.</strong> Reranking can reorder results but never add documents. The retrieval stage sets the ceiling.</p>
<hr />
<h2 id="llm-listwise-reranking">LLM Listwise Reranking</h2>
<p>The final stage uses an LLM to perform <strong>listwise</strong> reranking. Instead of scoring each document independently (pointwise), the LLM sees all top-10 results at once and outputs a complete ranking. This approach, inspired by <a href="https://arxiv.org/abs/2304.09542">RankGPT</a>, enables the model to compare products against each other ‚Äî something pointwise models cannot do.</p>
<p><img alt="LLM Reranking Approaches" src="../../../../assets/2026-02-08-search-ranking-stack/llm_reranking_approaches.svg" /></p>
<h3 id="the-listwise-prompt">The Listwise Prompt</h3>
<p>The prompt template asks the LLM to consider the ESCI relevance hierarchy:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># src/search_ranking_stack/stages/s05_llm_rerank.py</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_create_listwise_prompt</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">max_words</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

    <span class="n">doc_texts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">doc_id</span><span class="p">,</span> <span class="n">doc_text</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">doc_text</span><span class="o">.</span><span class="n">split</span><span class="p">()[:</span><span class="n">max_words</span><span class="p">]</span>
        <span class="n">doc_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">] </span><span class="si">{</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;I will provide you with </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> product listings, each indicated by &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;a numerical identifier [1] to [</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">]. Rank the products based on &quot;</span>
        <span class="sa">f</span><span class="s1">&#39;their relevance to the search query: &quot;</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s1">&quot;</span><span class="se">\n\n</span><span class="s1">&#39;</span>
        <span class="s2">&quot;Consider:</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;- Exact matches should rank highest</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;- Substitutes should rank above complements</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;- Irrelevant products should rank lowest</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">chr</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc_texts</span><span class="p">)</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Output ONLY a comma-separated list of identifiers: [3], [1], [2], ...</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;Do not explain your reasoning.&quot;</span>
    <span class="p">)</span>
</code></pre></div>
<h3 id="three-execution-modes">Three Execution Modes</h3>
<p>The demo supports three backends for LLM reranking:</p>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Model</th>
<th>How It Runs</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ollama</code></td>
<td><code>llama3.2:3b</code> (configurable)</td>
<td>Local via Ollama API</td>
</tr>
<tr>
<td><code>api</code></td>
<td><code>claude-haiku-4-5-20251001</code></td>
<td>Anthropic API</td>
</tr>
<tr>
<td><code>local</code></td>
<td><code>Qwen/Qwen2.5-1.5B-Instruct</code></td>
<td>HuggingFace Transformers</td>
</tr>
</tbody>
</table>
<h3 id="parsing-and-fallback">Parsing and Fallback</h3>
<p>LLM outputs are non-deterministic, so robust parsing and fallback logic are essential:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_parse_ranking</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parse LLM output to extract ranking order.&quot;&quot;&quot;</span>
    <span class="n">matches</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\[(\d+)\]&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">matches</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">positions</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">]</span>

    <span class="c1"># Pad with remaining positions if LLM returned partial output</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen</span><span class="p">:</span>
                <span class="n">positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">positions</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
</code></pre></div>
<p>If parsing fails entirely, the system falls back to the cross-encoder ordering. This fallback-first design is essential for any production LLM integration ‚Äî you should never let a parsing failure degrade results below the previous stage.</p>
<hr />
<h2 id="results-every-stage-earns-its-keep">Results: Every Stage Earns Its Keep</h2>
<p>Here are the results from running the full pipeline on ~500 ESCI queries:</p>
<p><img alt="Pipeline Results" src="../../../../assets/2026-02-08-search-ranking-stack/pipeline_results.svg" /></p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>NDCG@10</th>
<th>MRR@10</th>
<th>Recall@100</th>
<th>NDCG Delta</th>
</tr>
</thead>
<tbody>
<tr>
<td>BM25</td>
<td>0.585</td>
<td>0.812</td>
<td>0.741</td>
<td>--</td>
</tr>
<tr>
<td>Dense Bi-Encoder</td>
<td>0.611</td>
<td>0.808</td>
<td>0.825</td>
<td>+0.026</td>
</tr>
<tr>
<td>Hybrid (RRF)</td>
<td>0.628</td>
<td>0.834</td>
<td>0.842</td>
<td>+0.017</td>
</tr>
<tr>
<td>+ Cross-Encoder</td>
<td>0.645</td>
<td>0.860</td>
<td>0.842</td>
<td>+0.017</td>
</tr>
<tr>
<td>+ LLM Reranker</td>
<td>0.717</td>
<td>0.901</td>
<td>0.842</td>
<td>+0.072</td>
</tr>
</tbody>
</table>
<h3 id="key-observations">Key Observations</h3>
<p><strong>Hybrid search outperforms either method alone.</strong> RRF NDCG (0.628) exceeds the maximum of BM25 (0.585) and Dense (0.611). This is the core thesis of modern search: sparse and dense retrieval have complementary failure modes, and combining them recovers documents that would be missed by either method individually.</p>
<p><strong>Recall is set at retrieval.</strong> Recall@100 stays flat at 0.842 through both reranking stages. This confirms the fundamental rule: rerankers can reorder but never add documents. If you want higher recall, you must improve the retrieval layer.</p>
<p><strong>The LLM reranker provides the largest single jump.</strong> The +0.072 NDCG@10 gain from cross-encoder to LLM reranker is the biggest single-stage improvement in the pipeline. The LLM's ability to reason about product relevance ‚Äî understanding that a "wireless headphone stand" is a complement, not a match ‚Äî provides discrimination that statistical models miss.</p>
<p><strong>Dense beats BM25 on this dataset.</strong> ESCI's product search domain has severe vocabulary mismatch (users say "cheap laptop"; products say "budget notebook computer"), which favors semantic retrieval.</p>
<hr />
<h2 id="evaluation-measuring-what-matters">Evaluation: Measuring What Matters</h2>
<p>The demo uses three complementary metrics that together tell the full story:</p>
<h3 id="ndcg10-primary-metric">NDCG@10 (Primary Metric)</h3>
<p><strong>Normalized Discounted Cumulative Gain</strong> measures the quality of the top-10 ranking using graded relevance. It rewards placing highly relevant documents near the top with a logarithmic discount:</p>
<div class="arithmatex">\[
\text{DCG@k} = \sum_{i=1}^{k} \frac{2^{rel_i} - 1}{\log_2(i + 1)} \qquad \text{NDCG@k} = \frac{\text{DCG@k}}{\text{IDCG@k}}
\]</div>
<p>NDCG is the only metric that fully exploits ESCI's four-level graded relevance ‚Äî a system that places an Exact match at position 1 scores higher than one that places a Substitute there. This is why it is the primary metric for holistic search quality.</p>
<h3 id="mrr10-user-experience">MRR@10 (User Experience)</h3>
<p><strong>Mean Reciprocal Rank</strong> measures how quickly the user finds the first relevant result. If the first relevant result is at position 1, MRR = 1.0. At position 3, the reciprocal rank is 0.333. This captures "time to satisfaction" ‚Äî even if NDCG is high, users care most about the first good result.</p>
<h3 id="recall100-retrieval-coverage">Recall@100 (Retrieval Coverage)</h3>
<p>Recall measures what fraction of all relevant documents appear somewhere in the top-100. This is a ceiling metric ‚Äî if a relevant document is not retrieved, no reranker can fix it.</p>
<hr />
<h2 id="vector-indexing-hnsw-vs-ivf">Vector Indexing: HNSW vs. IVF</h2>
<p>The practical utility of dense embeddings depends on Approximate Nearest Neighbor (ANN) indexing. The demo uses brute-force cosine similarity (practical at ~8,500 documents), but production systems need specialized indexes.</p>
<h3 id="hnsw-hierarchical-navigable-small-world">HNSW (Hierarchical Navigable Small World)</h3>
<p><a href="https://arxiv.org/abs/1603.09320">HNSW</a> (Malkov &amp; Yashunin, 2016) is the gold standard for production environments requiring sub-100ms latency. It builds a multi-layered graph where upper layers provide "express" connections for coarse navigation and lower layers provide dense connections for precise refinement. Key tuning parameters are <code>M</code> (connections per node, typically 16-64) and <code>efSearch</code> (query-time beam width ‚Äî use at least 512 for recall targets above 0.95).</p>
<p>HNSW's weakness is the <strong>"Tombstone Problem"</strong>: when records are deleted, they leave phantom nodes in the graph. Over time, these create unreachable regions, effectively blinding your search engine to sections of data. This is not a theoretical concern ‚Äî even modern vector databases like Qdrant, which uses HNSW exclusively, <a href="https://github.com/qdrant/qdrant/issues/7147">report degraded search quality after heavy deletions</a> that requires full index rebuilds to resolve. If your dataset has frequent updates or deletions, plan for periodic reindexing or consider IVF-based alternatives.</p>
<h3 id="ivf-inverted-file">IVF (Inverted File)</h3>
<p>IVF indexes partition the vector space into Voronoi cells using k-means clustering. At query time, only the <code>nprobe</code> clusters closest to the query centroid are scanned. IVF is more memory-efficient and resilient to dynamic datasets ‚Äî deletions are clean, with no unreachable nodes. Build times are 4x-32x faster than HNSW.</p>
<p>For extreme scale, <strong><a href="https://arxiv.org/abs/2405.12497">IVF_RaBitQ</a></strong> (Gao &amp; Long, SIGMOD 2024) compresses floating-point vectors into single-bit representations. In high-dimensional space, a coordinate's sign (+/-) captures sufficient angular information for similarity computation.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>HNSW (Graph)</th>
<th>IVF (Cluster)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Query Speed</strong></td>
<td>Exceptional</td>
<td>Moderate</td>
</tr>
<tr>
<td><strong>Build Speed</strong></td>
<td>Slow</td>
<td>Fast (4x-32x faster)</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>High (RAM-bound)</td>
<td>Low</td>
</tr>
<tr>
<td><strong>Deletions</strong></td>
<td>Problematic (tombstones)</td>
<td>Clean</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Static, latency-critical</td>
<td>Dynamic, memory-constrained</td>
</tr>
</tbody>
</table>
<p><strong>Practical guidance from <a href="https://www.uber.com/blog/evolution-and-scale-of-ubers-delivery-search-platform/">Uber</a>:</strong> They optimized ANN retrieval by reducing the shard-level search parameter K from 1,200 to 200, achieving a 34% latency reduction and 17% CPU savings with minimal impact on recall.</p>
<hr />
<h2 id="the-llm-layer-beyond-reranking">The LLM Layer: Beyond Reranking</h2>
<p>LLMs are not just improving the reranking stage ‚Äî they are transforming the entire search pipeline.</p>
<h3 id="query-understanding">Query Understanding</h3>
<p>LLM-powered query expansion and rewriting addresses vocabulary mismatch before retrieval even begins. <a href="https://arxiv.org/abs/2303.07678">Query2doc</a> (Wang et al., EMNLP 2023) generates pseudo-documents via few-shot LLM prompting and concatenates them with original queries, yielding <strong>3-15% BM25 improvement</strong> on MS MARCO without any fine-tuning. The LLM "fills in" vocabulary that the user's terse query omits.</p>
<p>Practical patterns include abbreviation expansion, entity enrichment, sub-query decomposition for multi-hop reasoning, and <strong><a href="https://arxiv.org/abs/2402.03367">RAG-Fusion</a></strong> ‚Äî generating multiple query variants and combining results via RRF.</p>
<h3 id="llm-as-a-judge-for-evaluation">LLM-as-a-Judge for Evaluation</h3>
<p>LLMs are becoming the standard for search quality assessment. The <a href="https://arxiv.org/abs/2305.13233">TALEC</a> framework achieves <strong>over 80% correlation with human judgments</strong> using domain-specific evaluation criteria. <a href="https://arxiv.org/abs/2410.17152">Pinterest's approach</a> is instructive: Llama-3-8B serves as an offline teacher model that generates five-scale relevance labels on billions of search impressions, outperforming multilingual BERT-base by 12.5% in accuracy; these labels are then distilled into lightweight production models.</p>
<p>Key techniques for reliable LLM evaluation:</p>
<ul>
<li>Prompt models to <strong>explain their ratings</strong> (significantly improves human alignment)</li>
<li>Use <strong>panels of diverse models</strong> to reduce variability ("<a href="https://arxiv.org/abs/2404.18796">replacing judges with juries</a>")</li>
<li>Account for <strong>central tendency bias</strong> in LLM-generated labels</li>
</ul>
<h3 id="knowledge-distillation">Knowledge Distillation</h3>
<p>Running a full LLM for every query is prohibitively expensive. The solution is knowledge distillation:</p>
<ol>
<li>Use a powerful LLM (the teacher) to rerank thousands of training queries</li>
<li>Train a small, fast cross-encoder (the student, ~100M-200M parameters) to mimic the LLM's ranking distribution</li>
<li>Result: near-LLM performance with ~10ms latency</li>
</ol>
<p><a href="https://arxiv.org/abs/2401.06910">InRanker</a> distills MonoT5-3B into 60M and 220M parameter models ‚Äî a 50x size reduction with competitive performance. The <a href="https://arxiv.org/abs/2312.02969">Rank-Without-GPT</a> approach produces 7B open-source listwise rerankers that achieve 97% of GPT-4 effectiveness using QLoRA fine-tuning.</p>
<p>A cost optimization insight from <a href="https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025">ZeroEntropy</a>: reranking 75 candidates and sending only the top 20 to GPT-4o reduces API costs by <strong>72%</strong> ‚Äî from $162K/day to $44K/day at 10 QPS ‚Äî while preserving 95% of answer accuracy.</p>
<hr />
<h2 id="personalization-who-is-searching-matters">Personalization: Who Is Searching Matters</h2>
<p>Generic relevance is no longer enough. A search for "apple" should return iPhones for a tech enthusiast and apple recipes for someone who has been browsing cooking content.</p>
<h3 id="two-tower-models-for-personalization">Two-Tower Models for Personalization</h3>
<p>The dominant retrieval architecture for personalization uses a <strong>two-tower embedding model</strong>: the query tower encodes search queries plus user profile into embeddings; the item tower encodes items plus metadata. Dot-product similarity determines relevance, enabling sub-100ms ANN retrieval.</p>
<p><a href="https://dl.acm.org/doi/10.1145/3219819.3219885">Airbnb</a> pioneered listing embeddings using Word2Vec-style training on click sessions ‚Äî their Search and Similar Listings channels together drive <strong>99% of booking conversions</strong>. Pinterest's <a href="https://arxiv.org/abs/2404.16260">OmniSearchSage</a> (WWW 2024) jointly learns unified query, pin, and product embeddings, producing <strong>&gt;8% relevance improvement</strong> at 300K requests/second. <a href="https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/">Uber's Two-Tower Embeddings</a> power Eats Homefeed retrieval in ~100ms.</p>
<h3 id="position-bias-the-silent-distortion">Position Bias: The Silent Distortion</h3>
<p>Users click higher-ranked items regardless of true relevance, creating a self-reinforcing feedback loop. The most practical production approach (<a href="https://dl.acm.org/doi/10.1145/3298689.3347033">PAL</a>, Guo et al., RecSys 2019): include position as a training feature, then set position=1 for all items at serving time. This effectively debiases the model without requiring knowledge of the click model.</p>
<hr />
<h2 id="bridging-the-domain-gap">Bridging the Domain Gap</h2>
<p>A common failure in search strategy is assuming a model trained on general web data (like MS MARCO) will perform on specialized domains. This is the <strong>Out-of-Domain (OOD) problem</strong>.</p>
<h3 id="synthetic-data-generation">Synthetic Data Generation</h3>
<p>LLMs solve the labeled data scarcity problem through <strong>Generative Pseudo-Labeling</strong> (<a href="https://arxiv.org/abs/2112.07577">GPL</a>, <a href="https://arxiv.org/abs/2202.05144">InPars</a>):</p>
<ol>
<li>Take your domain-specific document corpus</li>
<li>Prompt an LLM to "Generate a search query that this document would answer"</li>
<li>Use these synthetic (Query, Document) pairs to fine-tune your retriever and reranker</li>
</ol>
<p>This technique has shown dramatic improvements on domain-specific tasks where real user queries are scarce. It is the practical bridge between Level 2 and Level 3 on the maturity path.</p>
<h3 id="rmsc-soft-tokens-for-domain-adaptation">RMSC: Soft Tokens for Domain Adaptation</h3>
<p>The <a href="https://arxiv.org/abs/2309.09828">RMSC</a> (Robust Multi-Supervision Combining) strategy introduces <strong>soft tokens</strong> ‚Äî special domain tokens <code>[S1]</code>, <code>[T1]</code> and relevance tokens <code>[H1]</code>, <code>[W1]</code> ‚Äî that explicitly signal to the model what domain it is processing and how confident the supervision signal is. By training with these tokens, the model stores domain-specific knowledge in the token embeddings rather than overwriting core backbone parameters.</p>
<hr />
<h2 id="the-practical-maturity-path">The Practical Maturity Path</h2>
<p>If you are building this stack today, do not start with the most complex architecture. Follow this maturity curve:</p>
<p><img alt="Practical Maturity Path" src="../../../../assets/2026-02-08-search-ranking-stack/maturity_path.svg" /></p>
<p><strong>Level 1 (Baseline):</strong> Postgres pgvector or Elasticsearch. Hybrid search with BM25 + vector retrieval. No reranker.</p>
<p><strong>Level 2 (The Reranker):</strong> Add a cross-encoder (e.g., <code>bge-reranker-v2-m3</code> or <code>ms-marco-MiniLM-L-12-v2</code>) to rerank the top 50 results. This typically yields the <strong>biggest ROI for the least effort</strong>. The <a href="https://www.elastic.co/search-labs/blog/elastic-semantic-reranker-part-2">Elastic Rerank</a> model (184M parameters, DeBERTa v3) reaches 0.565 average nDCG@10 on BEIR ‚Äî a 39% improvement over BM25.</p>
<p><strong>Level 3 (Fine-tuning):</strong> Fine-tune your embedding model and reranker on domain data using LLM-generated synthetic queries (GPL/InPars). This is where domain-specific performance really separates from generic models.</p>
<p><strong>Level 4 (State of the Art):</strong> Implement listwise LLM reranking for the top 5-10 results and inject personalization signals. Experiment with reasoning-based rerankers like <a href="https://arxiv.org/abs/2502.18418">Rank1</a>, which generates explicit reasoning chains before making relevance judgments.</p>
<p><strong>Level 2 is the sweet spot for most teams.</strong> Adding a cross-encoder reranker to an existing hybrid search setup can dramatically improve precision without architectural overhaul.</p>
<hr />
<h2 id="the-frontier-reasoning-rerankers-and-agentic-search">The Frontier: Reasoning Rerankers and Agentic Search</h2>
<p>Two patterns define where search is heading.</p>
<h3 id="reasoning-based-rerankers">Reasoning-Based Rerankers</h3>
<p><a href="https://arxiv.org/abs/2502.18418">Rank1</a> trains reranking models to generate explicit reasoning chains before making relevance judgments, inspired by DeepSeek-R1 and OpenAI o1. It distills from <strong>600,000+ reasoning trace examples</strong> and achieves state-of-the-art on the <a href="https://arxiv.org/abs/2407.12883">BRIGHT</a> reasoning benchmark ‚Äî sometimes <strong>2x improvement</strong> over same-sized rerankers. Remarkably, Rank1-0.5B performs comparably to RankLLaMA-13B despite being 25x smaller.</p>
<p>The practical implication: reasoning-heavy queries (legal research, scientific literature, complex product search) will benefit enormously from test-time compute scaling in rerankers.</p>
<h3 id="agentic-search">Agentic Search</h3>
<p><a href="https://arxiv.org/abs/2501.05366">Search-o1</a> (EMNLP 2025) enables reasoning models (specifically QwQ-32B) to autonomously generate search queries when they encounter uncertain knowledge mid-reasoning, achieving <strong>23.2% average exact match improvement</strong> over standard RAG on multi-hop QA benchmarks. The pattern is clear: search is becoming a tool that AI agents invoke dynamically rather than a standalone product.</p>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<ol>
<li>
<p><strong>Hybrid search is no longer optional.</strong> The empirical evidence across benchmarks and production systems is definitive. Every major vector database now supports it natively. Our demo shows RRF NDCG (0.628) exceeding both BM25 (0.585) and Dense (0.611).</p>
</li>
<li>
<p><strong>Retrieval sets the ceiling; reranking optimizes within it.</strong> Recall@100 stays flat at 0.842 through both reranking stages. Invest in retrieval quality first.</p>
</li>
<li>
<p><strong>Adding a cross-encoder is the highest-ROI single change for most teams.</strong> Even a small cross-encoder reranking 50 documents delivers significant NDCG uplift. Start here.</p>
</li>
<li>
<p><strong>LLM listwise reranking provides the largest single quality jump</strong> (+0.072 NDCG@10 in our demo), but at the cost of latency and compute. Use it selectively for the final top-10.</p>
</li>
<li>
<p><strong>Knowledge distillation is making LLM-quality reranking practical.</strong> Frontier model capabilities are being compressed into deployable sizes within months. A 7B model can achieve 97% of GPT-4 reranking effectiveness.</p>
</li>
<li>
<p><strong>The stack, not the model, determines production quality.</strong> Optimize the pipeline ‚Äî the interplay between retrieval, fusion, and reranking ‚Äî not just any single component.</p>
</li>
</ol>
<p>The complete pipeline code is available at <a href="https://github.com/slavadubrov/search-ranking-stack">github.com/slavadubrov/search-ranking-stack</a>. Clone it, run it, experiment with different models and parameters, and see the numbers for yourself.</p>
<hr />
<h2 id="references">References</h2>
<h3 id="papers">Papers</h3>
<ul>
<li><a href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf">Reciprocal Rank Fusion</a> ‚Äî Cormack et al., 2009</li>
<li><a href="https://arxiv.org/abs/2304.09542">RankGPT: LLMs as Zero-Shot Listwise Rerankers</a> ‚Äî Sun et al., EMNLP 2023 Outstanding Paper</li>
<li><a href="https://arxiv.org/abs/2502.18418">Rank1: Reasoning-Based Reranking</a> ‚Äî Weller et al., COLM 2025</li>
<li><a href="https://arxiv.org/abs/2405.04141">SCaLR: Self-Calibrated Listwise Reranking</a> ‚Äî Self-calibrating listwise reranking framework</li>
<li><a href="https://arxiv.org/abs/2402.12871">GCCP: Global-Consistent Comparative Pointwise</a> ‚Äî Addressing calibration in pointwise LLM ranking</li>
<li><a href="https://arxiv.org/abs/2405.07920">Rank-DistiLLM: Knowledge Distillation for Reranking</a> ‚Äî Schlatt et al., ECIR 2025</li>
<li><a href="https://arxiv.org/abs/2303.07678">Query2doc: LLM Query Expansion</a> ‚Äî Wang et al., EMNLP 2023</li>
<li><a href="https://arxiv.org/abs/2112.07577">GPL: Generative Pseudo Labeling</a> ‚Äî Domain adaptation for dense retrieval</li>
<li><a href="https://arxiv.org/abs/2401.06910">InRanker: Distilled Reranker</a> ‚Äî 50x size reduction with competitive performance</li>
<li><a href="https://arxiv.org/abs/2501.05366">Search-o1: Agentic Retrieval</a> ‚Äî EMNLP 2025</li>
<li><a href="https://arxiv.org/abs/2305.13233">TALEC: LLM-as-a-Judge for Search</a> ‚Äî Evaluation framework</li>
<li><a href="https://arxiv.org/abs/2309.09828">RMSC: Soft Tokens for Domain Adaptation</a> ‚Äî Multi-supervision combining</li>
<li><a href="https://arxiv.org/abs/2104.08663">BEIR Benchmark</a> ‚Äî Thakur et al., NeurIPS 2021</li>
<li><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT</a> ‚Äî Reimers &amp; Gurevych, EMNLP 2019</li>
<li><a href="https://arxiv.org/abs/1807.03748">InfoNCE / CPC</a> ‚Äî van den Oord et al., 2018</li>
<li><a href="https://arxiv.org/abs/2210.11773">SimANS: Hard Negative Sampling</a> ‚Äî Zhou et al., EMNLP 2022</li>
<li><a href="https://arxiv.org/abs/1901.04085">Passage Reranking with BERT</a> ‚Äî Nogueira &amp; Cho, 2019</li>
<li><a href="https://arxiv.org/abs/1603.09320">HNSW</a> ‚Äî Malkov &amp; Yashunin, 2016</li>
<li><a href="https://proceedings.neurips.cc/paper/2019/hash/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Abstract.html">DiskANN</a> ‚Äî Subramanya et al., NeurIPS 2019</li>
<li><a href="https://arxiv.org/abs/2405.12497">RaBitQ</a> ‚Äî Gao &amp; Long, SIGMOD 2024</li>
<li><a href="https://arxiv.org/abs/2404.18796">Replacing Judges with Juries</a> ‚Äî Verga et al., 2024</li>
<li><a href="https://arxiv.org/abs/2402.03367">RAG-Fusion</a> ‚Äî Rackauckas, 2024</li>
<li><a href="https://arxiv.org/abs/2202.05144">InPars</a> ‚Äî Bonifacio et al., SIGIR 2022</li>
<li><a href="https://arxiv.org/abs/2407.12883">BRIGHT Benchmark</a> ‚Äî Su et al., ICLR 2025</li>
<li><a href="https://arxiv.org/abs/2312.02969">Rank-without-GPT</a> ‚Äî Zhang et al., ECIR 2025</li>
<li><a href="https://arxiv.org/abs/2410.17152">Pinterest LLM Search Relevance</a> ‚Äî Wang et al., 2024</li>
<li><a href="https://arxiv.org/abs/2404.16260">OmniSearchSage</a> ‚Äî Agarwal et al., WWW 2024</li>
<li><a href="https://dl.acm.org/doi/10.1145/3298689.3347033">PAL: Position-bias Aware Learning</a> ‚Äî Guo et al., RecSys 2019</li>
</ul>
<h3 id="datasets-and-benchmarks">Datasets and Benchmarks</h3>
<ul>
<li><a href="https://github.com/amazon-science/esci-data">Amazon ESCI: Shopping Queries Dataset</a> ‚Äî KDD Cup 2022</li>
<li><a href="https://github.com/beir-cellar/beir">BEIR: Benchmarking IR</a> ‚Äî Heterogeneous benchmark for zero-shot evaluation</li>
<li><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB: Massive Text Embedding Benchmark</a> ‚Äî Embedding model leaderboard</li>
<li><a href="https://arxiv.org/abs/2206.06588">ESCI Paper</a> ‚Äî Reddy et al., 2022</li>
</ul>
<h3 id="models-used-in-the-demo">Models Used in the Demo</h3>
<ul>
<li><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a> ‚Äî 22M parameter bi-encoder</li>
<li><a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2">ms-marco-MiniLM-L-12-v2</a> ‚Äî 33M parameter cross-encoder</li>
<li><a href="https://www.sbert.net/">Sentence-Transformers</a> ‚Äî Neural retrieval model framework</li>
</ul>
<h3 id="tools-and-platforms">Tools and Platforms</h3>
<ul>
<li><a href="https://github.com/dorianbrown/rank_bm25">rank_bm25</a> ‚Äî BM25 implementation in Python</li>
<li><a href="https://github.com/cvangysel/pytrec_eval">pytrec_eval</a> ‚Äî TREC evaluation toolkit</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/retrievers.html">Elasticsearch</a> ‚Äî Hybrid search with Retrievers API</li>
<li><a href="https://vespa.ai/">Vespa</a> ‚Äî Unified search and recommendation engine</li>
<li><a href="https://weaviate.io/">Weaviate</a> ‚Äî Vector database with hybrid search</li>
<li><a href="https://qdrant.tech/">Qdrant</a> ‚Äî Vector database with multi-stage queries</li>
</ul>
<h3 id="industry-references">Industry References</h3>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3219819.3219885">Airbnb Listing Embeddings</a> ‚Äî Grbovic &amp; Cheng, KDD 2018</li>
<li><a href="https://www.uber.com/blog/evolution-and-scale-of-ubers-delivery-search-platform/">Uber Delivery Search</a> ‚Äî Uber Engineering, 2025</li>
<li><a href="https://www.uber.com/blog/innovative-recommendation-applications-using-two-tower-embeddings/">Uber Two-Tower Embeddings</a> ‚Äî Uber Engineering, 2023</li>
<li><a href="https://www.elastic.co/search-labs/blog/elastic-semantic-reranker-part-2">Elastic Rerank</a> ‚Äî Elastic, 2024</li>
<li><a href="https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025">ZeroEntropy Reranking Guide</a> ‚Äî ZeroEntropy, 2025</li>
</ul>
<h3 id="demo-project">Demo Project</h3>
<ul>
<li><a href="https://github.com/slavadubrov/search-ranking-stack">search-ranking-stack</a> ‚Äî Working demo with all code from this post</li>
</ul>







  
  




  


 
  </article>
</div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
    <a href="https://www.linkedin.com/in/slavadubrov" target="_blank" rel="noopener" title="LinkedIn" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://slavadubrov.substack.com/" target="_blank" rel="noopener" title="Substack" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22.539 8.242H1.46V5.406h21.08zM1.46 10.812V24L12 18.11 22.54 24V10.812zM22.54 0H1.46v2.836h21.08z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/slavadubrov" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
    <a href="/feed_rss_created.xml" target="_blank" rel="noopener" title="RSS" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19 7.38 20 6.18 20 5 20 4 19 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27zm0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.toc", "navigation.toc.sticky", "navigation.toc.maxdepth", "navigation.toc.title", "navigation.toc.collapse", "navigation.toc.collapse_empty_groups", "navigation.toc.collapse_single_children", "content.code.copy"], "search": "../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
      
        <script src="../../../../../javascripts/mermaid-init.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>