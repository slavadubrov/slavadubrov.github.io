{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Shared Intelligence","text":"<p>Tips &amp; war stories from the ML\u2011engineering trenches.</p> <p>\ud83d\udc4b Hi, I'm Slava!</p> <p>I'm a Berlin\u2011based Senior Machine Learning Engineer\u202fII at HubSpot (PhD) with 10+ years of experience turning cutting\u2011edge research into reliable, revenue\u2011generating products.</p> <p>I've built graph\u2011neural\u2011network fraud detection systems that save millions at Wayfair, large\u2011scale recommender systems and statistical experimentation tooling at OLX\u202fGroup, and now lead AI Signals initiatives at HubSpot.</p> <p>My professional passions include fraud\u202f&amp;\u202fscam detection, representation learning, recommender systems, LLMs\u202f&amp;\u202fAI\u202fagents, and MLOps\u2014topics you'll find explored throughout this blog.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<p>In this blog, you'll find:</p> <ul> <li>Technical tutorials and guides</li> <li>Machine Learning insights</li> <li>Best practices and tips</li> <li>Personal experiences and learnings</li> </ul> <p>My goal is to create a valuable resource for fellow practitioners and anyone interested in the real-world application of machine learning.</p>"},{"location":"#connect","title":"Connect","text":"<ul> <li>LinkedIn</li> <li>GitHub</li> </ul>"},{"location":"topics/","title":"\ud83d\udd0e Browse by Topic","text":""},{"location":"topics/#tag:deep-learning","title":"Deep Learning","text":"<ul> <li>            Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025          </li> </ul>"},{"location":"topics/#tag:distributed-training","title":"Distributed Training","text":"<ul> <li>            Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025          </li> </ul>"},{"location":"topics/#tag:gpu","title":"GPU","text":"<ul> <li>            Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025          </li> </ul>"},{"location":"topics/#tag:llm","title":"LLM","text":"<ul> <li>            Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025          </li> </ul>"},{"location":"topics/#tag:mac","title":"Mac","text":"<ul> <li>            Setting up a MacBook for AI Engineering          </li> </ul>"},{"location":"topics/#tag:parallelism","title":"Parallelism","text":"<ul> <li>            Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025          </li> </ul>"},{"location":"topics/#tag:python","title":"Python","text":"<ul> <li>            Managing Python on macOS with uv          </li> </ul>"},{"location":"topics/#tag:tooling","title":"Tooling","text":"<ul> <li>            Managing Python on macOS with uv          </li> </ul>"},{"location":"topics/#tag:tutorial","title":"Tutorial","text":"<ul> <li>            Managing Python on macOS with uv          </li> <li>            Setting up a MacBook for AI Engineering          </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/","title":"Managing Python like an AI Engineer on macOS with uv","text":"","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#tldr-bash-cheatsheet","title":"TL;DR Bash\u00a0Cheat\u2011sheet","text":"<pre><code>brew install uv        # install tool\nuv python install 3.12 # grab interpreter\nuv python pin          # lock version for repo\nuv venv                # create .venv\nuv pip install numpy pandas   # ML staples\nuv run train.py        # run with correct interpreter\nuv self upgrade        # update uv itself\n</code></pre>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#why-i-migrated-to-uv-and-you-should-too","title":"\ud83c\udf19 Why I Migrated to <code>uv</code> (And You Should Too)","text":"<p><code>uv</code> is a lightning-fast, all-in-one Python project tool written in Rust, combining package management, interpreter installation, and virtual environment creation. Key features include:</p> <ul> <li>Installing and switching between multiple CPython (and PyPy) builds</li> <li>Creating lightweight virtual environments</li> <li>Resolving dependencies with an absurdly fast pip-compatible resolver</li> <li>A <code>uvx</code> shim for running tools like Ruff or Black in isolated sandboxes:</li> <li><code>uvx black .</code> or <code>uvx ruff format .</code></li> </ul> <p>Result: fewer moving parts, faster setups, and consistent environments across laptop and CI images.</p>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#1-installing-uv","title":"1. Installing uv","text":"<pre><code># Install uv via Homebrew (Apple\u00a0Silicon &amp; Intel)\nbrew install uv\n</code></pre> <p>Note: <code>uv</code> auto-detects your architecture (Apple\u00a0Silicon or Intel).</p> <p>The same page shows a one\u2011liner curl installer if you're brew\u2011averse. Check it worked:</p> <pre><code># Check installation\nuv --version      # should print something like 0.6.x\nuv self upgrade   # keep it fresh\n</code></pre>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#2-installing-python-interpreters","title":"2. Installing Python interpreters","text":"<pre><code># Install specific Python versions\nuv python install 3.12.4          # exact version\nuv python install 3.13            # latest minor\nuv python install 3.9 3.10 3.11   # many at once\nuv python list                    # what's already cached\n</code></pre> <p>These archives live under <code>~/.cache/uv</code>, so they don't fight Homebrew or Xcode.</p> <p>Need the interpreter for this project only?</p> <pre><code># Pin Python version for the project\nuv python pin           # writes .python-version next to your code\n</code></pre> <p>Drop that file into Git and your team (or the CI) will automatically get the same binary.</p>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#3-virtual-environments-the-lazy-way","title":"3. Virtual environments the lazy way","text":"<pre><code># Create virtual environment\nuv venv                 # creates .venv with the pinned Python\nuv venv --python 3.11   # override if you're exploring\n</code></pre> <p>I rarely <code>activate</code> anymore as uv detects the <code>.venv</code> file and routes <code>uv pip</code>, <code>uv run</code>, or <code>uvx ruff</code> to the right interpreter. Pure convenience.</p> <p>A few patterns that shaved minutes from my workflow:</p> Task One\u2011liner Install deps into the current venv <code>uv pip install -r requirements.txt</code> Run a script with a different interpreter <code>uv run --python 3.10 scripts/train.py</code> Global tool in its own sandbox <code>uvx ruff format .</code>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#31-using-uvx-for-tools","title":"3.1 Using <code>uvx</code> for tools","text":"<p>With <code>uvx</code> you can run formatters or linters without touching your virtual environment:</p> <pre><code>uvx black .            # format code in an isolated sandbox\nuvx ruff check src/    # lint code without installing ruff globally\n</code></pre>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#4-coexisting-with-pyenv-if-you-must","title":"4. Co\u2011existing with pyenv (if you must)","text":"<ul> <li>Keep pyenv if you rely on its \"shim\" strategy to globally shadow <code>python</code> in your shell.</li> <li>Skip pyenv if project\u2011local versions and CI parity are your priority - uv handles that solo.</li> </ul> <p>From uv's perspective every interpreter in <code>$PATH</code> (even ones compiled by pyenv or Homebrew) is just \"system Python\". You can pass it to any <code>--python</code> flag and mix\u2011and\u2011match as needed.</p>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/17/managing-python-on-macos-with-uv/#5-mlspecific-niceties","title":"5. ML\u2011specific niceties","text":"<ul> <li>The PyTorch integration guide shows CUDA\u2011aware installs in one command - excellent for GPU vs. CPU builds on the same Mac.</li> <li>Binary wheels pulled by uv are cached, so re\u2011creating a venv to try a different version of scikit\u2011learn or TensorFlow feels instant.</li> </ul>","tags":["Tooling","Python","Tutorial"]},{"location":"blog/2025/04/19/setting-up-a-macbook-for-ai-engineering/","title":"Setting up a MacBook for AI Engineering","text":"<p>Here\u2019s my distilled, 10\u2011step workflow to transform a vanilla macOS install into a ready to-go AI engineering working station.</p> <ol> <li> <p>First of all, let's install Xcode Command Line Tools. These tools are the foundation for any type of software development (including DS).</p> <pre><code>xcode-select --install\n</code></pre> </li> <li> <p>Then install Homebrew. It is a package manager for macOS. You can follow instructions on their website or just run:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Then I install these dependencies with brew:</p> <pre><code>brew install openssl readline sqlite3 xz zlib\n</code></pre> </li> <li> <p>I used to prefer iTerm2 over the standard Terminal due to its flexible configuration, but recently migrated to Warp. Warp offers a modern, Rust-based terminal experience with AI features integrated. You can download it from the Warp website. However, if you still prefer iTerm2, here's how I used to configure it:</p> </li> <li> <p>For configuring iTerm I prefer to do the following:</p> <ul> <li> <p>Setup Natural text editing:</p> </li> <li> <p>Go to Preferences \u2192 Profiles \u2192 Keys \u2192 Key Mappings</p> </li> <li>Press Presets\u2026 dropdown button</li> <li> <p>Select Natural Text Editing</p> </li> <li> <p>For changing color select the preferred preset from this repo. Then:</p> </li> <li>Go to Preferences \u2192 Profiles \u2192 Colors \u2192 Color Presets\u2026 \u2192 Import</li> <li>After importing your new color will be displayed in Color Presets</li> </ul> </li> <li> <p>Then I install and configure Zsh and Oh My Zsh:</p> <pre><code>brew install zsh\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n</code></pre> </li> <li> <p>Now you can configure your terminal with a <code>~/.zshrc</code> file. I use the next zsh plugins in my daily routine:</p> <pre><code>plugins=(\n  git brew vscode iterm2\n  themes screen macos bgnotify\n  docker docker-compose gcloud aws\n  python pyenv pylint virtualenv\n  zsh-autosuggestions zsh-syntax-highlighting)\n</code></pre> <p>A description of the plugins you can find here.</p> <p>Only the last two plugins (zsh-autosuggestions and zsh-syntax-highlighting) require additional installation. It's pretty simple, just check the following links:</p> <ul> <li>zsh-autosuggestions: https://github.com/zsh-users/zsh-autosuggestions</li> <li>zsh-syntax-highlighting: https://github.com/zsh-users/zsh-syntax-highlighting/</li> </ul> </li> <li> <p>I'm using the Powerlevel10k theme in Zsh. It has installation assistance that helps you configure Zsh your way. Just follow the instruction on their website.</p> <p>If you have any issues with fonts in another terminal, you can install fonts separately. For example, I use VSCode and its internal terminal very often. This instruction could help you configure the VSCode terminal to work with the Powerlevel10k theme.</p> </li> <li> <p>For the terminal I also install:</p> <ul> <li>pyenv for managing global Python versions via shims,</li> <li>uv for managing Python project dependencies and virtual environments,</li> <li>htop for process monitoring,</li> <li>gitmoji for customizing commit messages.</li> </ul> </li> <li> <p>And of course, I always install Cursor as my main IDE, Docker for containerization, and Ollama for running LLMs locally.</p> </li> </ol>","tags":["Tutorial","Mac"]},{"location":"blog/2025/04/19/setting-up-a-macbook-for-ai-engineering/#final-thoughts","title":"Final thoughts","text":"<p>By now you have the exact stack I lean on every day as an AI engineer - just the essentials that remove friction between an idea and a running model.</p>","tags":["Tutorial","Mac"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/","title":"Scaling Large Language Models. Multi-GPU and Multi-Node Strategies in 2025","text":"<p>As LLMs continue to grow in complexity and size, efficient training and inference require leveraging multiple GPUs and, often, multiple systems. This guide explores prevalent strategies and tools in 2025 that facilitate such scalability, incorporating insights from Hugging Face's Ultra-Scale Playbook.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#1-parallelism-techniques","title":"1. Parallelism Techniques","text":"","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#11-data-parallelism-dp","title":"1.1 Data Parallelism (DP)","text":"<p>In classic data-parallel training every GPU keeps a full copy of the model. A large batch is split into N micro-batches; each rank runs forward + backward on its piece and then gradients are all-reduced (averaged) so that all replicas stay in sync before the optimizer step.</p> <p>Key ideas</p> <ul> <li>Simplicity first - almost zero code changes; works everywhere.</li> <li>Redundant memory - O(total params) on every GPU, so model size is bounded by a single card.</li> <li>Communication cost - one gradient all-reduce per step (~2 x parameter size).</li> <li>Throughput scaling - global batch = per-GPU batch x N; watch out for generalization when scaling batch too far.</li> </ul> <p>Mermaid Diagram</p> <pre><code>flowchart LR\n    subgraph DataLoader\n        D[Global batch] --&gt; |split| MB1[Micro-batch 1]\n        D[Global batch] --&gt; |split| MB2[Micro-batch 2]\n        D[Global batch] --&gt; |split| MBN[Micro-batch N]\n    end\n    subgraph GPU1\n        MB1[Micro-batch 1] --&gt; M1[Model copy]\n    end\n    subgraph GPU2\n        MB2[Micro-batch 2] --&gt; M2[Model copy]\n    end\n    subgraph GPUN\n        MBN[Micro-batch N] --&gt; MN[Model copy]\n    end\n    M1[Model copy] &amp; M2[Model copy] &amp; MN[Model copy] --&gt; G[All-reduce \u2192 average gradients]\n    G[All-reduce \u2192 average gradients] --&gt; U[Synchronised weight update]</code></pre> <ul> <li>Tools: PyTorch DDP, Horovod.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#111-fully-sharded-data-parallelism-fsdp","title":"1.1.1 Fully Sharded Data Parallelism (FSDP)","text":"<p>FSDP is a type of data-parallel training, but unlike traditional data-parallel, which maintains a per-GPU copy of a model's parameters, gradients and optimizer states, it shards all of these states across data-parallel workers and can optionally offload the sharded model parameters to CPUs. [Pytorch]</p> <p>Key ideas</p> <ul> <li>Memory scaling: O(total params / NGPU) - enables multi-billion-parameter models to fit on 24 GB cards.</li> <li>Zero redundancy: No GPU ever holds a full copy of the model; identical to DeepSpeed ZeRO-3.</li> <li>Overlap compute &amp; communication: PyTorch overlaps the all-gather with computation to hide latency.</li> <li>Granularity control: You can wrap the whole model or nest FSDP wrappers on sub-modules for finer control.</li> </ul> <p>Mermaid Diagram</p> <pre><code>flowchart TD\n    %% GPU-local state\n    subgraph \"GPU 1\"\n        direction TB\n        P1[Param shard P\u2081]\n        G1[Grad shard G\u2081]\n        O1[Opt shard O\u2081]\n    end\n    subgraph \"GPU 2\"\n        direction TB\n        P2[Param shard P\u2082]\n        G2[Grad shard G\u2082]\n        O2[Opt shard O\u2082]\n    end\n    subgraph \"GPU N\"\n        direction TB\n        PN[Param shard P\u2099]\n        GN[Grad shard G\u2099]\n        ON[Opt shard O\u2099]\n    end\n\n    %% Mini-batch pipeline\n    start([Start micro-batch]) --&gt; gather[Step 1: All-Gather]\n    gather --&gt; fwd[Step 2: Forward compute]\n    fwd --&gt; reshard[Step 3: Re-shard P]\n    reshard --&gt; bwd[Step 4: Backward compute]\n    bwd --&gt; reduce[Step 5: Reduce-Scatter]\n    reduce --&gt; update[Step 6: Optimizer update]\n\n    %% Collective edges (dotted to indicate broadcast)\n    P1 -.-&gt; gather\n    P2 -.-&gt; gather\n    PN -.-&gt; gather</code></pre> <p>Note: In the diagram above, P represents Parameters (model weights), G represents Gradients, and O represents Optimizer states. These are the three main components of model state that are sharded across GPUs in FSDP.</p> <ul> <li>Use Case: Training very large models (&gt; 10 B parameters) that do not fit on a single GPU.</li> <li>Tools: PyTorch FSDP, DeepSpeed ZeRO-3.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#12-tensor-parallelism-tp","title":"1.2 Tensor Parallelism (TP)","text":"<p>TP slices individual weight tensors across GPUs so each rank stores only a shard (e.g., specific columns or rows). During the forward pass each rank computes its partial matrix multiplication; intermediate activations are all-gathered or reduced to produce the layer output.</p> <p>Key ideas</p> <ul> <li>Shards compute &amp; memory - enables layers larger than a single GPU.</li> <li>Orthogonal to DP - combine TP x DP for higher scale (Megatron uses a 2-D \u00abTP x DP\u00bb grid).</li> <li>Best for dense GEMM(General Matrix Multiplication)-heavy blocks - attention &amp; FFN matrices.</li> </ul> <p>Mermaid Diagram</p> <pre><code>flowchart LR\n    A[X activations] --&gt; |broadcast| X1[GPU1]\n    A --&gt; |broadcast| X2[GPU2]\n    A --&gt; |broadcast| XN[GPUN]\n    subgraph ShardedWeights\n        W1[W shard\u2081] --- X1\n        W2[W shard\u2082] --- X2\n        WN[W shard\u2099] --- XN\n    end\n    X1 --&gt; P1[Partial Y\u2081]\n    X2 --&gt; P2[Partial Y\u2082]\n    XN --&gt; PN[Partial Y\u2099]\n    P1 &amp; P2 &amp; PN --&gt; C[Concat / reduce \u2192 Y]</code></pre> <ul> <li>Tools: Megatron-LM, TensorRT-LLM, ColossalAI.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#13-pipeline-parallelism-pp","title":"1.3 Pipeline Parallelism (PP)","text":"<p>PP distributes consecutive blocks of layers to different GPUs (pipeline stages). Micro-batches flow through stages like an assembly line, so computation and communication overlap.</p> <p>Key ideas</p> <ul> <li>Memory relief - each rank stores only its slice of the network depth.</li> <li>Bubble latency - first and last few micro-batches see idle time; mitigate with enough micro-batches or sophisticated scheduling.</li> <li>Composable with DP/TP - e.g., 2 x TP inside each stage x 4 x PP across depth.</li> </ul> <p>Mermaid Diagram</p> <pre><code>sequenceDiagram\n    participant S0 as GPU-Stage 0 (Layers 1-4)\n    participant S1 as GPU-Stage 1 (Layers 5-8)\n    participant S2 as GPU-Stage 2 (Layers 9-12)\n    Note over S0,S2: \u2190 time \u2192\n    S0-&gt;&gt;S0: Fwd/Bwd \u00b5-batch 0\n    S0-&gt;&gt;S1: send activations\n    S1-&gt;&gt;S1: Fwd/Bwd \u00b5-batch 0\n    S1-&gt;&gt;S2: send activations\n    S0-&gt;&gt;S0: Fwd/Bwd \u00b5-batch 1\n    S2-&gt;&gt;S2: Fwd/Bwd \u00b5-batch 0</code></pre> <ul> <li>Tools: DeepSpeed PP, Megatron-LM, GPipe.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#14-context-parallelism-cp","title":"1.4 Context Parallelism (CP)","text":"<p>CP (a.k.a. sequence parallelism) splits the sequence length / token dimension across GPUs so each rank handles a contiguous block of tokens, enabling context windows far beyond single-GPU memory.</p> <p>Key ideas</p> <ul> <li>Long-context enabler - reach 32 k, 64 k+ tokens.</li> <li>Attention communication - GPUs exchange keys/values (all-gather) for cross-token attention each layer.</li> <li>Pairs well with TP &amp; PP - CP handles tokens while others handle model axes.</li> <li>Early-stage technique - currently in research code (Picotron / Nanotron).</li> </ul> <p>Mermaid Diagram</p> <pre><code>flowchart LR\n    subgraph Input[\"Input Sequence\"]\n        S[Sequence 0-8191 tokens]\n    end\n\n    subgraph CrossGPU[\"Cross-GPU Processing\"]\n        direction LR\n        subgraph GPU1[\"GPU 1\"]\n            direction TB\n            T0[Tokens 0-4095]\n            A0[Self-Attention Block]\n            T0 --&gt; A0\n        end\n\n        subgraph GPU2[\"GPU 2\"]\n            direction TB\n            T1[Tokens 4096-8191]\n            A1[Self-Attention Block]\n            T1 --&gt; A1\n        end\n\n        GPU1 &lt;--&gt;|Exchange Keys/Values| GPU2\n    end\n\n    subgraph Output[\"Output Processing\"]\n        M[Merge Logits]\n        O[Output Sequence]\n        M --&gt; O\n    end\n\n    S --&gt; |Split| T0\n    S --&gt; |Split| T1\n\n    A0 --&gt; M\n    A1 --&gt; M</code></pre> <ul> <li>Tools: Picotron, Nanotron.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#15-expert-parallelism-or-mixture-of-experts","title":"1.5 Expert Parallelism (or Mixture of Experts)","text":"<p>MoE layers contain dozens (or even hundreds) of parallel experts (small feed-forward sub-networks). For every token a lightweight gating network selects the top-k experts, so only that subset runs. This decouples model capacity (total parameters) from per-token compute/FLOPs.</p> <p>Key ideas</p> <ul> <li>Sparse activation - With k = 2 out of 64 experts each token touches ~3 % of the parameters, yet the model still \"sees\" the full capacity during training.</li> <li>Conditional computation - Tokens route to different experts, letting each specialize (e.g., code vs poetry).</li> <li>Load-balancing loss - Extra loss term keeps expert usage uniform to avoid stragglers.</li> <li>Scale to trillions - Total parameters scale linearly with #experts, compute stays roughly constant.</li> </ul> <p>Mermaid Diagram</p> <pre><code>flowchart LR\n    subgraph Input_Tokens[\"Input Tokens\"]\n        T1[\"T\u2081\"]\n        T2[\"T\u2082\"]\n        T3[\"T\u2083\"]\n    end\n    G[\"Gating Network\"]\n    subgraph Experts[\"Experts\"]\n        E1[\"Expert 1\"]\n        E2[\"Expert 2\"]\n        E3[\"Expert 3\"]\n        E4[\"\u22ef\"]\n    end\n    T1 --&gt; G\n    T2 --&gt; G\n    T3 --&gt; G\n    G --&gt;|top-k routes| E1\n    G --&gt;|top-k routes| E2\n    G --&gt;|top-k routes| E3\n    E1 &amp; E2 &amp; E3 --&gt; O[\"Concatenate + Mix\"]</code></pre> <ul> <li>Use Case: Scaling to 100 B-1 T+ parameters without proportional compute cost.</li> <li>Tools: DeepSpeed-MoE, GShard / Switch Transformer.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#16-4d-5d-parallelism","title":"1.6 4D-5D Parallelism","text":"<ul> <li>4D composes Data (D), Tensor (T), Pipeline (P), and Context (C) parallelism so every axis of the workload can be distributed.   Picture the GPUs as a 4-D lattice: N = DxTxPxC ranks.</li> <li>5D combines 4D + Expert Parallelism.</li> </ul> <p>Key ideas</p> <ul> <li>Extreme scale - Easily maps 10\u00b3-10\u2074 GPUs for 100 B-parameter, 8 k-context models.</li> <li>Topology aware - Tune each dimension to match intra-node (NVLink), inter-node (IB), and rack-level bandwidth.</li> <li>Memory &amp; compute balance - TP shards big matrices, CP splits long sequences, PP handles depth, DP feeds throughput.</li> </ul> <p>Mermaid Diagram</p> <pre><code>graph TD\n    %% Example 2x2x2x2 grid (16 GPUs)\n    subgraph Stage0[\"Pipeline Stage 0\"]\n        subgraph TP0[\"Tensor Group 0\"]\n            R0000[\"GPU D0-C0\"]\n            R0001[\"GPU D0-C1\"]\n        end\n        subgraph TP1[\"Tensor Group 1\"]\n            R0010[\"GPU D0-C0\"]\n            R0011[\"GPU D0-C1\"]\n        end\n    end\n    subgraph Stage1[\"Pipeline Stage 1\"]\n        subgraph TP0S1[\"Tensor Group 0\"]\n            R0100[\"GPU D1-C0\"]\n            R0101[\"GPU D1-C1\"]\n        end\n        subgraph TP1S1[\"Tensor Group 1\"]\n            R0110[\"GPU D1-C0\"]\n            R0111[\"GPU D1-C1\"]\n        end\n    end\n    A[\"Micro-batches (DP)\"] --&gt; R0000\n    R0000 --&gt;|TP| R0010\n    R0010 --&gt;|PP| R0100\n    R0100 --&gt;|CP assemble| Z[\"Output\"]</code></pre> <ul> <li>Use Case: Training &gt; 100 B-parameter models with multi-node clusters and long context windows.</li> <li>Tools: Picotron, Nanotron.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#2-training-strategies","title":"2. Training Strategies","text":"<p>Rule of thumb - pick the simplest scheme that fits in memory and saturates your interconnect. Start with a shard-aware data-parallel variant (FSDP/ZeRO-3). Add Tensor \u2194 Pipeline \u2194 Context axes only when the model or the sequence length forces you.</p> Hardware scope Fastest link Go-to recipe When to switch 1 node (2-8 GPUs, NVLink / PCIe Gen5) 200-900 GB/s FSDP + small TP via <code>torchrun</code> or DeepSpeed Model &gt; 1 x GPU 2-16 nodes (\u2264128 GPUs, NVLink + InfiniBand) 25-200 GB/s \"TP inside, DP across\" + optional PP Model &gt; 1 x node &gt;16 nodes (hundreds-thousands GPUs) \u226425 GB/s 4-D grid (DPxTPxPPxCP) 70 B + params and 32 k + tokens","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#21-single-node-multi-gpu","title":"2.1 Single-Node, Multi-GPU","text":"<p>Combine zeRO-style Fully-Sharded Data Parallelism (FSDP) with a low-degree Tensor Parallelism group that stays inside the node.</p> <ul> <li>FSDP shards parameters, gradients, and optimizer states across all GPUs, so each GPU uses only about 1/n of the total model memory \u2014 significantly reducing memory usage per GPU.</li> <li>TP protects matmul kernels from weight-gather latency; keep <code>tp&lt;=2</code> on PCIe, up to <code>tp&lt;=4</code> on NVLink.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#22-multi-node-multi-gpu","title":"2.2 Multi-Node, Multi-GPU","text":"<p>Start with Tensor Parallelism inside a node and Data Parallelism across nodes; introduce Pipeline Parallelism when the model no longer fits on one node.</p> <ul> <li>Keep TP collectives inside the node to avoid slow inter-node all-reduces.</li> <li>Tune micro-batch = 4 x PP degree as recommended by the Ultra-Scale Playbook to limit the pipeline bubble.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#23-4d-5d-parallelism","title":"2.3 4D-5D Parallelism","text":"<p>When weights and sequence length both exceed a node, use every axis (DP x TP x PP x CP).</p> <p>Guidelines:</p> <ul> <li>TP groups stay inside nodes; PP/CP may span nodes.</li> <li>Increase DP first when you need a larger global batch; it is the cheapest axis communication-wise.</li> <li>Expect ~75 % scaling efficiency up to 512 GPUs on InfiniBand clusters (HF benchmarks, Feb 2025).</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#3-recommended-tools-and-libraries","title":"3. Recommended Tools and Libraries","text":"Tool/Library Description Link DeepSpeed Optimizes training and inference for large models DeepSpeed Megatron-LM Framework for training large transformer models with TP and PP Megatron-LM ColossalAI Provides a unified interface for various parallelism strategies ColossalAI Horovod Distributed training framework supporting multiple backends Horovod Hugging Face Accelerate Simplifies training and inference across devices Accelerate TensorRT-LLM High-performance inference library by NVIDIA TensorRT-LLM vLLM Efficient LLM inference engine vLLM Picotron Minimalistic 4D-parallelism distributed training framework for educational purposes Picotron Nanotron Minimalistic large language model 3D-parallelism training framework Nanotron","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#4-choosing-the-right-strategy","title":"4. Choosing the Right Strategy","text":"Scenario Recommended Approach Training on a single machine with multiple GPUs Combine DP with TP or PP using DeepSpeed or PyTorch FSDP. Training across multiple machines Utilize DeepSpeed with a combination of DP, TP, and PP. Training with very long context windows Use Picotron or Nanotron with Context Parallelism. Training extremely large models Leverage 4D parallelism with Picotron or Nanotron. Inference with latency constraints Deploy using TensorRT-LLM or vLLM. Inference for very large models Use DeepSpeed Inference with ZeRO-Offload. Quick deployment of models Leverage Hugging Face TGI.","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/04/28/scaling-large-language-models-multi-gpu-and-multi-node-strategies-in-2025/#cheatsheet-from-huggingface-folks","title":"Cheatsheet from HuggingFace Folks","text":"<p>\u2e3b</p> <p>By adopting these strategies and tools, you can effectively scale LLM training and inference across multiple GPUs and systems, ensuring optimal performance and resource utilization.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/archive/2025/","title":"2025","text":""}]}