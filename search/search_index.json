{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Shared Intelligence","text":"<p>Tips &amp; war stories from the ML\u2011engineering trenches.</p> <p>\ud83d\udc4b Hi, I'm Slava!</p> <p>I'm a Berlin\u2011based Senior Machine Learning Engineer\u202fII at HubSpot (PhD) with 10+ years of experience turning cutting\u2011edge research into reliable, revenue\u2011generating products.</p> <p>I've built graph\u2011neural\u2011network fraud detection systems that save millions at Wayfair, large\u2011scale recommender systems and statistical experimentation tooling at OLX\u202fGroup, and now lead AI Signals initiatives at HubSpot.</p> <p>My professional passions include fraud\u202f&amp;\u202fscam detection, representation learning, recommender systems, LLMs\u202f&amp;\u202fAI\u202fagents, and MLOps\u2014topics you'll find explored throughout this blog.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<p>In this blog, you'll find:</p> <ul> <li>Technical tutorials and guides</li> <li>Machine Learning insights</li> <li>Best practices and tips</li> <li>Personal experiences and learnings</li> </ul> <p>My goal is to create a valuable resource for fellow practitioners and anyone interested in the real-world application of machine learning.</p>"},{"location":"#connect","title":"Connect","text":"<ul> <li>LinkedIn</li> <li>GitHub</li> </ul>"},{"location":"topics/","title":"\ud83d\udd0e Browse by Topic","text":""},{"location":"topics/#tag:deep-learning","title":"Deep Learning","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:distributed-training","title":"Distributed Training","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:gpu","title":"GPU","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:llm","title":"LLM","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:parallelism","title":"Parallelism","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:genai","title":"genai","text":"<ul> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> </ul>"},{"location":"topics/#tag:guide","title":"guide","text":"<ul> <li>            Building a Custom FeatureStoreLite MCP Server Using uv          </li> <li>            Choosing the Right Open-Source LLM Variant &amp; File Format          </li> <li>            Quick-Guide on `pyproject.toml`          </li> <li>            Quick-Guide on managing Python on macOS with uv          </li> <li>            Quick-Guide on setting up a MacBook for AI Engineering          </li> <li>            Quick-Guide on ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> </ul>"},{"location":"topics/#tag:infrastructure","title":"infrastructure","text":"<ul> <li>            MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond          </li> </ul>"},{"location":"topics/#tag:llm","title":"llm","text":"<ul> <li>            Choosing the Right Open-Source LLM Variant &amp; File Format          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> </ul>"},{"location":"topics/#tag:llmops","title":"llmops","text":"<ul> <li>            MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond          </li> </ul>"},{"location":"topics/#tag:macos","title":"macos","text":"<ul> <li>            Quick-Guide on setting up a MacBook for AI Engineering          </li> <li>            Quick-Guide on ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> </ul>"},{"location":"topics/#tag:mcp","title":"mcp","text":"<ul> <li>            Building a Custom FeatureStoreLite MCP Server Using uv          </li> </ul>"},{"location":"topics/#tag:mlops","title":"mlops","text":"<ul> <li>            MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond          </li> </ul>"},{"location":"topics/#tag:python","title":"python","text":"<ul> <li>            Quick-Guide on `pyproject.toml`          </li> <li>            Quick-Guide on managing Python on macOS with uv          </li> </ul>"},{"location":"topics/#tag:tooling","title":"tooling","text":"<ul> <li>            Quick-Guide on managing Python on macOS with uv          </li> <li>            Quick-Guide on setting up a MacBook for AI Engineering          </li> <li>            Quick-Guide on ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> </ul>"},{"location":"topics/#tag:tools","title":"tools","text":"<ul> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/","title":"Quick-Guide on managing Python like an AI Engineer on macOS with uv","text":"","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#tldr-bash-cheatsheet","title":"TL;DR Bash Cheat\u2011sheet","text":"<pre><code>brew install uv        # install tool\nuv python install 3.12 # grab interpreter\n\n# New project workflow (modern)\nuv init                # create new project with pyproject.toml\nuv add pandas numpy    # add dependencies\nuv run train.py        # run with correct interpreter\n\n# Classical project workflow (requirements.txt)\nuv venv                           # create .venv\nuv pip install -r requirements.txt # install from requirements\nuv run train.py                   # run script\n\nbrew upgrade uv         # update uv itself (Homebrew install)\n</code></pre>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#why-i-migrated-to-uv-and-you-should-too","title":"\ud83c\udf19 Why I Migrated to <code>uv</code> (And You Should Too)","text":"<p><code>uv</code> is a lightning-fast, all-in-one Python project tool written in Rust, combining package management, interpreter installation, and virtual environment creation. Key features include:</p> <ul> <li>Installing and switching between multiple CPython (and PyPy) builds</li> <li>Creating lightweight virtual environments</li> <li>Resolving dependencies with an absurdly fast pip-compatible resolver</li> <li>Modern project management with <code>pyproject.toml</code></li> <li>A <code>uvx</code> shim for running tools like Ruff or Black in isolated sandboxes:</li> <li><code>uvx black .</code> or <code>uvx ruff format .</code></li> </ul> <p>Result: fewer moving parts, faster setups, and consistent environments across laptop and CI images.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#1-installing-uv","title":"1. Installing uv","text":"<pre><code># Install uv via Homebrew (Apple Silicon &amp; Intel)\nbrew install uv\n</code></pre> <p>Note: <code>uv</code> auto-detects your architecture (Apple Silicon or Intel).</p> <p>The same page shows a one\u2011liner curl installer if you're brew\u2011averse. Check it worked:</p> <pre><code># Check installation\nuv --version      # should print something like 0.6.x\nbrew upgrade uv   # keep it fresh (since we installed via Homebrew)\n</code></pre>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#2-installing-python-interpreters","title":"2. Installing Python interpreters","text":"<pre><code># Install specific Python versions\nuv python install 3.12.4          # exact version\nuv python install 3.13            # latest minor\nuv python install 3.9 3.10 3.11   # many at once\nuv python list                    # what's already cached\n</code></pre> <p>These archives live under <code>~/.cache/uv</code>, so they don't fight Homebrew or Xcode.</p> <p>Need the interpreter for this project only?</p> <pre><code># Pin Python version for the project\nuv python pin           # writes .python-version next to your code\n</code></pre> <p>Drop that file into Git and your team (or the CI) will automatically get the same binary.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#3-two-workflows-modern-vs-classical","title":"3. Two Workflows: Modern vs Classical","text":"","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#31-modern-workflow-new-projects-with-pyprojecttoml","title":"3.1 Modern Workflow: New Projects with <code>pyproject.toml</code>","text":"<p>For new projects or when you want to embrace the modern Python packaging ecosystem:</p> <pre><code># Start a new project\nuv init                    # creates pyproject.toml, README.md, .gitignore\n\n# Add dependencies\nuv add pip                 # needed for Jupyter notebooks in VS Code\nuv add pandas numpy        # add your ML packages\nuv add pytest --dev       # add development dependencies\n\n# Run your code\nuv run python script.py    # run scripts\nuv run jupyter lab         # run installed tools\nuv run pytest             # run tests\n</code></pre> <p>When cloning an existing uv project:</p> <pre><code>git clone &lt;repo&gt;\ncd &lt;repo&gt;\nuv sync                    # installs all dependencies from pyproject.toml\n</code></pre> <p>This creates a complete environment with locked dependencies, ensuring reproducible builds across your team.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#32-classical-workflow-existing-projects-with-requirementstxt","title":"3.2 Classical Workflow: Existing Projects with <code>requirements.txt</code>","text":"<p>For existing projects or when working with traditional Python setups:</p> <pre><code># Set up environment\nuv venv                           # creates .venv\nuv pip install -r requirements.txt # install dependencies\n\n# Run your code\nuv run python script.py           # run scripts\nuv run installed-package          # run any installed CLI tools\n</code></pre> <p>Pro tip: <code>uv</code> automatically detects the <code>.venv</code> directory, so you rarely need to manually activate environments.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#33-using-uvx-for-global-tools","title":"3.3 Using <code>uvx</code> for Global Tools","text":"<p>With <code>uvx</code> you can run formatters or linters without touching your virtual environment:</p> <pre><code>uvx black .            # format code in an isolated sandbox\nuvx ruff check src/    # lint code without installing ruff globally\nuvx jupyter lab        # run Jupyter without installing it locally\n</code></pre>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#4-quick-reference-table","title":"4. Quick Reference Table","text":"Task Modern (<code>pyproject.toml</code>) Classical (<code>requirements.txt</code>) Start new project <code>uv init</code> <code>touch requirements.txt</code> Add dependency <code>uv add package</code> <code>echo package &gt;&gt; requirements.txt</code> Install dependencies <code>uv sync</code> <code>uv pip install -r requirements.txt</code> Run script <code>uv run python script.py</code> <code>uv run python script.py</code> Run installed tool <code>uv run tool-name</code> <code>uv run tool-name</code> Create environment automatic with uv init <code>uv venv</code>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#5-coexisting-with-pyenv-if-you-must","title":"5. Co\u2011existing with pyenv (if you must)","text":"<ul> <li>Keep pyenv if you rely on its \"shim\" strategy to globally shadow <code>python</code> in your shell.</li> <li>Skip pyenv if project\u2011local versions and CI parity are your priority - uv handles that solo.</li> </ul> <p>From uv's perspective every interpreter in <code>$PATH</code> (even ones compiled by pyenv or Homebrew) is just \"system Python\". You can pass it to any <code>--python</code> flag and mix\u2011and\u2011match as needed.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-on-managing-python-on-macos-with-uv/#6-mlspecific-niceties","title":"6. ML\u2011specific niceties","text":"<ul> <li>The PyTorch integration guide shows CUDA\u2011aware installs in one command - excellent for GPU vs. CPU builds on the same Mac.</li> <li>Binary wheels pulled by uv are cached, so re\u2011creating a venv to try a different version of scikit\u2011learn or TensorFlow feels instant.</li> <li>Use <code>uv add pip</code> in new projects to ensure Jupyter notebooks work seamlessly in VS Code.</li> </ul>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/","title":"Quick-Guide on setting up a MacBook for AI Engineering","text":"<p>Here's my distilled, 10\u2011step workflow to transform a vanilla macOS install into a ready to-go AI engineering working station.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#1-xcode-command-line-tools","title":"1. Xcode Command Line Tools","text":"<p>First of all, let's install Xcode Command Line Tools. These tools are the foundation for any type of software development (including DS).</p> <pre><code>xcode-select --install\n</code></pre>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#2-homebrew","title":"2. Homebrew","text":"<p>Then install Homebrew. It is a package manager for macOS. You can follow instructions on their website or just run:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#3-main-dependencies-with-brew","title":"3. Main dependencies with brew","text":"<p>Then I install these dependencies with brew:</p> <pre><code>brew install openssl readline sqlite3 xz zlib pyenv uv htop gitmoji pandoc yt-dlp ncdu tmux\n</code></pre> <p>Descriptions:</p> <ul> <li>openssl for SSL/TLS cryptography,</li> <li>readline for command-line text editing support,</li> <li>sqlite3 for embedded SQL database engine,</li> <li>xz for LZMA2-based data compression,</li> <li>zlib for DEFLATE compression library,</li> <li>pyenv for managing global Python versions via shims,</li> <li>uv for managing Python project dependencies and virtual environments,</li> <li>htop for interactive process viewer and system monitor,</li> <li>gitmoji for customizing commit messages.</li> <li>pandoc for converting documents between various markup formats,</li> <li>yt-dlp for downloading videos from online platforms,</li> <li>ncdu for analyzing disk usage in a terminal interface,</li> <li>tmux for terminal session multiplexing.</li> </ul> <p>For more detailed information about using <code>uv</code> for Python development, check out my Quick-Guide on managing Python on macOS with uv.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#4-terminals","title":"4. Terminals","text":"<p>I used to prefer iTerm2 over the standard Terminal due to its flexible configuration, but recently migrated to Warp. Warp offers a modern, Rust-based terminal experience with AI features integrated. You can download it from the Warp website. However, if you still prefer iTerm2, here's how I used to configure it:</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#5-iterm-configuration","title":"5. iTerm configuration","text":"<p>For configuring iTerm I prefer to do the following:</p> <ul> <li> <p>Setup Natural text editing:</p> <ul> <li>Go to Preferences \u2192 Profiles \u2192 Keys \u2192 Key Mappings</li> <li>Press Presets\u2026 dropdown button</li> <li>Select Natural Text Editing</li> </ul> </li> <li> <p>For changing color select the preferred preset from this repo. Then:</p> <ul> <li>Go to Preferences \u2192 Profiles \u2192 Colors \u2192 Color Presets\u2026 \u2192 Import (or select)</li> <li>After importing your new color will be displayed in Color Presets</li> </ul> </li> </ul>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#6-zsh-oh-my-zsh","title":"6. Zsh + Oh My Zsh","text":"<p>Then I install and configure Zsh and Oh My Zsh:</p> <pre><code>brew install zsh\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n</code></pre>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#7-zsh-plugins","title":"7. Zsh plugins","text":"<p>Now you can configure your terminal with a <code>~/.zshrc</code> file. I use the next zsh plugins in my daily routine:</p> <pre><code>plugins=(\n    aws bgnotify brew docker docker-compose\n    emoji forklift gcloud git history iterm2\n    keychain kubectl macos pre-commit\n    pyenv pylint python screen themes\n    tmux virtualenv vscode\n    zsh-autosuggestions zsh-syntax-highlighting\n)\n</code></pre> <p>A description of the plugins you can find here.</p> <p>Only the last two plugins (zsh-autosuggestions and zsh-syntax-highlighting) require additional installation. It's pretty simple, just check the following links:</p> <ul> <li>zsh-autosuggestions</li> <li>zsh-syntax-highlighting</li> </ul>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#8-powerlevel10k","title":"8. Powerlevel10k","text":"<p>I'm using the Powerlevel10k theme in Zsh. It has installation assistance that helps you configure Zsh your way. Just follow the instruction on their website.</p> <p>If you have any issues with fonts in another terminal, you can install fonts separately. For example, to configure VSCode to use the Nerd Font either follow this instruction or do the next:</p> <ol> <li>Open VSCode Settings:</li> <li>Set the Terminal Font:<ol> <li>Search for terminal.integrated.fontFamily.</li> <li>Set its value to the name of the installed font, e.g., <code>MesloLGS NF</code>.</li> </ol> </li> </ol>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#9-ide","title":"9. IDE","text":"<p>I prefer VSCode or Cursor.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#10-other-developer-tools","title":"10. Other developer tools","text":"<ul> <li>GitHub Desctop</li> <li>Docker or its alternatives</li> <li>Local LLMs clients like Ollama or LMStudio</li> </ul>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#final-thoughts","title":"Final thoughts","text":"<p>By now you have the exact stack I lean on every day as an AI engineer - just the essentials that remove friction between an idea and a running model.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/","title":"Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025","text":"<p>The race to build bigger, better language models continues at breakneck speed. Today's state-of-the-art models require massive computing resources that no single GPU can handle. Whether you're training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.</p> <p>This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face's Ultra-Scale Playbook.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#why-scaling-matters","title":"Why Scaling Matters","text":"<p>Before diving into techniques, let's understand why this matters:</p> <ul> <li>Model size: A 70B parameter model requires ~140GB just to store in FP16 - far beyond any single GPU</li> <li>Training time: Even with 8 A100s, training a 13B model from scratch takes weeks</li> <li>Context length: Processing long contexts (32k+ tokens) often exceeds single-GPU memory</li> <li>Inference speed: Distributing inference can reduce latency for demanding applications</li> </ul> <p>Let's explore how to overcome these challenges.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#1-parallelism-techniques-explained-simply","title":"1. Parallelism Techniques Explained Simply","text":"","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#11-data-parallelism-dp","title":"1.1 Data Parallelism (DP)","text":"<p>Think of this as multiple workers all having the same instruction manual (model), but each working on different examples.</p> <p>How it works:</p> <ul> <li>Each GPU gets an identical copy of the model</li> <li>Each processes different data samples</li> <li>Results are combined by averaging gradients</li> </ul> <p>When to use it:</p> <ul> <li>Your model fits on a single GPU</li> <li>You want to process more data in parallel</li> <li>You need a simple solution with minimal code changes</li> </ul> <pre><code>flowchart LR\n    subgraph DataLoader\n        D[Global batch] --&gt; |split| MB1[Micro-batch 1]\n        D[Global batch] --&gt; |split| MB2[Micro-batch 2]\n        D[Global batch] --&gt; |split| MBN[Micro-batch N]\n    end\n    subgraph GPU1\n        MB1[Micro-batch 1] --&gt; M1[Model copy]\n    end\n    subgraph GPU2\n        MB2[Micro-batch 2] --&gt; M2[Model copy]\n    end\n    subgraph GPUN\n        MBN[Micro-batch N] --&gt; MN[Model copy]\n    end\n    M1[Model copy] &amp; M2[Model copy] &amp; MN[Model copy] --&gt; G[All-reduce -&gt; average gradients]\n    G[All-reduce -&gt; average gradients] --&gt; U[Synchronised weight update]</code></pre> <p>Tools: PyTorch DDP, Horovod.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#12-fully-sharded-data-parallelism-fsdp","title":"1.2 Fully Sharded Data Parallelism (FSDP)","text":"<p>FSDP is like DP but more memory-efficient - imagine each worker only keeping part of the instruction manual and borrowing pages from colleagues when needed.</p> <p>How it works:</p> <ul> <li>Model parameters, gradients, and optimizer states are split across GPUs</li> <li>During computation, GPUs gather needed parameters from others</li> <li>After backward pass, each GPU only updates its part</li> </ul> <p>Real-world impact:</p> <ul> <li>Training very large models (&gt; 10 B parameters) that do not fit on a single GPU.</li> </ul> <pre><code>flowchart TD\n    %% GPU-local state\n    subgraph \"GPU 1\"\n        direction TB\n        P1[Param shard P\u2081]\n        G1[Grad shard G\u2081]\n        O1[Opt shard O\u2081]\n    end\n    subgraph \"GPU 2\"\n        direction TB\n        P2[Param shard P\u2082]\n        G2[Grad shard G\u2082]\n        O2[Opt shard O\u2082]\n    end\n    subgraph \"GPU N\"\n        direction TB\n        PN[Param shard P\u2099]\n        GN[Grad shard G\u2099]\n        ON[Opt shard O\u2099]\n    end\n\n    %% Mini-batch pipeline\n    start([Start micro-batch]) --&gt; gather[Step 1: All-Gather]\n    gather --&gt; fwd[Step 2: Forward compute]\n    fwd --&gt; reshard[Step 3: Re-shard P]\n    reshard --&gt; bwd[Step 4: Backward compute]\n    bwd --&gt; reduce[Step 5: Reduce-Scatter]\n    reduce --&gt; update[Step 6: Optimizer update]\n\n    %% Collective edges (dotted to indicate broadcast)\n    P1 -.-&gt; gather\n    P2 -.-&gt; gather\n    PN -.-&gt; gather</code></pre> <p>Tools: PyTorch FSDP, DeepSpeed ZeRO-3.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#13-tensor-parallelism-tp","title":"1.3 Tensor Parallelism (TP)","text":"<p>Tensor parallelism splits individual layers across GPUs - like dividing a massive spreadsheet calculation across multiple people.</p> <p>How it works:</p> <ul> <li>Individual weight matrices are split across GPUs</li> <li>Each GPU computes part of each layer's output</li> <li>Results are combined before moving to the next layer</li> </ul> <p>Best for:</p> <ul> <li>Massive attention layers and FFNs</li> <li>When FSDP alone isn't enough</li> <li>Works well inside a node with fast GPU-GPU connections</li> </ul> <pre><code>flowchart LR\n    A[X activations] --&gt; |broadcast| X1[GPU1]\n    A --&gt; |broadcast| X2[GPU2]\n    A --&gt; |broadcast| XN[GPUN]\n    subgraph ShardedWeights\n        W1[W shard\u2081] --- X1\n        W2[W shard\u2082] --- X2\n        WN[W shard\u2099] --- XN\n    end\n    X1 --&gt; P1[Partial Y\u2081]\n    X2 --&gt; P2[Partial Y\u2082]\n    XN --&gt; PN[Partial Y\u2099]\n    P1 &amp; P2 &amp; PN --&gt; C[Concat / reduce -&gt; Y]</code></pre> <p>Tools: Megatron-LM, TensorRT-LLM, ColossalAI.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#14-pipeline-parallelism-pp","title":"1.4 Pipeline Parallelism (PP)","text":"<p>Pipeline parallelism splits the model across its depth - like an assembly line where each station handles different layers.</p> <p>How it works:</p> <ul> <li>Different layers run on different GPUs</li> <li>Data flows through the pipeline in micro-batches</li> <li>Each GPU only stores its assigned layers</li> </ul> <p>When to use:</p> <ul> <li>Very deep models</li> <li>When you need to scale beyond a single node</li> <li>Combined with other techniques for maximum scaling</li> </ul> <pre><code>sequenceDiagram\n    participant S0 as GPU-Stage 0 (Layers 1-4)\n    participant S1 as GPU-Stage 1 (Layers 5-8)\n    participant S2 as GPU-Stage 2 (Layers 9-12)\n    Note over S0,S2: \u2190 time -&gt;\n    S0-&gt;&gt;S0: Fwd/Bwd \u00b5-batch 0\n    S0-&gt;&gt;S1: send activations\n    S1-&gt;&gt;S1: Fwd/Bwd \u00b5-batch 0\n    S1-&gt;&gt;S2: send activations\n    S0-&gt;&gt;S0: Fwd/Bwd \u00b5-batch 1\n    S2-&gt;&gt;S2: Fwd/Bwd \u00b5-batch 0</code></pre> <p>Tools: DeepSpeed PP, Megatron-LM, GPipe.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#15-context-parallelism-cp","title":"1.5 Context Parallelism (CP)","text":"<p>For handling extremely long sequences - imagine different people each reading different paragraphs of a book, then sharing key information.</p> <p>How it works:</p> <ul> <li>Sequence/context length is split across GPUs</li> <li>Each GPU processes a chunk of the input sequence</li> <li>GPUs exchange information for cross-attention</li> </ul> <p>Real-world use case:</p> <ul> <li>Enables processing 100K+ tokens on consumer hardware</li> <li>Critical for document QA, code generation, and long-context reasoning</li> <li>Becoming essential as context windows expand</li> </ul> <pre><code>flowchart LR\n    subgraph Input[\"Input Sequence\"]\n        S[Sequence 0-8191 tokens]\n    end\n\n    subgraph CrossGPU[\"Cross-GPU Processing\"]\n        direction LR\n        subgraph GPU1[\"GPU 1\"]\n            direction TB\n            T0[Tokens 0-4095]\n            A0[Self-Attention Block]\n            T0 --&gt; A0\n        end\n\n        subgraph GPU2[\"GPU 2\"]\n            direction TB\n            T1[Tokens 4096-8191]\n            A1[Self-Attention Block]\n            T1 --&gt; A1\n        end\n\n        GPU1 &lt;--&gt;|Exchange Keys/Values| GPU2\n    end\n\n    subgraph Output[\"Output Processing\"]\n        M[Merge Logits]\n        O[Output Sequence]\n        M --&gt; O\n    end\n\n    S --&gt; |Split| T0\n    S --&gt; |Split| T1\n\n    A0 --&gt; M\n    A1 --&gt; M</code></pre> <p>Tools: Picotron, Nanotron.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#16-expert-parallelism-or-mixture-of-experts","title":"1.6 Expert Parallelism (or Mixture of Experts)","text":"<p>MoE models act like specialized consultants - rather than activating the entire model for every input, each token gets routed to only the \"experts\" it needs.</p> <p>How it works:</p> <ul> <li>Multiple specialized sub-networks (experts) exist within the model</li> <li>A routing mechanism determines which experts to use for each token</li> <li>Only a small subset of parameters is used for any given token</li> </ul> <p>Why it matters:</p> <ul> <li>Scales to massive models (1T+ parameters) without proportional compute costs</li> <li>Allows more efficient use of parameters</li> <li>Models like Mixtral and Grok use this approach</li> </ul> <pre><code>flowchart LR\n    subgraph Input_Tokens[\"Input Tokens\"]\n        T1[\"T\u2081\"]\n        T2[\"T\u2082\"]\n        T3[\"T\u2083\"]\n    end\n    G[\"Gating Network\"]\n    subgraph Experts[\"Experts\"]\n        E1[\"Expert 1\"]\n        E2[\"Expert 2\"]\n        E3[\"Expert 3\"]\n        E4[\"\u22ef\"]\n    end\n    T1 --&gt; G\n    T2 --&gt; G\n    T3 --&gt; G\n    G --&gt;|top-k routes| E1\n    G --&gt;|top-k routes| E2\n    G --&gt;|top-k routes| E3\n    E1 &amp; E2 &amp; E3 --&gt; O[\"Concatenate + Mix\"]</code></pre> <p>Tools: Picotron, Nanotron.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#2-practical-training-strategies","title":"2. Practical Training Strategies","text":"<p>For most practitioners, here's what I recommend based on your hardware:</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#21-single-machine-2-8-gpus","title":"2.1 Single Machine (2-8 GPUs)","text":"<p>Best approach: FSDP + small TP</p> <ul> <li>Start with pure FSDP using PyTorch or DeepSpeed</li> <li>If your model has huge layers, add TP=2 inside the node</li> <li>Use <code>accelerate</code> or <code>torchrun</code> for the simplest setup</li> </ul> <p>Real-world tip: On consumer hardware with PCIe connections, stick to TP\u22642. On servers with NVLink, you can go up to TP=4 efficiently.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#22-small-cluster-2-16-nodes-128-gpus","title":"2.2 Small Cluster (2-16 nodes, \u2264128 GPUs)","text":"<p>Best approach: \"TP inside, DP across\" + optional PP</p> <ul> <li>Keep TP groups within each node (utilizing fast NVLink)</li> <li>Use DP or FSDP across nodes</li> <li>Add PP when model depth exceeds single-node capacity</li> </ul> <p>Pro tip: When using PP, set micro-batch size to 4x your PP degree to minimize pipeline bubbles.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#23-large-cluster-hundreds-gpus","title":"2.3 Large Cluster (hundreds+ GPUs)","text":"<p>Best approach: 4D parallelism (DPxTPxPPxCP)</p> <ul> <li>Necessary for 70B+ models with 32k+ context windows</li> <li>Requires careful mapping to hardware topology</li> <li>Expect ~75% scaling efficiency with good InfiniBand networking</li> </ul> <p>Real-world example: Training a 70B model with 32k context might use:</p> <ul> <li>TP=8 (within each node)</li> <li>PP=4 (across nodes)</li> <li>CP=4 (for long sequence handling)</li> <li>DP=4 (for throughput)</li> <li>Total: 512 GPUs organized in a 4D grid</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#3-practical-tools-worth-learning","title":"3. Practical Tools Worth Learning","text":"Tool When to Use It Learning Curve PyTorch FSDP Training medium-large models (1-20B) on a single node \u2605\u2605\u2606\u2606\u2606 DeepSpeed ZeRO Multi-node training with simple setup \u2605\u2605\u2605\u2606\u2606 Megatron-LM Production training of very large models \u2605\u2605\u2605\u2605\u2606 vLLM Fast inference with attention caching \u2605\u2605\u2606\u2606\u2606 TensorRT-LLM Production inference with NVIDIA GPUs \u2605\u2605\u2605\u2605\u2606 Hugging Face Accelerate Simple distributed training with minimal code changes \u2605\u2606\u2606\u2606\u2606 Nanotron Research and education on 3D parallelism \u2605\u2605\u2605\u2606\u2606","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#4-making-the-right-choice-a-simple-decision-tree","title":"4. Making the Right Choice: A Simple Decision Tree","text":"<ol> <li> <p>Does your model fit on a single GPU?</p> <ul> <li>Yes -&gt; Use standard training</li> <li>No -&gt; Continue to question 2</li> </ul> </li> <li> <p>Do you have multiple GPUs in one machine?</p> <ul> <li>Yes -&gt; Try FSDP first</li> <li>No -&gt; Skip to question 4</li> </ul> </li> <li> <p>Is FSDP still not enough?</p> <ul> <li>Add TP=2 inside the node</li> <li>If still insufficient, add CP for long contexts</li> </ul> </li> <li> <p>Training across multiple nodes?</p> <ul> <li>Start with \"TP inside nodes, DP across nodes\"</li> <li>Add PP when model depth exceeds a single node</li> <li>For very large models with long contexts, consider 4D parallelism</li> </ul> </li> </ol> <p>Diagram for visualization</p> <pre><code>flowchart TD\n    start([LLM Scaling Decision]) --&gt; fit{Does model fit on single GPU?}\n    fit --&gt;|Yes| dp[Use standard training]\n    fit --&gt;|No| multiple{Multiple GPUs in one machine?}\n    multiple --&gt;|Yes| fsdp[Try FSDP first]\n    multiple --&gt;|No| multinode[Training across multiple nodes]\n    fsdp --&gt; enough{Is FSDP still not enough?}\n    enough --&gt;|Yes| tp[Add TP=2 inside the node]\n    tp --&gt; context{Need longer contexts?}\n    context --&gt;|Yes| cp[Add CP for long contexts]\n    multinode --&gt; hybrid[Start with TP inside nodes, DP across nodes]\n    hybrid --&gt; depth{Model depth exceeds single node?}\n    depth --&gt;|Yes| pp[Add PP when model depth exceeds a single node]\n    pp --&gt; large{Very large model with long contexts?}\n    large --&gt;|Yes| d4[Consider 4D parallelism]</code></pre>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#5-the-ultra-scale-cheatsheet","title":"5. The Ultra-Scale Cheatsheet","text":"<p>This visual guide from HuggingFace's team summarizes the key decision points:</p> <p></p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#conclusion","title":"Conclusion","text":"<p>Scaling LLMs is both an art and a science. While these techniques may seem complex at first, they follow logical patterns based on hardware constraints and model structure. Start simple with FSDP and add more dimensions of parallelism only as needed.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/","title":"MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond","text":"<p>The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models - from giant language models (LLMs) like GPT-4 to image diffusion models like Stable Diffusion. As a result, the way we build and operate ML systems (MLOps) looks very different today than it did just a few years ago. In this post, we'll explore how ML infrastructure and MLOps practices have evolved - contrasting the \"classic\" era of MLOps with the modern paradigms emerging to support foundation models. We'll highlight what's changed, what new patterns and workflows have emerged.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#1-the-mlops-landscape-a-few-years-ago-pre-foundation-model-era","title":"1. The MLOps Landscape a Few Years Ago (Pre-Foundation Model Era)","text":"<p>A few years back, MLOps primarily meant applying DevOps principles to ML: automating the model lifecycle from data preparation to deployment and monitoring. Typical ML systems were built around relatively smaller models, often trained from scratch or with moderate pre-training, on domain-specific data. Key characteristics of this \"classic\" MLOps era included:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#11-end-to-end-pipelines","title":"1.1. End-to-End Pipelines","text":"<p>Teams set up end-to-end pipelines for data extraction, training, validation, and deployment. Tools like Apache Airflow orchestrated ETL and training workflows, while CI/CD systems ran automated tests and pushed models to production. The focus was on reproducibility and automation - packaging models (e.g. in Docker containers) and deploying them via REST microservices or batch jobs.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#12-experiment-tracking-and-model-versioning","title":"1.2. Experiment Tracking and Model Versioning","text":"<p>Even then, managing experiments and versions was critical. Platforms such as MLflow or Weights &amp; Biases (W&amp;B) gained popularity to log training runs, hyperparameters, and metrics. This allowed data scientists to compare experiments and reliably reproduce results. Models were registered in model registries with version numbers, making it easier to roll back to a good model if a new one underperformed.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#13-continuous-training-cicd","title":"1.3. Continuous Training &amp; CI/CD","text":"<p>Classic MLOps pipelines emphasized continuous integration of new data and models. For instance, a pipeline might retrain a model nightly or weekly as new data arrived, then run a battery of tests. If tests passed, the new model would be deployed via a CI/CD pipeline. Automation tools (Jenkins, GitLab CI/CD, etc.) were configured to ensure that any change in data or code would trigger the pipeline and deliver updated models reliably.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#14-infrastructure-and-serving","title":"1.4. Infrastructure and Serving","text":"<p>In the pre-LLM era, serving a model in production often meant a relatively small footprint - perhaps a few CPU cores or a single GPU for real-time inference. Kubernetes and Docker became the de facto way to deploy scalable inference services, allowing organizations to replicate model instances to handle load. Monitoring focused on uptime and performance metrics (latency, throughput) as well as model-specific metrics like prediction accuracy on a rolling window of data, concept drift detection, etc.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#15-feature-stores-and-data-management","title":"1.5. Feature Stores and Data Management","text":"<p>For many ML applications (especially in industries like finance or e-commerce), engineered features were as important as models. Feature stores were introduced to provide a central place to manage features used in models, ensuring consistency between training and serving. The emphasis was on structured data pipelines and feature engineering, whereas unstructured data (text, images) often required custom handling outside these stores.</p> <p>In summary, \"classic\" MLOps revolved around relatively small-to-medium models and explicit feature engineering. The tooling was geared toward managing many experiments and deployments, and scaling out a large number of models (for different tasks) rather than scaling one enormous model. This paradigm worked well - until models started growing dramatically in size and capability, ushering in a new era.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#2-the-paradigm-shift-rise-of-large-scale-foundation-models","title":"2. The Paradigm Shift: Rise of Large-Scale Foundation Models","text":"<p>Around 2018-2020, researchers began introducing foundation models - extremely large models pretrained on vast corpora, capable of being adapted to many tasks. Examples include BERT and GPT-2 (NLP), followed by GPT-3 and PaLM, as well as image models like BigGAN and later diffusion models (DALL-E, Stable Diffusion). By 2023-2024, these models became ubiquitous in ML workflows. As one practitioner noted in early 2024, \"Today, foundational models are everywhere - from Hugging Face to built-in models in services like AWS Bedrock - a stark change from just two years ago\". This rise of foundation models led to major shifts in ML infrastructure:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#21-pretrained-from-scratch","title":"2.1. Pretrained &gt; From Scratch","text":"<p>Instead of developing many models from scratch, teams began with powerful pretrained models and fine-tuned them for specific tasks. This dramatically cut down training time and data needs for new tasks. It also meant that the largest models (with billions of parameters) were often reused via fine-tuning or even used as-is via APIs. As a result, the skillset for ML engineers started to include how to leverage and integrate these foundation models (sometimes via simple API calls) rather than only how to build new models. In fact, by 2024 some discussions suggested that ML/MLOps engineers should focus on integrating foundation models via their APIs - treating the model as a service - rather than reinventing the wheel.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#22-model-size-and-computational-demands","title":"2.2. Model Size and Computational Demands","text":"<p>The sheer scale of these models introduced new challenges:</p> <p>A model with billions of parameters cannot be handled with the same infrastructure as a model with millions. Training and even just deploying such models require powerful hardware (GPUs, TPUs) and often distributed computing. This gave rise to new techniques like model parallelism (sharding a single model across multiple GPUs) and distributed data parallel training (synchronizing multiple GPU workers). Libraries and optimizations like DeepSpeed and ZeRO (Zero Redundancy Optimizer) were developed to make training giant models feasible. Even for inference, serving a large model often meant using multiple GPUs or specialized inference runtimes to keep latency acceptable.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#23-emergence-of-llmops","title":"2.3. Emergence of LLMOps","text":"<p>It became clear that operating these large models in production required extensions to classic MLOps - leading to what many call LLMOps (Large Language Model Ops). LLMOps is essentially MLOps specialized for large models, especially LLMs. It builds on the same principles but \"addresses the unique challenges of deploying large language model... These models require substantial computational resources, prompt engineering, and ongoing monitoring to manage performance, ethics, and latency\". In other words, things that barely registered as issues for smaller models (like a single model's inference possibly producing biased text or leaking private training data) became major considerations when using LLMs at scale.</p> <p></p> <p>This diagram from NVIDIA illustrates how general MLOps (outer circle) has branched into specialized subfields like generative AI operations (for all generative models), LLMOps (for large language models), and even RAGOps for retrieval-augmented generation. The concentric circles indicate that these specializations build on the foundation of classic MLOps.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#24-foundation-models-as-a-service","title":"2.4. Foundation Models as a Service","text":"<p>Another shift was the increasing availability of models via API or model hubs. Companies like OpenAI, Cohere, and AI21 Labs offered hosted LLMs accessible through an API call, which many applications use directly rather than deploying their own model. Likewise, the open-source community (notably Hugging Face) created hubs where thousands of pretrained models can be downloaded or even run in the cloud. This changed ML system architecture - a production pipeline might call out to an external API for inference, which introduces new considerations (latency, cost, data privacy) but saves the effort of managing the model's infrastructure. Even big cloud platforms integrated foundation models: e.g. Google's Vertex AI added Model Garden to provide pretrained LLMs and diffusion models out-of-the-box, with tools to fine-tune them on custom data, all managed on Google's infrastructure.</p> <p>In essence, the rise of foundation models shifted ML development from a paradigm of \"your data + your model code = trained model\" to \"your data + adaptation of a massive pre-trained model = fine-tuned model (or sometimes just prompt the model with your data).\". It also meant MLOps had to handle far more computationally intensive processes and new workflows like prompt engineering and continuous monitoring for things like ethical compliance.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#3-new-requirements-and-capabilities-in-modern-ml-infrastructure","title":"3. New Requirements and Capabilities in Modern ML Infrastructure","text":"<p>With foundation models and LLMs becoming central, today's ML infrastructure must support capabilities that were niche or non-existent in the past. Let's discuss some of the key new requirements and how they differ from the classic era:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#31-distributed-training-and-model-parallelism","title":"3.1. Distributed Training and Model Parallelism","text":"<p>Training a model with hundreds of millions or billions of parameters is beyond the capacity of a single machine in most cases. Modern ML infrastructure often needs to orchestrate distributed training, where either the data or the model is split across multiple nodes. Model parallelism in particular is critical for large models - it involves splitting the model's layers or parameters across multiple GPUs or TPUs so that each accelerates a part of the model. Frameworks like PyTorch Lightning, Horovod, and libraries from hardware vendors (NVIDIA's Megatron-LM, Google's JAX/TPU ecosystem) help manage this. This contrasts with a few years ago when most teams could train models on a single server or small cluster without these complexities. Now, an ML platform is expected to handle launching jobs on GPU clusters, managing faults, and aggregating gradients from many workers seamlessly.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#32-efficient-fine-tuning-techniques","title":"3.2. Efficient Fine-Tuning Techniques","text":"<p>As training from scratch is often impractical with huge models, fine-tuning is the name of the game. However, even fine-tuning a multi-billion parameter model on new data can be extremely resource-intensive. This has led to new techniques like LoRA (Low-Rank Adaptation) - a method that \"reduces the computational cost of fine-tuning LLMs by updating only a small subset of the model's parameters (adapters) instead of the entire network\". Fine-tuning today might also involve methods like Prompt Tuning or Adapter modules, which allow adding task-specific capabilities without full re-training. ML infrastructure now must support these workflows - for example, loading a giant base model from a model hub, applying a delta of fine-tuned weights, and deploying that combined model. Traditional training pipelines had to evolve significantly to accommodate such multi-step model customization.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#33-prompt-engineering-management","title":"3.3. Prompt Engineering &amp; Management","text":"<p>One surprising new \"artifact\" in modern ML pipelines is the prompt. When using LLMs, a lot of the model's behavior is controlled through the text prompt or input format given to it. Engineering these prompts (and possibly chains of prompts) has become a new discipline. Teams now maintain prompt libraries or templates, and even use version control and A/B testing for prompts similar to code. This is quite different from classical ML, where the input to a model was usually just data features without this intermediate layer of natural language instructions. As a result, some MLOps setups treat prompts as a configurable part of the deployment - possibly storing prompt versions alongside model versions, and using prompt-management tools. We even see early frameworks that facilitate prompt design and optimization as first-class citizens of the ML pipeline (for example, prompt optimization modules in LangChain or prompt testing frameworks).</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#34-retrieval-augmented-generation-rag","title":"3.4. Retrieval-Augmented Generation (RAG)","text":"<p>Foundation models like GPT-3 have a fixed knowledge cutoff (training data) and context window. To keep responses up-to-date and accurate with external knowledge, a common pattern called retrieval-augmented generation has emerged. RAG involves using a vector database or search index to retrieve relevant documents which are then provided to the LLM as additional context (usually appended to the prompt). This pattern was rare a few years ago, but now it's \"becoming a best practice for improving LLM applications\", as noted in industry discussions. Instead of continuously retraining the model on new data (which is costly and slow), RAG allows the model to fetch information at query time. ML infrastructure has adapted by integrating new components: vector databases (like Pinecone, Weaviate, FAISS, or Milvus) are now part of the stack to handle fast similarity search on embeddings. Managing these embedding indexes and keeping them in sync with the latest data is a new responsibility for MLOps. In fact, in generative AI pipelines, \"using embeddings and vector databases replaces feature stores that were relevant to classic MLOps\", since unstructured data and semantic search have taken center stage over manual feature engineering.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#35-data-streaming-and-real-time-data-feeds","title":"3.5. Data Streaming and Real-Time Data Feeds","text":"<p>Many modern applications (especially those involving LLM-powered assistants or real-time personalization) continuously ingest data - chat conversations, sensor data, event streams - and need to update either the model's knowledge (via RAG) or trigger model responses in real-time. While classic ML pipelines often assumed periodic batch processing of data (e.g. a daily training job), today's systems might need to handle streaming data in real-time. This has led to increased use of technologies like Kafka or real-time databases in ML pipelines, and online feature stores or caches that update continuously. The boundary between data engineering and MLOps blurs further when dealing with streaming data for model consumption.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#36-scalable-and-specialized-serving-infrastructure","title":"3.6. Scalable and Specialized Serving Infrastructure","text":"<p>Serving a massive model is a challenge in itself. Modern ML infrastructure must support:</p> <ul> <li>High-Throughput, Low-Latency Serving</li> </ul> <p>For applications like interactive chatbots or image generators, users expect prompt responses. This often requires serving infrastructure that can utilize GPUs (or specialized ASICs like TPUs) to perform inference quickly. Techniques like model quantization (reducing precision to speed up inference) and GPU batching (serving multiple requests in parallel on one GPU) are employed. Some companies use model-specific serving optimizations, for example NVIDIA's TensorRT or Triton Inference Server for optimized GPU inference, or DeepSpeed-Inference for accelerated transformer model serving.</p> <ul> <li>Serverless and Elastic Scaling</li> </ul> <p>Interestingly, we see a trend toward serverless ML services for inference. Platforms like Modal have emerged, which \"is similar to AWS Lambda but with GPU support - a serverless platform where you provide the code and they handle infrastructure and scaling for you\". In such a setup, you don't have an always-running server for your model; instead, the platform spins up compute (with GPUs if needed) on-demand to handle requests, scaling to zero when idle. This is a departure from the always-on, containerized microservice model of the past. It promises cost savings (pay only per execution) and easier scaling, though one has to manage cold-start latency and statelessness in these systems. Modern MLOps may leverage such serverless inference for irregular workloads or use cases where managing GPU clusters is overhead.</p> <ul> <li>Distributed Model Serving</li> </ul> <p>If a model is too large for one machine or one GPU to serve, inference itself can be distributed. There are frameworks to shard the model across multiple machines for serving (similar to training) so that each handles part of the forward pass. This is complex, but needed for extreme cases (like serving a 175B parameter GPT-3 model on-premises might require multiple GPUs working together). Today's ML infra must be capable of launching such distributed inference replicas and routing requests appropriately.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#37-monitoring-observability-and-guardrails","title":"3.7. Monitoring, Observability, and Guardrails","text":"<p>With great power comes great responsibility - large models can generate incorrect or inappropriate outputs in ways small models typically did not. Modern ML systems need nuanced monitoring:</p> <ul> <li>Performance and Reliability</li> </ul> <p>Of course, we still monitor latency, throughput, memory usage, etc., since large models can be resource hogs. Ensuring an LLM-based service meets an SLA might involve autoscaling GPUs, or falling back to a smaller model if load is high.</p> <ul> <li>Output Quality and Safety</li> </ul> <p>We now also monitor the content of model outputs. For example, filtering for hate speech, PII, or other harmful content is a standard part of deploying generative models. Many pipelines include an automated moderation step - e.g., OpenAI's moderation API or custom filters - to catch problematic outputs. Bias evaluation tools might run in the background to detect drift in the model's responses or flag when the model starts producing biased results. This is part of \"guardrails\" that have become essential in LLMOps, intercepting adversarial inputs and ensuring outputs stay within acceptable bounds.</p> <ul> <li>Feedback Loops</li> </ul> <p>The notion of continuous improvement has extended to user feedback on model outputs. Modern MLops may incorporate a human feedback loop or at least a mechanism to collect user interactions (likes, corrections) with the model's outputs. This data can then be used to further fine-tune the model or adjust prompts. In the LLM world, techniques like Reinforcement Learning from Human Feedback (RLHF) explicitly use human ratings to refine model behavior. So the infrastructure must support collecting and managing this feedback data securely and effectively.</p> <p>In summary, today's ML infrastructure goes far beyond training and deploying a single model artifact. It needs to manage entire ecosystems of model components - the base model, fine-tuning adapters, prompt templates, retrieval indexes, monitoring detectors, and more - and orchestrate them to work together. The complexity is higher, but so is the capability unlocked.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#4-evolving-system-architecture-and-design-patterns","title":"4. Evolving System Architecture and Design Patterns","text":"<p>Given those new requirements, how are ML system architectures structured today? Let's highlight a few design patterns and compare them to earlier approaches:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#41-modular-pipelines-orchestration","title":"4.1. Modular Pipelines &amp; Orchestration","text":"<p>Orchestration frameworks from the past are still around - you might use Kubeflow Pipelines or Apache Airflow/Beam to orchestrate the fine-tuning process or batch scoring jobs. But for inference time orchestration (which needs low latency), lightweight frameworks or application code often replace heavyweight workflow engines. Also, new MLOps orchestration tools (like Metaflow, Flyte, or ZenML) have gained traction by focusing on Pythonic workflows that integrate well with modern ML libraries. They help manage the flow from data to deployment without forcing engineers to step out of their normal development environment.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#42-model-hubs-and-registries","title":"4.2. Model Hubs and Registries","text":"<p>Model management has evolved with the rise of hubs like Hugging Face. Instead of every team hosting their own model registry, many share and fetch models from centralized hubs. Internally, companies still maintain model registries (MLflow Registry, Amazon SageMaker Model Registry, etc.) for their bespoke models, but they might also pull foundation models from an external source. Hugging Face Hub not only hosts thousands of models but also versioned datasets and scripts, becoming a one-stop shop for ML components. This encourages a more plug-and-play architecture - e.g., a sentiment analysis service might directly fetch a pre-trained model checkpoint from a hub at startup. The ease of discovering and sharing models has accelerated the pace of development and also influenced design: engineers now plan for how to fine-tune and update third-party models rather than building everything in-house.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#43-feature-stores-vs-vector-databases","title":"4.3. Feature Stores vs. Vector Databases","text":"<p>As mentioned, the importance of traditional feature stores has slightly declined in apps where text and images are primary data. In their place, vector databases have become a new pillar in the architecture. Vector DBs (such as Pinecone, Weaviate, Chroma, or Milvus) are specialized for storing high-dimensional embeddings and performing similarity search quickly. They are used for semantic search, deduplication, recommendation, and as part of RAG for LLMs. In modern pipelines, you might see a vector DB alongside a more classical data warehouse - the former serving unstructured semantic lookup needs, the latter serving structured data analytics. An ML system might vectorize incoming data using an embedding model and continually update the vector index, enabling any LLM queries to fetch relevant context. This is a new pattern that didn't exist in older MLOps, and it's now quite common for AI applications dealing with text or image data.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#44-unified-platforms-end-to-end","title":"4.4. Unified Platforms (End-to-End)","text":"<p>The complexity of handling all these pieces has given momentum to end-to-end ML platforms that abstract many details. Cloud platforms like Google Vertex AI, AWS SageMaker, Azure Machine Learning have each evolved to support foundation model workflows. For example, Vertex AI offers training services that automatically distribute models across TPU pods, hosts a Model Garden with popular LLMs available, and provides endpoints to deploy models with one click (including scaling on GPUs). They also integrate data tools and monitoring for drift, etc. Similarly, SageMaker has added features for large model training (distributed training jobs, model parallel libraries) and even hosts proprietary models via its Bedrock service. These platforms embody the evolved best practices, providing building blocks like \"fine-tune this 20B parameter model on your data\" or \"embed and index your text data for retrieval\" as managed services. Meanwhile, open-source initiatives and startups also offer integrated solutions - for instance, MosaicML (now part of Databricks) provided tooling to train and deploy large models efficiently, Argilla and Label Studio help with data labeling and prompt dataset creation for LLMs, and ClearML or MLflow tie together experiment tracking with pipeline execution.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#45-inference-gateways-and-apis","title":"4.5. Inference Gateways and APIs","text":"<p>The proliferation of models and model sizes has led to architectures that include a model inference gateway or router. Companies might deploy multiple models (e.g., a small fast model and a large accurate model) and route requests to one or the other based on context (latency requirements, user subscription level, etc.). There are open-source tools and design patterns for these gateways, sometimes using a service mesh or simple web services to forward requests. The idea is to decouple the client-facing API from the actual model implementation behind it. This also helps in A/B testing models - a fraction of traffic can be served by a new model to compare outcomes. In legacy setups, one might simply deploy a new model at a new REST endpoint for testing. Now, more sophisticated routing is common.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#46-agentic-systems","title":"4.6. Agentic Systems","text":"<p>A cutting-edge pattern is the rise of \"agent\" architectures, where an AI system can dynamically choose sequences of actions (invoking different models or tools) to accomplish a task. This goes beyond static chains. For example, an AI agent might decide it needs to call an external calculator or search engine in the middle of answering a query. Frameworks enabling this (like LangChain's agent mode, OpenAI's function calling, etc.) are nascent, but point towards future ML systems that are even more complex - essentially a workflow decided at runtime by the model. MLOps is beginning to account for such systems (sometimes dubbed \"AgentOps\" in concept), which require robust monitoring to ensure the agent doesn't take unwanted actions and logging to trace its decisions. While not widespread in production yet, this is an emerging design pattern fueled by the flexibility of large models.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#5-conclusion-from-mlops-to-llmops-and-beyond","title":"5. Conclusion: From MLOps to LLMOps and Beyond","text":"<p>In just a few years, we've witnessed a transformation in how we approach machine learning in production. Classic MLOps principles - automation, reproducibility, collaboration between data science and engineering - still apply, but they have been extended and reshaped to handle the scale and scope of modern ML tasks. Large foundation models brought incredible capabilities, but also complexity that demanded new solutions. This gave rise to what is now called LLMOps, a specialization of MLOps, to manage the lifecycle of these powerful models. It's not just hype - the differences are tangible in day-to-day workflows, from how we fine-tune models, to how we deploy and monitor them with new infrastructure components (like vector databases or GPU clusters).</p> <p>The evolution is ongoing. As models continue to grow and as AI systems become more \"agentic\" or autonomous, we'll likely see further specialization (there's already talk of \"AgentOps\" for AI agents). However, the end goal remains the same: to reliably deliver the benefits of machine learning to end-users and business applications, at scale and with trustworthiness.</p> <p>Teams that successfully navigate this evolution are able to harness foundation models to build products faster than ever - while maintaining the reliability and efficiency that good operations provide.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#references","title":"References","text":"<p>The insights and examples in this post are supported by recent research and industry sources, including an MDPI review on transitioning from MLOps to LLMOps, NVIDIA's technical blogs on GenAIOps and LLMOps, and various practitioner articles and discussions capturing the state of ML in 2024. Platforms like Modal and Ray have published guides showing new deployment patterns (serverless GPUs, distributed serving) in action.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/","title":"Quick-Guide on ~/.zprofile vs ~/.zshrc \ud83d\ude80","text":"","tags":["tooling","macos","guide"]},{"location":"blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/#tldr","title":"TL;DR \u26a1","text":"<ul> <li><code>~/.zprofile</code> \u2192 one-shot, login-shell initialization (think \"environment/bootstrap\") \ud83d\udd27</li> <li><code>~/.zshrc</code> \u2192 every interactive prompt (think \"daily driving experience\") \ud83c\udfae</li> </ul> <p>Use both in tandem: keep your environment reliable with <code>~/.zprofile</code>, and your shell pleasant and tweakable with <code>~/.zshrc</code> \u2728</p>","tags":["tooling","macos","guide"]},{"location":"blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/#1-two-kinds-of-z-shells","title":"1. Two Kinds of Z-shells \ud83d\udc1a","text":"Shell starts as\u2026 Technically called Typical triggers Reads first Purpose Login shell login \u2022 New terminal tab/window on macOS\u2022 <code>ssh user@host</code>\u2022 <code>zsh -l</code> <code>~/.zprofile</code> One-time, \"session-wide\" setup Interactive shell interactive \u2022 Every prompt you see after the shell is running <code>~/.zshrc</code> Live, inter-prompt behavior <p>Note: A login shell is also interactive, but only login shells run the \"login-only\" startup files.</p>","tags":["tooling","macos","guide"]},{"location":"blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/#2-what-goes-where","title":"2. What Goes Where \ud83d\udcc1","text":"Put in <code>~/.zprofile</code> (login-only) \ud83d\udd10 Put in <code>~/.zshrc</code> (every prompt) \ud83d\udd04 \u2022 <code>export PATH=\u2026</code> and other environment variables that downstream programs need before they launch (e.g., GUI apps on macOS) \u2022 Aliases, functions, and shell options (<code>setopt autocd</code>) \u2022 <code>eval \"$(pyenv init -)\"</code>, NVM, ASDF, etc.\u2014tools that must adjust <code>$PATH</code> early \u2022 Prompt theming (<code>POWERLEVEL9K_*</code>), key-bindings, completion tweaks \u2022 <code>ulimit</code>, <code>ssh-agent</code> start-up, <code>launchctl</code> tweaks \u2022 History settings, auto-suggest-highlight, <code>bindkey</code> mappings \u2022 Anything you want to run once per terminal/SSH login \u2022 Anything you want to re-load with <code>source ~/.zshrc</code> without logging out","tags":["tooling","macos","guide"]},{"location":"blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/#3-practical-rules-of-thumb","title":"3. Practical Rules of Thumb \ud83d\udcdd","text":"<ol> <li> <p>Session vs. Prompt \u23f1\ufe0f    If it should happen once per session, choose <code>~/.zprofile</code>; if it should affect every new prompt, choose <code>~/.zshrc</code>.</p> </li> <li> <p>PATH-mangling early? \ud83d\udee3\ufe0f    Put it in <code>~/.zprofile</code> so every child process inherits it.</p> </li> <li> <p>Re-sourcing convenience? \ud83d\udd04    Keep interactive tweaks in <code>~/.zshrc</code>; you can iterate without closing the terminal.</p> </li> <li> <p>Platform nuance \ud83d\udcbb</p> </li> <li> <p>macOS Terminal &amp; iTerm2 open login shells by default \u21d2 both files run.</p> </li> <li> <p>Linux desktop terminals (GNOME, Kitty, Alacritty) usually start non-login shells \u21d2 only <code>~/.zshrc</code> runs; add <code>source ~/.zprofile</code> to <code>~/.zshrc</code> if you need login code there.</p> </li> <li> <p>Remote scripts (<code>#!/usr/bin/env zsh -l</code>)\u2014use a login shell if you rely on <code>~/.zprofile</code> \ud83c\udf10</p> </li> </ol>","tags":["tooling","macos","guide"]},{"location":"blog/2025/05/07/quick-guide-on-zprofile-vs-zshrc-/#4-minimal-template","title":"4. Minimal Template \ud83d\udccb","text":"<pre><code># ~/.zprofile  --------------------------------\n# Environment &amp; once-per-session setup\nexport PATH=\"$HOME/.local/bin:$PATH\"\neval \"$(fnm env)\"        # Node version manager\n\n# ~/.zshrc  -----------------------------------\n# Prompt, aliases, keybindings\nautoload -Uz promptinit; promptinit\nprompt pure\nalias gs='git status'\nbindkey -e\n</code></pre>","tags":["tooling","macos","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/","title":"Quick-Guide on <code>pyproject.toml</code>","text":"","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#tldr","title":"TL;DR","text":"<p>Think of <code>pyproject.toml</code> as the <code>package.json</code> for Python. Whether you prefer <code>.venv</code>, <code>pyenv</code>, or <code>uv</code>, putting all your project's metadata, dependencies, and tooling into one tidy TOML file simplifies development and boosts collaboration.</p>","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#1-what-is-pyprojecttoml","title":"1. What is <code>pyproject.toml</code>?","text":"<p><code>pyproject.toml</code> is a single, PEP-backed configuration file (TOML-format) that sits at the root of your repository.</p> <ul> <li>PEP 518 introduced it so that build tools could declare their requirements in a standard way via a <code>[build-system]</code> table.</li> <li>PEP 621 later standardised a <code>[project]</code> table for core package metadata (name, version, dependencies, etc.).</li> </ul> <p>Beyond packaging, many developer tools (Black, isort, pytest, Ruff, etc.) now read their settings from dedicated <code>[tool.*]</code> sections, so the file has become a universal \"one-stop\" project manifest.</p>","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#2-why-should-you-care","title":"2. Why should you care?","text":"<ul> <li>One file to rule them all - no more juggling <code>setup.py</code>, <code>setup.cfg</code>, <code>requirements.txt</code>, <code>MANIFEST.in</code>, and dotfiles.</li> <li>Backend-agnostic builds - pip reads <code>pyproject.toml</code> and installs required build tools automatically.</li> <li>Tooling ecosystem - linters, formatters, test runners, and type checkers agree on where to look for config.</li> </ul>","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#3-does-it-replace-requirementstxt","title":"3. Does it replace <code>requirements.txt</code>?","text":"<p>Mostly, yes. Modern tools like Poetry, PDM, Hatch, and uv store dependencies in <code>[project]</code> and use lockfiles for reproducibility. <code>requirements.txt</code> is only needed for legacy tools or simple CI.</p>","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#4-anatomy-at-a-glance","title":"4. Anatomy at a glance","text":"<pre><code>[build-system]\nrequires = [\"setuptools&gt;=69\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"awesome-app\"\nversion = \"0.1.0\"\ndescription = \"Short demo of pyproject.toml\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n  \"fastapi&gt;=0.111\",\n  \"uvicorn[standard]&gt;=0.30\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\", \"black\", \"ruff\"]\n\n[tool.black]\nline-length = 100\ntarget-version = [\"py312\"]\n\n[tool.isort]\nprofile = \"black\"\n</code></pre>","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#5-virtual-environments-pyprojecttoml","title":"5. Virtual environments &amp; <code>pyproject.toml</code>","text":"<p><code>pyproject.toml</code> does not create a venv itself; it declares what to install. How you create/activate an environment is up to you:</p> Tool Command Uses <code>pyproject.toml</code>? Plain Python <code>python -m venv .venv</code> Manual install with pip pyenv <code>pyenv virtualenv 3.12.2 env</code> Yes, if activated correctly uv <code>uv venv</code> or <code>uv add ...</code> Yes, automatic + fast","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#6-popular-real-world-uses","title":"6. Popular real-world uses","text":"Use Section Tools Why people like it Packaging <code>[project]</code> build, twine, uv Build once with python -m build, upload with twine or uv publish Dependencies <code>[project]</code> + lock Poetry, PDM, uv Reproducible installs; uv sync is very fast Formatting <code>[tool.black]</code> Black Keeps formatter settings in-repo Sorting imports <code>[tool.isort]</code> isort One config shared between IDE and CI Testing <code>[tool.pytest.ini_options]</code> pytest No more pytest.ini Typing <code>[tool.mypy]</code> mypy Optional if you prefer one file over mypy.ini","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#7-typical-workflow","title":"7. Typical workflow","text":"<p>New project with <code>uv</code>:</p> <pre><code>uv init my_app          # scaffolds folder, pyproject.toml and .venv\ncd my_app\nuv add requests fastapi # adds deps to [project] and installs\nuv run pytest           # runs inside the same venv\nuv build                # builds wheel/sdist using build-system table\n</code></pre> <p>Existing project:</p> <ol> <li>Add a minimal [build-system] to declare setuptools&gt;=61 and wheel.</li> <li>Move metadata and runtime deps into [project] (PEP 621).</li> <li>Convert dev-deps to [project.optional-dependencies].dev.</li> <li>Drop requirements.txt or generate it from the lock file for legacy CI.</li> <li>Configure tools under [tool.*].</li> </ol>","tags":["python","guide"]},{"location":"blog/2025/05/08/quick-guide-on-pyprojecttoml/#8-cheat-sheet","title":"8. Cheat-sheet","text":"Task Snippet Use Flit instead of Setuptools <code>[build-system]</code><code>requires=[\"flit_core&gt;=3.2\"]</code><code>build-backend=\"flit_core.buildapi\"</code> Pin Python version <code>[project]</code><code>requires-python=\"&gt;=3.12\"</code> Enable Black formatting <code>[tool.black]</code><code>line-length=88</code> Add test dependencies <code>[project.optional-dependencies]</code><code>test=[\"pytest\",\"coverage\"]</code> Export lockfile requirements <code>uv export &gt; requirements.txt</code>","tags":["python","guide"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/","title":"Quick-guide on Local Stable-Diffusion Toolkits for macOS","text":"<p>Running generative-AI models on-device means zero cloud costs, no upload limits, and full control of your checkpoints. Below is a quick guide to five of the most popular macOS-ready front-ends and launchers.</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#1-comfyui","title":"1. ComfyUI","text":"<ul> <li>Download: https://www.comfy.org/download</li> <li>What it is: A node-based graph editor that lets you wire together samplers, LoRA loaders, ControlNet, animation nodes and more.</li> <li>Pros<ul> <li>Visual graph makes complex pipelines transparent.</li> <li>Ships with MPS-enabled PyTorch wheels; smooth on M-series Macs.</li> <li>Huge community of custom nodes.</li> </ul> </li> <li>Cons<ul> <li>Steeper learning curve than point-and-click UIs.</li> <li>Initial setup still requires Python &amp; Homebrew.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#2-stable-diffusion-webui-automatic1111","title":"2. Stable Diffusion WebUI (AUTOMATIC1111)","text":"<ul> <li>Download / install guide: Installation on Apple Silicon</li> <li>What it is: The de-facto standard web interface, with extensions for almost everything (SDXL, ControlNet, Inpainting, DreamBooth, etc.).</li> <li>Pros<ul> <li>Feature-rich; thousands of extensions &amp; themes.</li> <li>Active development and community support.</li> </ul> </li> <li>Cons<ul> <li>Manual, terminal-centric install (Git + Python + brew deps).</li> <li>UI can feel cluttered for newcomers.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#3-diffusionbee","title":"3. DiffusionBee","text":"<ul> <li>Download: https://diffusionbee.com/download</li> <li>What it is: A one-click desktop app bundling Stable Diffusion, optimized for Apple Silicon.</li> <li>Pros<ul> <li>No command line - drag-and-drop install.</li> <li>Pre-bundled models; useful \"Upscale\" &amp; \"Remove BG\" tools.</li> </ul> </li> <li>Cons<ul> <li>Fewer tuning knobs; limited advanced workflows.</li> <li>Closed binary means slower updates to new samplers.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#4-invokeai","title":"4. InvokeAI","text":"<ul> <li>Download / quick-start: InvokeAI Quick Start</li> <li>What it is: A professional-leaning fork of the original \"lstein\" repo with both CLI and lightweight web UI.</li> <li>Pros<ul> <li>Powerful batch / workflow scripting.</li> <li>Good \"Unified Canvas\" for sketch-to-image iterations.</li> </ul> </li> <li>Cons<ul> <li>Conda-based install (\u22484 GB environment).</li> <li>Heavier RAM needs (recommend 16 GB+).</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#5-fooocus","title":"5. Fooocus","text":"<ul> <li>Repo: https://github.com/lllyasviel/Fooocus</li> <li>What it is: A simplified, Midjourney-style frontend (\"just prompt\") that auto-downloads models &amp; Loras.</li> <li>Pros<ul> <li>Minimal interface - great for fast idea-sketching.</li> <li>Automatic model &amp; VAE handling.</li> </ul> </li> <li>Cons<ul> <li>On-device generation slower on M-series (no discrete GPU).</li> <li>Fewer granular controls than A1111 or ComfyUI.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#side-by-side-snapshot","title":"Side-by-side snapshot","text":"Tool Install effort UI / Workflow Apple Silicon speed* Best for ComfyUI Medium (Python, Homebrew) Node-graph editor \u2605\u2605\u2605\u2605\u2606 Building custom pipelines A1111 WebUI High (manual CLI) Web tabs + extensions \u2605\u2605\u2605\u2606\u2606 Power users, extensions DiffusionBee One-click DMG Native app panels \u2605\u2605\u2605\u2606\u2606 Beginners, offline \"fire-and-forget\" InvokeAI Medium-high (Conda) Web + CLI + Canvas \u2605\u2605\u2605\u2606\u2606 Batch scripts, in-painting Fooocus Medium (Python zip) Minimal prompt box \u2605\u2605\u2606\u2606\u2606 Fast concepting, MJ-style <p>*Rating is relative to other Apple-Silicon solutions; all use PyTorch-MPS and run without Nvidia GPUs.</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#which-one-should-you-pick","title":"Which one should you pick?","text":"<ul> <li>New to Stable Diffusion? Start with DiffusionBee - no terminals, no dependencies.</li> <li>Need maximum control &amp; plug-ins? Go with AUTOMATIC1111 WebUI.</li> <li>Love visual programming or complex workflows (videos, LoRAs, ControlNet chains)? ComfyUI is unmatched.</li> <li>Want a production-friendly canvas and scripting? InvokeAI.</li> <li>Just want Midjourney-like simplicity offline? Fooocus.</li> </ul> <p>Because every app can load the same <code>.safetensors</code> checkpoints, you're free to test-drive a few and stick with the one that best matches your creative flow. Happy prompting!</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/","title":"Quick-guide on Running LLMs Locally on macOS","text":"<p>This guide compares the five most popular local toolchains, complete with download links, quick overviews, and pros &amp; cons. A comparison table follows for easy reference.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#1-ollama","title":"1. Ollama","text":"<p>Download: https://ollama.com/download/mac</p> <p>Ollama wraps <code>llama.cpp</code> in a slick native menu-bar app and CLI. It auto-downloads/quantises models (Llama 3, Mistral, Gemma \u2026\u200b) and speaks Apple Metal out of the box. Requires macOS 11+.</p> <p>Pros</p> <ul> <li>Zero-config install (drag-and-drop <code>.dmg</code>)</li> <li>GUI and script-friendly CLI (<code>ollama run \u2026</code>)</li> <li>Curated model library; automatic updates</li> </ul> <p>Cons</p> <ul> <li>Closed-source core (only the model files &amp; starter projects are OSS)</li> <li>Limited tuning - no token streaming API yet</li> <li>~3 GB disk footprint after first launch</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#2-lm-studio","title":"2. LM Studio","text":"<p>Download: https://lmstudio.ai</p> <p>A cross-platform GUI that bundles an open-source CLI/SDK, plus Apple-only MLX acceleration. You get a model catalogue, a local inference server, and simple RAG chat with your files.</p> <p>Pros</p> <ul> <li>Friendly \"App-Store\" model browser</li> <li>Ships both GUI and MIT-licensed SDK (Python &amp; JS)</li> <li>Runs GGUF or MLX models, ideal for Apple-silicon GPUs</li> </ul> <p>Cons</p> <ul> <li>GUI itself is closed source</li> <li>Heavier install (~750 MB); Intel Macs need Rosetta</li> <li>Fewer advanced CLI flags than raw <code>llama.cpp</code></li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#3-llamacpp","title":"3. llama.cpp","text":"<p>Repo: https://github.com/ggml-org/llama.cpp</p> <p>The reference C/C++ project behind most local LLM front-ends. Compile once via Homebrew/CMake and you have maximum control - quantisation, streaming, batching - direct from Terminal.</p> <p>Pros</p> <ul> <li>Fastest path to bleeding-edge features (updated daily)</li> <li>Full CLI flag set; linkable C API &amp; Python bindings</li> <li>Lean (&lt; 30 MB build) and truly open source (MIT)</li> </ul> <p>Cons</p> <ul> <li>Steeper learning curve (manual model downloads, GGUF knowledge required)</li> <li>No GUI - bring your own front-end</li> <li>Occasional breaking changes on master</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#4-gpt4all-desktop","title":"4. GPT4All Desktop","text":"<p>Download: https://gpt4all.io</p> <p>A Qt-based chat client from Nomic. One click fetches a model (Llama 3, DeepSeek, Nous-Hermes, etc.) and you're chatting offline. Also doubles as an OpenAI-compatible local server.</p> <p>Pros</p> <ul> <li>Privacy-first (all data stays local)</li> <li>Built-in \"LocalDocs\" RAG panel</li> <li>MIT-licensed core &amp; growing plugin ecosystem</li> </ul> <p>Cons</p> <ul> <li>GUI only - no headless mode yet</li> <li>Heavier RAM use than Ollama/LM Studio</li> <li>Fewer nerd knobs for quantisation or GPU tuning</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#5-koboldcpp","title":"5. KoboldCPP","text":"<p>Repo: https://github.com/LostRuins/koboldcpp</p> <p>A single-file, zero-install fork of <code>llama.cpp</code> aimed at storytellers (derives from the KoboldAI interface). Universal binaries are provided for M-series Macs; just <code>chmod +x</code> and run.</p> <p>Pros</p> <ul> <li>One executable - no CMake, no Brew</li> <li>Web UI tuned for long-form creative writing</li> <li>Supports mix-precision GGUF and GPU acceleration</li> </ul> <p>Cons</p> <ul> <li>Niche UI; less general-purpose than others</li> <li>AGPL-3 licence (copyleft) may deter commercial use</li> <li>Smaller maintainer team \u2192 slower feature parity with upstream</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#side-by-side-cheat-sheet","title":"Side-by-Side Cheat-Sheet","text":"Tool Interface Install Effort Apple-GPU / MLX Licence Best For Ollama Menu-bar app + CLI 1-click <code>.dmg</code> \u2714 (Metal) Proprietary core \"It should just work\" LM Studio Rich GUI + SDK 1-click (<code>.dmg</code>) \u2714 (Metal + MLX) MIT SDK / closed GUI Devs who want GUI and code API llama.cpp CLI / C API <code>brew install cmake</code> \u2714 (Metal) MIT Power users &amp; tinkerers GPT4All Desktop chat 1-click (.pkg) \u2714 MIT Privacy-first chat &amp; RAG KoboldCPP Web/CLI hybrid Download binary \u2714 AGPL-3 Fiction &amp; role-play sessions","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#choosing-in-one-minute","title":"Choosing in One Minute","text":"<ul> <li>Need the fastest path from idea \u2192 prompt? Pick Ollama.</li> <li>Prefer a full GUI and Python hooks? Go LM Studio.</li> <li>Want total control, scripting, or to embed an LLM in your own app? Compile llama.cpp.</li> <li>Just want a local ChatGPT-style app with zero cloud? GPT4All is the most polished.</li> <li>Writing interactive fiction? KoboldCPP has scene-and-memory features the others lack.</li> </ul> <p>Whichever route you choose, all five options run comfortably on Apple-silicon laptops and let you keep your data - and your GPU cycles - entirely on-device. Happy prompting!</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/","title":"Choosing the Right Open-Source LLM Variant &amp; File Format","text":"","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#1-why-all-these-tags-exist","title":"1. Why all these tags exist","text":"<p>Open-source LLMs are shipped in two axes of variation:</p> <ol> <li>Training / fine-tuning style \u2013 the suffixes you see in model names (<code>-Instruct</code>, <code>-Distill</code>, <code>-A3B</code>, \u2026) tell you how the checkpoint was produced and what it's good at.</li> <li>File &amp; quantization format \u2013 the extension (<code>.gguf</code>, <code>.gptq</code>, \u2026) tells you how the weights are packed for inference on different hardware.</li> </ol> <p>Understanding both axes lets you avoid downloading 20 GB for nothing or fighting CUDA errors at 3 a.m.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#2-model-type-cheat-sheet","title":"2. Model-type cheat sheet","text":"Tag What it means (plain English) Typical use-cases Trade-offs Base Raw pre-trained LLM, not aligned to human instructions. Further fine-tuning, research, creative (unfiltered) generation. Needs prompts wrapped carefully; can refuse or ramble. Instruct / Chat Extra supervised + RLHF rounds so the model follows instructions. Agents, RAG, chatbots, function-calling, day-to-day coding help. Slightly larger, a bit slower; may be less creative. Distill / Distilled A smaller \"student\" model trained to mimic a big \"teacher.\" Mobile / edge devices, cost-sensitive SaaS, latency-critical endpoints. Some loss in reasoning depth; great token-per-watt ratio. QAT Quantization-Aware Training: the model was re-trained while already in low-bit form. When you need 4- or 8-bit weights and near-FP16 accuracy (e.g., on consumer GPUs/CPUs). Training cost is higher than plain post-training quantization. A3B / A22B (MoE) Mixture-of-Experts with Activated X B parameters (e.g., \"A3B\" = 3 B active out of 30 B total). You want \"big-brain\" performance but only pay inference for a slice of it - ideal for local GPUs with \u226424 GB VRAM. Heavier disk size; not every framework supports MoE yet. <p>Rule of thumb:</p> <ul> <li>_Start with an Instruct model.</li> <li>If you hit latency or memory limits, drop to a Distill or A3B MoE variant.</li> <li>If you still need more speed, look for models explicitly tagged QAT or download a lower-bit quantized file._</li> </ul>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#3-picking-a-file-quantization-format","title":"3. Picking a file / quantization format","text":"Format Best for Highlights GGUF (<code>.gguf</code>) General local inference (CPU or GPU) via <code>llama.cpp</code>, Ollama, LM Studio Successor to GGML; single binary with rich metadata &amp; prompt template; supports 2-8 bit \"K-quants\"; now the de-facto community standard. GPTQ (<code>.safetensors</code> + <code>gptq.json</code>) Fast GPU inference at 3-/4-bit (NVIDIA/AMD) Block-wise second-order quantization; large ecosystem in <code>autoGPTQ</code>. AWQ (<code>awq.safetensors</code>) 4-bit CPU/GPU with very low memory Activation-aware; fewer outliers than GPTQ \u21d2 higher accuracy; easy via <code>autoawq</code>. EXL2 (<code>.exl2</code>) GPU power-users who want mixed-precision layers (2\u20138 bit) Allows per-layer bit-mixing; stellar throughput with ExLlama v2. PyTorch / Safetensors (FP16/BF16) Cloud GPUs, further fine-tuning Full fidelity, biggest VRAM &amp; disk footprint. ONNX / TFLite Edge devices, mobile Graph optimisations, hardware-specific accelerators. <p>Tip: If you're uncertain, grab a Q4_K_M GGUF first; it fits 13 B dense or 30 B-A3B MoE models into ~6\u20138 GB and runs on an 8-GB VRAM GPU or a modern CPU.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#4-decision-flow-quick-opinionated","title":"4. Decision flow (quick &amp; opinionated)","text":"<ol> <li> <p>What's my hardware?</p> <ul> <li><code>&lt;8 GB VRAM / only CPU</code> \u2192 GGUF Q4_K_M or AWQ 4-bit.</li> <li><code>\u226516 GB VRAM</code> \u2192 GPTQ 3-bit or GGUF Q3_K_S.</li> <li><code>H100/A100 class</code> \u2192 FP16 Safetensors or ONNX.</li> </ul> </li> <li> <p>What's my task?</p> <ul> <li>Chat / retrieval-augmented generation \u2192 Instruct or Chat.</li> <li>Batch creative writing \u2192 Dense Base model with a creative prompt.</li> <li>Private edge device \u2192 Distill or A3B + QAT/AWQ quantization.</li> </ul> </li> <li> <p>Do I care about devops simplicity?</p> <ul> <li>Yes \u2192 GGUF (single file, works everywhere).</li> <li>No, I'll optimise every ms \u2192 GPTQ / EXL2 on GPU.</li> </ul> </li> </ol>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#5-gotchas-to-remember","title":"5. Gotchas to remember","text":"<ul> <li>Not all quantizers understand MoE gating. Today <code>llama.cpp</code> (and forks) handle A3B/A22B in GGUF; <code>autoGPTQ</code> &amp; <code>autoAWQ</code> support is still evolving.</li> <li>QAT \u2260 plain quantization. A QAT model in 4-bit often matches an 8-bit PTQ model's accuracy - don't double-quantize it.</li> <li>Distill \u2260 small parameter count only. A distilled 7 B model may outperform a vanilla dense 13 B because knowledge from a stronger teacher was baked in.</li> <li>File format \u2260 quantization method. You can have a GGUF in 8-bit (near-FP16) or 2-bit (experimental); likewise, GPTQ can hold 4-bit or 8-bit blocks.</li> </ul>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#6-tldr","title":"6. TL;DR","text":"<p>If you just want something that works:</p> <ul> <li>Download a <code>&lt;model-name&gt;-Instruct.Q4_K_M.gguf</code> build.</li> <li>Run it with <code>llama.cpp</code>, LM Studio, or Ollama.</li> <li>If it's still slow, try an A3B MoE flavour or an AWQ 4-bit file.</li> </ul>","tags":["guide","llm"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/","title":"Building a Custom FeatureStoreLite MCP Server Using uv","text":"<p>A step-by-step guide that shows how to create your own lightweight feature store MCP server from scratch using FastMCP, run it through uv, and integrate it with Claude Desktop. This is a practical example of building a useful MCP server that ML engineers can actually use.</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#1-why-build-a-custom-featurestorelite-mcp-server","title":"1. Why build a custom \"FeatureStoreLite\" MCP server?","text":"<p>Let's create a practical MCP server example that solves a real problem: feature storage and retrieval for ML pipelines. Our custom FeatureStoreLite server will be a microservice responsible for storing and retrieving precomputed feature vectors via keys, allowing ML pipelines to share features efficiently without recomputation.</p> <p>This tutorial demonstrates how to build an MCP server that could be useful in a real-world ML pipeline.</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#2-setup-and-installation","title":"2. Setup and Installation","text":"<p>First, install uv (if you haven't already):</p> <pre><code># macOS/Linux with Homebrew\nbrew install uv\n\n# Or install directly from the official installer\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Then create your project with virtual environment and dependencies:</p> <pre><code># Create project directory\nmkdir mcp-featurestore &amp;&amp; cd mcp-featurestore\n\n# Initialize Python project (this creates pyproject.toml)\nuv init\n\n# Add dependencies\nuv add \"mcp[cli]\"\n</code></pre>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#3-implementing-our-custom-featurestorelite-server-with-fastmcp","title":"3. Implementing our custom FeatureStoreLite server with <code>FastMCP</code>","text":"<p>Let's build our MCP server from scratch.</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#31-create-the-database-module","title":"3.1. Create the database module","text":"<p>This module handles all database operations for storing and retrieving feature vectors.</p> <p>Create <code>database.py</code> file:</p> <pre><code>touch database.py\n</code></pre> <p>Add the following code to the file:</p> <pre><code># database.py\n\nimport sqlite3\nimport os\n\n\ndef get_db_path():\n    \"\"\"Get the database path - always in the script's directory\"\"\"\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    return os.path.join(script_dir, \"features.db\")\n\n\ndef init_db():\n    \"\"\"Initialize the feature store database\"\"\"\n    conn = sqlite3.connect(get_db_path())\n    conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS features (\n            key TEXT PRIMARY KEY,\n            vector TEXT NOT NULL,\n            metadata TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    \"\"\")\n\n    # Add example features for experimentation\n    example_features = [\n        (\n            \"user_123\", \n            \"[0.1, 0.2, -0.5, 0.8, 0.3, -0.1, 0.9, -0.4]\",\n            '{\"type\": \"user_embedding\", \"user_id\": 123, \"age\": 25, '\n            '\"category\": \"premium\"}'\n        ),\n        (\n            \"product_abc\", \n            \"[0.7, -0.3, 0.4, 0.1, -0.8, 0.6, 0.2, -0.5]\",\n            '{\"type\": \"product_embedding\", \"product_id\": \"abc\", '\n            '\"price\": 29.99, \"category\": \"electronics\"}'\n        ),\n        (\n            \"doc_guide_001\", \n            \"[-0.2, 0.5, 0.9, -0.1, 0.4, 0.7, -0.6, 0.3]\",\n            '{\"type\": \"document_embedding\", \"doc_id\": \"guide_001\", '\n            '\"title\": \"Getting Started Guide\", \"section\": \"introduction\"}'\n        ),\n        (\n            \"recommendation_engine\", \n            \"[0.4, 0.8, -0.2, 0.6, -0.7, 0.1, 0.5, -0.9]\",\n            '{\"type\": \"model_embedding\", \"model\": \"collaborative_filter\", '\n            '\"version\": \"1.2\", \"accuracy\": 0.85}'\n        )\n    ]\n\n    # Insert example features only if they don't exist\n    for key, vector, metadata in example_features:\n        existing = conn.execute(\n            \"SELECT 1 FROM features WHERE key = ?\", (key,)\n        ).fetchone()\n        if not existing:\n            conn.execute(\n                \"INSERT INTO features (key, vector, metadata) \"\n                \"VALUES (?, ?, ?)\",\n                (key, vector, metadata)\n            )\n\n    conn.commit()\n    conn.close()\n\n\ndef get_db_connection():\n    \"\"\"Get a database connection\"\"\"\n    return sqlite3.connect(get_db_path())\n\n\nif __name__ == \"__main__\":\n    init_db()\n    print(\"Database initialized successfully!\")\n</code></pre> <p>Run the database initialization:</p> <pre><code>uv run python database.py\n</code></pre>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#32-create-the-mcp-server","title":"3.2. Create the MCP server","text":"<p>Create <code>featurestore_server.py</code>:</p> <pre><code>touch featurestore_server.py\n</code></pre> <p>Add the following code to the file:</p> <pre><code># featurestore_server.py\n\nimport json\nfrom mcp.server.fastmcp import FastMCP\nfrom database import get_db_connection, init_db\n\nmcp = FastMCP(\"FeatureStoreLite\")\n\n# Initialize database\ninit_db()\n\n\n@mcp.resource(\"schema://main\")\ndef get_schema() -&gt; str:\n    \"\"\"Provide the database schema as a resource\"\"\"\n    conn = get_db_connection()\n    try:\n        schema = conn.execute(\n            \"SELECT sql FROM sqlite_master WHERE type='table'\"\n        ).fetchall()\n        if not schema:\n            return \"No tables found in database\"\n        return \"\\n\".join(sql[0] for sql in schema if sql[0])\n    except Exception as e:\n        return f\"Error getting schema: {str(e)}\"\n    finally:\n        conn.close()\n\n\n@mcp.tool()\ndef store_feature(key: str, vector: str, metadata: str | None = None) -&gt; str:\n    \"\"\"Store a feature vector with optional metadata\"\"\"\n    conn = get_db_connection()\n    try:\n        # Validate vector format (JSON array)\n        json.loads(vector)\n        conn.execute(\n            \"INSERT OR REPLACE INTO features (key, vector, metadata) \"\n            \"VALUES (?, ?, ?)\",\n            (key, vector, metadata)\n        )\n        conn.commit()\n        return f\"Feature '{key}' stored successfully\"\n    except json.JSONDecodeError:\n        return \"Error: vector must be valid JSON\"\n    except Exception as e:\n        return f\"Error storing feature: {str(e)}\"\n    finally:\n        conn.close()\n\n\n@mcp.tool()\ndef get_feature(key: str) -&gt; str:\n    \"\"\"Retrieve a feature vector by key\"\"\"\n    conn = get_db_connection()\n    try:\n        result = conn.execute(\n            \"SELECT vector, metadata FROM features WHERE key = ?\", (key,)\n        ).fetchone()\n        if result:\n            return json.dumps({\n                \"key\": key,\n                \"vector\": json.loads(result[0]),\n                \"metadata\": json.loads(result[1]) if result[1] else None\n            })\n        else:\n            return f\"Feature '{key}' not found\"\n    except Exception as e:\n        return f\"Error retrieving feature: {str(e)}\"\n    finally:\n        conn.close()\n\n\n@mcp.tool()\ndef list_features() -&gt; str:\n    \"\"\"List all available feature keys\"\"\"\n    conn = get_db_connection()\n    try:\n        result = conn.execute(\n            \"SELECT key, created_at FROM features ORDER BY created_at DESC\"\n        ).fetchall()\n        features = [{\"key\": row[0], \"created_at\": row[1]} for row in result]\n        return json.dumps(features)\n    except Exception as e:\n        return f\"Error listing features: {str(e)}\"\n    finally:\n        conn.close()\n\n\n@mcp.resource(\"features://{key}\")\ndef feature_resource(key: str) -&gt; str:\n    \"\"\"Expose feature data via URI\"\"\"\n    return get_feature(key)\n\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre> <p>Test the server in development mode:</p> <pre><code>uv run mcp dev featurestore_server.py\n</code></pre>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#4-connecting-to-claude-desktop","title":"4. Connecting to Claude Desktop","text":"<p>To use the FeatureStoreLite server with Claude Desktop, you need to update your Claude configuration file.</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#41-configuration-file-location","title":"4.1. Configuration file location","text":"<ul> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%/Claude/claude_desktop_config.json</code></li> </ul>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#42-configuration-setup","title":"4.2. Configuration setup","text":"<p>Add the following configuration to your <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"featurestore\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"mcp\",\n        \"run\",\n        \"/path/to/your/featurestore_server.py\"\n      ]\n    }\n  }\n}\n</code></pre> <p>\ud83d\udca1 Important Tip: If you are getting errors when connecting to the server, you can use the next command: <pre><code>uv run mcp install featurestore_server.py\n</code></pre> This command will automatically install and configure the server for Claude Desktop. After running this command, check your Claude Desktop config file to see how the server has been configured.</p> <p>This is often the easiest way to get started, especially if you're having trouble with manual configuration!</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#43-testing-the-server","title":"4.3  Testing the server","text":"<p>After updating the config:</p> <ol> <li>Restart Claude Desktop completely (quit and reopen)</li> <li>Look for connection status in Claude's interface</li> <li>Try asking Claude to interact with your feature store</li> </ol> <p>Example queries to test:</p> <ul> <li>\"Show me the database schema for the feature store\"</li> </ul> <p></p> <ul> <li>\"List all available features in the store\"</li> </ul> <p></p> <ul> <li>\"Retrieve the feature vector for product_abc\"</li> </ul> <p></p> <ul> <li>Try your own queries!</li> </ul>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#44-production-deployment-considerations","title":"4.4. Production deployment considerations","text":"<p>Please note that this is a simple example to demonstrate the MCP server usage and not production-ready.</p> <p>For production use, consider:</p> <ul> <li>Using a proper database (PostgreSQL, MySQL) instead of SQLite</li> <li>Adding authentication and authorization</li> <li>Implementing proper logging and monitoring</li> <li>Adding data validation and sanitization</li> <li>Using environment variables for configuration</li> </ul>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#5-alternative-client-configurations","title":"5. Alternative client configurations","text":"","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#51-generic-mcp-client-configuration","title":"5.1. Generic MCP client configuration","text":"<p>For other MCP clients, you can use exactly the same configuration pattern as we did for Claude Desktop:</p> <pre><code>{\n  \"mcpServers\": {\n    \"FeatureStoreLite\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp[cli]\",\n        \"mcp\",\n        \"run\",\n        \"path/to/featurestore_server.py\"\n      ]\n    }\n  }\n}\n</code></pre>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#6-observability-and-debugging-with-inspector","title":"6. Observability and debugging with Inspector","text":"<p>The MCP development tools provide excellent observability features.</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#61-running-in-development-mode","title":"6.1. Running in development mode","text":"<pre><code>uv run mcp dev featurestore_server.py\n</code></pre> <p>This starts the server with the MCP Inspector, which provides:</p> <ul> <li>Real-time request/response monitoring</li> <li>Tool and resource exploration</li> <li>Interactive testing interface</li> <li>Performance metrics</li> </ul>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#62-using-the-inspector","title":"6.2. Using the Inspector","text":"<p>When running in development mode, the Inspector is available at the URL shown in the terminal output (typically <code>http://localhost:6274</code>).</p> <p>The Inspector allows you to:</p> <ul> <li>Browse Resources: View available resources like the database schema</li> <li>Test Tools: Interactively test each tool with different parameters</li> <li>Monitor Traffic: See all MCP protocol messages in real-time</li> <li>Debug Issues: Identify problems with tool calls or resource access</li> </ul> <p></p> <p>You can check manually the resources available in the server:</p> <p></p> <p>As well as the tools available:</p> <p></p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#7-conclusion","title":"7. Conclusion","text":"<p>In this tutorial, we built a custom FeatureStoreLite MCP server using FastMCP, ran it through uv, and integrated it with Claude Desktop. We also explored how to use the <code>mcp inspector</code> to see the server's capabilities and the requests and responses it is sending and receiving.</p>","tags":["guide","mcp"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#8-references","title":"8. References","text":"<ul> <li>The repo of this tutorial example</li> <li>Introduction to MCP</li> <li>MCP Python SDK</li> <li>Claude Desktop</li> <li>uv</li> </ul>","tags":["guide","mcp"]},{"location":"blog/archive/2025/","title":"2025","text":""}]}