{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Edge of Context","text":"<p>Practical lessons from building ML systems that scale.</p> <p>\ud83d\udc4b Hi, I'm Slava!</p> <p>I\u2019m a Machine Learning Engineering Tech Lead II at HubSpot with 10+ years of experience shipping production AI systems across embeddings, recommender systems, LLM-powered applications, forecasting, churn detection, scam and fraud prevention. Right now I\u2019m building Embedding Hub on top of Qdrant and establishing a Context Layer that helps AI agents stay grounded, efficient, and compliant.</p> <p>I lead cross-functional ML teams, mentor engineers, and partner with product, data, and go-to-market stakeholders to launch customer-facing AI features worldwide. I thrive at the intersection of infrastructure and product\u2014owning the roadmap from data pipelines and model lifecycle to agent orchestration, evaluation, and safety guardrails.</p> <p>Day to day, I stay focused on embedding systems, retrieval quality, LLM evaluation, and agentic workflows\u2014experimenting with Google ADK, FastMCP, LlamaIndex, LangGraph, CrewAI, and SmolAgents to push new ideas into production-ready form.</p> <p>I write here about practical aspects of developing LLM applications and AI agents, and I stay active in the broader community through mentorship and knowledge sharing.</p> <p>Check out my experiments on GitHub: github.com/slavadubrov.</p> <p>If you're exploring LLM-powered products, agent frameworks, or robust ML platforms, let's connect\u2014I'm always happy to swap ideas.</p>"},{"location":"#tech-radar","title":"Tech Radar","text":"<p>Python (PyTorch, JAX) \u00b7 Vertex AI \u00b7 AWS \u00b7 Spark \u00b7 FastAPI \u00b7 Airflow \u00b7 DBT \u00b7 Qdrant \u00b7 LangChain \u00b7 LangGraph \u00b7 LlamaIndex \u00b7 CrewAI</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<p>In this blog, you'll find:</p> <ul> <li>Technical tutorials and guides</li> <li>Machine Learning insights</li> <li>Best practices and tips</li> <li>Personal experiences and learnings</li> </ul> <p>My goal is to create a valuable resource for fellow practitioners and anyone interested in the real-world application of machine learning.</p>"},{"location":"#subscribe","title":"Subscribe","text":"<p>Get new posts delivered to your inbox:</p> <p> Subscribe </p> <p>No spam. Unsubscribe anytime.</p>"},{"location":"#connect","title":"Connect","text":"<ul> <li>GitHub</li> <li>LinkedIn</li> </ul>"},{"location":"topics/","title":"\ud83d\udd0e Browse by Topic","text":""},{"location":"topics/#tag:deep-learning","title":"Deep Learning","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:deployment","title":"Deployment","text":"<ul> <li>            LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes          </li> </ul>"},{"location":"topics/#tag:distributed-training","title":"Distributed Training","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:gpu","title":"GPU","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:inference","title":"Inference","text":"<ul> <li>            LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes          </li> </ul>"},{"location":"topics/#tag:kubernetes","title":"Kubernetes","text":"<ul> <li>            LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes          </li> </ul>"},{"location":"topics/#tag:llm","title":"LLM","text":"<ul> <li>            LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes          </li> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:lora","title":"LoRA","text":"<ul> <li>            LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes          </li> </ul>"},{"location":"topics/#tag:parallelism","title":"Parallelism","text":"<ul> <li>            Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025          </li> </ul>"},{"location":"topics/#tag:agents","title":"agents","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> <li>            Domain-driven design for AI agents: a beginner-friendly guide          </li> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:ai-engineering","title":"ai-engineering","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> <li>            Domain-driven design for AI agents: a beginner-friendly guide          </li> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:architecture","title":"architecture","text":"<ul> <li>            Domain-driven design for AI agents: a beginner-friendly guide          </li> </ul>"},{"location":"topics/#tag:best-practices","title":"best-practices","text":"<ul> <li>            The Ultimate Guide to `pyproject.toml`          </li> </ul>"},{"location":"topics/#tag:context-compression","title":"context-compression","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:context-layer","title":"context-layer","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:context-optimization","title":"context-optimization","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:domain-driven-design","title":"domain-driven-design","text":"<ul> <li>            Domain-driven design for AI agents: a beginner-friendly guide          </li> </ul>"},{"location":"topics/#tag:genai","title":"genai","text":"<ul> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> </ul>"},{"location":"topics/#tag:guardrails","title":"guardrails","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:guide","title":"guide","text":"<ul> <li>            Building a Custom FeatureStoreLite MCP Server Using uv          </li> <li>            Choosing the Right Open-Source LLM Variant &amp; File Format          </li> <li>            Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> <li>            Quick Guide: Managing Python on macOS with uv          </li> <li>            Quick-Guide on setting up a MacBook for AI Engineering          </li> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> <li>            The Ultimate Guide to `pyproject.toml`          </li> </ul>"},{"location":"topics/#tag:infrastructure","title":"infrastructure","text":"<ul> <li>            MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond          </li> </ul>"},{"location":"topics/#tag:llm","title":"llm","text":"<ul> <li>            Choosing the Right Open-Source LLM Variant &amp; File Format          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:llmops","title":"llmops","text":"<ul> <li>            MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond          </li> </ul>"},{"location":"topics/#tag:macos","title":"macos","text":"<ul> <li>            Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> <li>            Quick-Guide on setting up a MacBook for AI Engineering          </li> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> </ul>"},{"location":"topics/#tag:mcp","title":"mcp","text":"<ul> <li>            Building a Custom FeatureStoreLite MCP Server Using uv          </li> </ul>"},{"location":"topics/#tag:memory","title":"memory","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:mlops","title":"mlops","text":"<ul> <li>            MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond          </li> </ul>"},{"location":"topics/#tag:python","title":"python","text":"<ul> <li>            Building a Custom FeatureStoreLite MCP Server Using uv          </li> <li>            Quick Guide: Managing Python on macOS with uv          </li> <li>            The Ultimate Guide to `pyproject.toml`          </li> </ul>"},{"location":"topics/#tag:rag","title":"rag","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:retrieval","title":"retrieval","text":"<ul> <li>            Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It          </li> </ul>"},{"location":"topics/#tag:sgr","title":"sgr","text":"<ul> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:structured-output","title":"structured-output","text":"<ul> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:tooling","title":"tooling","text":"<ul> <li>            Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> <li>            Quick Guide: Managing Python on macOS with uv          </li> <li>            Quick-Guide on setting up a MacBook for AI Engineering          </li> </ul>"},{"location":"topics/#tag:tools","title":"tools","text":"<ul> <li>            Quick-guide on Local Stable-Diffusion Toolkits for macOS          </li> <li>            Quick-guide on Running LLMs Locally on macOS          </li> </ul>"},{"location":"topics/#tag:uv","title":"uv","text":"<ul> <li>            Building a Custom FeatureStoreLite MCP Server Using uv          </li> </ul>"},{"location":"topics/#tag:vllm","title":"vllm","text":"<ul> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:xgrammar","title":"xgrammar","text":"<ul> <li>            Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines          </li> </ul>"},{"location":"topics/#tag:zsh","title":"zsh","text":"<ul> <li>            Mastering Zsh Startup: ~/.zprofile vs ~/.zshrc \ud83d\ude80          </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/","title":"Quick Guide: Managing Python on macOS with uv","text":"","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#quick-start","title":"Quick Start","text":"<pre><code># Install uv\nbrew install uv\n\n# For new projects (modern workflow)\nuv init                # create project structure\nuv add pandas numpy    # add dependencies\nuv run train.py        # run your script\n\n# For existing projects (legacy workflow)\nuv venv                             # create virtual environment\nuv pip install -r requirements.txt  # install dependencies\nuv run train.py                     # run your script\n\n# Run tools without installing them\nuvx ruff check .       # run linter\nuvx black .            # run formatter\n</code></pre>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#why-uv","title":"Why uv?","text":"<p>If you've been using Python for a while, you're likely familiar with the \"tool fatigue\" of managing <code>pip</code>, <code>virtualenv</code>, <code>pip-tools</code>, <code>pyenv</code>, and <code>poetry</code>.</p> <p><code>uv</code> replaces all of them.</p> <p>Written in Rust, it is designed to be a drop-in replacement that is 10-100x faster than existing tools. It unifies your workflow into a single, cohesive experience.</p> <p></p> <p>It handles:</p> <ul> <li>Package management (replacing <code>pip</code> and <code>pip-tools</code>)</li> <li>Python installation (replacing <code>pyenv</code>)</li> <li>Virtual environments (replacing <code>virtualenv</code> and <code>venv</code>)</li> <li>Tool execution (replacing <code>pipx</code>)</li> <li>Project management (replacing <code>poetry</code> or <code>pdm</code>)</li> </ul>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#installing-uv","title":"Installing uv","text":"<p>The easiest way to install <code>uv</code> on macOS is via Homebrew:</p> <pre><code>brew install uv\n</code></pre> <p><code>uv</code> automatically detects your Mac's architecture (Apple Silicon or Intel), so no extra configuration is needed.</p> <p>Keep it updated:</p> <pre><code>brew upgrade uv\n# OR\nuv self update\n</code></pre>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#core-concepts","title":"Core Concepts","text":"<p><code>uv</code> simplifies Python development by handling three distinct use cases:</p> <ol> <li>Projects: Building an application or library with dependencies.</li> <li>Scripts: Running a single-file Python script with inline dependencies.</li> <li>Tools: Running command-line utilities (like <code>ruff</code> or <code>httpie</code>) globally.</li> </ol>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#1-modern-project-management","title":"1. Modern Project Management","text":"<p>For new projects, <code>uv</code> uses the standard <code>pyproject.toml</code> for configuration and a cross-platform <code>uv.lock</code> for reproducible builds.</p> <p></p> <p>Start a new project:</p> <pre><code>uv init my-project\ncd my-project\n</code></pre> <p>This creates a clean project structure with a <code>pyproject.toml</code>, <code>.gitignore</code>, and a <code>hello.py</code>.</p> <p>Add dependencies:</p> <pre><code># Add runtime dependencies\nuv add pandas requests\n\n# Add development dependencies\nuv add pytest ruff --dev\n</code></pre> <p>Run your code:</p> <pre><code>uv run hello.py\n</code></pre> <p><code>uv</code> automatically manages the virtual environment in <code>.venv</code>. You never need to manually activate it!</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#2-managing-python-versions","title":"2. Managing Python Versions","text":"<p>Forget <code>pyenv</code>. <code>uv</code> can install and manage Python versions for you, keeping them isolated in <code>~/.cache/uv</code>.</p> <p>Install a specific version:</p> <pre><code>uv python install 3.12\n</code></pre> <p>Pin a version for your project:</p> <pre><code>uv python pin 3.11\n</code></pre> <p>This creates a <code>.python-version</code> file. When you run <code>uv run</code>, it will automatically use the pinned version, downloading it if necessary. This ensures your entire team and CI pipeline use the exact same Python version.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#3-running-tools-with-uvx","title":"3. Running Tools with <code>uvx</code>","text":"<p>Use <code>uvx</code> (an alias for <code>uv tool run</code>) to execute Python command-line tools without polluting your global environment or project dependencies.</p> <pre><code># Run a linter\nuvx ruff check .\n\n# Run a formatter\nuvx black .\n\n# Start a temporary Jupyter server\nuvx --from jupyterlab jupyter lab\n</code></pre> <p>Each tool runs in its own isolated, temporary environment. It's fast, clean, and safe.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#legacy-projects-requirementstxt","title":"Legacy Projects (requirements.txt)","text":"<p>If you have an existing project using <code>requirements.txt</code>, <code>uv</code> works as a drop-in replacement for <code>pip</code> and <code>venv</code>.</p> <p>Setup:</p> <pre><code># Create a virtual environment\nuv venv\n\n# Install dependencies (lightning fast!)\nuv pip install -r requirements.txt\n</code></pre> <p>Run:</p> <pre><code>uv run python app.py\n</code></pre>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#performance-notes","title":"Performance Notes","text":"<p>Why is <code>uv</code> so fast?</p> <ol> <li>Rust: It's built with performance in mind, without the overhead of Python startup times.</li> <li>Global Cache: It caches built wheels globally. If you've installed <code>numpy</code> in one project, installing it in another is instant (using copy-on-write links on macOS).</li> <li>Parallelism: It downloads and installs packages in parallel, maximizing your bandwidth.</li> </ol>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/17/quick-guide-managing-python-on-macos-with-uv/#summary","title":"Summary","text":"Task Old Way The uv Way Install Python <code>pyenv install 3.12</code> <code>uv python install 3.12</code> New Project <code>mkdir proj &amp;&amp; cd proj &amp;&amp; python -m venv .venv</code> <code>uv init proj</code> Install Package <code>pip install pandas &amp;&amp; pip freeze &gt; requirements.txt</code> <code>uv add pandas</code> Run Script <code>source .venv/bin/activate &amp;&amp; python script.py</code> <code>uv run script.py</code> Run Tool <code>pipx run black</code> <code>uvx black</code> <p>Switching to <code>uv</code> on macOS is one of the highest-ROI changes you can make to your Python workflow today. It's faster, simpler, and standard-compliant.</p>","tags":["tooling","python","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/","title":"Quick-Guide on setting up a MacBook for AI Engineering","text":"<p>Setting up a new MacBook for AI development doesn't have to be overwhelming. Here's my streamlined 10-step process to transform a fresh macOS installation into a fully functional AI engineering workstation.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#1-install-xcode-command-line-tools","title":"1. Install Xcode Command Line Tools","text":"<p>Start by installing the Xcode Command Line Tools. These are essential building blocks for any software development on macOS, including AI and data science work.</p> <pre><code>xcode-select --install\n</code></pre> <p>This command opens a dialog that walks you through the installation process.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#2-install-homebrew","title":"2. Install Homebrew","text":"<p>Next, install Homebrew, the go-to package manager for macOS. It makes installing and managing software incredibly simple. Run this command:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>The installer will guide you through the process and may ask for your password.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#3-install-essential-development-tools","title":"3. Install essential development tools","text":"<p>Now let's install the core tools you'll need for AI engineering:</p> <pre><code>brew install openssl readline sqlite3 xz zlib pyenv uv htop gitmoji pandoc ncdu tmux\n</code></pre> <p>Here's what each tool does:</p> <p>Python environment:</p> <ul> <li>pyenv \u2014 manage multiple Python versions seamlessly</li> <li>uv \u2014 fast Python package manager and environment handler</li> </ul> <p>System libraries:</p> <ul> <li>openssl \u2014 SSL/TLS cryptography support</li> <li>readline \u2014 command-line text editing</li> <li>sqlite3 \u2014 lightweight embedded database</li> <li>xz \u2014 advanced data compression</li> <li>zlib \u2014 compression library</li> </ul> <p>Productivity tools:</p> <ul> <li>htop \u2014 visual system monitor and process viewer</li> <li>tmux \u2014 manage multiple terminal sessions</li> <li>ncdu \u2014 analyze disk usage interactively</li> <li>gitmoji \u2014 add emojis to commit messages</li> <li>pandoc \u2014 convert documents between formats</li> </ul> <p>Note: For more detailed information about using <code>uv</code> for Python development, check out my Quick-Guide on managing Python on macOS with uv.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#4-choose-your-terminal","title":"4. Choose your terminal","text":"<p>The default macOS Terminal works fine, but I've found better alternatives. I recently switched from iTerm2 to Warp. Warp is a modern, Rust-based terminal with built-in AI features that make your workflow smoother.</p> <p>You can download Warp from their website.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#optional-iterm2-configuration","title":"Optional: iTerm2 configuration","text":"<p>If you prefer the battle-tested iTerm2, here's my recommended setup:</p> <p>Enable natural text editing:</p> <ol> <li>Open Preferences \u2192 Profiles \u2192 Keys \u2192 Key Mappings</li> <li>Click the Presets\u2026 dropdown</li> <li>Select \"Natural Text Editing\"</li> </ol> <p>Choose a color theme:</p> <ol> <li>Browse themes at iTerm2-Color-Schemes</li> <li>Open Preferences \u2192 Profiles \u2192 Colors \u2192 Color Presets\u2026</li> <li>Click Import and select your downloaded theme</li> </ol>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#5-set-up-zsh-with-oh-my-zsh","title":"5. Set up Zsh with Oh My Zsh","text":"<p>Modern macOS comes with Zsh as the default shell, but we'll enhance it with Oh My Zsh, a framework that makes Zsh more powerful and easier to customize:</p> <pre><code>brew install zsh\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n</code></pre> <p>The Oh My Zsh installer will back up your existing Zsh configuration and set up the new one.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#6-add-zsh-plugins-for-superpowers","title":"6. Add Zsh plugins for superpowers","text":"<p>Plugins make your terminal smarter and more productive. Edit your <code>~/.zshrc</code> file to add these plugins:</p> <pre><code>plugins=(\n    aws bgnotify brew docker docker-compose\n    emoji forklift gcloud git history iterm2\n    keychain kubectl macos pre-commit\n    pyenv pylint python screen themes\n    tmux virtualenv vscode\n    zsh-autosuggestions zsh-syntax-highlighting\n)\n</code></pre> <p>You can find detailed descriptions of all plugins in the Oh My Zsh plugins wiki.</p> <p>Extra installation required:</p> <p>The last two plugins need separate installation (but it's quick!):</p> <ul> <li>zsh-autosuggestions \u2014 suggests commands as you type based on your history</li> <li>zsh-syntax-highlighting \u2014 highlights commands in real-time to catch errors</li> </ul> <p>Follow the installation instructions on their respective GitHub pages.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#7-make-your-terminal-beautiful-with-powerlevel10k","title":"7. Make your terminal beautiful with Powerlevel10k","text":"<p>Powerlevel10k is a gorgeous Zsh theme that displays useful information like your current directory, Git status, Python environment, and more. The best part? It comes with an interactive configuration wizard that walks you through customizing it to your preferences.</p> <p>Follow the installation instructions on their GitHub page.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#font-setup-for-other-editors","title":"Font setup for other editors","text":"<p>If you use VSCode or other editors with integrated terminals, you'll want to use compatible fonts:</p> <ol> <li>Open your editor's settings</li> <li>Search for <code>terminal.integrated.fontFamily</code></li> <li>Set it to <code>MesloLGS NF</code> (this font is installed with Powerlevel10k)</li> </ol> <p>For detailed font setup instructions, check the Powerlevel10k font guide.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#8-pick-your-code-editor-and-ai-assistant","title":"8. Pick your code editor and AI assistant","text":"<p>For AI engineering, you'll want both a powerful IDE and AI coding assistants. Here's my setup:</p> <p>IDE:</p> <ul> <li>Cursor \u2014 a VSCode fork with native AI pair programming features</li> <li>VSCode \u2014 the industry standard with an enormous extension ecosystem</li> </ul> <p>AI Assistants:</p> <ul> <li>OpenAI Codex \u2014 OpenAI's code generation model for intelligent code completion</li> <li>Claude \u2014 Anthropic's AI assistant for complex coding tasks and architecture discussions</li> </ul> <p>My preference: I use Cursor as my IDE alongside Codex (or Claude Code) running in parallel.</p>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#9-additional-developer-tools","title":"9. Additional developer tools","text":"<p>Round out your setup with these applications:</p> <ul> <li>GitHub Desktop \u2014 visual Git client for managing repositories</li> <li>Docker \u2014 containerization platform (or check out alternatives like Podman)</li> <li>Ollama or LM Studio \u2014 run large language models locally on your Mac</li> </ul>","tags":["macos","tooling","guide"]},{"location":"blog/2025/04/19/quick-guide-on-setting-up-a-macbook-for-ai-engineering/#10-youre-ready-to-build","title":"10. You're ready to build","text":"<p>That's it! You now have a complete AI engineering setup that mirrors what I use daily. This configuration removes the friction between having an idea and building with AI models. From here, you can:</p> <ul> <li>Start new Python projects with <code>uv</code></li> <li>Run local LLMs for development and testing</li> <li>Manage your code with Git and GitHub</li> <li>Work efficiently in a beautiful, customized terminal</li> </ul>","tags":["macos","tooling","guide"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/","title":"Scaling Large Language Models - Practical Multi-GPU and Multi-Node Strategies for 2025","text":"<p>The race to build bigger, better language models continues at breakneck speed. Today's state-of-the-art models require massive computing resources that no single GPU can handle. Whether you're training a custom LLM or deploying one for inference, understanding how to distribute this workload is essential.</p> <p>This guide walks through practical strategies for scaling LLMs across multiple GPUs and nodes, incorporating insights from Hugging Face's Ultra-Scale Playbook.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#prerequisites","title":"Prerequisites","text":"<p>Before diving in, you should be familiar with:</p> <ul> <li>Basic Deep Learning: Backpropagation, gradients, and optimizers (AdamW).</li> <li>Transformer Architecture: Attention mechanisms, Feed-Forward Networks (FFN).</li> <li>PyTorch Basics: <code>nn.Module</code>, <code>DataLoader</code>, and the training loop.</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#why-scaling-matters","title":"Why Scaling Matters","text":"<p>Modern LLMs have outgrown single GPUs. Here's why scaling is no longer optional:</p> <ul> <li>Model size: A 70B parameter model needs ~140GB in FP16 format - that's nearly 2x what an A100 (80GB) can hold</li> <li>Training time: Even with 8 top-tier A100 GPUs, training a 13B model from scratch takes weeks</li> <li>Context length: Long contexts (32k+ tokens) easily exceed single-GPU memory limits</li> <li>Inference speed: For production workloads, distributing inference reduces latency and increases throughput</li> </ul> <p>The solution? Split the workload across multiple GPUs. Let's explore how.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#1-parallelism-techniques-explained-simply","title":"1. Parallelism Techniques Explained Simply","text":"","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#11-data-parallelism-dp","title":"1.1 Data Parallelism (DP)","text":"<p>The idea: Multiple workers with identical instruction manuals (the model), each working on different examples.</p> <p>How it works:</p> <ol> <li>Each GPU gets a complete copy of the model</li> <li>Each GPU processes different batches of data</li> <li>After computing gradients, all GPUs synchronize by averaging their gradients</li> <li>Everyone updates their model copy with the averaged gradients</li> </ol> <p>When to use it:</p> <ul> <li>Your model fits comfortably on a single GPU</li> <li>You want to process more data faster</li> <li>You need the simplest distributed setup with minimal code changes</li> </ul> <p>Limitation: Memory inefficient - every GPU stores the full model, so you're not saving memory, just increasing throughput.</p> <p></p> <p>Tools: PyTorch DDP, Horovod.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#12-fully-sharded-data-parallelism-fsdp","title":"1.2 Fully Sharded Data Parallelism (FSDP)","text":"<p>The idea: Like Data Parallelism, but memory-efficient. Each worker keeps only part of the instruction manual and borrows pages from colleagues when needed.</p> <p>How it works:</p> <ol> <li>Model parameters, gradients, and optimizer states are sharded (split) across all GPUs</li> <li>During forward pass: each GPU gathers the parameters it needs from other GPUs</li> <li>After using them, it discards those borrowed parameters to save memory</li> <li>During backward pass: same gathering happens for gradient computation</li> <li>After backward pass: gradients are reduced and each GPU updates only its own parameter shard</li> </ol> <p>When to use it:</p> <ul> <li>Your model is too large for a single GPU (typically &gt;10B parameters)</li> <li>You want to train bigger models without changing your code much</li> <li>You're working on a single machine with multiple GPUs</li> </ul> <p>Real-world impact: FSDP lets you train models 4-8x larger than what fits on one GPU.</p> <p></p> <p>[!NOTE] &gt; Understanding ZeRO Stages FSDP is often described in terms of \"ZeRO stages\" (Zero Redundancy Optimizer):</p> <ul> <li>Stage 1: Shard optimizer states only (4x memory savings).</li> <li>Stage 2: Shard gradients + optimizer states (8x memory savings).</li> <li>Stage 3: Shard parameters + gradients + optimizer states (Linear memory savings with N GPUs).</li> </ul> <p>PyTorch FSDP defaults to Stage 3 behavior.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#example-enabling-fsdp-in-pytorch","title":"Example: Enabling FSDP in PyTorch","text":"<pre><code>from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\n# 1. Wrap your model\nmodel = MyLLM()\nmodel = FSDP(model)\n\n# 2. Train as usual\noutput = model(input)\nloss = output.sum()\nloss.backward()\noptimizer.step()\n</code></pre> <p>Tools: PyTorch FSDP, DeepSpeed ZeRO-3.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#13-tensor-parallelism-tp","title":"1.3 Tensor Parallelism (TP)","text":"<p>The idea: Split individual layers across GPUs - like dividing a massive spreadsheet calculation where each person computes a few columns.</p> <p>How it works:</p> <ol> <li>Take a single layer's weight matrix and split it into chunks</li> <li>Each GPU gets one chunk and computes its portion of the output</li> <li>Results are combined (via all-reduce or concatenation) before passing to the next layer</li> <li>This happens at every layer in the model</li> </ol> <p>When to use it:</p> <ul> <li>Individual layers are too large even with FSDP (e.g., huge attention or FFN layers)</li> <li>You have fast GPU-to-GPU connections (NVLink/NVSwitch)</li> <li>You're working within a single node (TP doesn't scale well across nodes due to communication overhead)</li> </ul> <p>Sweet spot: TP degree of 2-8 within a single machine with NVLink.</p> <p></p> <p>Tools: Megatron-LM, TensorRT-LLM, ColossalAI.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#14-pipeline-parallelism-pp","title":"1.4 Pipeline Parallelism (PP)","text":"<p>The idea: Split the model vertically by layers - like an assembly line where each station handles specific layers.</p> <p>How it works:</p> <ol> <li>Divide your model into stages (e.g., layers 1-10, 11-20, 21-30)</li> <li>Assign each stage to a different GPU</li> <li>Send micro-batches through the pipeline: GPU 1 processes batch 1, sends output to GPU 2, then starts on batch 2</li> <li>Multiple micro-batches flow through simultaneously to keep all GPUs busy</li> </ol> <p>When to use it:</p> <ul> <li>Very deep models that don't fit on available GPUs even with FSDP</li> <li>Multi-node training where inter-node bandwidth is limited</li> <li>Combined with TP and FSDP for massive models</li> </ul> <p>Challenge: Pipeline \"bubbles\" (idle time) at the start and end of each batch. Use multiple micro-batches to minimize this.</p> <p></p> <p>Tools: DeepSpeed PP, Megatron-LM, GPipe.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#15-context-parallelism-cp","title":"1.5 Context Parallelism (CP)","text":"<p>The idea: For handling extremely long sequences - different people read different paragraphs of a book, then share key information.</p> <p>How it works:</p> <ol> <li>Split a long sequence (e.g., 64K tokens) across multiple GPUs (e.g., 4 GPUs \u00d7 16K tokens each)</li> <li>Each GPU runs self-attention on its local chunk</li> <li>GPUs exchange keys and values to compute cross-attention (how tokens in one chunk relate to tokens in other chunks)</li> <li>Results are merged to produce the final output</li> </ol> <p>When to use it:</p> <ul> <li>Processing very long contexts (64K, 128K, or even 1M+ tokens)</li> <li>Document analysis, long-form code generation, or book-length reasoning</li> <li>When context length is the bottleneck, not model size</li> </ul> <p>Real-world impact: Context Parallelism enables 100K+ token processing on consumer hardware that would otherwise max out at 8K tokens.</p> <p></p> <p>Tools: Picotron, Nanotron.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#16-expert-parallelism-mixture-of-experts-moe","title":"1.6 Expert Parallelism (Mixture of Experts - MoE)","text":"<p>The idea: Specialized consultants - instead of activating the entire model for every input, each token gets routed only to the \"experts\" it needs.</p> <p>How it works:</p> <ol> <li>Replace dense feed-forward layers with multiple \"expert\" networks (e.g., 8 or 64 experts)</li> <li>A gating network decides which experts (usually top-2) should process each token</li> <li>Only those selected experts activate for that token</li> <li>Different experts can live on different GPUs</li> </ol> <p>When to use it:</p> <ul> <li>You want a model with 100B+ total parameters but only want to activate 13B per token</li> <li>You need better parameter efficiency than dense models</li> <li>You're okay with more complex training dynamics</li> </ul> <p>Real-world examples: Mixtral-8x7B (56B total params, 13B active), Grok, DeepSeek-V2.</p> <p>Trade-off: More parameters with less compute per token, but training can be trickier due to load balancing between experts.</p> <p></p> <p>Tools: Picotron, Nanotron.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#quick-comparison-which-parallelism-should-you-use","title":"Quick Comparison: Which Parallelism Should You Use?","text":"Technique What It Splits Best For Memory Savings Communication Cost Data Parallelism (DP) Data batches Models that fit on 1 GPU None (copies model) Low (only gradients) FSDP Model + optimizer + gradients Models too big for 1 GPU High (4-8x) Medium Tensor Parallelism (TP) Individual layers Huge layers, fast GPUs Medium High (per layer) Pipeline Parallelism (PP) Layer groups (stages) Very deep models Medium Low (between stages) Context Parallelism (CP) Sequence length Long contexts (64K+ tokens) High (for activations) Medium Expert Parallelism (MoE) Experts in MoE layers Massive sparse models None (more params, less FLOPs) Medium <p>Rule of thumb: Start with FSDP. Add TP if individual layers are too big. Add PP if you need multiple nodes. Add CP if context length is your bottleneck.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#2-practical-training-strategies","title":"2. Practical Training Strategies","text":"<p>Now that you understand the techniques, here's what to actually do based on your hardware setup.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#21-single-machine-2-8-gpus","title":"2.1 Single Machine (2-8 GPUs)","text":"<p>Recommended approach: FSDP, optionally + TP</p> <p>What to do:</p> <ol> <li>Start with pure FSDP using PyTorch FSDP or DeepSpeed ZeRO-2/ZeRO-3</li> <li>If your model has huge attention or FFN layers that still don't fit, add TP=2</li> <li>Use Hugging Face <code>accelerate</code> or PyTorch <code>torchrun</code> for easy setup</li> </ol> <p>Hardware-specific tips:</p> <ul> <li>Consumer GPUs (RTX 4090, etc.) with PCIe: Stick to TP=1 or TP=2 max</li> <li>Server GPUs (A100, H100) with NVLink: You can efficiently use TP=2 to TP=4</li> <li>8 GPUs in one box: FSDP alone often works great for models up to 70B</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#22-small-cluster-2-16-nodes-128-gpus","title":"2.2 Small Cluster (2-16 nodes, \u2264128 GPUs)","text":"<p>Recommended approach: 2D or 3D parallelism (TP + FSDP, optionally + PP)</p> <p>What to do:</p> <ol> <li>Use TP within each node (e.g., TP=4 or TP=8 per node with NVLink)</li> <li>Use FSDP across nodes for data parallelism</li> <li>If your model is extremely deep, add PP to split it vertically across nodes</li> </ol> <p>Why this works:</p> <ul> <li>Fast intra-node connections (NVLink) handle TP's high communication needs</li> <li>Slower inter-node connections (InfiniBand) only need to sync FSDP shards</li> <li>Minimizes cross-node bandwidth requirements</li> </ul> <p>Pro tip: When using Pipeline Parallelism, set your number of micro-batches to at least 4\u00d7 your pipeline degree to keep GPUs busy and minimize \"bubbles.\"</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#23-large-cluster-hundreds-or-thousands-of-gpus","title":"2.3 Large Cluster (Hundreds or Thousands of GPUs)","text":"<p>Recommended approach: 4D parallelism (DP \u00d7 TP \u00d7 PP \u00d7 CP)</p> <p>What to do:</p> <ol> <li>Combine all four parallelism strategies to handle the largest models</li> <li>Carefully map parallelism strategies to your hardware topology</li> <li>Use tools like Megatron-LM or Nanotron that support 4D parallelism out of the box</li> </ol> <p>When you need this:</p> <ul> <li>Training models with 70B+ parameters and 32K+ context windows</li> <li>Pretraining from scratch (not fine-tuning)</li> <li>Production-scale model training at big labs</li> </ul> <p>Performance expectations:</p> <ul> <li>With good InfiniBand networking: ~70-80% scaling efficiency</li> <li>With excellent setup and tuning: ~85% scaling efficiency possible</li> </ul> <p>Real-world example: Training a 70B model with 32K context on 512 GPUs:</p> <ul> <li>TP=8 (within each 8-GPU node)</li> <li>PP=4 (pipeline across 4 nodes)</li> <li>CP=4 (split context across 4 chunks)</li> <li>DP=4 (data parallelism for throughput)</li> <li>Total: 8 \u00d7 4 \u00d7 4 \u00d7 4 = 512 GPUs</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#3-practical-tools-worth-learning","title":"3. Practical Tools Worth Learning","text":"<p>Here's a quick guide to the most useful tools and when to reach for them:</p> Tool When to Use It Learning Curve Best For Hugging Face Accelerate Any distributed training with minimal code changes \u2605\u2606\u2606\u2606\u2606 Beginners, quick prototypes PyTorch FSDP Medium-large models (1-30B) on single node \u2605\u2605\u2606\u2606\u2606 Most common use case DeepSpeed ZeRO Multi-node training with good documentation \u2605\u2605\u2605\u2606\u2606 Production training Megatron-LM Very large models (70B+), 3D/4D parallelism \u2605\u2605\u2605\u2605\u2606 Advanced/production at scale Nanotron Learning/research on modern parallelism strategies \u2605\u2605\u2605\u2606\u2606 Education, experimentation vLLM Fast inference with PagedAttention and KV caching \u2605\u2605\u2606\u2606\u2606 Serving models in production TensorRT-LLM Maximum inference speed on NVIDIA GPUs \u2605\u2605\u2605\u2605\u2606 Production inference optimization","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#example-accelerate-config-for-fsdp","title":"Example: Accelerate Config for FSDP","text":"<p>To get started with FSDP using Hugging Face Accelerate, you can run <code>accelerate config</code> or create a <code>config.yaml</code> like this:</p> <pre><code>compute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\nfsdp_config:\n    fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n    fsdp_backward_prefetch: BACKWARD_PRE\n    fsdp_state_dict_type: SHARDED_STATE_DICT\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 8\nuse_cpu: false\n</code></pre> <p>My recommendation for getting started: Start with Hugging Face Accelerate for learning, then graduate to PyTorch FSDP or DeepSpeed when you need more control.</p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#4-making-the-right-choice-a-decision-framework","title":"4. Making the Right Choice: A Decision Framework","text":"<p>Still not sure what to use? Follow this decision tree:</p> <p>Step 1: Does your model fit on a single GPU?</p> <ul> <li>\u2705 Yes \u2192 Use standard training (no parallelism needed)</li> <li>\u274c No \u2192 Continue to Step 2</li> </ul> <p>Step 2: Do you have multiple GPUs in one machine?</p> <ul> <li>\u2705 Yes \u2192 Start with FSDP</li> <li>\u274c No \u2192 You'll need a cluster or smaller model (skip to Step 4)</li> </ul> <p>Step 3: Is FSDP alone enough?</p> <ul> <li>\u2705 Yes \u2192 You're done! Use pure FSDP</li> <li>\u274c No, individual layers are too big \u2192 Add TP=2 or TP=4</li> <li>\u274c No, context is too long \u2192 Add CP</li> </ul> <p>Step 4: Training across multiple nodes?</p> <ul> <li>Start with: TP within nodes + FSDP across nodes</li> <li>If model is very deep: Add PP to split layers across nodes</li> <li>If you have 100+ GPUs and long contexts: Consider 4D parallelism (TP + PP + DP + CP)</li> </ul> <p>Visual decision tree:</p> <p></p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#5-the-ultra-scale-cheatsheet","title":"5. The Ultra-Scale Cheatsheet","text":"<p>For a comprehensive visual summary, check out this guide from Hugging Face's team:</p> <p></p>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/04/scaling-large-language-models---practical-multi-gpu-and-multi-node-strategies-for-2025/#conclusion","title":"Conclusion","text":"<p>Scaling LLMs is both an art and a science. The key takeaways:</p> <ol> <li>Start simple: Most people should begin with FSDP. It handles the majority of use cases.</li> <li>Add complexity only when needed: Don't jump straight to 4D parallelism unless you're training at massive scale.</li> <li>Match strategy to hardware: TP works best within nodes, FSDP across nodes, PP for extreme depth.</li> <li>Tools matter: Use Accelerate to learn, FSDP or DeepSpeed for production.</li> </ol> <p>The techniques here follow logical patterns based on hardware constraints and model architecture. With the right approach, you can scale from a single GPU to thousands, training models that would have been impossible just a few years ago.</p> <p>Further resources:</p> <ul> <li>Hugging Face Ultra-Scale Playbook - Interactive guide with more details</li> <li>PyTorch FSDP Tutorial - Official getting started guide</li> <li>DeepSpeed Tutorials - Comprehensive DeepSpeed documentation</li> </ul>","tags":["LLM","Distributed Training","Deep Learning","GPU","Parallelism"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/","title":"MLOps in the Age of Foundation Models. Evolving Infrastructure for LLMs and Beyond","text":"<p>The field of machine learning has undergone a seismic shift with the rise of large-scale foundation models. From giant language models like GPT-4 to image diffusion models like Stable Diffusion, these powerful models have fundamentally changed how we build and operate ML systems.</p> <p>In this post, I'll explore how ML infrastructure and MLOps practices have evolved to support foundation models. We'll contrast the \"classic\" era of MLOps with modern paradigms, examine what's changed, and look at the new patterns and workflows that have emerged. Think of it as upgrading from a standard toolbox to a fully automated factory\u2014the principles are similar, but the scale and complexity are on a different level.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#1-the-mlops-landscape-a-few-years-ago-pre-foundation-model-era","title":"1. The MLOps Landscape a Few Years Ago (Pre-Foundation Model Era)","text":"<p>A few years back, MLOps primarily meant applying DevOps principles to machine learning. The goal was simple: automate the model lifecycle from data preparation to deployment and monitoring.</p> <p>Back then, ML systems were built around relatively smaller models, often trained from scratch on domain-specific data. Here's what the \"classic\" MLOps era looked like:</p> <p></p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#11-end-to-end-pipelines","title":"1.1. End-to-End Pipelines","text":"<p>Teams built end-to-end pipelines for data extraction, training, validation, and deployment. Apache Airflow orchestrated ETL and training workflows, while CI/CD systems ran automated tests and pushed models to production. The focus was on reproducibility and automation: package models in Docker containers, deploy them as REST microservices or batch jobs, and keep everything running smoothly.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#12-experiment-tracking-and-model-versioning","title":"1.2. Experiment Tracking and Model Versioning","text":"<p>Managing experiments and versions was critical. Platforms like MLflow and Weights &amp; Biases (W&amp;B) became popular for logging training runs, hyperparameters, and metrics. Data scientists could compare experiments and reliably reproduce results. Models were registered in model registries with version numbers, making rollbacks straightforward when a new model underperformed.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#13-continuous-training-cicd","title":"1.3. Continuous Training &amp; CI/CD","text":"<p>Classic MLOps pipelines emphasized continuous integration of new data and models. A typical pipeline might retrain a model nightly or weekly as new data arrived, run a battery of tests, and if tests passed, deploy the new model automatically. Automation tools like Jenkins and GitLab CI/CD ensured that any change in data or code would trigger the pipeline reliably.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#14-infrastructure-and-serving","title":"1.4. Infrastructure and Serving","text":"<p>Serving a model in production meant a relatively small footprint\u2014perhaps a few CPU cores or a single GPU for real-time inference. Kubernetes and Docker became the standard for deploying scalable inference services. Monitoring focused on:</p> <ul> <li>Performance metrics: latency, throughput, memory usage</li> <li>Model metrics: prediction accuracy, concept drift detection</li> <li>System health: uptime, error rates</li> </ul>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#15-feature-stores-and-data-management","title":"1.5. Feature Stores and Data Management","text":"<p>For many ML applications (especially in finance or e-commerce), engineered features were as important as models. Feature stores provided a central place to manage features, ensuring consistency between training and serving. The emphasis was on structured data pipelines and feature engineering. Unstructured data like text and images required custom handling outside these stores.</p> <p>In summary: Classic MLOps revolved around small-to-medium models and explicit feature engineering. The tooling was designed for managing many experiments and deployments\u2014scaling out a large number of models for different tasks rather than scaling one enormous model. This paradigm worked well until models started growing dramatically in size and capability.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#2-the-paradigm-shift-rise-of-large-scale-foundation-models","title":"2. The Paradigm Shift: Rise of Large-Scale Foundation Models","text":"<p>Around 2018-2020, everything changed. Researchers began introducing foundation models\u2014extremely large models pretrained on vast corpora, capable of being adapted to many tasks.</p> <p>The progression was rapid:</p> <ul> <li>2018-2019: BERT and GPT-2 showed the power of transfer learning</li> <li>2020-2021: GPT-3 and PaLM demonstrated what massive scale could achieve</li> <li>2021-2023: Image models like DALL-E and Stable Diffusion brought generative AI to the mainstream</li> <li>2023-2024: Foundation models became ubiquitous\u2014available everywhere from Hugging Face to AWS Bedrock</li> </ul> <p>As one practitioner noted in early 2024: \"Foundational models are everywhere now\u2014a stark change from just two years ago.\"</p> <p>This shift created a fundamentally different paradigm. If classic models were like specialized kitchen gadgets (a toaster, a blender), foundation models are like a professional chef who can learn to cook anything with a little instruction.</p> <p></p> <p>Here's how foundation models changed ML infrastructure:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#21-pretrained-beats-from-scratch","title":"2.1. Pretrained Beats From Scratch","text":"<p>Instead of training models from scratch, teams started with powerful pretrained models and fine-tuned them for specific tasks. This approach:</p> <ul> <li>Cuts training time from weeks to hours or days</li> <li>Reduces data requirements from millions to thousands of examples</li> <li>Enables smaller teams to build sophisticated AI applications</li> </ul> <p>The largest models (with billions of parameters) are often used as-is via APIs or fine-tuned minimally. By 2024, the ML engineer's skillset shifted from \"how to build models\" to \"how to leverage and integrate foundation models\"\u2014treating the model as a service rather than reinventing the wheel.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#22-model-size-and-computational-demands","title":"2.2. Model Size and Computational Demands","text":"<p>The sheer scale of these models introduced new challenges. A model with 175 billion parameters cannot be handled with the same infrastructure as one with 50 million parameters.</p> <p>Key scaling challenges:</p> <ul> <li>Training: Requires powerful hardware (GPUs, TPUs) and distributed computing</li> <li>Model parallelism: Sharding a single model across multiple GPUs</li> <li>Data parallelism: Synchronizing multiple GPU workers during training</li> <li>Inference: Often requires multiple GPUs or specialized runtimes to keep latency acceptable</li> </ul> <p>Libraries like DeepSpeed and ZeRO (Zero Redundancy Optimizer) were developed specifically to make training giant models feasible. The infrastructure requirements jumped by orders of magnitude.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#23-emergence-of-llmops","title":"2.3. Emergence of LLMOps","text":"<p>It became clear that operating these large models in production required extensions to classic MLOps. This led to LLMOps (Large Language Model Operations)\u2014essentially MLOps specialized for large models.</p> <p>LLMOps builds on classic MLOps principles but addresses unique challenges:</p> <ul> <li>Computational resources: Managing expensive GPU clusters</li> <li>Prompt engineering: Optimizing model behavior through input design</li> <li>Safety monitoring: Detecting bias, harmful content, and data leakage</li> <li>Performance management: Balancing latency, quality, and cost</li> </ul> <p>Issues that barely registered for smaller models\u2014like producing biased text or leaking training data\u2014became major considerations at LLM scale.</p> <p></p> <p>This diagram from NVIDIA illustrates how general MLOps (outer circle) has branched into specialized subfields like generative AI operations (for all generative models), LLMOps (for large language models), and even RAGOps for retrieval-augmented generation. The concentric circles indicate that these specializations build on the foundation of classic MLOps.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#24-foundation-models-as-a-service","title":"2.4. Foundation Models as a Service","text":"<p>Another major shift was the rise of models as a service. Instead of deploying their own models, many applications now call external APIs:</p> <p>API Providers:</p> <ul> <li>OpenAI, Cohere, AI21 Labs offer hosted LLMs</li> <li>Google's Vertex AI provides Model Garden with pretrained models</li> <li>AWS Bedrock hosts proprietary foundation models</li> </ul> <p>Model Hubs:</p> <ul> <li>Hugging Face hosts thousands of pretrained models</li> <li>Models can be downloaded or run in the cloud</li> <li>Version control and community sharing became standard</li> </ul> <p>This changed ML architecture fundamentally. Production pipelines might call external APIs for inference, introducing new considerations:</p> <ul> <li>Latency: Network calls add overhead</li> <li>Cost: Pay-per-token pricing models</li> <li>Data privacy: Sending data to third parties</li> <li>Vendor lock-in: Dependency on external services</li> </ul> <p>But it also saves the massive effort of managing model infrastructure.</p> <p>The paradigm shift: From \"your data + your model code = trained model\" to \"your data + adaptation of a pretrained model = fine-tuned model (or just prompt it with your data).\"</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#3-new-requirements-and-capabilities-in-modern-ml-infrastructure","title":"3. New Requirements and Capabilities in Modern ML Infrastructure","text":"<p>With foundation models at the center, today's ML infrastructure must support capabilities that were niche or non-existent just a few years ago. Here are the key new requirements:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#31-distributed-training-and-model-parallelism","title":"3.1. Distributed Training and Model Parallelism","text":"<p>Training a model with billions of parameters is beyond the capacity of a single machine. Modern ML infrastructure orchestrates distributed training across multiple nodes:</p> <p></p> <p>Two main approaches:</p> <ul> <li>Model parallelism: Split the model's layers across multiple GPUs (each GPU handles part of the model)</li> <li>Data parallelism: Replicate the model across GPUs and split the training data (synchronize gradients)</li> </ul> <p>Tools that enable this:</p> <ul> <li>PyTorch Lightning, Horovod for general distributed training</li> <li>NVIDIA's Megatron-LM for massive transformer models</li> <li>Google's JAX/TPU ecosystem for TPU clusters</li> </ul> <p>A few years ago, most teams trained models on a single server. Now, ML platforms must handle launching jobs on GPU clusters, managing faults, and aggregating gradients from dozens or hundreds of workers seamlessly.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#32-efficient-fine-tuning-techniques","title":"3.2. Efficient Fine-Tuning Techniques","text":"<p>Training from scratch is impractical for huge models, but even fine-tuning a multi-billion parameter model can be resource-intensive. This led to parameter-efficient fine-tuning methods:</p> <p>Modern fine-tuning approaches:</p> <ul> <li>LoRA (Low-Rank Adaptation): Updates only a small subset of parameters (adapters) instead of the entire network, dramatically reducing computational cost</li> <li>Prompt Tuning: Optimizes only the prompt embeddings, keeping the model frozen</li> <li>Adapter Modules: Adds small trainable layers between frozen model layers</li> </ul> <p>Here is a simple example of how you might configure LoRA using the <code>peft</code> library:</p> <pre><code>from peft import LoraConfig, get_peft_model\n\n# Configure LoRA\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply to base model\n# model = get_peft_model(base_model, peft_config)\n# model.print_trainable_parameters()\n</code></pre> <p>ML infrastructure must now support complex workflows: load a base model from a hub, apply fine-tuned weight deltas, and deploy the combined model. Traditional training pipelines evolved significantly to accommodate this multi-step customization.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#33-prompt-engineering-management","title":"3.3. Prompt Engineering &amp; Management","text":"<p>One surprising new artifact in modern ML pipelines is the prompt. With LLMs, much of the model's behavior is controlled through the text prompt or input format you give it.</p> <p>This created an entirely new discipline. Teams now:</p> <ul> <li>Maintain prompt libraries and templates</li> <li>Use version control for prompts (just like code)</li> <li>Run A/B tests to compare prompt variants</li> <li>Store prompt versions alongside model versions</li> </ul> <p>This is fundamentally different from classic ML, where inputs were just data features\u2014not natural language instructions. Frameworks like LangChain now include prompt optimization as a first-class feature.</p> <p>Example prompt evolution:</p> <pre><code>v1: \"Classify this text as positive or negative: {text}\"\nv2: \"You are a sentiment analyzer. Classify: {text}\"\nv3: \"Analyze sentiment. Return only 'positive' or 'negative': {text}\"\n</code></pre> <p>Each version can produce different results, so tracking and testing prompts became as important as tracking model weights.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#34-retrieval-augmented-generation-rag","title":"3.4. Retrieval-Augmented Generation (RAG)","text":"<p>Foundation models have a fixed knowledge cutoff and limited context windows. To keep responses accurate and up-to-date, Retrieval-Augmented Generation (RAG) has become a best practice.</p> <p>How RAG works:</p> <p></p> <p>Instead of continuously retraining the model on new data (costly and slow), RAG fetches information at query time. The retrieved documents are appended to the prompt as additional context.</p> <p>Here's a simplified view of how this looks in code using LangChain:</p> <pre><code>from langchain.vectorstores import Pinecone\nfrom langchain.llms import OpenAI\nfrom langchain.chains import RetrievalQA\n\n# 1. Load the vector database\n# vector_db = Pinecone.from_existing_index(\"my-index\", embeddings)\n\n# 2. Initialize the LLM\n# llm = OpenAI(temperature=0)\n\n# 3. Create the RAG chain\n# qa_chain = RetrievalQA.from_chain_type(\n#     llm=llm,\n#     chain_type=\"stuff\",\n#     retriever=vector_db.as_retriever()\n# )\n\n# 4. Ask a question\n# response = qa_chain.run(\"How does LLMOps differ from MLOps?\")\n</code></pre> <p>New infrastructure components:</p> <ul> <li>Vector databases (Pinecone, Weaviate, FAISS, Milvus) for fast similarity search on embeddings</li> <li>Embedding models to convert documents into vectors</li> <li>Index management to keep embeddings in sync with the latest data</li> </ul> <p>In many ways, vector databases have replaced traditional feature stores. Unstructured data and semantic search took center stage over manual feature engineering.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#35-data-streaming-and-real-time-data-feeds","title":"3.5. Data Streaming and Real-Time Data Feeds","text":"<p>Modern applications\u2014especially LLM-powered assistants\u2014continuously ingest data: chat conversations, sensor data, event streams. This data needs to update the model's knowledge (via RAG) or trigger responses in real-time.</p> <p>The shift:</p> <ul> <li>Classic MLOps: Batch processing (daily/weekly training jobs)</li> <li>Modern LLMOps: Real-time streaming data pipelines</li> </ul> <p>Technologies driving this:</p> <ul> <li>Kafka and event streaming platforms</li> <li>Real-time databases (Redis, DynamoDB)</li> <li>Online feature stores with continuous updates</li> <li>Streaming embeddings that update vector indexes in real-time</li> </ul> <p>The boundary between data engineering and MLOps has blurred. Data pipelines now directly feed model inference rather than just training.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#36-scalable-and-specialized-serving-infrastructure","title":"3.6. Scalable and Specialized Serving Infrastructure","text":"<p>Serving a massive model is challenging. Modern ML infrastructure must support three key capabilities:</p> <p>High-Throughput, Low-Latency Serving</p> <p>Interactive applications (chatbots, image generators) demand fast responses. This requires:</p> <ul> <li>GPU/TPU acceleration for quick inference</li> <li>Model quantization to reduce precision and speed up serving</li> <li>GPU batching to serve multiple requests in parallel</li> <li>Optimized serving engines like NVIDIA's TensorRT, Triton Inference Server, or DeepSpeed-Inference</li> </ul> <p>Serverless and Elastic Scaling</p> <p>A new trend toward serverless ML has emerged. Platforms like Modal offer \"AWS Lambda but with GPU support\"\u2014you provide code, they handle infrastructure and scaling.</p> <p>Benefits:</p> <ul> <li>No always-running servers</li> <li>Compute spins up on-demand</li> <li>Scale to zero when idle (pay only per execution)</li> <li>Automatic scaling under load</li> </ul> <p>Tradeoffs:</p> <ul> <li>Cold-start latency when spinning up</li> <li>Managing statelessness</li> <li>Less control over infrastructure</li> </ul> <p>This works well for irregular workloads where managing GPU clusters is overkill.</p> <p>Distributed Model Serving</p> <p>For models too large for a single GPU, inference itself can be distributed. The model is sharded across multiple machines, each handling part of the forward pass.</p> <p>Example: Serving a 175B parameter model on-premises requires multiple GPUs working together. Modern ML infrastructure must launch distributed inference replicas and route requests appropriately.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#37-monitoring-observability-and-guardrails","title":"3.7. Monitoring, Observability, and Guardrails","text":"<p>With great power comes great responsibility. Large models can generate incorrect or inappropriate outputs in ways small models never did. Modern ML systems need three layers of monitoring:</p> <p>Performance and Reliability</p> <p>The basics still matter:</p> <ul> <li>Latency, throughput, memory usage</li> <li>GPU utilization and costs</li> <li>Autoscaling policies (scale up under load, fall back to smaller models if needed)</li> </ul> <p>Output Quality and Safety</p> <p>We now monitor the content of outputs:</p> <ul> <li>Content filtering: Detect hate speech, PII, harmful content</li> <li>Moderation APIs: Use OpenAI's moderation API or custom filters</li> <li>Bias detection: Continuously evaluate for biased responses</li> <li>Guardrails: Intercept adversarial inputs and ensure outputs stay within bounds</li> </ul> <p>These \"guardrails\" have become essential in LLMOps\u2014they're not optional.</p> <p>Feedback Loops</p> <p>Continuous improvement now includes human feedback:</p> <ul> <li>Collect user interactions (likes, corrections, ratings)</li> <li>Use feedback to fine-tune models or adjust prompts</li> <li>RLHF (Reinforcement Learning from Human Feedback): Explicitly use human ratings to refine behavior</li> </ul> <p>The infrastructure must support collecting and managing this feedback data securely.</p> <p>In summary: Today's ML infrastructure manages entire ecosystems\u2014base models, fine-tuning adapters, prompt templates, retrieval indexes, monitoring detectors, and more. The complexity is higher, but so is the capability.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#4-evolving-system-architecture-and-design-patterns","title":"4. Evolving System Architecture and Design Patterns","text":"<p>Given these new requirements, how are ML systems actually structured today? Here are the key design patterns that have emerged:</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#41-modular-pipelines-orchestration","title":"4.1. Modular Pipelines &amp; Orchestration","text":"<p>Classic tools (Kubeflow Pipelines, Apache Airflow) are still used for:</p> <ul> <li>Fine-tuning workflows</li> <li>Batch scoring jobs</li> <li>Periodic model retraining</li> </ul> <p>New tools have emerged for modern needs:</p> <ul> <li>Metaflow, Flyte, ZenML: Pythonic workflows that integrate seamlessly with ML libraries</li> <li>Lightweight orchestration: For low-latency inference, application code often replaces heavyweight workflow engines</li> </ul> <p>The key difference: engineers no longer need to leave their development environment to manage the flow from data to deployment.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#42-model-hubs-and-registries","title":"4.2. Model Hubs and Registries","text":"<p>Model management evolved with centralized hubs:</p> <p>External hubs:</p> <ul> <li>Hugging Face Hub: Thousands of models, datasets, and scripts</li> <li>One-stop shop for ML components</li> <li>Plug-and-play architecture (fetch models at startup)</li> </ul> <p>Internal registries:</p> <ul> <li>MLflow Registry, SageMaker Model Registry for bespoke models</li> <li>Combined with external foundation models</li> </ul> <p>The shift: Instead of building everything in-house, engineers now plan for how to fine-tune and adapt third-party models. This has accelerated development dramatically.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#43-feature-stores-vs-vector-databases","title":"4.3. Feature Stores vs. Vector Databases","text":"<p>The data layer has fundamentally changed:</p> <p></p> <p>Traditional feature stores handled structured data with manual feature engineering.</p> <p>Modern vector databases (Pinecone, Weaviate, Chroma, Milvus) handle:</p> <ul> <li>High-dimensional embeddings</li> <li>Fast similarity search</li> <li>Semantic search and deduplication</li> <li>RAG for LLMs</li> </ul> <p>You'll often see both: a vector DB for unstructured semantic lookup and a data warehouse for structured analytics.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#44-unified-platforms-end-to-end","title":"4.4. Unified Platforms (End-to-End)","text":"<p>The complexity of modern ML has driven adoption of end-to-end platforms that abstract infrastructure details.</p> <p>Cloud platforms evolved to support foundation models:</p> <ul> <li>Google Vertex AI: Auto-distributed training on TPU pods, Model Garden with LLMs, one-click deployment</li> <li>AWS SageMaker: Distributed training, model parallelism, and Bedrock for hosted foundation models</li> <li>Azure Machine Learning: Integrated training, deployment, and monitoring</li> </ul> <p>These platforms provide managed services like \"fine-tune this 20B parameter model on your data\" or \"embed and index your text data for retrieval.\"</p> <p>Open-source and startups:</p> <ul> <li>MosaicML (now Databricks): Efficient training and deployment for large models</li> <li>Argilla, Label Studio: Data labeling and prompt dataset creation</li> <li>ClearML, MLflow: Experiment tracking tied to pipeline execution</li> </ul>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#45-inference-gateways-and-apis","title":"4.5. Inference Gateways and APIs","text":"<p>The proliferation of model sizes led to inference gateways\u2014routers that intelligently direct requests:</p> <p></p> <p>Use cases:</p> <ul> <li>Route based on latency requirements</li> <li>Different models for different subscription tiers</li> <li>A/B testing new models on a fraction of traffic</li> <li>Fallback to smaller models under high load</li> </ul> <p>This decouples the client-facing API from model implementation, allowing seamless model swaps and testing.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#46-agentic-systems","title":"4.6. Agentic Systems","text":"<p>A cutting-edge pattern: AI agents that dynamically choose sequences of actions to accomplish tasks.</p> <p>Unlike static chains, agents can:</p> <ul> <li>Call external tools (calculators, search engines, databases)</li> <li>Decide workflows at runtime based on context</li> <li>Invoke different models for different subtasks</li> </ul> <p>Enabling frameworks:</p> <ul> <li>LangChain's agent mode</li> <li>OpenAI's function calling</li> <li>AutoGPT and similar systems</li> </ul> <p>This emerging pattern requires new operational practices (sometimes called \"AgentOps\"):</p> <ul> <li>Robust monitoring to prevent unwanted actions</li> <li>Detailed logging to trace decision paths</li> <li>Safety guardrails to limit agent capabilities</li> </ul> <p></p> <p>While not yet widespread in production, agentic systems represent the frontier of LLMOps.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#5-getting-started-with-llmops","title":"5. Getting Started with LLMOps","text":"<p>If you are new to this field, the ecosystem can feel overwhelming. Here is a recommended path to get your hands dirty:</p> <ol> <li>Play with APIs: Start by using OpenAI or Anthropic APIs to understand prompt engineering.</li> <li>Build a RAG App: Use LangChain or LlamaIndex to build a simple \"Chat with your PDF\" app. This introduces you to vector databases and retrieval.</li> <li>Try Fine-Tuning: Use Hugging Face to fine-tune a small model (like Llama-3-8B or Mistral-7B) on a custom dataset using Google Colab.</li> <li>Deploy: Try deploying your fine-tuned model using vLLM or Ollama locally, then move to a cloud provider.</li> </ol>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#6-conclusion-from-mlops-to-llmops-and-beyond","title":"6. Conclusion: From MLOps to LLMOps and Beyond","text":"<p>In just a few years, we've witnessed a transformation in how we approach machine learning in production.</p> <p>What remains the same:</p> <ul> <li>Automation, reproducibility, collaboration</li> <li>Focus on reliability and efficiency</li> <li>DevOps principles applied to ML</li> </ul> <p>What changed dramatically:</p> <ul> <li>Scale: From millions to billions of parameters</li> <li>Approach: From training from scratch to adapting foundation models</li> <li>Infrastructure: From single servers to distributed GPU clusters</li> <li>Data layer: From feature stores to vector databases</li> <li>Monitoring: From performance metrics to content safety guardrails</li> </ul> <p>This gave rise to LLMOps\u2014a specialization of MLOps for managing the lifecycle of large models. It's not just hype. The differences are tangible in day-to-day workflows:</p> <ul> <li>How we fine-tune models (LoRA, adapters)</li> <li>How we deploy them (distributed serving, serverless GPUs)</li> <li>How we monitor them (content filtering, bias detection)</li> <li>What infrastructure we need (vector databases, GPU clusters)</li> </ul> <p>The evolution continues. As models grow and AI systems become more autonomous, we're already seeing:</p> <ul> <li>AgentOps for managing AI agents</li> <li>RAGOps for retrieval-augmented systems</li> <li>Even more specialized operational practices</li> </ul> <p>But the end goal remains: reliably deliver the benefits of machine learning to end-users and business applications, at scale and with trustworthiness.</p> <p>Teams that successfully navigate this evolution harness foundation models to build products faster than ever\u2014while maintaining the reliability and efficiency that good operations provide.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/06/mlops-in-the-age-of-foundation-models-evolving-infrastructure-for-llms-and-beyond/#references","title":"References","text":"<p>The insights and examples in this post are supported by recent research and industry sources, including an MDPI review on transitioning from MLOps to LLMOps, NVIDIA's technical blogs on GenAIOps and LLMOps, and various practitioner articles and discussions capturing the state of ML in 2024. Platforms like Modal and Ray have published guides showing new deployment patterns (serverless GPUs, distributed serving) in action.</p>","tags":["mlops","llmops","infrastructure"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/","title":"Mastering Zsh Startup: <code>~/.zprofile</code> vs <code>~/.zshrc</code> \ud83d\ude80","text":"<p>If you've ever wondered why your terminal feels slow, or why your environment variables aren't loading where you expect them to, you're likely battling the Zsh startup order.</p> <p>The distinction between <code>~/.zprofile</code> and <code>~/.zshrc</code> is one of the most common sources of confusion for developers moving to Zsh (especially on macOS).</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#tldr","title":"TL;DR \u26a1","text":"<ul> <li><code>~/.zprofile</code> is for Environment Setup. It runs once when you log in (or open a terminal tab on macOS). Put your <code>PATH</code>, <code>EDITOR</code>, and language version managers (like <code>fnm</code>, <code>pyenv</code>) here.</li> <li><code>~/.zshrc</code> is for Interactive Configuration. It runs every time you start a new shell instance. Put your aliases, prompt themes, and key bindings here.</li> </ul>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#the-shell-startup-flow","title":"The Shell Startup Flow \ud83d\udc1a","text":"<p>To understand where to put things, you need to understand when files are loaded. Zsh has a specific hierarchy of configuration files.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#login-vs-interactive-shells","title":"Login vs. Interactive Shells","text":"<ol> <li>Login Shell: The first shell you enter after authentication. On macOS, every new terminal tab or window is a login shell by default. This is a key difference from Linux, where opening a terminal usually starts a non-login interactive shell.</li> <li>Interactive Shell: Any shell where you can type commands.</li> </ol> <p>Here is the actual flow of execution when you open a terminal on macOS:</p> <p></p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#the-loading-order","title":"The Loading Order","text":"<ol> <li><code>~/.zshenv</code>: (Optional) Runs for every shell script and command. Avoid putting output or heavy logic here, as it can break scripts. Use it only for essential environment variables that must exist everywhere (rarely needed for average users).</li> <li><code>~/.zprofile</code>: Runs only for login shells. This is your \"setup\" phase.</li> <li><code>~/.zshrc</code>: Runs for interactive shells. This is your \"customization\" phase.</li> <li><code>~/.zlogin</code>: (Optional) Runs at the very end of a login shell startup.</li> </ol>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#what-goes-where","title":"What Goes Where? \ud83d\udcc1","text":"","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#1-zprofile-the-environment-layer","title":"1. <code>~/.zprofile</code>: The Environment Layer \ud83c\udf0d","text":"<p>Think of this as the foundation of your house. It sets up the rules of physics (paths, variables) that everything else relies on.</p> <p>What belongs here:</p> <ul> <li><code>PATH</code> modifications: Adding directories to your executable path.</li> <li>Environment Variables: <code>EDITOR</code>, <code>LANG</code>, <code>GOPATH</code>, <code>JAVA_HOME</code>.</li> <li>Tool Initialization: Things that modify the environment, like <code>pyenv</code>, <code>rbenv</code>, <code>fnm</code>, or <code>cargo</code>.</li> </ul> <p>Why? These only need to be calculated once. If you put them in <code>.zshrc</code>, they will be re-calculated every time you open a sub-shell or run a script, which is wasteful and can lead to duplicate entries in your <code>PATH</code>.</p> <pre><code># ~/.zprofile\n\n# 1. Set up your PATH\n# Ensure local bin is first so your tools override system ones\nexport PATH=\"$HOME/.local/bin:/opt/homebrew/bin:$PATH\"\n\n# 2. Set global variables\nexport EDITOR=\"nvim\"\nexport VISUAL=\"nvim\"\nexport LANG=\"en_US.UTF-8\"\n\n# 3. Initialize Version Managers (The Heavy Lifters)\n# Doing this here keeps your shell startup snappy!\neval \"$(fnm env --use-on-cd)\"\neval \"$(pyenv init -)\"\n</code></pre>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#2-zshrc-the-interactive-layer","title":"2. <code>~/.zshrc</code>: The Interactive Layer \ud83c\udfae","text":"<p>Think of this as the interior decoration. It makes the house comfortable to live in.</p> <p>What belongs here:</p> <ul> <li>Aliases: <code>alias g='git'</code>.</li> <li>Prompt: Starship, Powerlevel10k, or Pure.</li> <li>Completions: <code>compinit</code>.</li> <li>Key Bindings: <code>bindkey</code>.</li> <li>Shell Options: <code>setopt autocd</code>, <code>setopt histignorealldups</code>.</li> </ul> <p>Why? These settings only matter when a human is typing at the keyboard. A script running in the background doesn't need your fancy prompt or your git aliases.</p> <pre><code># ~/.zshrc\n\n# 1. Load your prompt (Visuals)\nautoload -Uz promptinit &amp;&amp; promptinit\nprompt pure\n\n# 2. Aliases (Shortcuts)\nalias ll='ls -lah'\nalias g='git'\nalias gs='git status'\n\n# 3. Shell Options (Behavior)\nsetopt autocd              # cd by just typing directory name\nsetopt histignorealldups   # Don't record duplicate history entries\nsetopt share_history       # Share history between tabs\n\n# 4. Completions (The Magic)\nautoload -Uz compinit &amp;&amp; compinit\n</code></pre>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#why-not-just-put-everything-in-one-file","title":"Why Not Just Put Everything in One File? \ud83e\udd14","text":"<p>You might be asking: \"Why can't I just put my aliases in <code>.zprofile</code> and run them once? Why do I need to reload them?\"</p> <p>It comes down to Inheritance vs. Re-definition.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#1-environment-variables-inherit","title":"1. Environment Variables Inherit \ud83e\uddec","text":"<p>When you set <code>export EDITOR=\"vim\"</code> in a parent shell (like your login shell), every child process (sub-shells, scripts, programs) inherits that variable. You set it once, and it propagates everywhere. This is why <code>.zprofile</code> is perfect for <code>export</code>.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#2-aliases-and-functions-do-not-inherit","title":"2. Aliases and Functions Do Not Inherit \ud83d\udeab","text":"<p>Aliases (<code>alias g='git'</code>) and shell functions are local to the current shell instance. They are not passed down to child shells.</p> <ul> <li>If you define an alias in <code>.zprofile</code>, it exists in your top-level login shell.</li> <li>If you then type <code>zsh</code> to start a sub-shell, or run a script, that alias disappears.</li> <li>To make aliases available everywhere, you must re-define them in every new interactive shell. That is exactly what <code>.zshrc</code> does.</li> </ul>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#3-scripts-dont-need-human-features","title":"3. Scripts Don't Need \"Human\" Features \ud83e\udd16","text":"<p>When you run a shell script (e.g., <code>./deploy.sh</code>), it starts a new, non-interactive shell.</p> <ul> <li>It doesn't need your fancy prompt.</li> <li>It doesn't need your <code>git</code> aliases.</li> <li>It definitely doesn't want to wait for <code>oh-my-zsh</code> to load.</li> </ul> <p>By keeping interactive config in <code>.zshrc</code>, you ensure that your scripts run fast and clean, without being polluted by your personal customization.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#common-pitfalls-best-practices","title":"Common Pitfalls &amp; Best Practices \ud83d\udeab","text":"","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#pitfall-1-putting-nvm-or-pyenv-in-zshrc","title":"\ud83d\uded1 Pitfall 1: Putting <code>nvm</code> or <code>pyenv</code> in <code>.zshrc</code>","text":"<p>The Symptom: You open a new terminal tab, and it takes 2-3 seconds before you can type anything. The Cause: Version managers often have heavy initialization scripts. If you put them in <code>.zshrc</code>, they run every single time. The Fix: Move them to <code>~/.zprofile</code>.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#pitfall-2-growing-path","title":"\ud83d\uded1 Pitfall 2: Growing <code>PATH</code>","text":"<p>The Symptom: Your <code>$PATH</code> variable has the same directories listed 5 times. The Cause: You have <code>export PATH=\"$HOME/bin:$PATH\"</code> in your <code>.zshrc</code>. Every time you reload the config (<code>source ~/.zshrc</code>) or open a sub-shell, it appends the path again. The Fix: Move <code>PATH</code> definitions to <code>~/.zprofile</code>.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#pro-tip-the-reload-trick","title":"\ud83d\udca1 Pro Tip: The \"Reload\" Trick","text":"<p>If you make changes to <code>~/.zprofile</code>, they won't apply to your current shell immediately because <code>.zprofile</code> is only read at login. You have two options:</p> <ol> <li>Close the tab and open a new one (easiest).</li> <li>Manually source it: <code>source ~/.zprofile</code>.</li> </ol> <p>For <code>.zshrc</code> changes, you can always just run:</p> <pre><code>source ~/.zshrc\n</code></pre>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#a-robust-configuration-strategy","title":"A Robust Configuration Strategy \ud83d\udee0\ufe0f","text":"<p>If you use multiple machines (e.g., macOS at work, Linux at home), you might want a setup that handles both gracefully.</p> <p>Since Linux terminals often start as non-login shells, they might skip <code>~/.zprofile</code>. A common pattern to support both is to source <code>.zprofile</code> from <code>.zshrc</code> if it hasn't been loaded.</p> <p>In your <code>~/.zshrc</code>:</p> <pre><code># ~/.zshrc\n\n# If we are on Linux/Non-login shell, ensure environment is set\nif [[ -o interactive &amp;&amp; ! -o login ]]; then\n    [[ -f ~/.zprofile ]] &amp;&amp; source ~/.zprofile\nfi\n\n# ... rest of your interactive config\n</code></pre>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/07/mastering-zsh-startup-zprofile-vs-zshrc-/#summary","title":"Summary","text":"File Purpose Examples <code>~/.zshenv</code> Critical Env Vars <code>ZDOTDIR</code> (Advanced users only) <code>~/.zprofile</code> Environment Setup <code>PATH</code>, <code>EDITOR</code>, <code>eval \"$(pyenv init -)\"</code> <code>~/.zshrc</code> Interactive Config <code>alias</code>, <code>prompt</code>, <code>bindkey</code>, <code>compinit</code> <p>Keep your environment in <code>.zprofile</code> and your experience in <code>.zshrc</code>, and you'll have a fast, clean, and reliable terminal experience.</p>","tags":["tooling","macos","guide","zsh"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/","title":"The Ultimate Guide to <code>pyproject.toml</code>","text":"","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#tldr","title":"TL;DR","text":"<p>Think of <code>pyproject.toml</code> as the <code>package.json</code> for Python. It's a single configuration file that holds your project's metadata, dependencies, and tool settings. Whether you use <code>.venv</code>, <code>pyenv</code>, or <code>uv</code>, this one file simplifies development and makes collaboration smoother.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#what-is-pyprojecttoml","title":"What is <code>pyproject.toml</code>?","text":"<p><code>pyproject.toml</code> is a standardized configuration file that lives at the root of your Python project. It uses the TOML format (think INI files but better) and is backed by official Python Enhancement Proposals (PEPs).</p> <p>The file evolved in two key stages:</p> <ul> <li>PEP 518 (2016) introduced the <code>[build-system]</code> table so build tools could declare their requirements in a standard way.</li> <li>PEP 621 (2020) added the <code>[project]</code> table for core package metadata\u2014name, version, dependencies, and more.</li> </ul> <p>Today, most Python developer tools (Black, isort, pytest, Ruff, mypy) read their configuration from <code>[tool.*]</code> sections in this file, making it the central hub for your entire project setup.</p> <p></p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#why-should-you-care","title":"Why should you care?","text":"","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#1-one-file-to-rule-them-all","title":"1. One file to rule them all","text":"<p>Before <code>pyproject.toml</code>, you'd juggle <code>setup.py</code>, <code>setup.cfg</code>, <code>requirements.txt</code>, <code>MANIFEST.in</code>, and various dotfiles (<code>.flake8</code>, <code>.coveragerc</code>). Now everything lives in one place.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#2-backend-agnostic-builds","title":"2. Backend-agnostic builds","text":"<p>When you run <code>pip install .</code>, pip reads <code>pyproject.toml</code> and automatically installs whatever build tools your project needs (setuptools, flit, hatchling, etc.). You are no longer tied to <code>setuptools</code>.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#3-universal-tool-configuration","title":"3. Universal tool configuration","text":"<p>Linters, formatters, test runners, and type checkers all know to look here for their settings. Your IDE, CI pipeline, and teammates all read from the same source of truth.</p> <p></p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#anatomy-of-a-pyprojecttoml","title":"Anatomy of a <code>pyproject.toml</code>","text":"<p>Here's what a typical file looks like with the three main sections:</p> <pre><code># 1. Build system - tells pip/build how to package your project\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n# 2. Project metadata and dependencies\n[project]\nname = \"awesome-app\"\nversion = \"0.1.0\"\ndescription = \"Short demo of pyproject.toml\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n  \"fastapi&gt;=0.111\",\n  \"uvicorn[standard]&gt;=0.30\",\n]\n\n# Expose CLI commands\n[project.scripts]\nawesome-cli = \"awesome_app.cli:main\"\n\n# Optional dependencies (e.g., for development)\n[project.optional-dependencies]\ndev = [\"pytest\", \"ruff\", \"mypy\"]\n\n# 3. Tool configuration\n[tool.ruff]\nline-length = 100\ntarget-version = \"py312\"\n\n[tool.pytest.ini_options]\naddopts = \"-ra -q\"\ntestpaths = [\"tests\"]\n</code></pre>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#breaking-it-down","title":"Breaking it down","text":"","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#build-system","title":"<code>[build-system]</code>","text":"<p>Required if you want to package/distribute your project. Tells pip and build tools (like <code>python -m build</code>) which backend to use.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#project","title":"<code>[project]</code>","text":"<p>Your package metadata. This is where dependencies live instead of <code>requirements.txt</code>.</p> <ul> <li><code>dependencies</code>: The runtime requirements for your package.</li> <li><code>optional-dependencies</code>: Groups of extra dependencies (e.g., <code>dev</code>, <code>test</code>, <code>docs</code>).</li> <li><code>scripts</code>: Creates executable commands. In the example above, installing the package creates an <code>awesome-cli</code> command that runs the <code>main</code> function in <code>awesome_app/cli.py</code>.</li> </ul>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#tool","title":"<code>[tool.*]</code>","text":"<p>Configuration for any tool that supports it. Each tool gets its own namespace (e.g., <code>[tool.pytest.ini_options]</code>, <code>[tool.mypy]</code>).</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#does-it-replace-requirementstxt","title":"Does it replace <code>requirements.txt</code>?","text":"<p>In modern workflows, yes. Tools like Poetry, PDM, Hatch, and uv store dependencies directly in the <code>[project]</code> section and generate lockfiles for reproducibility.</p> <p>You only need <code>requirements.txt</code> if:</p> <ul> <li>You're working with legacy deployment systems that expect it.</li> <li>You have simple CI scripts that haven't been updated.</li> </ul> <p>Most modern tools can export a <code>requirements.txt</code> from your <code>pyproject.toml</code> when needed:</p> <pre><code>uv export &gt; requirements.txt\n</code></pre>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#choosing-a-build-backend","title":"Choosing a Build Backend","text":"<p>One of the confusing parts of <code>pyproject.toml</code> is choosing a build backend. Here is a quick comparison:</p> Backend Best For Pros Cons Hatchling Modern standard Fast, extensible, supports plugins Newer, less legacy support Flit Simple packages Extremely simple, zero config Not for complex builds (C extensions) Setuptools Legacy / Complex Supports everything (C extensions, etc.) Slower, complex configuration Poetry Poetry users Integrated with Poetry ecosystem Locked into Poetry workflow <p>Recommendation: Use Hatchling for new pure-Python projects. It's the default for <code>uv</code> and is becoming the industry standard.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#migrating-an-existing-project","title":"Migrating an existing project","text":"<p>If you have a legacy Python project, here's how to modernize it:</p> <p></p> <p>Step-by-step:</p> <ol> <li>Add <code>[build-system]</code> - Start with setuptools if you're not sure: <code>requires = [\"setuptools&gt;=61\", \"wheel\"]</code>.</li> <li>Move to <code>[project]</code> - Transfer name, version, dependencies from <code>setup.py</code> or <code>setup.cfg</code>.</li> <li>Convert dev dependencies - Put them in <code>[project.optional-dependencies].dev</code>.</li> <li>Configure tools - Add <code>[tool.*]</code> sections for Black, pytest, mypy, etc.</li> <li>Handle <code>requirements.txt</code> - Either drop it or generate it from lockfile for legacy systems.</li> </ol> <p>After migration, you can delete <code>setup.py</code>, <code>setup.cfg</code>, and most config dotfiles.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#advanced-features","title":"Advanced Features","text":"","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#cli-entry-points","title":"CLI Entry Points","text":"<p>Instead of the old <code>console_scripts</code> in <code>setup.py</code>, use <code>[project.scripts]</code>:</p> <pre><code>[project.scripts]\nmy-tool = \"my_package.main:run\"\n</code></pre> <p>When a user installs your package, they can type <code>my-tool</code> in their terminal.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#workspaces-monorepos","title":"Workspaces (Monorepos)","text":"<p>Tools like <code>uv</code> and <code>hatch</code> support workspaces, allowing you to manage multiple packages in a single repo.</p> <pre><code>[tool.uv.workspace]\nmembers = [\"packages/*\"]\n</code></pre> <p>This allows you to develop multiple interdependent packages and install them all into a single virtual environment for testing.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#typical-workflows-with-uv","title":"Typical Workflows with <code>uv</code>","text":"","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#starting-a-new-project","title":"Starting a new project","text":"<pre><code>uv init my_app          # creates folder with pyproject.toml and .venv\ncd my_app\nuv add requests fastapi # adds to [project.dependencies] and installs\nuv run pytest           # runs tests in the venv\n</code></pre>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#running-scripts","title":"Running scripts","text":"<p>You can define scripts in <code>pyproject.toml</code> (if using a task runner like <code>poe</code> or <code>hatch</code>) or just use <code>uv run</code>:</p> <pre><code>uv run python main.py\n</code></pre>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/08/the-ultimate-guide-to-pyprojecttoml/#best-practices","title":"Best Practices","text":"<ol> <li>Don't pin exact versions in libraries: Use ranges (e.g., <code>requests&gt;=2.30</code>) so your library doesn't conflict with others.</li> <li>Do pin versions in applications: Use a lockfile (<code>uv.lock</code> or <code>poetry.lock</code>) to ensure reproducible builds.</li> <li>Group dev dependencies: Keep testing, linting, and docs dependencies in separate optional groups (e.g., <code>dev</code>, <code>test</code>, <code>docs</code>).</li> <li>Keep it clean: Don't dump every possible config option in there. Stick to project-wide defaults.</li> </ol> <p>Bottom line: <code>pyproject.toml</code> brings Python's project setup into the modern era. Whether you're packaging a library, managing dependencies, or configuring tools, this one file is your command center. Start with <code>uv</code> for the smoothest experience, or integrate it into your existing workflow gradually.</p>","tags":["python","guide","best-practices"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/","title":"Quick-guide on Local Stable-Diffusion Toolkits for macOS","text":"<p>Running generative AI models locally is a game-changer. It means zero cloud costs, no censorship, total privacy, and unlimited experimentation. Whether you're generating character portraits, architectural concepts, or just having fun, your Mac is more than capable of handling the workload thanks to Apple Silicon.</p> <p>But with so many tools available, where do you start?</p> <p>Below is a practical guide to the best macOS-ready interfaces. Each tool wraps the same powerful Stable Diffusion models but offers a completely different experience\u2014from \"Apple-like\" simplicity to \"developer-grade\" control.</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#how-these-tools-work","title":"How these tools work","text":"<p>At their core, all these applications do the same thing: they provide a user interface (UI) for the Stable Diffusion models. They handle the complex \"plumbing\"\u2014loading heavy model weights, managing memory, and talking to your Mac's GPU.</p> <p></p> <p>Because they all share the same underlying architecture, you can usually share model files (<code>.safetensors</code>) between them. Download a model once, and try it in different apps to see which workflow suits you.</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#the-apple-silicon-advantage","title":"The Apple Silicon Advantage","text":"<p>Why is the Mac so good for this? It comes down to Unified Memory. Unlike a PC with a separate graphics card (where you might have only 8GB or 12GB of VRAM), your Mac's GPU has access to your entire system RAM.</p> <p></p> <p>This means a MacBook Pro with 32GB or 64GB of RAM can load massive models (like SDXL or Flux) that would bring a typical gaming PC to its knees.</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#1-draw-things-the-powerhouse-app","title":"1. Draw Things: The Powerhouse App","text":"<ul> <li>Download: App Store Link</li> <li>What it is: A native iOS/macOS app that feels like a professional design tool. It's surprisingly powerful, supporting ControlNet, Inpainting, and LoRAs right out of the box.</li> <li>Best for: Users who want a native app experience (no terminal!) but don't want to sacrifice advanced features.</li> <li>Pros:<ul> <li>Native Performance: Highly optimized for Apple Silicon.</li> <li>Feature Rich: Supports Inpainting, Outpainting, ControlNet, and scriptable workflows.</li> <li>Offline First: Runs completely offline after downloading models.</li> </ul> </li> <li>Cons:<ul> <li>UI can be a bit dense on smaller screens (it's designed to scale from iPhone to Mac).</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#2-diffusionbee-the-one-click-wonder","title":"2. DiffusionBee: The \"One-Click\" Wonder","text":"<ul> <li>Download: diffusionbee.com</li> <li>What it is: The simplest way to run Stable Diffusion on a Mac. It strips away all the jargon. You don't \"load a checkpoint\"; you just select a style.</li> <li>Best for: Absolute beginners who just want to make cool images now.</li> <li>Pros:<ul> <li>Zero Setup: Download the DMG, drag to Applications, run.</li> <li>Clean UI: Very \"Apple-like\" design.</li> <li>Built-in Tools: Includes simple upscaling and background removal.</li> </ul> </li> <li>Cons:<ul> <li>Limited Control: You can't easily tweak advanced sampler settings or complex node pipelines.</li> <li>Slower Updates: New features (like the latest ControlNet models) take longer to arrive.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#3-comfyui-the-node-based-lab","title":"3. ComfyUI: The Node-Based Lab","text":"<ul> <li>Download: comfy.org</li> <li>What it is: A visual programming environment. Instead of sliders, you connect \"nodes\" with wires to build your image generation pipeline.</li> <li>Best for: Power users, technical artists, and anyone who wants to understand exactly how the image is being made.</li> <li>Pros:<ul> <li>Ultimate Control: Build custom workflows for specific tasks (e.g., \"Generate image -&gt; Upscale -&gt; Face Restore\").</li> <li>Speed: Often faster than other UIs because it executes only what's needed.</li> <li>Ecosystem: Thousands of custom nodes created by the community.</li> </ul> </li> <li>Cons:<ul> <li>Steep Learning Curve: It looks like a bowl of spaghetti until you learn to read it.</li> <li>Setup: Requires some comfort with Python/Terminal (though installers exist).</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#4-stable-diffusion-webui-automatic1111","title":"4. Stable Diffusion WebUI (AUTOMATIC1111)","text":"<ul> <li>Install Guide: Installation on Apple Silicon</li> <li>What it is: The \"Swiss Army Knife\" of Stable Diffusion. It runs in your browser and has a tab for everything.</li> <li>Best for: Enthusiasts who want to use the latest community extensions immediately.</li> <li>Pros:<ul> <li>Extensions: If a new AI technique is released today, A1111 will have an extension for it tomorrow.</li> <li>Tutorials: The vast majority of YouTube tutorials use this interface.</li> </ul> </li> <li>Cons:<ul> <li>Clunky UI: It's functional but chaotic.</li> <li>Heavy: Can be slower and more memory-hungry than ComfyUI or Draw Things.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#5-fooocus-midjourney-on-your-mac","title":"5. Fooocus: Midjourney on Your Mac","text":"<ul> <li>Repo: github.com/lllyasviel/Fooocus</li> <li>What it is: An interface designed to mimic the ease of Midjourney. It automates all the technical choices (samplers, steps, refiners) so you can focus on the prompt.</li> <li>Best for: High-quality results with minimal tweaking.</li> <li>Pros:<ul> <li>Smart Defaults: It \"just works\" and produces beautiful images.</li> <li>Minimalist: No overwhelming sliders.</li> </ul> </li> <li>Cons:<ul> <li>Less Customization: Harder to force it to do something specific if it fights you.</li> <li>Performance: Often optimized for NVIDIA GPUs first, so Mac performance can vary.</li> </ul> </li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#comparison-table","title":"Comparison Table","text":"Tool Install Difficulty Interface Style Best For Draw Things \u2605\u2606\u2606\u2606\u2606 (App Store) Native App (Pro) The Sweet Spot (Power + Ease) DiffusionBee \u2605\u2606\u2606\u2606\u2606 (DMG) Native App (Simple) Beginners &amp; Casual Use ComfyUI \u2605\u2605\u2605\u2606\u2606 (Python) Node Graph Complex Workflows &amp; Automation A1111 WebUI \u2605\u2605\u2605\u2605\u2606 (Terminal) Browser Dashboard Extensions &amp; Community Support Fooocus \u2605\u2605\u2605\u2606\u2606 (Python) Minimalist Midjourney-style Prompting","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#decision-flowchart","title":"Decision Flowchart","text":"<p>Not sure which one to pick? Follow this path:</p> <p></p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#essential-tips-for-mac-users","title":"Essential Tips for Mac Users","text":"","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#1-system-requirements","title":"1. System Requirements","text":"<ul> <li>RAM is King:<ul> <li>8GB: Doable for basic 512x512 images, but expect slowness and crashes with newer models (SDXL).</li> <li>16GB: The comfortable minimum. You can run most things, including SDXL.</li> <li>32GB+: The dream. You can keep multiple models loaded and multitask while generating.</li> </ul> </li> <li>Storage: AI models are huge (2GB - 6GB each). Get an external SSD if your Mac is low on space.</li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#2-where-to-get-models","title":"2. Where to get Models","text":"<p>The software is just the engine; you need fuel (models).</p> <ul> <li>Civitai: The largest community for models. Look for \"Checkpoints\" that are compatible with SD 1.5 or SDXL.</li> <li>Hugging Face: The \"GitHub of AI\". More technical, but the official source for base models from Stability AI.</li> <li>File Types: Always look for <code>.safetensors</code> files. Avoid <code>.ckpt</code> files if possible, as they can theoretically contain malicious code (though rare).</li> </ul>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-local-stable-diffusion-toolkits-for-macos/#3-start-simple","title":"3. Start Simple","text":"<p>Don't try to install ComfyUI on day one. Start with DiffusionBee or Draw Things. Get a feel for how prompting works. Once you hit a wall (\"I wish I could control the pose of this character...\"), then look into ControlNet and more advanced tools.</p>","tags":["macos","guide","tools","genai"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/","title":"Quick-guide on Running LLMs Locally on macOS","text":"<p>Running Large Language Models (LLMs) locally on your Mac is a game-changer. It means faster responses, complete privacy, and zero API bills. But with so many tools popping up every week, which one should you choose?</p> <p>This guide breaks down the top options\u2014from dead-simple menu bar apps to full-control command-line tools. We'll cover what makes each special, their trade-offs, and how to get started.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#why-run-locally","title":"Why Run Locally?","text":"<p>Before we dive into the tools, let's look at the benefits:</p> <ul> <li>\ud83d\udd12 Privacy: Your data and prompts never leave your machine. Perfect for sensitive work.</li> <li>\u26a1 Speed: No network latency. On Apple Silicon, responses can be faster than cloud APIs.</li> <li>\ud83d\udcb0 Cost: One-time download. No monthly subscriptions or token fees.</li> <li>\u2708\ufe0f Offline: Work from a plane, a cabin, or a coffee shop with spotty Wi-Fi.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#key-concepts-for-beginners","title":"Key Concepts for Beginners","text":"<p>If you're new to local LLMs, here are three terms you'll see often:</p> <ol> <li>Inference: The act of \"running\" the model to generate text.</li> <li>Quantization (GGUF): A technique to shrink model sizes with minimal quality loss. You'll see filenames like <code>llama-3-8b-Q4_K_M.gguf</code>. The <code>Q4</code> means \"4-bit quantization\"\u2014it uses less RAM than the full 16-bit model.</li> <li>Apple Silicon (Metal): Apple's M1/M2/M3 chips have \"Unified Memory,\" allowing the CPU and GPU to share RAM. This makes Macs uniquely powerful for running huge models that would require expensive dedicated GPUs on a PC.</li> </ol>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#prerequisites","title":"Prerequisites","text":"<ul> <li>Hardware: A Mac with Apple Silicon (M1, M2, M3, or M4) is highly recommended. Intel Macs work but will be significantly slower.</li> <li>RAM:<ul> <li>8GB: Can run small models (Mistral 7B, Llama 3 8B) comfortably.</li> <li>16GB+: Recommended for larger models and multitasking.</li> </ul> </li> <li>Disk Space: Models take up space! Plan for ~10-20GB for a good starter library.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#1-ollama-the-just-works-option","title":"1. Ollama - The \"Just Works\" Option","text":"<p>Download: ollama.com</p> <p>Think of Ollama as the \"Docker for LLMs.\" It wraps the complex engine (<code>llama.cpp</code>) in a sleek, native package. You install it, run one command, and you're chatting. It handles all the messy details like model downloading and hardware acceleration automatically.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#best-for","title":"Best For","text":"<ul> <li>Beginners who want to get started in 5 minutes.</li> <li>Developers who want a simple CLI tool.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#example-workflow","title":"Example Workflow","text":"<pre><code># 1. Download and run Llama 3 (it auto-downloads if needed)\nollama run llama3\n\n# 2. Use it in your code via the local API\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3\",\n  \"prompt\": \"Explain quantum computing to a 5-year-old\",\n  \"stream\": false\n}'\n</code></pre>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#pros-cons","title":"Pros &amp; Cons","text":"\u2705 Pros \u274c Cons Easiest setup (Drag-and-drop <code>.dmg</code>) Core application is closed-source Great CLI (<code>ollama list</code>, <code>ollama pull</code>) Less granular control over generation parameters Huge library of pre-configured models","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#2-lm-studio-the-visual-explorer","title":"2. LM Studio - The Visual Explorer","text":"<p>Download: lmstudio.ai</p> <p>LM Studio is for those who prefer a beautiful Graphical User Interface (GUI) over a terminal. It features a built-in \"App Store\" style browser for models, letting you search HuggingFace directly. It also supports Apple's native MLX format, which can be faster on some Macs.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#best-for_1","title":"Best For","text":"<ul> <li>Visual learners who want to explore and test different models.</li> <li>Developers needing an OpenAI-compatible local server.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#example-workflow_1","title":"Example Workflow","text":"<p>LM Studio has a great Python SDK, but it also provides a local server that mimics OpenAI's API, meaning you can use standard libraries:</p> <pre><code># Using the official LM Studio SDK\nfrom lmstudio import LMStudio\n\nclient = LMStudio()\nresponse = client.complete(\n    model=\"llama-3-8b\",\n    prompt=\"Write a haiku about debugging.\"\n)\nprint(response.content)\n</code></pre>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#pros-cons_1","title":"Pros &amp; Cons","text":"\u2705 Pros \u274c Cons Beautiful, easy-to-use interface GUI is closed-source Native support for both GGUF and MLX models Larger download (~750MB) Built-in RAG (Chat with your PDFs)","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#3-llamacpp-the-power-users-tool","title":"3. llama.cpp - The Power User's Tool","text":"<p>Repo: github.com/ggml-org/llama.cpp</p> <p>This is the engine that powers almost everyone else. If you want maximum performance, bleeding-edge features, or to embed an LLM into your own C++ application, this is the source. It's bare-metal, lightweight, and incredibly powerful.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#best-for_2","title":"Best For","text":"<ul> <li>Engineers and Power Users.</li> <li>Running on older or constrained hardware.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#example-workflow_2","title":"Example Workflow","text":"<pre><code># 1. Install via Homebrew\nbrew install llama.cpp\n\n# 2. Download a model manually (e.g., from HuggingFace)\nhuggingface-cli download TheBloke/Llama-3-8B-Instruct-GGUF --local-dir .\n\n# 3. Run inference with full control\nllama-cli -m llama-3-8b-instruct.Q4_K_M.gguf \\\n  -p \"Write a python script to sort a list\" \\\n  -n 512 \\\n  --temp 0.7 \\\n  --ctx-size 4096\n</code></pre>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#pros-cons_2","title":"Pros &amp; Cons","text":"\u2705 Pros \u274c Cons Ultimate control over every parameter Steep learning curve (CLI only) MIT Licensed (Open Source) Manual model management Extremely lightweight (&lt;30MB)","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#4-gpt4all-privacy-first-rag","title":"4. GPT4All - Privacy-First &amp; RAG","text":"<p>Download: gpt4all.io</p> <p>GPT4All focuses heavily on privacy and documents. Its standout feature is \"LocalDocs,\" which lets you point the app at a folder of PDFs, notes, or code, and chat with them instantly. It runs completely offline with no telemetry.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#best-for_3","title":"Best For","text":"<ul> <li>Privacy advocates.</li> <li>Users who want to chat with their own documents (RAG) easily.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#pros-cons_3","title":"Pros &amp; Cons","text":"\u2705 Pros \u274c Cons \"LocalDocs\" RAG is excellent and easy GUI-only (no headless mode) Completely offline &amp; private Heavier resource usage than Ollama Cross-platform (Mac, Windows, Linux)","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#5-koboldcpp-for-storytellers","title":"5. KoboldCPP - For Storytellers","text":"<p>Repo: github.com/LostRuins/koboldcpp</p> <p>A fork of <code>llama.cpp</code> tailored for creative writing and Role-Playing Games (RPGs). It features a web interface designed for long-form text generation, with tools to manage \"World Info,\" character memory, and story consistency.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#best-for_4","title":"Best For","text":"<ul> <li>Writers, Novelists, and RPG players.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#example-workflow_3","title":"Example Workflow","text":"<pre><code># 1. Download the single binary\nwget https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-mac.zip\n\n# 2. Run it (launches a web server)\n./koboldcpp --model llama-3-8b.gguf --port 5001 --smartcontext\n</code></pre>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#pros-cons_4","title":"Pros &amp; Cons","text":"\u2705 Pros \u274c Cons Best-in-class tools for creative writing Niche UI (not great for coding/chat) Single file executable (no installation) AGPL license (restrictive for commercial use)","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#honorable-mention-mlx-lm","title":"Honorable Mention: MLX-LM","text":"<p>If you are a Python developer specifically targeting Apple Silicon, check out MLX-LM by Apple. It's a framework optimized specifically for the M-series chips. While less \"user-friendly\" than Ollama, it's often the fastest way to run models if you're comfortable with Python.</p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#summary-which-tool-is-right-for-you","title":"Summary: Which Tool is Right for You?","text":"<p>Here is a quick decision tree to help you decide:</p> <p></p>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#quick-comparison-table","title":"Quick Comparison Table","text":"Tool Interface Difficulty Best Feature Ollama CLI / Menu Bar \u2b50 (Easy) \"Just Works\" experience LM Studio GUI \u2b50 (Easy) Model discovery &amp; UI GPT4All GUI \u2b50 (Easy) Chat with local docs (RAG) KoboldCPP Web UI \u2b50\u2b50 (Medium) Creative writing tools llama.cpp CLI \u2b50\u2b50\u2b50 (Hard) Raw performance &amp; control","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/10/quick-guide-on-running-llms-locally-on-macos/#final-thoughts","title":"Final Thoughts","text":"<p>You can't really go wrong with any of these. They all run locally, they all respect your privacy, and they all leverage the incredible power of Apple Silicon.</p> <ul> <li>Start with Ollama if you just want to see what the fuss is about.</li> <li>Try LM Studio if you want to browse models visually.</li> <li>Dive into llama.cpp if you want to understand how it all works under the hood.</li> </ul>","tags":["macos","guide","llm","tools"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/","title":"Choosing the Right Open-Source LLM Variant &amp; File Format","text":"","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#why-do-open-source-llms-have-so-many-confusing-names","title":"Why do open-source LLMs have so many confusing names?","text":"<p>You've probably seen model names like <code>Llama-3.1-8B-Instruct.Q4_K_M.gguf</code> or <code>Mistral-7B-v0.3-A3B.awq</code> and wondered what all those suffixes mean. It looks like a secret code, but the short answer is: they tell you two critical things.</p> <p>Open-source LLMs vary along two independent dimensions:</p> <ol> <li>Model variant \u2013 the suffix in the name (<code>-Instruct</code>, <code>-Distill</code>, <code>-A3B</code>, etc.) describes how the model was trained and what it's optimized for.</li> <li>File format \u2013 the extension (<code>.gguf</code>, <code>.gptq</code>, <code>.awq</code>, etc.) describes how the weights are stored and where they run best (CPU, GPU, mobile, etc.).</li> </ol> <p>Think of it like this: the model variant is the recipe, and the file format is the container. You can put the same soup (recipe) into a thermos, a bowl, or a takeout box (container) depending on where you plan to eat it.</p> <p></p> <p>Understanding both dimensions helps you avoid downloading 20 GB of the wrong model at midnight and then spending hours debugging CUDA errors.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#model-variants-explained-the-recipe","title":"Model variants explained (the recipe)","text":"<p>This is about the brain of the model. How was it taught?</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#base-models","title":"Base models","text":"<p>What it is: The raw, pre-trained model straight from the training run. Think of it as the unfiltered brain that learned language patterns from massive text datasets (the entire internet) but hasn't been taught to follow instructions. It just predicts the next word.</p> <p>When to use it:</p> <ul> <li>You're planning to fine-tune it for your specific domain.</li> <li>You're doing research and need the \"pure\" foundation.</li> <li>You want maximum creative freedom (no safety guardrails).</li> </ul> <p>Trade-offs: Won't reliably follow instructions. If you ask \"What is the capital of France?\", it might reply \"and what is the capital of Germany?\" because it thinks it's completing a list of questions.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#instruct-chat-models","title":"Instruct / Chat models","text":"<p>What it is: A base model that went through additional training (Supervised Fine-Tuning + RLHF) to understand and follow human instructions. This is what most people actually want when they say \"I want an LLM.\"</p> <p>When to use it:</p> <ul> <li>Building chatbots, AI agents, or RAG applications.</li> <li>Function calling and tool use.</li> <li>Day-to-day coding assistance.</li> <li>95% of production use cases.</li> </ul> <p>Trade-offs: Slightly larger and slower than base models due to the extra training layers. May be less \"creative\" due to alignment training that makes it more predictable and helpful.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#reasoning-cot-models-new","title":"Reasoning / CoT Models (New!)","text":"<p>What it is: A new breed of models (like DeepSeek-R1 or o1-derivatives) trained with \"Chain of Thought\" (CoT) reinforcement learning. They \"think\" before they speak, generating internal reasoning tokens to solve complex logic, math, or coding problems before outputting the final answer.</p> <p>When to use it:</p> <ul> <li>Complex coding tasks and debugging.</li> <li>Math problems and logic puzzles.</li> <li>When you need the model to double-check its work and avoid hallucinations.</li> </ul> <p>Trade-offs: Slower inference. They generate many \"thought\" tokens that you might not see but still have to wait for. They can also be overly verbose for simple \"hello world\" tasks.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#distilled-models","title":"Distilled models","text":"<p>What it is: A smaller \"student\" model trained to mimic the behavior of a larger \"teacher\" model. Think of it as compressed knowledge\u2014you get 70-80% of the performance at 30-50% of the size.</p> <p>When to use it:</p> <ul> <li>Mobile or edge devices with limited resources.</li> <li>Cost-sensitive SaaS where every millisecond counts.</li> <li>High-throughput scenarios where you need to serve many requests.</li> </ul> <p>Trade-offs: Some loss in complex reasoning ability, but excellent efficiency. The token-per-watt ratio is hard to beat.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#moe-mixture-of-experts-a3b-a22b-etc","title":"MoE (Mixture-of-Experts): A3B, A22B, etc.","text":"<p>What it is: A clever architecture where the model has many \"expert\" sub-networks, but only activates a subset for each token. \"A3B\" means \"3 billion parameters active\" out of a much larger total (often 30B+).</p> <p>When to use it:</p> <ul> <li>You want \"big model\" smarts but only have 12-24 GB VRAM.</li> <li>You need the reasoning power of a 30B model but with 7B inference costs.</li> <li>You're running locally and want the best performance-per-memory ratio.</li> </ul> <p>Trade-offs: Takes more disk space (you're storing all the experts). Not every inference framework supports MoE routing yet\u2014check compatibility first.</p> <p></p> <p>Rule of thumb:</p> <ul> <li>Start with an Instruct model\u2014it's what most people need.</li> <li>Hit memory or latency limits? Try a Distilled or MoE variant.</li> <li>Need to solve a complex riddle? Try a Reasoning model.</li> </ul>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#file-formats-explained-the-container","title":"File formats explained (the container)","text":"<p>Now that you know what kind of model you want, you need to pick how it's packaged. File formats determine where your model runs best and how much memory it needs.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#quantization-101-why-do-we-shrink-models","title":"Quantization 101: Why do we shrink models?","text":"<p>Before we talk formats, let's talk Quantization. Standard models use 16-bit numbers (FP16) for every weight. That's precise but huge. Quantization reduces these to 8-bit, 4-bit, or even 2-bit numbers.</p> <ul> <li>FP16: 2 bytes per parameter. (13B model \u2248 26 GB)</li> <li>4-bit: 0.5 bytes per parameter. (13B model \u2248 6.5 GB)</li> </ul> <p>You lose a tiny bit of \"intelligence\" but gain massive speed and memory savings.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#gguf-gguf","title":"GGUF (<code>.gguf</code>)","text":"<p>What it is: The successor to GGML and now the de-facto standard for local inference. A single file that contains the model weights, metadata, and even the prompt template.</p> <p>Best for:</p> <ul> <li>Apple Silicon (M1/M2/M3): It works natively with Metal acceleration.</li> <li>CPU Inference: If you don't have a dedicated GPU.</li> <li>Easy Setup: Works with <code>llama.cpp</code>, Ollama, and LM Studio.</li> </ul> <p>Why it's great: One file, works everywhere. Supports multiple quantization levels.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#decoding-gguf-names-q3_k_m-q5_k_m-etc","title":"Decoding GGUF Names (Q3_K_M, Q5_K_M, etc.)","text":"<p>You'll often see a long list of files like <code>Q3_K_M</code>, <code>Q4_K_S</code>, <code>Q5_K_M</code>. These aren't random; they are specific \"K-quant\" formats.</p> <p>How to read <code>Q3_K_M</code>:</p> <ul> <li>Q3: Average 3-bit quantization for the weights.</li> <li>K: Uses the K-quant scheme (a newer, smarter quantization method that uses non-uniform precision).</li> <li>M: Medium block size. This refers to the internal layout (<code>S</code> = Small, <code>L</code> = Large).</li> </ul> <p>Which one should you pick?</p> <ul> <li>Q3_K_M (The \"Budget\" Choice): Use when you are tight on memory. It has a noticeable quality drop but allows you to run larger models on weaker hardware.</li> <li>Q4_K_M (The \"Standard\"): The sweet spot. Best balance of speed, size, and perplexity. Indistinguishable from uncompressed for most tasks.</li> <li>Q5_K_M (The \"Premium\"): Use if you have VRAM to spare. Quality is very close to FP16 / Q8 while still being quite compact.</li> </ul> <p>Pro Tip: It is often better to run a larger model at lower quantization (e.g., Llama-70B at Q3) than a smaller model at high quantization (e.g., Llama-8B at Q8). Intelligence scales with parameter count more than precision.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#gptq-safetensors-configjson","title":"GPTQ (<code>.safetensors</code> + <code>config.json</code>)","text":"<p>What it is: Post-training quantization optimized specifically for Nvidia GPUs. It uses second-order information to minimize accuracy loss when compressing.</p> <p>Best for:</p> <ul> <li>Production Servers: Running on Linux with Nvidia GPUs (CUDA).</li> <li>High Throughput: Very fast inference at 4-bit.</li> <li>ExLlamaV2: Can be run with the ExLlamaV2 loader for extreme speed.</li> </ul> <p>Watch out for: Requires a GPU. Won't run efficiently on CPU or Mac.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#awq-safetensors","title":"AWQ (<code>.safetensors</code>)","text":"<p>What it is: Activation-Aware Weight Quantization. It analyzes which weights matter most during inference and preserves their precision better than naive quantization.</p> <p>Best for:</p> <ul> <li>Accuracy: Often matches FP16 accuracy more closely than GPTQ at 4-bit.</li> <li>vLLM: Supported natively by the vLLM serving engine.</li> </ul> <p>Why it's great: It's \"smarter\" quantization. If you care about squeezing every drop of quality out of a 4-bit model, AWQ is often the winner.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#pytorch-safetensors-fp16bf16","title":"PyTorch / Safetensors (FP16/BF16)","text":"<p>What it is: Full-precision weights with no quantization. The original format most models are released in.</p> <p>Best for:</p> <ul> <li>Cloud inference with powerful GPUs (A100, H100).</li> <li>Fine-tuning and continued training.</li> <li>When accuracy is paramount and memory isn't a constraint.</li> </ul> <p>Trade-offs: Largest memory and disk footprint. A 70B model in FP16 needs ~140 GB VRAM!</p> <p></p> <p>Tip: When in doubt, start with GGUF Q4_K_M. It's the Swiss Army knife of LLM formats\u2014runs on 8GB VRAM GPUs, modern CPUs, and everything in between. You can always optimize later.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#how-to-actually-run-these-serving-engines","title":"How to actually run these? (Serving Engines)","text":"<p>You have the file. Now what? You need an engine to run it.</p> <ol> <li> <p>Ollama: The easiest CLI tool.</p> <ul> <li>Uses: GGUF.</li> <li>Good for: Mac, Linux, Windows, Local development.</li> <li>Command: <code>ollama run llama3</code></li> </ul> </li> <li> <p>LM Studio: A beautiful GUI application.</p> <ul> <li>Uses: GGUF.</li> <li>Good for: Beginners, testing models visually, Mac/Windows.</li> </ul> </li> <li> <p>vLLM: The production standard.</p> <ul> <li>Uses: AWQ, GPTQ, Safetensors (FP16).</li> <li>Good for: High-performance servers, deploying APIs, Linux/Docker.</li> <li>Note: Doesn't support GGUF well (yet).</li> </ul> </li> <li> <p>Llama.cpp: The engine behind Ollama.</p> <ul> <li>Uses: GGUF.</li> <li>Good for: Low-level integration, running on Raspberry Pis, Android, etc.</li> </ul> </li> </ol>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#putting-it-all-together-a-decision-framework","title":"Putting it all together: a decision framework","text":"<p>Here's a practical flowchart to help you choose. Start with your constraints (hardware and use case), then pick the appropriate combination.</p> <p></p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#quick-recommendations-by-scenario","title":"Quick recommendations by scenario","text":"<p>Scenario 1: Building a chatbot on a MacBook Pro (16GB RAM)</p> <ul> <li>Model: Instruct (Llama-3-8B or Mistral-7B)</li> <li>Format: GGUF Q4_K_M</li> <li>Why: Runs smoothly on CPU/Metal, fits in memory, one-file simplicity.</li> </ul> <p>Scenario 2: RAG system on a server with RTX 4090 (24GB VRAM)</p> <ul> <li>Model: Instruct or MoE (Mixtral 8x7B or Qwen-14B)</li> <li>Format: EXL2 (via ExLlamaV2) or AWQ 4-bit</li> <li>Why: Maximizes the 24GB VRAM. EXL2 is blazing fast on Nvidia cards.</li> </ul> <p>Scenario 3: Fine-tuning for domain-specific use on cloud GPU</p> <ul> <li>Model: Base</li> <li>Format: FP16 Safetensors</li> <li>Why: You need full precision for training. Start with the unaligned base model.</li> </ul> <p>Scenario 4: High-throughput API with cost constraints</p> <ul> <li>Model: Distilled (DeepSeek-Distill or similar)</li> <li>Format: AWQ 4-bit running on vLLM</li> <li>Why: vLLM + AWQ offers incredible throughput (tokens/sec) per dollar.</li> </ul>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#common-pitfalls-and-misconceptions","title":"Common pitfalls and misconceptions","text":"","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#all-4-bit-models-are-the-same-quality","title":"\"All 4-bit models are the same quality\"","text":"<p>Not true. A QAT 4-bit model (trained in 4-bit) often beats an 8-bit post-training quantized model. The method matters. AWQ typically preserves more accuracy than naive GPTQ.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#moe-models-work-with-any-inference-engine","title":"\"MoE models work with any inference engine\"","text":"<p>Not yet. <code>llama.cpp</code> handles MoE routing well. Support in other engines varies. Always check compatibility before downloading a 50GB MoE model.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#distilled-models-are-just-smaller-versions","title":"\"Distilled models are just smaller versions\"","text":"<p>Nope. A distilled 7B model can outperform a vanilla 13B model because it learned from a much larger teacher (often 70B+). It's compressed knowledge, not just compressed parameters.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#i-should-quantize-my-qat-model-further-to-save-space","title":"\"I should quantize my QAT model further to save space\"","text":"<p>Don't. QAT models were already trained in low-bit precision. Quantizing them again usually degrades quality significantly. Use them as-is.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#bigger-is-always-better","title":"\"Bigger is always better\"","text":"<p>Context matters. A well-tuned 8B Instruct model often outperforms a poorly-aligned 70B base model for specific tasks. Match the model variant to your use case\u2014size isn't everything.</p>","tags":["guide","llm"]},{"location":"blog/2025/05/11/choosing-the-right-open-source-llm-variant--file-format/#tldr-just-tell-me-what-to-download","title":"TL;DR - Just tell me what to download","text":"<p>If you just want something that works:</p> <ol> <li>Download a <code>&lt;model-name&gt;-Instruct.Q4_K_M.gguf</code> file from Hugging Face.</li> <li>Run it with Ollama or LM Studio.</li> <li>If it's too slow \u2192 try a smaller model or Distilled variant.</li> <li>If you're out of memory \u2192 try a Q3_K_M quantization.</li> <li>If quality isn't good enough \u2192 move up to Q5_K_M or switch to a larger model.</li> </ol> <p>Start simple, optimize only when needed. The defaults are good enough for 90% of use cases.</p>","tags":["guide","llm"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/","title":"Building a Custom FeatureStoreLite MCP Server Using uv","text":"<p>A step-by-step guide that shows how to create your own lightweight feature store MCP server from scratch using FastMCP, run it through uv, and integrate it with Claude Desktop.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#1-introduction","title":"1. Introduction","text":"<p>The Model Context Protocol (MCP) is an open standard that enables AI assistants (like Claude) to connect to external data and tools. Instead of building custom integrations for every tool, MCP provides a universal language for AI models to interact with your world.</p> <p>In this tutorial, we will build a FeatureStoreLite MCP server. This server will act as a bridge between an LLM and a feature store (a database of precomputed ML features), allowing the LLM to query and retrieve feature vectors for users, products, or documents.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#why-build-this","title":"Why build this?","text":"<p>Imagine you are an ML engineer debugging a pipeline. Instead of writing SQL queries or Python scripts to check feature values, you can simply ask Claude: \"What is the feature vector for user_123?\" or \"Show me the metadata for product_abc\".</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#why-use-uv","title":"Why use <code>uv</code>?","text":"<p>We will use uv, an extremely fast Python package installer and project manager. It simplifies dependency management and makes running our server reproducible and fast.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#architecture-overview","title":"Architecture Overview","text":"<p>Here is how the components interact:</p> <p></p> <ol> <li>User: Asks a question in natural language.</li> <li>Claude Desktop: The MCP Client that interprets the question and decides which tool to call.</li> <li>MCP Server: Our Python application running <code>FastMCP</code> that exposes tools (<code>get_feature</code>, <code>store_feature</code>).</li> <li>SQLite: The backing storage for our feature vectors.</li> </ol>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#2-setup-and-installation","title":"2. Setup and Installation","text":"","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#21-install-uv","title":"2.1. Install <code>uv</code>","text":"<p>If you haven't installed <code>uv</code> yet, get it now. It's a game-changer for Python development.</p> <pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or via Homebrew\nbrew install uv\n</code></pre>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#22-initialize-the-project","title":"2.2. Initialize the Project","text":"<p>Create a new directory and initialize a Python project. <code>uv init</code> creates a <code>pyproject.toml</code> for you.</p> <pre><code># Create project directory\nmkdir mcp-featurestore\ncd mcp-featurestore\n\n# Initialize Python project\nuv init\n\n# Add the MCP SDK with CLI tools\nuv add \"mcp[cli]\"\n</code></pre>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#3-building-the-server","title":"3. Building the Server","text":"<p>We will split our application into two files:</p> <ol> <li><code>database.py</code>: Handles SQLite operations.</li> <li><code>featurestore_server.py</code>: The MCP server definition.</li> </ol>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#31-the-database-layer-databasepy","title":"3.1. The Database Layer (<code>database.py</code>)","text":"<p>This module manages the SQLite connection and provides helper functions. We'll also seed it with some dummy data so we have something to query.</p> <p>Create <code>database.py</code>:</p> <pre><code># database.py\nimport json\nimport os\nimport sqlite3\n\n\ndef get_db_path() -&gt; str:\n    \"\"\"Get the database path - always in the script's directory\"\"\"\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    return os.path.join(script_dir, \"features.db\")\n\n\ndef init_db() -&gt; None:\n    \"\"\"Initialize the feature store database with table and sample data\"\"\"\n    conn = sqlite3.connect(get_db_path())\n    conn.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS features (\n            key TEXT PRIMARY KEY,\n            vector TEXT NOT NULL,\n            metadata TEXT,\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )\n    \"\"\")\n\n    # Sample data for experimentation\n    example_features = [\n        (\n            \"user_123\",\n            \"[0.1, 0.2, -0.5, 0.8, 0.3, -0.1, 0.9, -0.4]\",\n            json.dumps({\"type\": \"user\", \"id\": 123, \"segment\": \"premium\"}),\n        ),\n        (\n            \"product_abc\",\n            \"[0.7, -0.3, 0.4, 0.1, -0.8, 0.6, 0.2, -0.5]\",\n            json.dumps({\"type\": \"product\", \"id\": \"abc\", \"category\": \"electronics\"}),\n        ),\n    ]\n\n    # Insert if not exists\n    for key, vector, metadata in example_features:\n        try:\n            conn.execute(\n                \"INSERT INTO features (key, vector, metadata) VALUES (?, ?, ?)\",\n                (key, vector, metadata),\n            )\n        except sqlite3.IntegrityError:\n            pass  # Already exists\n\n    conn.commit()\n    conn.close()\n\n\ndef get_db_connection() -&gt; sqlite3.Connection:\n    \"\"\"Get a database connection\"\"\"\n    return sqlite3.connect(get_db_path())\n\n\nif __name__ == \"__main__\":\n    init_db()\n    print(\"\u2705 Database initialized successfully!\")\n</code></pre> <p>Initialize the database:</p> <pre><code>uv run python database.py\n</code></pre>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#32-the-mcp-server-featurestore_serverpy","title":"3.2. The MCP Server (<code>featurestore_server.py</code>)","text":"<p>Now for the exciting part. We use <code>FastMCP</code> to define our server. It uses decorators to turn standard Python functions into MCP Tools and Resources.</p> <p>Create <code>featurestore_server.py</code>:</p> <pre><code># featurestore_server.py\nimport json\n\nfrom mcp.server.fastmcp import FastMCP\n\nfrom database import get_db_connection, init_db\n\n# Initialize the MCP Server\nmcp = FastMCP(\"FeatureStoreLite\")\n\n# Ensure DB is ready when server starts\ninit_db()\n\n\n@mcp.resource(\"schema://main\")\ndef get_schema() -&gt; str:\n    \"\"\"\n    Resource: Provide the database schema.\n    Resources are passive data that LLMs can read like files.\n    \"\"\"\n    conn = get_db_connection()\n    try:\n        schema = conn.execute(\n            \"SELECT sql FROM sqlite_master WHERE type='table'\"\n        ).fetchall()\n        return \"\\n\".join(sql[0] for sql in schema if sql[0]) or \"No tables found.\"\n    finally:\n        conn.close()\n\n\n@mcp.tool()\ndef store_feature(key: str, vector: str, metadata: str | None = None) -&gt; str:\n    \"\"\"\n    Tool: Store a feature vector.\n    Tools are executable functions that LLMs can call to perform actions.\n    \"\"\"\n    conn = get_db_connection()\n    try:\n        # Validate that vector is valid JSON\n        json.loads(vector)\n\n        conn.execute(\n            \"INSERT OR REPLACE INTO features (key, vector, metadata) VALUES (?, ?, ?)\",\n            (key, vector, metadata),\n        )\n        conn.commit()\n        return f\"Successfully stored feature '{key}'\"\n    except json.JSONDecodeError:\n        return \"Error: Vector must be a valid JSON array string (e.g., '[0.1, 0.2]')\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n    finally:\n        conn.close()\n\n\n@mcp.tool()\ndef get_feature(key: str) -&gt; str:\n    \"\"\"\n    Tool: Retrieve a feature vector by key.\n    \"\"\"\n    conn = get_db_connection()\n    try:\n        row = conn.execute(\n            \"SELECT vector, metadata FROM features WHERE key = ?\", (key,)\n        ).fetchone()\n\n        if row:\n            return json.dumps(\n                {\n                    \"key\": key,\n                    \"vector\": json.loads(row[0]),\n                    \"metadata\": json.loads(row[1]) if row[1] else None,\n                },\n                indent=2,\n            )\n        return f\"Feature '{key}' not found.\"\n    finally:\n        conn.close()\n\n\n@mcp.tool()\ndef list_features() -&gt; str:\n    \"\"\"\n    Tool: List all available feature keys.\n    \"\"\"\n    conn = get_db_connection()\n    try:\n        rows = conn.execute(\"SELECT key FROM features\").fetchall()\n        return json.dumps([row[0] for row in rows])\n    finally:\n        conn.close()\n\n\nif __name__ == \"__main__\":\n    mcp.run()\n</code></pre>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#4-testing-with-mcp-inspector","title":"4. Testing with MCP Inspector","text":"<p>Before connecting to Claude, use the MCP Inspector to verify your server works. This web interface lets you test tools and view resources.</p> <pre><code>uv run mcp dev featurestore_server.py\n</code></pre> <p>This command starts the server and opens the Inspector in your browser (usually <code>http://localhost:5173</code> or similar).</p> <p></p> <p>Try calling <code>get_feature</code> with <code>key=\"user_123\"</code> in the Inspector to confirm it returns the JSON data.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#5-connecting-to-claude-desktop","title":"5. Connecting to Claude Desktop","text":"<p>Now let's connect our server to Claude Desktop so we can talk to it.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#51-configure-claude","title":"5.1. Configure Claude","text":"<p>Edit your Claude Desktop configuration file:</p> <ul> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%/Claude/claude_desktop_config.json</code></li> </ul> <p>Add your server to the <code>mcpServers</code> object:</p> <pre><code>{\n    \"mcpServers\": {\n        \"featurestore\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"run\",\n                \"--with\",\n                \"mcp[cli]\",\n                \"mcp\",\n                \"run\",\n                \"/ABSOLUTE/PATH/TO/mcp-featurestore/featurestore_server.py\"\n            ]\n        }\n    }\n}\n</code></pre> <p>\u26a0\ufe0f Important: You must use the absolute path to your <code>featurestore_server.py</code> file.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#52-how-the-interaction-works","title":"5.2. How the Interaction Works","text":"<p>When you ask Claude a question, the following workflow occurs:</p> <p></p> <ol> <li>Claude sees the available tools (<code>get_feature</code>, <code>list_features</code>, etc.).</li> <li>It determines that your question requires data from the feature store.</li> <li>It constructs a tool call and sends it to your server.</li> <li>Your server executes the Python function and returns the result.</li> <li>Claude uses that result to answer your question.</li> </ol>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#53-example-queries","title":"5.3. Example Queries","text":"<p>Restart Claude Desktop and try these prompts:</p> <ol> <li> <p>\"List all available features.\"    </p> </li> <li> <p>\"Get the feature vector for user_123.\"    </p> </li> <li> <p>\"Store a new feature for 'new_item' with vector [0.5, 0.5] and metadata {'type': 'test'}.\"</p> </li> </ol>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#6-troubleshooting","title":"6. Troubleshooting","text":"<p>If things aren't working, check these common issues:</p> <ul> <li> <p>\"Server connection failed\":</p> <ul> <li>Check the logs: <code>tail -f ~/Library/Logs/Claude/mcp.log</code> (macOS).</li> <li>Ensure you used the absolute path in the config file.</li> <li>Verify <code>uv</code> is in your system PATH or use the full path to the <code>uv</code> binary.</li> </ul> </li> <li> <p>\"Tool execution error\":</p> <ul> <li>Use the Inspector (<code>uv run mcp dev ...</code>) to debug the specific tool.</li> <li>Check if your <code>database.py</code> is creating the <code>features.db</code> file in the correct location.</li> </ul> </li> </ul>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#7-conclusion","title":"7. Conclusion","text":"<p>You've just built a functional MCP server that extends Claude's capabilities! This pattern\u2014using <code>FastMCP</code> for the server and <code>uv</code> for execution\u2014is a powerful way to build robust AI tools quickly.</p>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/06/10/building-a-custom-featurestorelite-mcp-server-using-uv/#references","title":"References","text":"<ul> <li>The repo of this tutorial example</li> <li>Introduction to MCP</li> <li>MCP Python SDK</li> <li>Claude Desktop</li> <li>uv</li> </ul>","tags":["guide","mcp","python","uv"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/","title":"Context Engineering in the Agentic\u2011AI Era \u2014 and How to Cook It","text":"","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#tldr","title":"TL;DR","text":"<p>Context engineering (the context layer) is the pipeline that selects, structures, and governs what the model sees at the moment of decision: Instructions, Examples, Knowledge, Memory, Skills, Tools, Guardrails. Agentic systems live or die by this layer. Below is a field\u2011tested blueprint and patterns.</p> <p>The problem: You build an agent. It works in demos, fails in production. Why? The model gets the wrong context at the wrong time\u2014stale memory, irrelevant docs, no safety checks, ambiguous instructions.</p> <p>The fix: Design the context layer deliberately. This guide shows you how.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#why-now","title":"Why now","text":"<p>Picture this: your customer support agent runs for three weeks. It handles 200 tickets. Then it suddenly starts hallucinating product details, mixing up customers, and calling the wrong APIs. The model didn't get worse\u2014the context did.</p> <p>Here's why context engineering became critical in 2025:</p> <ul> <li> <p>Agents moved from chat to action. Multi\u2011step planning, tool use, and sub\u2011agents raised the bar for repeatable context assembly vs. one\u2011off prompts. A single bad context decision can cascade through a 10\u2011step plan.</p> </li> <li> <p>Memory and standards arrived. Centralized user/org memory (and standards like MCP) make it feasible to load personal/org context safely\u2014if you design the layer properly. Without governance, you leak PII or overload the window.</p> </li> <li> <p>Retrieval matured. Hybrid search, reranking, and graph\u2011aware retrieval (e.g., GraphRAG) reduce hallucinations and token waste. But only if you route queries to the right retrieval strategy.</p> </li> <li> <p>Value focus shifted. Many \"agentic\" pilots stall not because of model quality but because of weak context design/governance. A deliberate context layer is the fix.</p> </li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#key-concepts-for-beginners","title":"Key Concepts for Beginners","text":"<p>Before we dive in, let's define a few terms that will appear frequently:</p> <ul> <li>Context Window: The \"working memory\" of the model. It's the maximum amount of text (measured in tokens) the model can process at once. If you exceed it, the model crashes or forgets the beginning.</li> <li>Tokens: The basic units of text for an LLM. Roughly, 1,000 tokens \u2248 750 words.</li> <li>Attention Budget: Language models process tokens through attention mechanisms that create pairwise relationships between all tokens. For n tokens, this creates n\u00b2 relationships. As context grows, this budget gets stretched thin\u2014meaning information in the middle of context receives less attention than information at the beginning or end.</li> <li>Embeddings: Numerical representations of text. We use them to search for \"meaning\" rather than just keywords (e.g., searching for \"dog\" might find \"puppy\").</li> <li>JSON Schema: A standard way to describe the structure of JSON data. It allows us to force the model to output specific fields (like <code>{\"answer\": \"...\", \"citations\": [...]}</code>).</li> <li>MCP (Model Context Protocol): An open standard that enables AI models to interact with external data and tools securely. Think of it as a \"USB port\" for AI apps to connect to your local files, databases, or Slack.</li> </ul> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#what-is-the-context-layer","title":"What is the context layer?","text":"<p>A pipeline + policy that (1) selects &amp; structures inputs per step, (2) applies controls (format/safety/policy), and (3) feeds the model/agent with just\u2011enough, just\u2011in\u2011time context.</p> <p>Think of it as the assembly line that prepares exactly what the model needs to make a good decision\u2014nothing more, nothing less.</p> <p>Context is a finite resource. Like humans with limited working memory, language models have an attention budget that depletes as context grows. Every new token consumes some of this budget. The engineering problem is finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.</p> <p>There's no single canonical definition. Different teams ship different stacks. But a practical, shared decomposition is:</p> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#1-instructions","title":"1) Instructions","text":"<p>What: A durable contract for behavior: role, tone, constraints, output schema, evaluation goals. Modern models respect instruction hierarchies (system &gt; developer &gt; user).</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-instructions","title":"When to use Instructions","text":"<ul> <li>You need consistent output (reports, SQL, API calls, JSON).</li> <li>You must apply policy (e.g., redact PII, reject unsupported asks).</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#instruction-patterns","title":"Instruction Patterns","text":"<ul> <li>Role &amp; policy blocks: keep rules separate from the user task.</li> <li>Structured outputs: JSON Schema \u2192 deterministic downstream.</li> <li>Instruction hierarchy: split system, developer, user explicitly.</li> </ul> <p>Plain example (policy block)</p> <p></p> <pre><code>SYSTEM RULES\n- Role: support assistant for ACME.\n- Always output valid JSON per AnswerSchema.\n- If a request needs account data, ask for the account ID.\n- Never include secrets or internal URLs.\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#schemaguided-reasoning-sgr","title":"Schema\u2011Guided Reasoning (SGR)","text":"<p>What: Drive the agent with JSON Schemas for the plan, tool arguments, intermediate results, and the final answer. The model emits/consumes JSON at each step; your code validates it.</p> <p>Why: Reduces ambiguity, makes retries/repairs deterministic, and improves safety by enforcing types and required fields throughout the loop.</p> <p>How it works:</p> <ol> <li>Define schemas for <code>Plan</code>, <code>ToolArgs</code>, <code>StepResult</code>, and <code>FinalAnswer</code>.</li> <li>At each agent step, the model outputs JSON matching one of these schemas.</li> <li>Your code validates the JSON before proceeding.</li> <li>If validation fails, attempt one automatic repair (e.g., add missing required fields with defaults).</li> <li>If repair fails, refuse and log the error.</li> </ol> <p>Concrete example: Instead of the model saying \"I'll search for the customer's tickets\", it outputs:</p> <pre><code>{\n    \"action\": \"call_tool\",\n    \"tool\": \"search_tickets\",\n    \"args\": { \"customer_id\": \"A-123\", \"limit\": 10 },\n    \"expected_schema\": \"TicketList\"\n}\n</code></pre> <p>Your code validates <code>args</code> against the tool's schema before calling the API.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#2-examples","title":"2) Examples","text":"<p>What: A few short input\u2192output examples that show the exact format, tone, and steps the model should follow.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-examples","title":"When to use Examples","text":"<ul> <li>You need the model to match a specific template (tables, JSON, SQL, API calls).</li> <li>You want domain\u2011specific phrasing/labels or consistent tone.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#example-patterns","title":"Example Patterns","text":"<ul> <li>Canonical demos: show the exact target structure.</li> <li>Bad vs. good: contrast common mistakes with the desired result.</li> <li>Schema\u2011first + examples: pair your JSON Schema with 2\u20133 short demos.</li> <li>Keep it short: many small, focused demos beat one long example.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#3-knowledge","title":"3) Knowledge","text":"<p>What: Grounding via retrieval (vector + keyword), reranking, graphs, web, or enterprise sources.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-knowledge","title":"When to use Knowledge","text":"<ul> <li>You need fresh or private facts.</li> <li>You want cited, defensible answers.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#knowledge-patterns","title":"Knowledge Patterns","text":"<ul> <li>Hybrid retrieval (BM25 + dense) with reranker to shrink tokens.</li> <li>Graph\u2011aware retrieval (GraphRAG) for cross\u2011doc relations.</li> <li>Adaptive RAG: route between no retrieval, single\u2011shot, and iterative.</li> </ul> <p>Params that matter:</p> <ul> <li>Chunking: split by semantic boundary (paragraphs, sections) &gt; fixed size.</li> <li>top\u2011k: start with 10\u201320 for hybrid, rerank to 3\u20135.</li> <li>MMR (diversity) \u03bb: 0.7 as default.</li> <li>Citations: Always include source references and quotes.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#4-memory","title":"4) Memory","text":"<p>What: Durable context across turns/sessions: short\u2011term (conversation state), long\u2011term (user/app facts), episodic (events), semantic (facts/entities).</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-memory","title":"When to use Memory","text":"<ul> <li>You want personalization and continuity.</li> <li>Multiple agents coordinate over days/weeks.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#memory-patterns","title":"Memory Patterns","text":"<ul> <li>Entity memories (names, IDs, preferences) + expiry policies.</li> <li>Scoped retrieval from long-term store (vector/kv/graph).</li> <li>Compression integration: When short-term memory grows large, apply Context Compression Strategies [7].</li> </ul> <p>Expiry rules:</p> <ul> <li>Preferences: 365 days</li> <li>Episodic events: 90 days</li> <li>Short-term state: clear after session ends</li> <li>Entities: no expiry, but require periodic validation</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#5-skills","title":"5) Skills","text":"<p>What: Composable domain expertise that agents discover and load dynamically. Agent Skills are a framework introduced by Anthropic for equipping agents with specialized capabilities by capturing and sharing procedural knowledge.</p> <p>Building a skill for an agent is like putting together an onboarding guide for a new hire. \u2014 Anthropic Engineering Blog</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-skills","title":"When to use Skills","text":"<ul> <li>You need domain-specific expertise (PDF manipulation, git operations, data analysis).</li> <li>You want reusable procedures across agents or organizations.</li> <li>You need to specialize an agent without hardcoding behaviors.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#what-is-a-skill","title":"What is a Skill?","text":"<p>A Skill is an organized folder containing instructions, scripts, and resources that agents can discover and load dynamically:</p> <ul> <li>SKILL.md file: Contains name, description (in YAML frontmatter), and the skill's instructions</li> <li>Additional files: Scripts, references, templates that the skill can reference</li> <li>Code: Python scripts or other executables the agent can run as tools</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#skills-pattern-progressive-disclosure","title":"Skills Pattern: Progressive Disclosure","text":"<p>The key design principle is progressive disclosure\u2014loading information only when needed. See The Progressive Disclosure Principle for the general pattern.</p> <ol> <li>Level 1 (Startup): Only skill names and descriptions are loaded into context</li> <li>Level 2 (Activation): When relevant, the full SKILL.md is loaded</li> <li>Level 3+ (Deep dive): Additional referenced files loaded only as needed</li> </ol> <p>This means skill content is effectively unbounded\u2014agents don't need to load everything into context at once.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Context Window                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 System Prompt                                           \u2502\n\u2502 \u251c\u2500\u2500 Core instructions                                   \u2502\n\u2502 \u2514\u2500\u2500 Skill metadata (name + description only)           \u2502\n\u2502     \u2022 pdf: \"Manipulate PDF documents\"                   \u2502\n\u2502     \u2022 git: \"Advanced git operations\"                    \u2502\n\u2502     \u2022 context-compression: \"Manage long sessions\"       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 [User triggers task requiring PDF skill]                \u2502\n\u2502                                                         \u2502\n\u2502 \u2192 Agent reads pdf/SKILL.md into context                 \u2502\n\u2502 \u2192 Agent reads pdf/forms.md (only if filling forms)      \u2502\n\u2502 \u2192 Agent executes pdf/extract_fields.py (without loading)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#skill-best-practices","title":"Skill Best Practices","text":"<p>From Anthropic's guidelines:</p> <ol> <li>Start with evaluation: Identify gaps by running agents on representative tasks, build skills to address shortcomings</li> <li>Structure for scale: Split large SKILL.md into separate files, keeping paths separate for mutually exclusive contexts</li> <li>Think from Claude's perspective: Monitor skill usage, iterate on name/description to improve triggering accuracy</li> <li>Iterate with Claude: Ask Claude to capture successful approaches into reusable skill content</li> </ol>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#security-considerations","title":"Security Considerations","text":"<p>Skills can introduce vulnerabilities since they provide new capabilities through instructions and code:</p> <ul> <li>Install skills only from trusted sources</li> <li>Audit skills before use\u2014review bundled files, code dependencies, and network connections</li> <li>Pay attention to instructions that connect to external sources</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#6-tools","title":"6) Tools","text":"<p>What: Function calls to fetch data or take actions (APIs, DB, search, file ops, \"computer use\").</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-tools","title":"When to use Tools","text":"<ul> <li>You want deterministic side\u2011effects and data fidelity.</li> <li>You orchestrate plan \u2192 call \u2192 verify \u2192 continue loops.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#tool-patterns","title":"Tool Patterns","text":"<ul> <li>Tool\u2011first planning + post\u2011call validators.</li> <li>Structured outputs between steps.</li> <li>Fallbacks when tools fail (retry \u2192 degrade \u2192 human\u2011in\u2011loop).</li> </ul> <p>Key concepts:</p> <ul> <li>Idempotent: safe to retry without side effects (GET=yes, POST/DELETE=no).</li> <li>Postconditions: checks after each call (non_empty_result, status==\"ok\", valid_json).</li> <li>Fallback chain: retry \u2192 degrade gracefully \u2192 human-in-loop.</li> </ul> <p>A Note on MCP (Model Context Protocol)</p> <p>MCP is becoming the standard for how agents connect to tools and data [6]. Instead of custom API wrappers for every service, you run an \"MCP Server\" for each. Your agent automatically discovers available tools and resources.</p> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#7-guardrails","title":"7) Guardrails","text":"<p>What: Input/output validation, safety filters, jailbreak defense, schema enforcement, content policy.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-to-use-guardrails","title":"When to use Guardrails","text":"<ul> <li>You need compliance/brand integrity.</li> <li>You want typed, correct outputs and safe behavior.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#guardrail-patterns","title":"Guardrail Patterns","text":"<ul> <li>Programmable rails (policy rules + actions).</li> <li>Schema + semantic validators (types, regex, evals).</li> <li>Central policy + observability (dashboards, red\u2011teaming).</li> </ul> <p>Repair vs refuse flow:</p> <ul> <li>Schema violations: Attempt automatic repair once. If repair fails, refuse with clear error.</li> <li>Policy violations: Refuse immediately (no repair attempt). Suggest safe alternative.</li> </ul> <p>Common guardrail types:</p> <ol> <li>Input guards: PII detection, prompt injection defense, toxicity filters</li> <li>Output guards: schema validation, content policy, factual consistency</li> <li>Tool guards: rate limiting, permission checks, cost thresholds</li> <li>Memory guards: PII redaction before storage, expiry enforcement</li> </ol>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#concrete-example-support-bot-answering-a-ticket","title":"Concrete example: support bot answering a ticket","text":"<p>Let's see how all these components work together. When a customer asks \"Why is my API key not working?\", the context layer assembles:</p> <ul> <li>Instructions: role = helpful support assistant for ACME, cite sources, return JSON {answer, sources, next_steps}.</li> <li>Examples: 2 short Q\u2192A pairs showing tone and JSON shape (one about API keys, one about billing).</li> <li>Knowledge: search the help center and product runbooks for \"API key troubleshooting\"; include relevant quotes.</li> <li>Memory: customer name \"Sam\", account_id \"A-123\", plan \"Pro\", last interaction was \"API key created 3 days ago\".</li> <li>Skills: load <code>ticket-handling</code> skill with troubleshooting procedures, escalation policies, and resolution templates.</li> <li>Tools: <code>search_tickets(customer_id)</code>, <code>check_api_key_status(key)</code>, <code>create_issue(description)</code>.</li> <li>Guardrails: redact any API key values in output; if schema fails, repair once; if policy violated (e.g., requesting to delete production data), refuse politely.</li> </ul> <p>The model receives all of this structured context, generates an answer, and the guardrails validate it before sending to the customer.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-fundamentals-deep-dive","title":"Context Fundamentals Deep Dive","text":"<p>Understanding the anatomy of context is prerequisite to effective context engineering. This section provides the foundational knowledge for everything that follows.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#the-anatomy-of-context","title":"The Anatomy of Context","text":"<p>Context comprises several distinct components, each with different characteristics and constraints:</p> <p>System Prompts</p> <p>System prompts establish the agent's core identity, constraints, and behavioral guidelines. They are loaded once at session start and typically persist throughout the conversation.</p> <p>The key is finding the right \"altitude\"\u2014specific enough to guide behavior effectively, yet flexible enough to provide strong heuristics. Too low (hardcoded brittle logic) creates fragility; too high (vague guidance) fails to give concrete signals.</p> <p>Organize prompts into distinct sections using XML tagging or Markdown headers:</p> <pre><code>&lt;BACKGROUND_INFORMATION&gt;\nYou are a Python expert helping a development team.\nCurrent project: Data processing pipeline in Python 3.9+\n&lt;/BACKGROUND_INFORMATION&gt;\n\n&lt;INSTRUCTIONS&gt;\n- Write clean, idiomatic Python code\n- Include type hints for function signatures\n- Add docstrings for public functions\n&lt;/INSTRUCTIONS&gt;\n\n&lt;TOOL_GUIDANCE&gt;\nUse bash for shell operations, python for code tasks.\nFile operations should use pathlib for cross-platform compatibility.\n&lt;/TOOL_GUIDANCE&gt;\n</code></pre> <p>Tool Definitions</p> <p>Tool definitions specify the actions an agent can take. Each tool includes a name, description, parameters, and return format. Tool descriptions collectively steer agent behavior\u2014poor descriptions force agents to guess; optimized descriptions include usage context, examples, and defaults.</p> <p>The consolidation principle: If a human engineer cannot definitively say which tool should be used in a given situation, an agent cannot be expected to do better.</p> <p>Retrieved Documents</p> <p>Agents use retrieval augmented generation to pull relevant documents into context at runtime rather than pre-loading all possible information. The just-in-time approach maintains lightweight identifiers (file paths, stored queries, web links) and loads data dynamically.</p> <p>Message History</p> <p>Message history contains the conversation between user and agent, including previous queries, responses, and reasoning. For long-running tasks, message history can grow to dominate context usage. It serves as scratchpad memory where agents track progress and maintain task state.</p> <p>Tool Outputs</p> <p>Tool outputs are the results of agent actions: file contents, search results, command execution output, API responses. Research shows observations (tool outputs) can reach over 80% of total context usage in typical agent trajectories [4]. This creates pressure for strategies like observation masking and compaction.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-windows-and-attention-mechanics","title":"Context Windows and Attention Mechanics","text":"<p>The Attention Budget Constraint</p> <p>Language models process tokens through attention mechanisms that create pairwise relationships between all tokens. For n tokens, this creates n\u00b2 relationships that must be computed. As context length increases, the model's ability to capture relationships gets stretched thin.</p> <p>Models develop attention patterns from training data where shorter sequences predominate. This means models have less experience with context-wide dependencies. The result is an \"attention budget\" that depletes as context grows.</p> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#the-progressive-disclosure-principle","title":"The Progressive Disclosure Principle","text":"<p>Progressive disclosure manages context efficiently by loading information only as needed. At startup, agents load only skill names and descriptions\u2014sufficient to know when a skill might be relevant. Full content loads only when activated for specific tasks.</p> <pre><code># Instead of loading all documentation at once:\n\n# Step 1: Load summary\n\ndocs/api_summary.md # Lightweight overview\n\n# Step 2: Load specific section as needed\n\ndocs/api/endpoints.md # Only when API calls needed\ndocs/api/authentication.md # Only when auth context needed\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-quality-versus-quantity","title":"Context Quality Versus Quantity","text":"<p>The assumption that larger context windows solve memory problems has been empirically debunked. Context engineering means finding the smallest possible set of high-signal tokens.</p> <p>Several factors create pressure for context efficiency:</p> <ul> <li>Cost: Processing cost grows disproportionately with context length\u2014not just double for double the tokens, but exponentially more in time and computing resources.</li> <li>Degradation: Model performance degrades beyond certain context lengths even when the window technically supports more.</li> <li>Latency: Long inputs remain expensive even with prefix caching.</li> </ul> <p>The guiding principle: Informativity over exhaustiveness. Include what matters for the decision at hand, exclude what does not.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-degradation-patterns","title":"Context Degradation Patterns","text":"<p>Language models exhibit predictable degradation patterns as context length increases. Understanding these patterns is essential for diagnosing failures and designing resilient systems.</p> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#the-lost-in-middle-phenomenon","title":"The Lost-in-Middle Phenomenon","text":"<p>The most well-documented degradation pattern is \"lost-in-middle,\" where models demonstrate U-shaped attention curves [1]. Information at the beginning and end of context receives reliable attention, while information in the middle suffers from 10-40% lower recall accuracy.</p> <p>Why it happens: Models allocate massive attention to the first token (often the BOS token) to stabilize internal states, creating an \"attention sink\" that consumes attention budget [2]. As context grows, middle tokens fail to garner sufficient attention weight.</p> <p>Practical fix: Place critical information at the beginning or end of context. Use summary structures that surface key information at attention-favored positions.</p> <p></p> <pre><code># Organize context with critical info at edges\n\n[CURRENT TASK] # At start\n\n- Goal: Generate quarterly report\n- Deadline: End of week\n\n[DETAILED CONTEXT] # Middle (less attention)\n\n- 50 pages of data\n- Multiple analysis sections\n- Supporting evidence\n\n[KEY FINDINGS] # At end\n\n- Revenue up 15%\n- Costs down 8%\n- Growth in Region A\n</code></pre> <p>Prompt duplication technique: For critical instructions that must not be missed, duplicate them at both the beginning AND end of the context. Since models attend strongly to both edges, placing the same instruction in both positions ensures it gets proper attention regardless of context length. This is particularly useful for:</p> <ul> <li>System constraints that must always be followed</li> <li>Output format requirements</li> <li>Safety policies and content guidelines</li> </ul> <pre><code># Example: Duplicating critical instructions\n\n[SYSTEM - START]\nCRITICAL: Always respond in JSON format. Never include PII.\n\n[... long context with documents, history, tools ...]\n\n[SYSTEM - REMINDER]\nCRITICAL: Always respond in JSON format. Never include PII.\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-poisoning","title":"Context Poisoning","text":"<p>Context poisoning occurs when hallucinations, errors, or incorrect information enters context and compounds through repeated reference. Once poisoned, context creates feedback loops that reinforce incorrect beliefs.</p> <p>How poisoning occurs:</p> <ol> <li>Tool outputs contain errors or unexpected formats</li> <li>Retrieved documents have incorrect or outdated information</li> <li>Model-generated summaries introduce hallucinations that persist</li> </ol> <p>Detection symptoms:</p> <ul> <li>Degraded output quality on tasks that previously succeeded</li> <li>Tool misalignment (agents call wrong tools or parameters)</li> <li>Persistent hallucinations despite correction attempts</li> </ul> <p>Recovery: Truncate context to before the poisoning point, explicitly note the poisoning and request re-evaluation, or restart with clean context preserving only verified information.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-distraction","title":"Context Distraction","text":"<p>Context distraction emerges when context grows so long that models over-focus on provided information at the expense of their training knowledge.</p> <p>Research shows that even a single irrelevant document reduces performance on tasks involving relevant documents [8]. The effect follows a step function\u2014the presence of any distractor triggers degradation.</p> <p>Key insight: Models cannot \"skip\" irrelevant context. They must attend to everything provided, creating distraction even when irrelevant information is clearly not useful.</p> <p>Mitigation: Apply relevance filtering before loading documents. Use namespacing to make irrelevant sections easy to ignore. Consider whether information needs to be in context or can be accessed through tool calls.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-confusion","title":"Context Confusion","text":"<p>Context confusion arises when irrelevant information influences responses in ways that degrade quality. If you put something in context, the model has to pay attention to it\u2014it may incorporate irrelevant information, use inappropriate tool definitions, or apply constraints from different contexts.</p> <p>Signs: Responses address the wrong aspect of queries, tool calls seem appropriate for different tasks, outputs mix requirements from multiple sources.</p> <p>Fix: Explicit task segmentation (different tasks get different context windows), clear transitions between task contexts, state management that isolates context.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-clash","title":"Context Clash","text":"<p>Context clash develops when accumulated information directly conflicts, creating contradictory guidance. This differs from poisoning where one piece is incorrect\u2014in clash, multiple correct pieces contradict each other.</p> <p>Common sources: Multi-source retrieval with contradictory information, version conflicts (outdated and current information both in context), perspective conflicts (valid but incompatible viewpoints).</p> <p>Resolution: Explicit conflict marking, priority rules establishing which source takes precedence, version filtering to exclude outdated information.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#model-specific-degradation-thresholds","title":"Model-Specific Degradation Thresholds","text":"<p>Research provides concrete data on when performance degradation begins. Note that \"effective context length\" (where models maintain optimal performance) is often significantly smaller than advertised maximum context windows [3]:</p> Model Max Context Effective Context Degradation Notes GPT-4 Turbo 128K tokens ~32K tokens Retrieval degrades after 32K, accuracy suffers beyond 64K GPT-4o 128K tokens ~8K tokens Complex NIAH accuracy drops from 99% to 70% at 32K Claude 3.5 Sonnet 200K tokens ~4K tokens Complex NIAH accuracy drops from 88% to 30% at 32K Gemini 1.5 Pro 1M tokens ~128K tokens 99% NIAH recall at 1M, best long-context performance Gemini 2.0 Flash 1M tokens ~32K tokens Complex NIAH accuracy drops from 94% to 48% at 32K <p>Sources: RULER benchmark [3], NoLiMa benchmark [9], Google technical reports</p> <p>Key finding [3]: Only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens. Near-perfect scores on simple needle-in-haystack tests do not translate to real long-context understanding\u2014complex reasoning tasks show much steeper degradation.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-compression-strategies","title":"Context Compression Strategies","text":"<p>Terminology note: Context compression is an umbrella category that includes several techniques: summarization (for conversation history and memory), observation masking (for tool outputs), and selective trimming [5]. The memory summarization referenced in the Memory section is one application of these broader compression strategies.</p> <p>When agent sessions generate millions of tokens of conversation history, compression becomes mandatory. The naive approach is aggressive compression to minimize tokens per request. The correct optimization target is tokens per task [4]: total tokens consumed to complete a task, including re-fetching costs when compression loses critical information.</p> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#when-compression-is-needed","title":"When Compression is Needed","text":"<p>Activate compression when:</p> <ul> <li>Agent sessions exceed context window limits</li> <li>Agents \"forget\" what files they modified</li> <li>Debugging long-running coding or debugging sessions</li> <li>Performance degrades in extended conversations</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#three-production-ready-approaches","title":"Three Production-Ready Approaches","text":"<p>1. Anchored Iterative Summarization</p> <p>Maintain structured, persistent summaries with explicit sections for session intent, file modifications, decisions, and next steps. When compression triggers, summarize only the newly-truncated span and merge with the existing summary.</p> <p>Key insight: Structure forces preservation. Dedicated sections act as checklists that the summarizer must populate, preventing silent information drift.</p> <pre><code>## Session Intent\n\nDebug 401 Unauthorized error on /api/auth/login despite valid credentials.\n\n## Root Cause\n\nStale Redis connection in session store. JWT generated correctly but session could not be persisted.\n\n## Files Modified\n\n- auth.controller.ts: No changes (read only)\n- config/redis.ts: Fixed connection pooling configuration\n- services/session.service.ts: Added retry logic for transient failures\n- tests/auth.test.ts: Updated mock setup\n\n## Test Status\n\n14 passing, 2 failing (mock setup issues)\n\n## Next Steps\n\n1. Fix remaining test failures (mock session service)\n2. Run full test suite\n3. Deploy to staging\n</code></pre> <p>2. Opaque Compression</p> <p>Produce compressed representations optimized for reconstruction fidelity. Achieves highest compression ratios (99%+) but sacrifices interpretability. Use when maximum token savings required and re-fetching costs are low.</p> <p>3. Regenerative Full Summary</p> <p>Generate detailed structured summaries on each compression. Produces readable output but may lose details across repeated compression cycles.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#compression-comparison","title":"Compression Comparison","text":"<p>Research from Factory.ai [4] compared compression strategies on real production agent sessions:</p> Method Compression Ratio Quality Score Best For Anchored Iterative 98.6% 3.70/5 Long sessions, file tracking Regenerative 98.7% 3.44/5 Clear phase boundaries Opaque 99.3% 3.35/5 Maximum savings, short sessions <p>Understanding the metrics:</p> <ul> <li> <p>Compression Ratio (98.6%): The percentage of tokens removed. A 98.6% ratio means if you had 100,000 tokens of conversation history, only 1,400 tokens remain after compression (98,600 were removed).</p> </li> <li> <p>Quality Score (3.70/5): Measured via probe-based evaluation\u2014after compression, the agent is asked questions that require recalling specific details from the truncated history (e.g., \"What files did we modify?\", \"What was the error message?\"). A score of 3.70/5 means the agent answered ~74% of probes correctly.</p> </li> <li> <p>0.7% additional tokens: Comparing Anchored Iterative (98.6%) to Opaque (99.3%), the difference is 0.7%. For a 100K token session: Anchored keeps 1,400 tokens, Opaque keeps only 700 tokens. That extra 700 tokens (0.7% of original) buys 0.35 quality points (3.70 vs 3.35)\u2014meaning significantly better recall of task-critical details.</p> </li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#the-artifact-trail-problem","title":"The Artifact Trail Problem","text":"<p>Artifact trail integrity is the weakest dimension across all compression methods, scoring 2.2-2.5 out of 5.0. Coding agents need to know which files were created, modified, read\u2014and compression often loses this.</p> <p>Recommendation: Implement a separate artifact index or explicit file-state tracking in agent scaffolding, beyond general summarization.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#compression-trigger-strategies","title":"Compression Trigger Strategies","text":"Strategy Trigger Point Trade-off Fixed threshold 70-80% utilization Simple but may compress early Sliding window Keep last N turns + summary Predictable context size Importance-based Compress low-relevance first Complex but preserves signal Task-boundary Compress at task completions Clean summaries, unpredictable timing","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#probe-based-evaluation","title":"Probe-Based Evaluation","text":"<p>Traditional metrics like ROUGE fail to capture functional compression quality. A summary may score high on lexical overlap while missing the one file path the agent needs.</p> <p>Probe-based evaluation directly measures quality by asking questions after compression:</p> Probe Type What It Tests Example Question Recall Factual retention \"What was the original error message?\" Artifact File tracking \"Which files have we modified?\" Continuation Task planning \"What should we do next?\" Decision Reasoning chain \"What did we decide about the Redis issue?\" <p>If compression preserved the right information, the agent answers correctly. If not, it guesses or hallucinates.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-optimization-techniques","title":"Context Optimization Techniques","text":"<p>Context optimization extends effective capacity through strategic compression, masking, caching, and partitioning. These techniques build on the Context Compression Strategies covered earlier, applying them systematically for production use. Effective optimization can double or triple effective context capacity without requiring larger models.</p> <p></p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#compaction-strategies","title":"Compaction Strategies","text":"<p>Compaction summarizes context contents when approaching limits, then reinitializes with the summary. This distills contents in a high-fidelity manner, enabling continued operation with minimal degradation.</p> <p>Priority for compression:</p> <ol> <li>Tool outputs \u2192 replace with summaries</li> <li>Old turns \u2192 summarize early conversation</li> <li>Retrieved docs \u2192 summarize if recent versions exist</li> <li>Never compress: system prompt</li> </ol> <p>Effective summaries preserve different elements by type:</p> <ul> <li>Tool outputs: Preserve key findings, metrics, conclusions. Remove verbose raw output.</li> <li>Conversational turns: Preserve decisions, commitments, context shifts. Remove filler.</li> <li>Retrieved documents: Preserve key facts and claims. Remove supporting elaboration.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#observation-masking","title":"Observation Masking","text":"<p>Tool outputs can comprise over 80% of token usage [4]. Once an agent has used a tool output to make a decision, keeping the full output provides diminishing value.</p> <p>Masking decision matrix:</p> Category Action Reasoning Current task observations Never mask Critical to current work Most recent turn Never mask Immediately relevant Active reasoning Never mask In-progress thought 3+ turns ago Consider masking Purpose likely served Repeated outputs Always mask Redundant Boilerplate Always mask Low signal","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#kv-cache-optimization","title":"KV-Cache Optimization","text":"<p>The KV-cache stores Key and Value tensors computed during inference. Caching across requests with identical prefixes avoids recomputation, dramatically reducing cost and latency.</p> <p>Optimize for caching:</p> <pre><code># Stable content first (cacheable)\ncontext = [system_prompt, tool_definitions]\n# Frequently reused elements\ncontext += [reused_templates]\n# Unique elements last\ncontext += [unique_content]\n</code></pre> <p>Design for cache stability: Avoid dynamic content like timestamps in prompts, use consistent formatting, keep structure stable across sessions.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#context-partitioning","title":"Context Partitioning","text":"<p>The most aggressive optimization is partitioning work across sub-agents with isolated contexts. Each sub-agent operates in a clean context focused on its subtask without carrying accumulated context from other subtasks.</p> <p>Result aggregation: Validate all partitions completed, merge compatible results, summarize if still too large.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#optimization-decision-framework","title":"Optimization Decision Framework","text":"<p>When to optimize:</p> <ul> <li>Context utilization exceeds 70%</li> <li>Response quality degrades in extended conversations</li> <li>Costs increase due to long contexts</li> <li>Latency increases with conversation length</li> </ul> <p>What to apply:</p> <ul> <li>Tool outputs dominate \u2192 observation masking</li> <li>Retrieved documents dominate \u2192 summarization or partitioning</li> <li>Message history dominates \u2192 compaction with summarization</li> <li>Multiple components \u2192 combine strategies</li> </ul> <p>Target metrics:</p> <ul> <li>Compaction: 50-70% token reduction with &lt;5% quality degradation</li> <li>Masking: 60-80% reduction in masked observations</li> <li>Cache optimization: 70%+ hit rate for stable workloads</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#how-to-cook-it-stepbystep","title":"How to cook it (step\u2011by\u2011step)","text":"<p>Here's a practical recipe to implement the context layer in your agentic system. Start simple, then add complexity only when needed.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-1-write-the-contract","title":"Step 1: Write the contract","text":"<p>Define what your agent must do and how it should behave.</p> <p>Actions:</p> <ul> <li>Write system-level policies: role, constraints, safety rules</li> <li>Write developer guidelines: output format, tone, citation requirements</li> <li>Define JSON Schemas for all outputs: <code>AnswerSchema</code>, <code>PlanSchema</code>, <code>StepResultSchema</code></li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-2-pick-retrieval-strategy","title":"Step 2: Pick retrieval strategy","text":"<p>Start with hybrid retrieval (BM25 + vector) + reranker.</p> <p>Decision tree:</p> <ul> <li>Query is general knowledge? \u2192 No retrieval (parametric)</li> <li>Query needs fresh/private facts? \u2192 Single-shot RAG (hybrid + rerank)</li> <li>Query is complex/multi-part? \u2192 Iterative RAG (break into subqueries)</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-3-design-memory","title":"Step 3: Design memory","text":"<p>Split short-term (conversation state) from long-term (user facts, history).</p> <p>Actions:</p> <ul> <li>Short-term: store conversation state, last few turns. Clear after session.</li> <li>Long-term: store user entities, preferences, episodic events.</li> <li>Set expiry rules: preferences 365d, episodic 90d, short-term session-only.</li> <li>Add PII redaction before storing anything.</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-4-specify-tools","title":"Step 4: Specify tools","text":"<p>Define clear tool signatures with validation and fallback strategies.</p> <p>Actions:</p> <ul> <li>For each tool: write clear docstring, input schema, output schema</li> <li>Mark idempotency: is it safe to retry?</li> <li>Define postconditions and fallback chains</li> <li>Validate tool arguments against schema before calling</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-5-install-guardrails","title":"Step 5: Install guardrails","text":"<p>Add input and output validation, safety filters, and policy enforcement.</p> <p>Quick checklist:</p> <ul> <li>[ ] Redact PII (emails, SSNs, credit cards) before processing</li> <li>[ ] Validate all outputs against JSON Schema</li> <li>[ ] Block prompt injection attempts</li> <li>[ ] Rate limit tool calls</li> <li>[ ] Log all policy violations for auditing</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-6-add-observability-evals","title":"Step 6: Add observability &amp; evals","text":"<p>Instrument your context layer so you can debug and improve it.</p> <p>Essential traces:</p> <ul> <li>Which context sources loaded?</li> <li>Token counts: input, output, cost</li> <li>Retrieval metrics: query, top-k results, sources cited</li> <li>Tool calls: which tools, arguments, results, failures</li> <li>Guardrail triggers: input blocks, output repairs, policy refusals</li> </ul> <p>Metrics to track:</p> <ol> <li>Exactness (schema validity): Target 99%+</li> <li>Groundedness (citation rate): Target 90%+ for knowledge queries</li> <li>Latency: Target &lt;2s p95</li> <li>Cost: Target &lt;$0.05 per query</li> </ol>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#step-7-iterate","title":"Step 7: Iterate","text":"<p>Start with basics. Add advanced patterns only when you hit clear limits.</p> <p>When to add:</p> <ul> <li>Reflections: when error rate &gt; 5%</li> <li>Planners: when tasks require &gt; 3 sequential steps</li> <li>Sub-agents: when you have distinct domains</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#antipatterns","title":"Anti\u2011patterns","text":"<p>Common mistakes that kill agentic systems. Avoid these.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#1-stuff-the-window","title":"1. Stuff-the-window","text":"<p>What: Dump every possible document, memory, and example into context on every query.</p> <p>Why it fails: Context rot. Signal-to-noise ratio collapses. See Context Degradation Patterns for details on distraction and confusion.</p> <p>Fix: Route adaptively. Use compression and masking. See Context Optimization Techniques.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#2-unvalidated-tool-results","title":"2. Unvalidated tool results","text":"<p>What: Agent calls a tool, gets back data, and immediately feeds it to the model without checking.</p> <p>Why it fails: Malformed data crashes downstream logic. Null results cause hallucinations. This is a primary vector for Context Poisoning.</p> <p>Fix: Always validate tool results against schema and postconditions.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#3-one-shot-everything","title":"3. One-shot everything","text":"<p>What: Cram system policy, developer guidelines, examples, user query, memory, and knowledge into a single monolithic prompt.</p> <p>Why it fails: No separation of concerns. Context window fills with duplicate boilerplate.</p> <p>Fix: Separate durable instructions from step-specific context. Use KV-Cache Optimization patterns.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#4-unbounded-memory","title":"4. Unbounded memory","text":"<p>What: Store every user interaction forever. Load all of it on every query.</p> <p>Why it fails: Context fills with stale, irrelevant memories. Privacy risks. See Lost-in-Middle for why middle content gets ignored anyway.</p> <p>Fix: Set retention policies. Implement scoped retrieval. Redact PII.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#5-rag-everywhere","title":"5. RAG everywhere","text":"<p>What: Retrieve documents for every single query, even \"What is 2+2?\" or \"Hello\".</p> <p>Why it fails: Wastes latency and cost. Retrieval can inject noise that causes Context Distraction.</p> <p>Fix: Implement adaptive RAG routing. Use classifiers or heuristics.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#6-ignoring-guardrail-triggers","title":"6. Ignoring guardrail triggers","text":"<p>What: Log guardrail violations but never review them.</p> <p>Why it fails: You miss real attacks. You miss UX issues. Schema repairs shouldn't be frequent\u2014if they are, your instructions are unclear.</p> <p>Fix: Review guardrail triggers weekly.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#7-no-evals","title":"7. No evals","text":"<p>What: Ship context layer changes without testing them.</p> <p>Why it fails: Silent regressions. No way to compare variants objectively.</p> <p>Fix: Define 5\u201310 eval scenarios before shipping. Run on every change. Use Probe-Based Evaluation to catch compression quality issues.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#quick-wins-ship-these-today","title":"Quick wins: ship these today","text":"<p>If you already have an agent in production and want immediate improvements, start here. Each takes &lt; 1 day.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#1-add-output-schema-validation","title":"1. Add output schema validation","text":"<p>Impact: Catch the majority of errors before they reach users.</p> <pre><code>from jsonschema import validate, ValidationError\n\ndef validate_output(output: dict) -&gt; dict:\n    try:\n        validate(instance=output, schema=ANSWER_SCHEMA)\n        return output\n    except ValidationError as e:\n        repaired = auto_repair(output, e)\n        validate(instance=repaired, schema=ANSWER_SCHEMA)\n        return repaired\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#2-instrument-basic-tracing","title":"2. Instrument basic tracing","text":"<p>Impact: Debug significantly faster when things break.</p> <pre><code>logger.info(json.dumps({\n    \"request_id\": request_id,\n    \"query\": query,\n    \"context_loaded\": {\"instructions\": True, \"memory\": True, \"knowledge\": True},\n    \"tokens\": {\"input\": 1200, \"output\": 150},\n    \"latency_ms\": 1120,\n    \"result\": \"success\"\n}))\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#3-split-system-vs-user-messages","title":"3. Split system vs user messages","text":"<p>Impact: Reduce token waste significantly by enabling KV-Cache Optimization.</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": SYSTEM_POLICY + DEVELOPER_GUIDELINES},\n    {\"role\": \"user\", \"content\": f\"Query: {query}\\nMemory: {memory}\\nKnowledge: {knowledge}\"}\n]\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#4-add-citation-requirements","title":"4. Add citation requirements","text":"<p>Impact: Build trust, enable auditing, reduce hallucinations.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#5-set-memory-expiry","title":"5. Set memory expiry","text":"<p>Impact: Prevent context pollution and privacy risks.</p> <pre><code>def load_memory(customer_id: str) -&gt; dict:\n    entries = db.get_memory(customer_id)\n    now = datetime.now()\n    return {\n        k: v for k, v in entries.items()\n        if v.get(\"expires_at\", now) &gt; now\n    }\n</code></pre>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#conclusion","title":"Conclusion","text":"<p>Context engineering is the discipline that separates demo agents from production agents. The model didn't get worse\u2014the context did. By understanding:</p> <ul> <li>Fundamentals: Context is finite, attention is limited, progressive disclosure is key</li> <li>Degradation: Lost-in-middle, poisoning, distraction, confusion, clash</li> <li>Compression: Tokens-per-task over tokens-per-request, structured summaries</li> <li>Optimization: Compaction, masking, caching, partitioning</li> </ul> <p>...you can build agents that work reliably at scale.</p> <p>Start with the quick wins. Add complexity only when you hit clear limits. And always measure\u2014you can't improve what you don't trace.</p> <p>This article incorporates content from the Agent Skills for Context Engineering collection, a set of reusable knowledge modules for building better AI agents.</p>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#references","title":"References","text":"<ol> <li> <p>Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp; Liang, P. (2023). \"Lost in the Middle: How Language Models Use Long Contexts.\" arXiv preprint arXiv:2307.03172. https://arxiv.org/abs/2307.03172</p> <ul> <li>Key finding: 10-40% lower recall accuracy for information in the middle of context vs. beginning/end.</li> </ul> </li> <li> <p>Xiao, G., Tian, Y., Chen, B., Han, S., &amp; Lewis, M. (2023). \"Efficient Streaming Language Models with Attention Sinks.\" ICLR 2024. https://arxiv.org/abs/2309.17453</p> <ul> <li>Introduces the \"attention sink\" phenomenon where LLMs allocate disproportionate attention to initial tokens.</li> </ul> </li> <li> <p>Hsieh, C. Y., et al. (2024). \"RULER: What's the Real Context Size of Your Long-Context Language Models?\" COLM 2024. https://arxiv.org/abs/2404.06654</p> <ul> <li>Key finding: Only 50% of models claiming 32K+ context maintain satisfactory performance at 32K tokens.</li> </ul> </li> <li> <p>Factory.ai Research. (2025). \"Evaluating Context Compression for AI Agents.\" https://www.factory.ai/blog/evaluating-context-compression</p> <ul> <li>Source for compression strategy comparisons, tokens-per-task optimization, and probe-based evaluation methodology.</li> </ul> </li> <li> <p>Li, Y., et al. (2023). \"Compressing Context to Enhance Inference Efficiency of Large Language Models.\" EMNLP 2023.</p> <ul> <li>Research on selective context pruning using self-information metrics.</li> </ul> </li> <li> <p>Anthropic. (2024). \"Model Context Protocol (MCP) Specification.\" https://modelcontextprotocol.io/</p> <ul> <li>Official specification for the MCP standard for AI-tool integration.</li> </ul> </li> <li> <p>LangChain/LangGraph. (2024). \"How to add memory to the prebuilt ReAct agent.\" https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/    \u2014 Demonstrates that summarization is one technique for managing memory within context limits.</p> </li> <li> <p>Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., &amp; Berant, J. (2024). \"Making Retrieval-Augmented Language Models Robust to Irrelevant Context.\" ICLR 2024. https://arxiv.org/abs/2310.01558</p> <ul> <li>Key finding: Even a single irrelevant document can significantly reduce RAG performance, creating a \"distracting effect.\"</li> </ul> </li> <li> <p>Maekawa, S., et al. (2025). \"NoLiMa: Long-Context Evaluation Beyond Literal Matching.\" ICML 2025. https://arxiv.org/abs/2502.05167</p> <ul> <li>Key finding: GPT-4o effective context ~8K tokens, Claude 3.5 Sonnet ~4K tokens when latent reasoning is required (vs. literal matching). At 32K tokens, GPT-4o drops from 99.3% to 69.7% accuracy.</li> </ul> </li> </ol>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/05/context-engineering-in-the-agenticai-era--and-how-to-cook-it/#additional-resources","title":"Additional Resources","text":"<ul> <li>Anthropic Claude Documentation: Best practices for long-context usage</li> <li>OpenAI Cookbook: Strategies for managing context windows</li> <li>LangChain Documentation: Memory and retrieval patterns</li> <li>LlamaIndex Documentation: RAG and chunking strategies</li> </ul>","tags":["ai-engineering","agents","context-layer","rag","retrieval","memory","guardrails","context-compression","context-optimization"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/","title":"Domain-driven design for AI agents: a beginner-friendly guide","text":"","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#tldr","title":"TL;DR","text":"<p>Domain-driven design (DDD) gives AI agent teams a shared language, clear boundaries, and code that mirrors the real world. Use it to tame prompt spaghetti, enforce business rules, and evolve systems without breaking everything.</p> <p></p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#why-domain-driven-design-matters-for-ai-agents","title":"Why domain-driven design matters for AI agents","text":"<p>AI agent projects fail for a surprising reason: not because the code is bad, but because developers and domain experts can't understand each other. You've seen it\u2014business teams ask for a \"policy check\" and get back a <code>process_data()</code> method. Nobody knows what it does, so requirements drift and systems calcify.</p> <p>Domain-driven design (DDD) fixes this by putting the business domain at the center. Not the database schema. Not the prompt template. The actual real-world process you're trying to model. This alignment delivers three immediate wins:</p> <ul> <li>Shared language. Everyone\u2014product, ops, engineering\u2014uses the same words. When compliance says \"refund request\", that's what appears in your code, prompts, and documentation.</li> <li>Focused scope. You build what matters: the core workflows, compliance rules, and critical metrics. Not mountains of glue code that break when requirements shift.</li> <li>Adaptability. When policies change (and they will), you update one well-defined slice instead of hunting through a monolithic tangle.</li> </ul> <p>This matters most in complex domains where rules evolve constantly\u2014think finance, healthcare, operations, or any regulated industry. DDD gives you a fighting chance to keep up.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#strategic-building-blocks","title":"Strategic building blocks","text":"<p>DDD isn't one big idea\u2014it's a toolkit of patterns that work together. It's often split into two parts:</p> <ol> <li>Strategic Design: The \"big picture\" stuff. Defining boundaries, teams, and how systems talk. This is crucial for multi-agent systems.</li> <li>Tactical Design: The code-level patterns (Entities, Aggregates). This keeps your agent's internal logic clean.</li> </ol> <p>Here are the core concepts you'll use every day.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#ubiquitous-language","title":"Ubiquitous language","text":"<p>This is the shared vocabulary that shows up everywhere: in meetings, documentation, prompts, and method names. No translation layers between \"business speak\" and \"code speak.\"</p> <p>If compliance says \"policy check\", your method is <code>run_policy_check()</code>, not <code>process_data()</code>. If doctors say \"admit patient\", you write <code>admit_patient()</code>, not <code>add_user()</code>.</p> <pre><code>class PatientRegistry:\n    def admit_patient(self, patient_id: str) -&gt; None:\n        \"\"\"Admit a patient to the registry - term used by medical staff.\"\"\"\n        ...\n</code></pre> <p>This eliminates translation gaps and makes code self-documenting. When requirements change, the language change is obvious and localized.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#bounded-contexts","title":"Bounded contexts","text":"<p>Large systems need explicit boundaries. Why? Because the same word means different things in different parts of the business.</p> <p>Take \"product\" in e-commerce. In the Inventory context, a product is a catalog item with SKUs and stock counts. In the Billing context, it's a line item with pricing rules and tax calculations. In Order Management, it's a quantity and delivery promise.</p> <p>Bounded contexts let each subdomain have its own definition without conflict. Translation layers or interfaces connect them when they need to talk.</p> <p>Bounded contexts let each subdomain have its own definition without conflict. Translation layers or interfaces connect them when they need to talk.</p> <p></p> <p>This keeps each model lean and prevents the \"one size fits all\" model that becomes unwieldy as complexity grows.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#entities-and-value-objects","title":"Entities and value objects","text":"<p>These are the basic building blocks of your domain model. Understanding the difference is key.</p> <p>Entities have identity that persists over time. A <code>Task</code> with ID <code>123</code> is the same task even if you change its description, status, or due date. Two entities are equal if they have the same ID, regardless of their attributes.</p> <pre><code>from pydantic import BaseModel\n\nclass SupportTicket(BaseModel):\n    ticket_id: str  # This is the identity\n    customer: str\n    issue: str\n    status: str = \"OPEN\"\n\n    def close(self) -&gt; None:\n        if self.status != \"OPEN\":\n            raise ValueError(\"Ticket already closed\")\n        self.status = \"CLOSED\"\n</code></pre> <p>Value objects have no identity\u2014they're defined entirely by their attributes. Two <code>TimeSlot</code> objects with the same start and end times are interchangeable. Value objects are immutable; instead of changing them, you create new ones.</p> <pre><code>from pydantic import BaseModel\n\nclass TimeSlot(BaseModel):\n    start: str  # e.g., \"2025-10-18 09:00\"\n    end: str    # e.g., \"2025-10-18 10:00\"\n\n    @property\n    def duration(self) -&gt; int:\n        # Compute duration from start to end\n        ...\n</code></pre> <p>Use entities for things that have lifecycles (<code>Order</code>, <code>User</code>, <code>AgentSession</code>). Use value objects for descriptions and measurements (<code>EmailAddress</code>, <code>Priority</code>, <code>Location</code>).</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#aggregates","title":"Aggregates","text":"<p>Aggregates are clusters of related entities and value objects that get treated as one unit. Think of them as consistency boundaries\u2014within an aggregate, business rules must always hold true.</p> <p>Every aggregate has one aggregate root\u2014an entity that controls access to everything inside. Want to modify something in the aggregate? Go through the root. This enforces invariants and prevents invalid states.</p> <pre><code>from datetime import date\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    id: str\n    description: str\n    completed: bool = False\n\nclass Plan(BaseModel):  # This is the aggregate root\n    id: str\n    tasks: list[Task] = Field(default_factory=list)\n\n    def add_task(self, task: Task) -&gt; None:\n        # Business rule enforced here: no duplicate task IDs\n        if any(t.id == task.id for t in self.tasks):\n            raise ValueError(\"Task ID already exists\")\n        self.tasks.append(task)\n</code></pre> <p>External code never touches the <code>tasks</code> list directly\u2014it always calls <code>add_task()</code>. This guarantees the \"no duplicate IDs\" rule can never be violated. When you save to a database, you typically save the entire aggregate at once.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#repositories","title":"Repositories","text":"<p>Repositories abstract away persistence. To your domain code, it feels like working with an in-memory collection\u2014no SQL queries, no database sessions, just clean methods like <code>save()</code> and <code>get()</code>.</p> <p>This separation has real benefits:</p> <ul> <li>Domain logic stays clean. It doesn't care if data lives in Postgres, MongoDB, or a JSON file.</li> <li>Testing is trivial. Swap in an in-memory repository for tests without touching domain code.</li> <li>Storage can evolve. Switch from SQLite to Redis without rewriting business rules.</li> </ul> <pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    id: str\n    description: str\n    completed: bool = False\n\nclass Plan(BaseModel):\n    id: str\n    tasks: list[Task] = Field(default_factory=list)\n\nclass PlanRepository(ABC):\n    \"\"\"Domain layer defines the interface.\"\"\"\n    @abstractmethod\n    def save(self, plan: Plan) -&gt; None:\n        ...\n\n    @abstractmethod\n    def get(self, plan_id: str) -&gt; Plan | None:\n        ...\n\nclass InMemoryPlanRepository(PlanRepository):\n    \"\"\"Infrastructure layer provides the implementation.\"\"\"\n    def __init__(self) -&gt; None:\n        self.storage: dict[str, Plan] = {}\n\n    def save(self, plan: Plan) -&gt; None:\n        self.storage[plan.id] = plan\n\n    def get(self, plan_id: str) -&gt; Plan | None:\n        return self.storage.get(plan_id)\n</code></pre> <p>Your domain code only knows about <code>PlanRepository</code> (the interface). The infrastructure layer plugs in the actual implementation.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#domain-events","title":"Domain events","text":"<p>Domain events capture important things that happen in your system. They're named in past tense\u2014<code>OrderPlaced</code>, <code>TaskCompleted</code>, <code>PaymentFailed</code>\u2014because they represent facts.</p> <p>Events make implicit side effects explicit. Instead of one module directly calling another when something happens, the domain raises an event. Other parts of the system subscribe and react independently.</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel\n\nclass TaskCompleted(BaseModel):\n    task_id: str\n    completed_at: datetime\n</code></pre> <p>When a task finishes, you emit <code>TaskCompleted</code>. A notification service might listen for this event and send an email. A reporting service might log it for analytics. The important part: the task aggregate doesn't need to know about emails or analytics. It just announces what happened.</p> <p>This decouples workflows and makes cross-context communication clean. It's especially powerful in multi-agent systems where agents react to each other's events.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#translating-ddd-to-agent-architectures","title":"Translating DDD to agent architectures","text":"<p>AI agents deal with complexity\u2014multi-step workflows, unreliable LLM outputs, evolving requirements. DDD's patterns map surprisingly well to these challenges. Here's how the concepts translate:</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#bounded-contexts-become-agents-or-skills","title":"Bounded contexts become agents or skills","text":"<p>Each agent (or major capability) is a bounded context. A research orchestrator might coordinate three specialized agents:</p> <ul> <li>Trends Agent \u2014 gathers market data using its own vocabulary and tools</li> <li>Compliance Agent \u2014 runs policy checks with regulatory terminology</li> <li>Cost Agent \u2014 estimates expenses with finance-specific rules</li> </ul> <p>Each has its own model, terminology, and invariants. They communicate through well-defined interfaces or events.</p> <p>Each has its own model, terminology, and invariants. They communicate through well-defined interfaces or events.</p> <p></p> <p>Even in a single-agent system, you might define internal contexts\u2014a Planning module and an Execution module, each with its own domain model.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#prompts-honor-the-ubiquitous-language","title":"Prompts honor the ubiquitous language","text":"<p>Use domain terms in system prompts, tool descriptions, and function signatures. If compliance experts say \"policy check\", that exact phrase appears in your prompts and code. This keeps humans and agents synchronized and makes the system easier to review and debug.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#state-becomes-explicit-entities","title":"State becomes explicit entities","text":"<p>LLMs are often stateless, but complex agents maintain state\u2014conversation sessions, goals, intermediate results, tool outputs. Model these as entities or value objects:</p> <ul> <li><code>ConversationSession</code> entity with ID and message history</li> <li><code>Task</code> entity representing units of work</li> <li><code>ToolOutput</code> value object for immutable results</li> </ul> <p>Explicit modeling enables validation, business rules, and reuse. You can enforce rules like \"a task can't be completed until dependencies finish\" directly in the entity methods.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#aggregates-express-agent-plans","title":"Aggregates express agent plans","text":"<p>A <code>Plan</code> aggregate root can govern task lists, enforce limits, and maintain priorities. When an LLM proposes adding 50 tasks, the aggregate enforces a maximum of 10. When it suggests duplicate work, the aggregate rejects it. This keeps AI proposals within business constraints.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#domain-events-drive-orchestration","title":"Domain events drive orchestration","text":"<p>Agents raise events\u2014<code>ResearchCompleted</code>, <code>ThresholdExceeded</code>, <code>PolicyViolationDetected</code>. Other agents or services listen and react without tight coupling. This event-driven approach is the future of scalable agent systems: it lets multiple agents collaborate in real-time without being hard-wired together.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#business-rules-wrap-ai-actions","title":"Business rules wrap AI actions","text":"<p>LLM outputs flow through domain services or entity methods. If an LLM suggests a refund amount beyond policy limits, your <code>RefundRequest</code> value object validates and rejects it. The AI can improvise, but business rules have the final say. This keeps agents safe and aligned with policy.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#the-anti-corruption-layer-acl","title":"The Anti-Corruption Layer (ACL)","text":"<p>When working with LLMs, you are dealing with a probabilistic, creative, and occasionally chaotic entity. Your domain model, however, must be deterministic and safe. You cannot let the raw output of an LLM leak directly into your domain logic.</p> <p>Enter the Anti-Corruption Layer (ACL).</p> <p></p> <p>The ACL acts as a gatekeeper. It translates the \"wild\" output of the LLM into the \"strict\" language of your domain.</p> <ol> <li>Ingest: Receive raw text or JSON from the LLM.</li> <li>Validate: Use Pydantic models to check structure and types.</li> <li>Sanitize: Ensure values fall within acceptable ranges (e.g., no negative prices).</li> <li>Translate: Convert DTOs (Data Transfer Objects) into Domain Entities.</li> </ol> <p>If validation fails, the ACL rejects the data\u2014often sending an error message back to the LLM so it can correct itself. This loop ensures that only valid data ever touches your core business logic.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#example-a-task-assistant-modeled-with-ddd","title":"Example: a task assistant modeled with DDD","text":"<p>Let's build a personal task assistant that handles requests like \"Remind me to buy milk tomorrow\" or \"What's on my to-do list?\" We'll apply DDD principles step by step.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#1-map-the-contexts","title":"1. Map the contexts","text":"<p>Start by breaking the problem into subdomains:</p> <ul> <li>Task Management \u2014 handling to-do items and reminders (core domain)</li> <li>Scheduling \u2014 calendar events and meetings</li> <li>Notifications \u2014 sending alerts and emails</li> </ul> <p>We'll focus on Task Management first. The others can evolve as separate bounded contexts or companion agents.</p> <p>We'll focus on Task Management first. The others can evolve as separate bounded contexts or companion agents.</p> <p></p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#2-speak-the-same-language","title":"2. Speak the same language","text":"<p>Establish the vocabulary with domain experts (or just common sense for personal tasks): \"task\", \"deadline\", \"reminder\", \"priority\". Use these exact terms everywhere\u2014prompt templates, method names, UI labels. No translation layers.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#3-capture-entities-value-objects-and-events","title":"3. Capture entities, value objects, and events","text":"<p>Now model the core concepts:</p> <ul> <li>Entity: <code>Task</code> with identity (<code>id</code>) and mutable state (<code>completed</code>)</li> <li>Value object: <code>Priority</code> enum (immutable, defined by its value)</li> <li>Domain event: <code>TaskCompletedEvent</code> to signal when work finishes</li> </ul> <pre><code>from datetime import datetime, date, timezone\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\n\nclass Priority(Enum):\n    \"\"\"Value object: priority is defined by its value alone.\"\"\"\n    LOW = 1\n    NORMAL = 2\n    HIGH = 3\n\nclass TaskCompletedEvent(BaseModel):\n    \"\"\"Domain event: announces a task was completed.\"\"\"\n    task_id: str\n    time: datetime\n\nclass Task(BaseModel):\n    \"\"\"Entity: identity persists even as attributes change.\"\"\"\n    id: str\n    description: str\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n    due_date: date | None = None\n    priority: Priority = Priority.NORMAL\n    completed: bool = False\n\n    def mark_completed(self) -&gt; TaskCompletedEvent:\n        \"\"\"Business rule: can't complete an already-completed task.\"\"\"\n        if self.completed:\n            raise ValueError(\"Task is already completed.\")\n        self.completed = True\n        return TaskCompletedEvent(task_id=self.id, time=datetime.now(timezone.utc))\n</code></pre> <p>Notice how business rules live in the entity methods, not scattered across prompt templates.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#4-shape-the-aggregate","title":"4. Shape the aggregate","text":"<p>The <code>TaskList</code> is our aggregate root. It holds multiple <code>Task</code> entities and enforces consistency rules across them. All modifications go through the root's methods.</p> <pre><code>from datetime import date\nfrom pydantic import BaseModel, Field\n\nclass Task(BaseModel):\n    id: str\n    description: str\n    due_date: date | None = None\n    completed: bool = False\n\nclass TaskList(BaseModel):\n    \"\"\"Aggregate root: enforces invariants across all tasks.\"\"\"\n    owner: str\n    tasks: list[Task] = Field(default_factory=list)\n\n    def add_task(self, task: Task) -&gt; None:\n        \"\"\"Business rule: no duplicate tasks on the same day.\"\"\"\n        if any(\n            existing.description == task.description\n            and existing.due_date == task.due_date\n            for existing in self.tasks\n        ):\n            raise ValueError(\"A similar task on that date already exists.\")\n        self.tasks.append(task)\n\n    def get_pending(self) -&gt; list[Task]:\n        \"\"\"Query helper: find tasks that aren't done yet.\"\"\"\n        return [task for task in self.tasks if not task.completed]\n\n\nTaskList.model_rebuild()  # Resolve forward references for Pydantic.\n</code></pre> <p>External code never manipulates <code>tasks</code> directly\u2014it always goes through <code>add_task()</code> or other root methods. This guarantees the \"no duplicates\" rule holds.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#5-wrap-persistence-in-a-repository","title":"5. Wrap persistence in a repository","text":"<p>The repository abstracts storage. Domain code doesn't know if tasks live in memory, a database, or a JSON file.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Task(BaseModel):\n    id: str\n    description: str\n    completed: bool = False\n\n\nclass TaskList(BaseModel):\n    owner: str\n    tasks: list[Task] = Field(default_factory=list)\n\n\nTaskList.model_rebuild()  # Resolve forward references for Pydantic.\n\n\nclass TaskRepository:\n    \"\"\"Abstracts task storage - in-memory implementation for simplicity.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._data: dict[str, TaskList] = {}\n\n    def get_task_list(self, owner: str) -&gt; TaskList:\n        \"\"\"Retrieve a user's task list, or create a new empty one.\"\"\"\n        return self._data.get(owner, TaskList(owner=owner))\n\n    def save_task_list(self, task_list: TaskList) -&gt; None:\n        \"\"\"Persist changes to the task list.\"\"\"\n        self._data[task_list.owner] = task_list\n</code></pre> <p>In production, you'd swap this for a database implementation\u2014say, using SQLAlchemy or Postgres\u2014without touching the domain logic.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#6-run-the-flow","title":"6. Run the flow","text":"<p>Here's how everything fits together when a user makes a request:</p> <pre><code>from datetime import date, timedelta\nfrom uuid import uuid4\nfrom pydantic import BaseModel, Field\n\n\nclass Task(BaseModel):\n    id: str\n    description: str\n    due_date: date | None = None\n    completed: bool = False\n\n\nclass TaskList(BaseModel):\n    owner: str\n    tasks: list[Task] = Field(default_factory=list)\n\n    def add_task(self, task: Task) -&gt; None:\n        if any(\n            existing.description == task.description\n            and existing.due_date == task.due_date\n            for existing in self.tasks\n        ):\n            raise ValueError(\"A similar task on that date already exists.\")\n        self.tasks.append(task)\n\n\nclass TaskRepository:\n    def __init__(self) -&gt; None:\n        self._data: dict[str, TaskList] = {}\n\n    def get_task_list(self, owner: str) -&gt; TaskList:\n        return self._data.get(owner, TaskList(owner=owner))\n\n    def save_task_list(self, task_list: TaskList) -&gt; None:\n        self._data[task_list.owner] = task_list\n\n\nTaskList.model_rebuild()  # Resolve forward references for Pydantic.\n\n\n# User says: \"Remind me to buy milk tomorrow\"\n# (In reality, an LLM would parse this into structured data)\nuser_input = \"Remind me to buy milk tomorrow\"\nintent = \"add_task\"\n\n# Initialize repository\nrepo = TaskRepository()\n\nif intent == \"add_task\":\n    # 1. Load the user's task list\n    task_list = repo.get_task_list(owner=\"User123\")\n\n    # 2. Create a new task entity\n    task = Task(\n        id=str(uuid4()),\n        description=\"buy milk\",\n        due_date=date.today() + timedelta(days=1),\n    )\n\n    # 3. Domain layer enforces business rules\n    try:\n        task_list.add_task(task)\n        repo.save_task_list(task_list)\n        print(f\"Task '{task.description}' added for {task.due_date}.\")\n    except Exception as exc:\n        print(f\"Sorry, I couldn't add that task: {exc}\")\n</code></pre> <p>Notice the separation of concerns:</p> <ul> <li>LLM layer parses natural language into structured data (intent + parameters)</li> <li>Domain layer enforces business rules through entity methods</li> <li>Repository layer handles persistence without leaking into domain logic</li> </ul> <p>The LLM can be creative with parsing, but the domain ensures consistency. If the LLM tries to add a duplicate task, the aggregate root rejects it\u2014no special-casing needed in prompts.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#tooling-to-bring-the-model-to-life","title":"Tooling to bring the model to life","text":"<p>DDD doesn't require special frameworks, but certain tools make implementation smoother\u2014especially for AI agents.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#fastapi","title":"FastAPI","text":"<p>FastAPI pairs beautifully with DDD. Use routers to separate bounded contexts (<code>/tasks</code>, <code>/schedule</code>), Pydantic models for request/response validation, and dependency injection to wire up repositories.</p> <p>Structure your project in layers:</p> <pre><code>project/\n\u251c\u2500\u2500 domain/          # Pure business logic (entities, aggregates, value objects)\n\u251c\u2500\u2500 application/     # Use cases and command handlers\n\u251c\u2500\u2500 infrastructure/  # Repositories, databases, external APIs\n\u2514\u2500\u2500 interface/       # FastAPI routers and HTTP contracts\n</code></pre> <p>This layering (sometimes called \"onion architecture\") keeps changes from rippling through your codebase. Swap the database? Touch only <code>infrastructure/</code>. Change the UI? Touch only <code>interface/</code>.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#pydantic-and-pydantic-ai","title":"Pydantic and Pydantic AI","text":"<p>Pydantic enforces invariants and validates data at runtime. Use it for entities, value objects, and especially for validating LLM outputs.</p> <p>Pydantic AI takes this further: it ensures LLM responses conform to your domain schemas. Define an <code>AddTaskCommand</code> with required fields, and Pydantic AI validates that the LLM's JSON output matches before you act on it. This brings structure to the chaotic world of AI outputs.</p> <p>Another excellent tool is Instructor, which patches OpenAI (and other) clients to return Pydantic models directly. It's a lightweight way to implement your Anti-Corruption Layer.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#ddd-helper-libraries","title":"DDD helper libraries","text":"<ul> <li>DDDesign \u2014 provides base classes for entities, repositories, and value objects built on Pydantic</li> <li>Protean \u2014 a full framework for DDD, CQRS, and event sourcing if you want something that comes with a lot of ready-made features out of the box</li> </ul> <p>Most Python developers skip these and use vanilla classes with Pydantic, but they're worth exploring for large projects.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#event-driven-tooling","title":"Event-driven tooling","text":"<p>For domain events, consider:</p> <ul> <li>blinker \u2014 lightweight in-process event dispatcher</li> <li>redis-py Pub/Sub or RabbitMQ \u2014 for distributed events across services or agents</li> <li>asyncio event patterns \u2014 if you're already async</li> </ul> <p>Events are crucial for multi-agent orchestration. An agent emits <code>ResearchCompleted</code>, others react\u2014no tight coupling.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#agent-frameworks","title":"Agent frameworks","text":"<p>LangChain, LangGraph, Haystack, Semantic Kernel, LlamaIndex, AutoGen, Google ADK, smolagents, and CrewAI provide structure for modern agent workflows. Use them within your domain layer, but wrap them in your own interfaces. That way, swapping frameworks doesn't break your business logic.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#testing","title":"Testing","text":"<p>One of DDD's biggest wins: your domain layer tests without the whole stack running.</p> <ul> <li>PyTest for unit tests on entities and aggregates</li> <li>Fake repositories (in-memory) for integration tests</li> <li>LLM stubs that return predetermined outputs</li> </ul> <p>Your domain code should never require a live LLM to test. The LLM is an implementation detail\u2014your tests validate business rules.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/20/domain-driven-design-for-ai-agents-a-beginner-friendly-guide/#getting-started-checklist","title":"Getting started checklist","text":"<p>Ready to apply DDD to your next agent project? Here's your roadmap:</p> <ol> <li>Interview domain experts. Draft the ubiquitous language\u2014the vocabulary everyone will use. Document it.</li> <li>Map bounded contexts. Draw the subdomains and mark where they need to talk to each other. Start with one core context.</li> <li>Model entities and value objects. What things have identity? What things are just values? Bake invariants into their methods.</li> <li>Define aggregate roots. Bundle related entities under one root that enforces consistency rules.</li> <li>Create repository interfaces. Don't implement storage yet\u2014just define <code>save()</code> and <code>get()</code> methods. Keep the domain clean.</li> <li>Emit domain events. For meaningful changes (order placed, task completed), raise events. Wire listeners later as needed.</li> <li>Wrap LLM outputs in schemas. Use Pydantic models to enforce contracts. Don't let free-form text leak into your domain.</li> <li>Add orchestration. Build application services that coordinate agents via structured commands or events.</li> </ol> <p>The golden rule: start with the domain, not the tech stack. Understand the business problem first. Model it explicitly. Then let the AI tooling serve that model\u2014not the other way around.</p>","tags":["ai-engineering","agents","architecture","domain-driven-design"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/","title":"LoRAX Playbook - Orchestrating Thousands of LoRA Adapters on Kubernetes","text":"<p>Serving dozens of fine-tuned large language models used to mean provisioning one GPU per model. LoRAX (LoRA eXchange) flips that math on its head: keep a single base model in memory and hot-swap lightweight LoRA adapters per request.</p> <p>This guide shows you how LoRAX achieves near-constant cost per token regardless of how many fine-tunes you're serving. We'll cover:</p> <ul> <li>What LoRA is and why it's a game-changer.</li> <li>LoRAX vs. vLLM: When to use which.</li> <li>Kubernetes Deployment: A production-ready Helm guide.</li> <li>API Usage: REST, Python, and OpenAI-compatible examples.</li> </ul>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#background-what-is-lora","title":"Background: What is LoRA?","text":"<p>Low-Rank Adaptation (LoRA) is a fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture.</p> <p>In simple terms: instead of retraining the entire model (which is slow and produces massive files), LoRA trains a tiny set of \"diffs\" that represent the new knowledge.</p> <ul> <li>Full Fine-tuning: Produces a 20GB+ file for a 7B model.</li> <li>LoRA Fine-tuning: Produces a ~100MB adapter file.</li> </ul> <p>This massive size reduction is what makes dynamic serving possible. You can store thousands of adapters on disk and load them into GPU memory in milliseconds.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#the-problem-lorax-solves","title":"The problem LoRAX solves","text":"<p>Traditional multi-model serving is expensive. Each fine-tuned model needs its own GPU memory allocation, which means serving 50 customer-specific models requires 50 separate deployments\u2014or at least 50x the memory. The costs scale linearly with every new variant you add.</p> <p>LoRAX is an Apache 2.0 project from Predibase that extends the Hugging Face Text Generation Inference server with three critical features: dynamic adapter loading, tiered weight caching, and multi-adapter batching. These let you serve hundreds of tenant-specific LoRA adapters on a single Ampere-class GPU without sacrificing throughput or latency.</p> <p>Here's the key insight: LoRA fine-tuning produces small delta weights (adapters) rather than full model copies. LoRAX exploits this by loading just the base model into GPU memory and injecting adapter weights on demand. Unused adapters consume zero VRAM.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#how-it-works-four-core-innovations","title":"How it works: four core innovations","text":"<p>1. Dynamic adapter loading</p> <p>Adapter weights are injected just-in-time for each request. The base model stays resident in GPU memory while adapters load on the fly without blocking other requests. This means you can catalog thousands of adapters but only pay memory costs for the ones actively serving traffic.</p> <p>2. Tiered weight caching</p> <p>LoRAX stages adapters across three layers: GPU VRAM for hot adapters, CPU RAM for warm ones, and disk for cold storage. This hierarchy prevents out-of-memory crashes while keeping swap times fast enough that users don't notice the difference.</p> <p>3. Continuous multi-adapter batching</p> <p>Here's where the magic happens. LoRAX extends continuous batching strategies to work across different adapters in parallel. Requests targeting different fine-tunes can share the same forward pass, keeping the GPU fully utilized. Benchmarks from Predibase show that processing 1M tokens spread across 32 different adapters takes about the same time as 1M tokens on a single model.</p> <p>4. Battle-tested foundation</p> <p>LoRAX builds on Hugging Face's Text Generation Inference (TGI) server, inheriting production-grade optimizations: FlashAttention 2, paged attention, SGMV kernels for multi-adapter inference, and streaming responses. You get the stability of TGI plus the flexibility of dynamic adapter switching.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#the-economics-near-constant-cost-scaling","title":"The economics: near-constant cost scaling","text":"<p>The chart below demonstrates the cost advantage. While traditional dedicated deployments (dark gray) scale linearly\u2014double the models means double the cost\u2014LoRAX (orange) keeps per-token costs nearly flat regardless of how many adapters you serve. Even hosted API fine-tunes from providers like OpenAI (light gray) can't match this efficiency for multi-model scenarios.</p> <p></p> <p>Cost per million tokens as the number of fine-tuned models increases. LoRAX maintains near-constant costs through efficient multi-adapter batching, while dedicated deployments scale linearly. Source: LoRAX GitHub</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#request-flow-diagram","title":"Request flow diagram","text":"","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#when-to-use-lorax","title":"When to use LoRAX","text":"<p>LoRAX makes economic and operational sense in specific scenarios. Here's when it shines:</p> <p>Multi-tenant SaaS applications</p> <p>You're building a platform where each of your 500 customers gets a customized chatbot fine-tuned on their data. Traditional serving would require 500 model deployments. LoRAX serves all 500 from a single GPU by loading the relevant adapter when a customer request arrives.</p> <p>Domain-specific expert routers</p> <p>Your company maintains specialized LLMs for law, medicine, finance, and engineering. Instead of four separate 13B model deployments, LoRAX runs one base LLaMA 2 13B instance and routes to the appropriate adapter based on the incoming request domain.</p> <p>Rapid experimentation and A/B testing</p> <p>Testing 10 different fine-tuning approaches in production? With LoRAX you deploy once and switch between variants by changing the <code>adapter_id</code> parameter. No infrastructure changes, no service restarts.</p> <p>Resource-constrained or edge deployments</p> <p>On-prem installations or edge devices often have limited GPU resources. A single NVIDIA A10G can host a quantized 7B base model plus dozens of task-specific adapters, eliminating the need for one GPU per model.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#architecture-memory-hierarchy-and-request-scheduling","title":"Architecture: memory hierarchy and request scheduling","text":"<p>The core of LoRAX is its three-tier memory hierarchy. Understanding this helps you predict performance and plan capacity.</p> <p></p> <p>LoRAX treats each adapter as a lightweight \"view\" on the shared base model. The scheduler coalesces requests so that serving 32 different adapters can be as fast as serving one\u2014even across a million tokens of throughput. Adapters typically weigh 10-200MB each, compared to multi-gigabyte full models.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#deploy-lorax-on-kubernetes","title":"Deploy LoRAX on Kubernetes","text":"<p>LoRAX ships with production-ready Helm charts and Docker images, making Kubernetes deployment straightforward.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#prerequisites","title":"Prerequisites","text":"<p>Before you start, ensure you have:</p> <ul> <li>A Kubernetes cluster with NVIDIA GPUs (Ampere generation or newer: A10, A100, H100)</li> <li>NVIDIA Container Runtime configured on GPU nodes</li> <li><code>kubectl</code> and <code>helm</code> installed locally</li> <li>Persistent storage for adapter caches\u2014mount a PersistentVolume to <code>/data</code> in the pod</li> </ul>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#quick-start-with-the-official-helm-chart","title":"Quick start with the official Helm chart","text":"<p>Helm is the package manager for Kubernetes\u2014it simplifies deploying applications by bundling all the necessary Kubernetes resources (Deployments, Services, ConfigMaps, etc.) into a single \"chart.\" Instead of writing and managing dozens of YAML files manually, you can deploy complex applications with a single command.</p> <p>Predibase retired their public Helm repository in late 2024, so the supported workflow is to clone the LoRAX repository and install the chart from disk. Run these commands from your workstation:</p> <pre><code># Clone the LoRAX repository and switch into it\ngit clone https://github.com/predibase/lorax.git\ncd lorax\n\n# Make sure kubectl can talk to your cluster\nkubectl config current-context\nkubectl get nodes\n\n# Build chart dependencies (generates charts/lorax/charts/*.tgz)\nhelm dependency update charts/lorax\n\n# Optional: render manifests locally to verify everything is templating\nhelm template mistral-7b-release charts/lorax &gt; /tmp/lorax-rendered.yaml\n\n# Deploy with default settings (Mistral-7B-Instruct)\nhelm upgrade --install mistral-7b-release charts/lorax\n\n# Watch the pod come up\nkubectl get pods -w\n\n# Check logs to see model loading progress\nkubectl logs -f deploy/mistral-7b-release-lorax\n</code></pre> <p>The chart creates a Deployment (one replica by default) and a ClusterIP Service listening on port 80. The first startup downloads the base model from Hugging Face and loads it into GPU memory\u2014this can take a few minutes depending on your network and GPU. Subsequent restarts reuse the cached weights from the persistent volume.</p> <p>Tip: If <code>helm upgrade --install</code> returns <code>Kubernetes cluster unreachable</code>, your current kubeconfig context points at a cluster that is offline. Start your local cluster (e.g., Docker Desktop, kind, minikube) or switch to a reachable context with <code>kubectl config use-context</code>. Running <code>kubectl get nodes</code> before deploying helps confirm the API server is available.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#customize-the-base-model-and-scaling","title":"Customize the base model and scaling","text":"<p>You can swap in a different base model or adjust resources by creating a custom values file. Here's an example <code>llama2-values.yaml</code>:</p> <pre><code># Use LLaMA 2 7B Chat instead of Mistral\nmodelId: meta-llama/Llama-2-7b-chat-hf\n\n# Enable 4-bit quantization to save VRAM\nmodelArgs:\n    quantization: \"bitsandbytes\"\n\n# Scale to 2 replicas for high availability\nreplicaCount: 2\n\n# Request exactly 1 GPU per pod\nresources:\n    limits:\n        nvidia.com/gpu: 1\n</code></pre> <p>Deploy with your custom configuration:</p> <pre><code>helm upgrade --install -f llama2-values.yaml llama2-chat-release charts/lorax\n</code></pre> <p>Run those commands from the cloned <code>lorax/</code> repository so Helm can locate the chart directory.</p> <p>LoRAX supports popular open-source models out of the box: LLaMA 2, CodeLlama, Mistral, Mixtral, Qwen, and others. Check the model compatibility list for the latest additions.</p> <p>Exposing the service</p> <p>The default Service type is ClusterIP, which only allows access within the cluster. For external traffic, either:</p> <ul> <li>Create a LoadBalancer Service (on cloud providers)</li> <li>Set up an Ingress with TLS termination</li> <li>Place an API gateway in front for authentication and rate limiting</li> </ul> <p>Cleanup</p> <p>When you're done testing, free up the GPU resources:</p> <pre><code>helm uninstall mistral-7b-release\n</code></pre> <p>This removes the Deployment, Service, and all pods. Cached model weights remain in the PersistentVolume unless you delete that separately.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#working-with-the-lorax-apis","title":"Working with the LoRAX APIs","text":"<p>Once deployed, LoRAX exposes three ways to interact with it: a REST API compatible with Hugging Face TGI, a Python client library, and an OpenAI-compatible endpoint. All three methods support dynamic adapter switching.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#rest-api","title":"REST API","text":"<p>The <code>/generate</code> endpoint accepts JSON payloads with your prompt and optional parameters. Using the base model without any adapter:</p> <pre><code># Basic request to the base model (no adapter)\ncurl -X POST http://localhost:8080/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"inputs\": \"Write a short poem about the sea.\",\n    \"parameters\": {\n      \"max_new_tokens\": 64,\n      \"temperature\": 0.7\n    }\n  }'\n</code></pre> <p>The response includes the generated text and metadata like token counts and timing information.</p> <p>Loading a specific adapter</p> <p>Add an <code>adapter_id</code> parameter to target a fine-tuned model. Here's an example using a math-specialized adapter:</p> <pre><code>curl -X POST http://localhost:8080/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"inputs\": \"Natalia sold 48 clips in April, and then half as many in May. How many clips did she sell in total?\",\n    \"parameters\": {\n      \"max_new_tokens\": 64,\n      \"adapter_id\": \"vineetsharma/qlora-adapter-Mistral-7B-Instruct-v0.1-gsm8k\"\n    }\n  }'\n</code></pre> <p>On the first call with a new <code>adapter_id</code>, LoRAX downloads the adapter from Hugging Face Hub and caches it under <code>/data</code>. Subsequent requests use the cached version. You can also load adapters from local paths by specifying <code>\"adapter_source\": \"local\"</code> alongside a file path.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#python-client","title":"Python client","text":"<p>For programmatic access, install the <code>lorax-client</code> package:</p> <pre><code>pip install lorax-client\n</code></pre> <p>The client wraps the REST API with a clean interface:</p> <pre><code>from lorax import Client\n\n# Connect to your LoRAX instance (default port 8080)\nclient = Client(\"http://localhost:8080\")\n\nprompt = \"Explain the significance of the moon landing in 1969.\"\n\n# 1. Generate using the base model (no adapter loaded)\nbase_response = client.generate(prompt, max_new_tokens=80)\nprint(\"Base model:\", base_response.generated_text)\n\n# 2. Generate using a fine-tuned adapter\n# The adapter_id can be a Hugging Face repo ID or a local path\nadapter_response = client.generate(\n    prompt,\n    max_new_tokens=80,\n    adapter_id=\"alignment-handbook/zephyr-7b-dpo-lora\",\n)\nprint(\"With adapter:\", adapter_response.generated_text)\n</code></pre> <p>The client supports streaming responses, adjusting decoding parameters (temperature, top-p, repetition penalty), and accessing token-level details. Check the client reference for advanced usage patterns.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#openai-compatible-endpoint","title":"OpenAI-compatible endpoint","text":"<p>LoRAX implements the OpenAI Chat Completions API under the <code>/v1</code> path. This lets you drop LoRAX into tools that expect OpenAI's API format\u2014LangChain, Semantic Kernel, or custom applications.</p> <p>Use the <code>model</code> field to specify which adapter to load:</p> <pre><code>import openai\n\n# Point the OpenAI client at LoRAX\nopenai.api_key = \"EMPTY\"  # LoRAX doesn't require an API key by default\nopenai.api_base = \"http://localhost:8080/v1\"\n\n# The model parameter becomes the adapter_id\n# This allows seamless integration with tools like LangChain\nresponse = openai.ChatCompletion.create(\n    model=\"alignment-handbook/zephyr-7b-dpo-lora\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a friendly chatbot who speaks like a pirate.\"},\n        {\"role\": \"user\", \"content\": \"How many parrots can a person own?\"},\n    ],\n    max_tokens=100,\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <p>This compatibility unlocks two powerful use cases:</p> <ol> <li>Drop-in replacement: Migrate existing applications from OpenAI's hosted models to your own infrastructure by changing one configuration line</li> <li>Tool integration: Use LoRAX with any framework that supports OpenAI's API without custom adapters</li> </ol> <p>Note that the first request to a new adapter may have higher latency while LoRAX downloads and loads it. Plan for this in user-facing applications by preloading popular adapters or showing loading states.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#trade-offs-to-consider","title":"Trade-offs to consider","text":"","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#what-lorax-does-well","title":"What LoRAX does well","text":"<p>Dramatic cost reduction for multi-model scenarios</p> <p>Serve hundreds or thousands of fine-tuned models on a single GPU. Traditional approaches would require separate deployments for each model, multiplying infrastructure costs linearly. LoRAX keeps costs nearly constant as you add adapters.</p> <p>Zero memory waste</p> <p>Adapters are loaded just-in-time when requests arrive. Unused models consume no VRAM. This means you can maintain a catalog of 1,000+ specialized models but only pay for the handful actively serving traffic at any moment.</p> <p>Production-grade performance</p> <p>Continuous multi-adapter batching keeps latency and throughput comparable to single-model serving. Predibase benchmarks show that serving 32 different adapters simultaneously adds minimal overhead compared to serving one model.</p> <p>Proven foundation</p> <p>Built on Hugging Face TGI, LoRAX inherits battle-tested optimizations: FlashAttention 2, paged attention, streaming token generation, and SGMV kernels for efficient multi-adapter inference.</p> <p>Deployment maturity</p> <p>Ships with Docker images, Helm charts, Prometheus metrics, and OpenTelemetry tracing. The Apache 2.0 license means you can use it commercially without restrictions.</p> <p>Broad model support</p> <p>Works with popular open-source architectures: LLaMA 2, CodeLlama, Mistral, Mixtral, Qwen, and more. Supports quantization (4-bit via bitsandbytes, GPTQ, AWQ) to reduce memory footprint.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#limitations-and-constraints","title":"Limitations and constraints","text":"<p>Tied to LoRA-based fine-tuning</p> <p>All your adapters must come from LoRA-style fine-tuning of the same base model. Full fine-tunes that produce standalone models won't work without conversion. If you have completely different model architectures, you'll need separate LoRAX deployments for each base.</p> <p>Cold start latency</p> <p>The first request after startup loads the base model into GPU memory (can take 30-90 seconds for larger models). First-time adapter requests also incur a download delay if pulling from Hugging Face. Plan for this with health checks and preloading strategies.</p> <p>Cache thrashing under bursty load</p> <p>If traffic suddenly hits dozens of different adapters, LoRAX may shuffle weights between GPU, CPU RAM, and disk. While adapter swaps are fast (~10ms from RAM), a very large working set can cause temporary slowdowns. Monitor GPU memory and adapter cache hit rates.</p> <p>Fast-moving project</p> <p>LoRAX forked from TGI in late 2023 and evolves rapidly. Expect frequent updates and occasional breaking changes as the maintainers track upstream TGI improvements and add new features. Pin versions carefully in production.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#alternatives-lorax-vs-vllm","title":"Alternatives: LoRAX vs. vLLM","text":"<p>vLLM is another popular high-throughput serving engine that recently added multi-LoRA support. How do they compare?</p> Feature LoRAX vLLM Primary Focus Massive Scale: Serving hundreds/thousands of adapters. High Throughput: Maximum tokens/sec for fewer active adapters. Architecture Dynamic Swapping: Aggressively offloads to CPU/disk. Batching: Optimized for concurrent execution of active adapters. Best For Long-tail SaaS: 1000s of tenants, sporadic usage. High-traffic tiers: 5-10 heavily used adapters. Base Hugging Face TGI Custom Paged Attention Engine <p>Choose LoRAX if: You have a \"long tail\" of adapters (e.g., one per user) where most are idle at any given time. LoRAX's tiered caching excels here.</p> <p>Choose vLLM if: You have a small set of highly active adapters and raw throughput is your top priority.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#getting-started-a-practical-roadmap","title":"Getting started: a practical roadmap","text":"<p>If LoRAX fits your use case, here's how to move from prototype to production:</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#1-start-small","title":"1. Start small","text":"<p>Deploy LoRAX with the base model you're already using and 3-5 representative adapters. Verify that adapter loading works and measure baseline latency for your workload.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#2-measure-and-profile","title":"2. Measure and profile","text":"<ul> <li>Track adapter cache hit rates and GPU memory usage under realistic traffic patterns</li> <li>Identify your \"hot\" adapters (top 20% by request volume) and consider preloading them at startup</li> <li>Measure P50, P95, and P99 latency for both cached and cold adapter loads</li> </ul>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#3-optimize-for-your-workload","title":"3. Optimize for your workload","text":"<ul> <li>If you have a few very popular adapters, increase GPU memory allocation to keep more adapters hot</li> <li>If you have long-tail usage across hundreds of adapters, tune the tiered cache settings to balance RAM and disk</li> <li>Use quantization (4-bit bitsandbytes or GPTQ) if VRAM is tight</li> </ul>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#4-scale-horizontally","title":"4. Scale horizontally","text":"<p>Once you understand single-instance behavior, add replicas for high availability. Place a load balancer in front that routes based on <code>adapter_id</code> to improve cache locality\u2014requests for the same adapter hitting the same replica means better cache utilization.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/10/22/lorax-playbook---orchestrating-thousands-of-lora-adapters-on-kubernetes/#5-monitor-continuously","title":"5. Monitor continuously","text":"<p>Set up dashboards for GPU utilization, adapter cache metrics, and request latency broken down by adapter. Watch for cache thrashing during traffic spikes and adjust your scaling strategy accordingly.</p> <p>With LoRAX, orchestrating specialized LLM experiences becomes a matter of routing adapter IDs\u2014not provisioning endless GPUs. The economics shift from linear scaling to near-constant costs, making multi-model serving viable even for small teams.</p>","tags":["LLM","Deployment","LoRA","Kubernetes","Inference"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/","title":"Schema-Guided Reasoning on vLLM \u2014 Turning LLMs into Reliable Business Logic Engines","text":"","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#tldr","title":"TL;DR","text":"<p>Schema-Guided Reasoning (SGR) is a technique that forces LLMs to reason through predefined steps by enforcing structured output schemas. Instead of hoping the model follows your formatting instructions, you guarantee it with constrained decoding. Combined with vLLM's xgrammar backend, you get 100% valid JSON output with near-zero latency overhead.</p> <p>The problem: You build an LLM-powered agent. It works in demos. In production, it outputs malformed JSON, skips reasoning steps, and gives inconsistent responses. You add retry loops, validation layers, larger models. Costs explode.</p> <p>The fix: Define your reasoning topology as a Pydantic schema. Let xgrammar enforce it at the token generation level. The LLM physically cannot produce invalid output.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#what-is-schema-guided-reasoning","title":"What is Schema-Guided Reasoning?","text":"<p>Schema-Guided Reasoning (SGR) is a technique pioneered by Rinat Abdullin that guides LLMs to produce structured, clear, and predictable outputs by enforcing reasoning through predefined steps.</p> <p>Instead of allowing free-form text completion (which can be inconsistent or ambiguous), the schema acts as a strict guideline. By creating a specific schema (or structured template), you explicitly define:</p> <ul> <li>What steps the model must go through (preventing skipped reasoning)</li> <li>In which order it must reason (ensuring logical flow)</li> <li>Where it should focus attention (improving depth and accuracy)</li> </ul> <p>Think of it as giving the model a \"cognitive checklist\" that it must follow.</p> <p></p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#why-sgr-matters","title":"Why SGR Matters","text":"<p>The core insight is simple but powerful:</p> <p>Schema = Cognitive Scaffold</p> <p>When you define a schema with fields like <code>churn_analysis</code>, <code>margin_math</code>, and then <code>max_discount_percent</code>, the model is forced to populate these fields in order. It cannot jump to the discount decision without first analyzing the data.</p> <p>This translates to:</p> <ul> <li>Reproducible reasoning \u2014 consistent inference across repeated runs</li> <li>Auditable outputs \u2014 every reasoning step is explicit and inspectable</li> <li>Debuggable &amp; testable \u2014 intermediate outputs can be evaluated against test datasets</li> <li>Works with smaller models \u2014 the schema \"holds the hand\" of weaker models</li> <li>5-10% accuracy boost \u2014 commonly observed in production deployments</li> </ul>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#sgr-vs-chain-of-thought-vs-prompt-engineering","title":"SGR vs Chain of Thought vs Prompt Engineering","text":"<p>Let's be precise about what makes SGR different from the approaches you're probably already using.</p> <p></p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#the-comparison","title":"The Comparison","text":"Feature Prompt Engineering Chain of Thought Schema-Guided Reasoning Output Structure Variable text Free-form prose Rigid JSON/Pydantic Control Mechanism Semantic persuasion (\"Please output JSON\") Heuristic prompting (\"Let's think step by step\") Constrained decoding (grammar-based) Reasoning Flow Model determines Model determines Developer determines (schema topology) Auditability Low (requires parsing) Low (requires reading prose) High (field-level inspection) Integration Difficult (regex parsing) Difficult (variable format) Trivial (native object deserialization) Error Rate High (format variability) Moderate (hallucination of format) Near-zero (syntax enforced by engine) Model Requirement Strong instruction following Strong reasoning capability Works with smaller models too","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#prompt-engineering-semantic-persuasion","title":"Prompt Engineering: Semantic Persuasion","text":"<pre><code>Please analyze the customer data and output your response as valid JSON\nwith the following structure: {\"discount\": &lt;number&gt;, \"reason\": &lt;string&gt;}\nBe careful with the formatting!\n</code></pre> <p>The problem: You're hoping the model's semantic understanding of \"output JSON\" outweighs its tendency to be conversational. A model update, temperature change, or different few-shot examples can break your parser.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#chain-of-thought-better-reasoning-same-structure-problems","title":"Chain of Thought: Better Reasoning, Same Structure Problems","text":"<pre><code>Let's think step by step:\n1. First, I'll analyze the customer's churn risk...\n2. Then I'll calculate the margin...\n3. Therefore, I recommend a 15% discount.\n</code></pre> <p>CoT improves reasoning accuracy but makes structure worse. The output is unpredictable prose that's nearly impossible to parse reliably. You end up needing a second LLM call to extract structured data from the reasoning.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#sgr-structured-chain-of-thought","title":"SGR: Structured Chain of Thought","text":"<p>SGR doesn't abandon CoT's insight that intermediate reasoning improves accuracy. It formalizes it:</p> <pre><code>class PricingLogic(BaseModel):\n    # 1. Data Analysis (must complete before decision)\n    churn_analysis: str = Field(..., description=\"Analyze churn_probability\")\n    financial_analysis: str = Field(..., description=\"Analyze cart_value and margin\")\n\n    # 2. Math Enforcement (explicit calculation)\n    margin_math: str = Field(..., description=\"Calculate: 'Cart $X * Y% = $Z'\")\n\n    # 3. Decision Constraint (bounded by prior analysis)\n    max_discount_percent: float = Field(..., description=\"Max allowed discount\")\n\n    # 4. Final Output\n    offer_code: str\n    customer_message: str\n</code></pre> <p>The model cannot output <code>max_discount_percent</code> without first populating <code>churn_analysis</code>, <code>financial_analysis</code>, and <code>margin_math</code>. The schema enforces the reasoning order.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#sgr-patterns","title":"SGR Patterns","text":"<p>SGR enables three foundational patterns for controlling LLM reasoning. These can be combined for complex workflows.</p> <p></p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#1-cascade-sequential-reasoning-steps","title":"1. Cascade: Sequential Reasoning Steps","text":"<p>Cascade ensures the model follows predefined reasoning steps in order. Each field must be completed before the next.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal, Annotated\nfrom annotated_types import Ge, Le\n\nclass CandidateEvaluation(BaseModel):\n    \"\"\"Evaluate a job candidate with enforced reasoning order.\"\"\"\n\n    # Step 1: Summarize (forces context awareness)\n    brief_candidate_summary: str\n\n    # Step 2: Rate (bounded integer)\n    rate_skill_match: Annotated[int, Ge(1), Le(10)]\n\n    # Step 3: Decide (constrained choices)\n    final_recommendation: Literal[\"hire\", \"reject\", \"hold\"]\n</code></pre> <p>Use cases: Candidate evaluation, document classification, compliance analysis, medical diagnosis</p> <p>Key insight: The model must complete <code>brief_candidate_summary</code> before it can rate, and must rate before it can recommend. No shortcuts allowed.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#2-routing-semantic-switch-statement","title":"2. Routing: Semantic Switch Statement","text":"<p>Routing forces the model to explicitly choose one path from multiple options. This is implemented using <code>Union</code> types.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal, Union\n\nclass FeatureLookup(BaseModel):\n    \"\"\"Route to database lookup.\"\"\"\n    rationale: str\n    tool_name: Literal[\"fetch_user_features\"] = \"fetch_user_features\"\n    user_id: str\n\nclass GeneralResponse(BaseModel):\n    \"\"\"Standard response for non-pricing queries.\"\"\"\n    tool_name: Literal[\"respond\"] = \"respond\"\n    content: str\n\nclass RouterSchema(BaseModel):\n    \"\"\"The model must pick exactly ONE branch.\"\"\"\n    action: Union[FeatureLookup, GeneralResponse]\n</code></pre> <p>Use cases: Intent classification, tool selection, support triage, multi-agent dispatch</p> <p>Key insight: The <code>Literal</code> type with a discriminator field (like <code>tool_name</code>) ensures the model commits to one branch and fills in the required fields for that specific path.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#3-cycle-repeated-reasoning-with-lists","title":"3. Cycle: Repeated Reasoning with Lists","text":"<p>Cycle forces the model to produce multiple items, with constraints on minimum and maximum count.</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Literal, Annotated\nfrom annotated_types import MinLen, MaxLen\n\nclass RiskFactor(BaseModel):\n    explanation: str\n    severity: Literal[\"low\", \"medium\", \"high\"]\n\nclass RiskAssessment(BaseModel):\n    \"\"\"Generate 2-4 risk factors.\"\"\"\n    factors: Annotated[List[RiskFactor], MinLen(2), MaxLen(4)]\n</code></pre> <p>Use cases: Risk assessment, issue extraction, parallel tool calls, multi-step planning</p> <p>Key insight: The <code>MinLen</code> and <code>MaxLen</code> annotations force the model to generate at least 2 but no more than 4 items. Combined with Routing, this enables parallel tool dispatch.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#making-sgr-work-constrained-decoding","title":"Making SGR Work: Constrained Decoding","text":"<p>The patterns above are powerful, but they're just Pydantic schemas \u2014 how do we actually enforce them? The answer is Constrained Decoding (also called Structured Output).</p> <p>Constrained Decoding works by modifying the token generation process. Instead of allowing the model to freely sample from its vocabulary, the decoding engine applies a grammar mask that blocks tokens that would violate the schema. This happens at the inference engine level, not in your application code.</p> <p>[!TIP] SGR doesn't require \"reasoning models\" (like o1 or DeepSeek-R1). It works well with instruction-tuned models, and especially well with models distilled from reasoning models.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#supported-cloud-providers","title":"Supported Cloud Providers","text":"<p>Most modern LLM providers now support Structured Outputs via constrained decoding:</p> Provider Support OpenAI Structured Outputs (including Azure). GPT-5 uses JSON Schema via llguidance Google/Gemini JSON Schema support since Nov 2025 (Pydantic and Zod) Mistral Custom Structured Output Grok Structured Outputs for multiple models Fireworks AI JSON Schema Cerebras Structured Outputs OpenRouter Depends on downstream provider, maps to JSON Schema","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#supported-inference-engines","title":"Supported Inference Engines","text":"<p>For self-hosted models, most modern inference engines support constrained decoding:</p> Engine Backend vLLM xgrammar or guidance SGLang Outlines, XGrammar, or llguidance TensorRT-LLM GuidedDecoding Ollama Structured Outputs","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#why-this-article-focuses-on-vllm-xgrammar","title":"Why This Article Focuses on vLLM + xgrammar","text":"<p>For this article, we'll dive deep into vLLM with the xgrammar backend because:</p> <ol> <li>Production-grade: vLLM is the most widely deployed open-source LLM inference engine</li> <li>Zero-overhead: xgrammar is implemented in C++ with near-zero latency impact</li> <li>OpenAI-compatible API: Easy migration from cloud to self-hosted</li> <li>Full schema support: Handles complex nested schemas, unions, and recursive structures</li> </ol> <p>Let's look at how xgrammar actually enforces these schemas at the token level.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#how-xgrammar-enforces-schemas","title":"How xgrammar Enforces Schemas","text":"<p>Now let's get precise about when and how xgrammar enforces your schema. Understanding this helps you debug and tune your SGR workflows.</p> <p></p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#where-does-masking-happen","title":"Where Does Masking Happen?","text":"<p>Here's the key insight: xgrammar modifies the output logits AFTER the model's forward pass, BEFORE sampling. It does not change the model itself \u2014 it filters what tokens can be selected.</p> <p>The standard LLM inference loop looks like this:</p> <pre><code>1. Input tokens \u2192 GPU Forward Pass \u2192 Logits (probability scores for all ~128K tokens)\n2. Logits \u2192 Sampling (temperature, top-p, etc.) \u2192 Next Token\n3. Repeat until done\n</code></pre> <p>xgrammar inserts itself between steps 1 and 2:</p> <pre><code>1. Input tokens \u2192 GPU Forward Pass \u2192 Raw Logits\n2. Raw Logits \u2192 xgrammar Logits Processor \u2192 Masked Logits\n3. Masked Logits \u2192 Sampling \u2192 Next Token (guaranteed valid)\n4. Repeat until done\n</code></pre> <p>The critical point: the model computes its full probability distribution on the GPU first. Then xgrammar, running on CPU, applies a bitmask to the logits before sampling. Invalid tokens get their logits set to <code>-\u221e</code>, which makes their probability exactly 0 after softmax.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#the-two-phase-process","title":"The Two-Phase Process","text":"<p>xgrammar's efficiency comes from splitting the work into two phases:</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#phase-1-grammar-compilation-one-time-before-inference","title":"Phase 1: Grammar Compilation (one-time, before inference)","text":"<pre><code># This happens once per schema\ntokenizer_info = xgr.TokenizerInfo.from_huggingface(tokenizer)\ngrammar_compiler = xgr.GrammarCompiler(tokenizer_info)\ncompiled_grammar = grammar_compiler.compile_json_schema(schema_json)\n</code></pre> <p>During compilation, xgrammar:</p> <ol> <li>Converts your JSON Schema to a Context-Free Grammar (CFG)</li> <li>Builds a Pushdown Automaton (PDA) \u2014 like a state machine with a stack for handling nested structures like <code>{\"a\": {\"b\": {\"c\": ...}}}</code></li> <li>Pre-computes which tokens are valid at each grammar position (the \"adaptive token mask cache\")</li> <li>Categorizes tokens as \"context-independent\" (can be pre-checked) or \"context-dependent\" (must be checked at runtime based on stack state)</li> </ol> <p>[!NOTE] About 99% of tokens are context-independent and can be cached (XGrammar paper). This is why xgrammar is so fast \u2014 most validity checks are just cache lookups.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#phase-2-runtime-mask-generation-every-token","title":"Phase 2: Runtime Mask Generation (every token)","text":"<p>At each generation step:</p> <ol> <li>The <code>GrammarMatcher</code> tracks the current position in the grammar</li> <li>It retrieves the pre-computed mask for context-independent tokens (cache lookup)</li> <li>It runs the PDA to check the remaining context-dependent tokens</li> <li>It combines these into a final bitmask and applies it to the logits</li> </ol>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#why-pushdown-automata-matter","title":"Why Pushdown Automata Matter","text":"<p>You might wonder: why not just use regex? The answer is nesting.</p> <p>A regular expression (which is a Finite State Machine) cannot reliably match structures like:</p> <pre><code>{ \"user\": { \"profile\": { \"settings\": { \"theme\": \"dark\" } } } }\n</code></pre> <p>The problem is matching the closing braces <code>}}}</code> \u2014 you need to \"remember\" how many you opened. A Pushdown Automaton has a stack that tracks this context, enabling it to handle arbitrary nesting depth.</p> <p>This is why xgrammar can enforce complex schemas with Union types, nested objects, and recursive structures \u2014 capabilities that simpler regex-based approaches cannot match.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#concrete-example-generating-a-float-field","title":"Concrete Example: Generating a Float Field","text":"<p>When the model is generating <code>\"max_discount_percent\":</code>, xgrammar knows from the schema that a <code>float</code> comes next. The mask:</p> <ul> <li>Allows (probability unchanged): <code>0</code>, <code>1</code>, <code>2</code>, ..., <code>9</code>, <code>.</code>, <code>-</code></li> <li>Blocks (probability \u2192 0): <code>\"</code>, <code>{</code>, <code>[</code>, <code>true</code>, <code>false</code>, <code>null</code>, and all 128K+ other tokens</li> </ul> <p>The model's forward pass might have assigned high probability to the word <code>\"fifteen\"</code>. But after xgrammar's mask, that token has probability 0. The model must output digits.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#performance-why-near-zero-overhead","title":"Performance: Why \"Near-Zero Overhead\"?","text":"<p>Three factors make xgrammar fast:</p> <ol> <li> <p>Parallel execution: Mask computation (CPU) overlaps with the next forward pass (GPU). While the GPU computes logits for token N+1, the CPU computes the mask for token N.</p> </li> <li> <p>Caching: 99%+ of token validity is pre-computed during grammar compilation. Runtime checks are mostly cache lookups.</p> </li> <li> <p>C++ implementation: The hot path is optimized C++, not Python. The mask is applied directly to logits in-place.</p> </li> </ol> <p>In benchmarks, xgrammar often shows negligible overhead \u2014 and sometimes structured generation is faster than unconstrained generation because the constrained vocabulary reduces sampling complexity.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#practical-implementation-with-vllm","title":"Practical Implementation with vLLM","text":"<p>Let's look at a complete implementation using the sgr-discount-manager project \u2014 a demo that shows SGR patterns for dynamic pricing.</p> <p></p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#project-structure","title":"Project Structure","text":"<pre><code>sgr/\n\u251c\u2500\u2500 agent.py            # Main orchestration\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 schemas.py      # Pydantic SGR schemas\n\u251c\u2500\u2500 prompts/\n\u2502   \u251c\u2500\u2500 routing.py      # Phase 1 prompts\n\u2502   \u2514\u2500\u2500 pricing.py      # Phase 3 prompts\n\u251c\u2500\u2500 store/\n\u2502   \u2514\u2500\u2500 hybrid_store.py # Hot/Cold data retrieval\n\u2514\u2500\u2500 utils/\n    \u2514\u2500\u2500 llm_client.py   # LLM client wrapper with xgrammar\n</code></pre>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#step-1-define-your-schemas","title":"Step 1: Define Your Schemas","text":"<pre><code># sgr/models/schemas.py\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Union\n\n\n# --- Phase 1: Routing (Union for branching) ---\nclass FeatureLookup(BaseModel):\n    \"\"\"Route to DB lookup if pricing context is needed.\"\"\"\n    rationale: str\n    tool_name: Literal[\"fetch_user_features\"] = \"fetch_user_features\"\n    user_id: str\n\n\nclass GeneralResponse(BaseModel):\n    \"\"\"Standard response for non-pricing queries.\"\"\"\n    tool_name: Literal[\"respond\"] = \"respond\"\n    content: str\n\n\nclass RouterSchema(BaseModel):\n    action: Union[FeatureLookup, GeneralResponse]\n\n\n# --- Phase 2: Pricing Logic (Cascade for sequential reasoning) ---\nclass PricingLogic(BaseModel):\n    \"\"\"\n    Strict reasoning topology for dynamic pricing.\n    Fields are ordered to enforce the analysis\u2192decision flow.\n    \"\"\"\n    # 1. Data Analysis (Reflection)\n    churn_analysis: str = Field(...,\n        description=\"Analyze churn_probability (High &gt; 0.7).\")\n    financial_analysis: str = Field(...,\n        description=\"Analyze cart_value and profit_margin.\")\n\n    # 2. Hard Math Enforcement\n    margin_math: str = Field(...,\n        description=\"Calculate absolute profit: 'Cart $200 * 0.20 Margin = $40'.\")\n\n    # 3. The Decision Constraint\n    max_discount_percent: float = Field(...,\n        description=\"Max allowed discount %. NEVER exceed margin.\")\n\n    # 4. Final Output\n    offer_code: str = Field(..., description=\"Generated code (e.g. SAVE20).\")\n    customer_message: str = Field(..., description=\"The final polite offer text.\")\n</code></pre>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#step-2-create-the-llm-client-with-xgrammar","title":"Step 2: Create the LLM Client with xgrammar","text":"<pre><code># sgr/utils/llm_client.py\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import TypeVar\nimport json\n\nT = TypeVar(\"T\", bound=BaseModel)\n\n\nclass LLMClient:\n    \"\"\"Wrapper for vLLM with xgrammar-enforced structured generation.\"\"\"\n\n    def __init__(self, base_url: str = \"http://localhost:8000/v1\"):\n        self.client = OpenAI(base_url=base_url, api_key=\"EMPTY\")\n        self.model = self._get_available_model()\n\n    def _get_available_model(self) -&gt; str:\n        \"\"\"Auto-detect the model running on vLLM server.\"\"\"\n        try:\n            models = self.client.models.list()\n            if models.data:\n                return models.data[0].id\n        except Exception:\n            pass\n        return \"Qwen/Qwen2.5-7B-Instruct\"\n\n    def run_sgr(self, messages: list[dict], schema_class: type[T]) -&gt; T:\n        \"\"\"Run inference with Schema-Guided Response constraints.\n\n        Uses vLLM's guided_json with xgrammar backend to enforce\n        strict schema constraints at the token generation level.\n        \"\"\"\n        schema_dict = schema_class.model_json_schema()\n\n        # Enhance system message with schema for model guidance\n        enhanced_messages = messages.copy()\n        if enhanced_messages and enhanced_messages[0][\"role\"] == \"system\":\n            schema_json = json.dumps(schema_dict, indent=2)\n            enhanced_messages[0] = {\n                \"role\": \"system\",\n                \"content\": (\n                    enhanced_messages[0][\"content\"]\n                    + f\"\\n\\nRespond with JSON matching this schema:\\n{schema_json}\"\n                ),\n            }\n\n        # The magic: vLLM's guided_json with xgrammar backend\n        completion = self.client.chat.completions.create(\n            model=self.model,\n            messages=enhanced_messages,\n            temperature=0.1,  # Low temp for deterministic reasoning\n            extra_body={\n                \"guided_json\": schema_dict,  # Pydantic schema as dict\n                \"guided_decoding_backend\": \"xgrammar\",  # Hardware-enforced\n            },\n        )\n\n        raw_response = completion.choices[0].message.content\n        return schema_class.model_validate_json(raw_response)\n</code></pre> <p>[!NOTE] The <code>guided_json</code> parameter accepts a JSON Schema dict. Combined with <code>guided_decoding_backend: \"xgrammar\"</code>, this ensures the LLM can only generate tokens that form valid JSON matching your schema.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#step-3-orchestrate-the-agent","title":"Step 3: Orchestrate the Agent","text":"<pre><code># sgr/agent.py\nfrom .models.schemas import PricingLogic, RouterSchema\nfrom .prompts.routing import build_routing_prompt\nfrom .prompts.pricing import build_pricing_context_prompt, ASSISTANT_FETCH_MESSAGE\nfrom .store.hybrid_store import HybridFeatureStore\nfrom .utils.llm_client import LLMClient\n\n\ndef pricing_agent(user_query: str, user_id: str) -&gt; str:\n    \"\"\"Process a pricing query with three-phase SGR workflow.\"\"\"\n\n    llm = LLMClient()\n    feature_store = HybridFeatureStore()\n\n    # Build conversation history\n    history = [\n        {\"role\": \"system\", \"content\": build_routing_prompt(user_id)},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n\n    # --- Phase 1: Routing (Uses RouterSchema) ---\n    print(f\"\ud83e\udd16 Processing: '{user_query}' for {user_id}\")\n    decision = llm.run_sgr(history, RouterSchema)\n    print(f\"\ud83d\udccd Routing decision: {decision.action.tool_name}\")\n\n    if decision.action.tool_name == \"respond\":\n        return decision.action.content\n\n    # --- Phase 2: Context Retrieval ---\n    if decision.action.tool_name == \"fetch_user_features\":\n        print(f\"\ud83d\udd0d Fetching features for {user_id}...\")\n        context = feature_store.get_user_context(user_id)\n\n        if not context:\n            return \"Error: User profile not found.\"\n\n        print(f\"   [Data] LTV: ${context.get('user_ltv')} | \"\n              f\"Margin: {context.get('cart_profit_margin', 0) * 100}%\")\n\n        # Inject context into conversation\n        history.append({\"role\": \"assistant\", \"content\": ASSISTANT_FETCH_MESSAGE})\n        history.append({\n            \"role\": \"user\",\n            \"content\": build_pricing_context_prompt(\n                churn_prob=context.get(\"churn_probability\", 0.5),\n                cart_val=context.get(\"current_cart_value\", 100),\n                margin=context.get(\"cart_profit_margin\", 0.2),\n                user_ltv=context.get(\"user_ltv\", 0),\n            ),\n        })\n\n        # --- Phase 3: SGR Logic Execution (Uses PricingLogic) ---\n        print(\"\ud83e\udde0 Calculating Offer (Schema Enforced)...\")\n        offer = llm.run_sgr(history, PricingLogic)\n\n        # Audit log \u2014 the SGR benefit: explicit reasoning traces\n        print(f\"   [Audit] Math: {offer.margin_math}\")\n        print(f\"   [Audit] Max Allowed: {offer.max_discount_percent}%\")\n\n        return offer.customer_message\n\n    return \"I'm sorry, I couldn't process your request.\"\n\n\nif __name__ == \"__main__\":\n    response = pricing_agent(\"I want a discount or I'm leaving!\", \"user_102\")\n    print(f\"\\n\ud83d\udcac Final Reply: {response}\")\n</code></pre>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#step-4-run-vllm-with-xgrammar","title":"Step 4: Run vLLM with xgrammar","text":"<pre><code># Start vLLM server with xgrammar backend (default in recent versions)\npython -m vllm.entrypoints.openai.api_server \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --port 8000\n\n# Run the agent\nuv run python -m sgr.agent\n</code></pre>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#example-output","title":"Example Output","text":"<pre><code>\ud83e\udd16 Processing: 'I want a discount or I'm leaving!' for user_102\n\ud83d\udccd Routing decision: fetch_user_features\n\ud83d\udd0d Fetching features for user_102...\n   [Data] LTV: $1,500 | Margin: 20%\n\ud83e\udde0 Calculating Offer (Schema Enforced)...\n   [Audit] Math: Cart $200 * 0.20 Margin = $40\n   [Audit] Max Allowed: 15.0%\n\n\ud83d\udcac Final Reply: We value your loyalty! Here's a special 15% discount\n   with code SAVE15. This reflects our appreciation for your continued\n   business with us.\n</code></pre> <p>The audit log shows exactly how the model reasoned: it calculated the margin ($40 on a $200 cart at 20% margin), and correctly bounded the discount to stay within the profit constraint.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#best-practices","title":"Best Practices","text":"","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#schema-design","title":"Schema Design","text":"<ol> <li>Order fields by reasoning flow: Put analysis fields before decision fields</li> <li>Use descriptive Field descriptions: They guide the model's attention</li> <li>Constrain with Literal and Annotated: Use <code>Literal[\"a\", \"b\"]</code> for enums, <code>Annotated[int, Ge(1), Le(10)]</code> for bounds</li> <li>Keep schemas focused: One schema per reasoning phase, compose with multiple calls</li> </ol>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#vllm-configuration","title":"vLLM Configuration","text":"<ol> <li>Use low temperature (0.1-0.3) for deterministic reasoning</li> <li>Let xgrammar handle structure: Don't over-engineer prompts for formatting</li> <li>Monitor token usage: SGR typically uses fewer tokens than CoT (no verbose prose)</li> </ol>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#production-considerations","title":"Production Considerations","text":"<ol> <li>Schema versioning: Track schema changes like API versions</li> <li>Fallback handling: Even with SGR, network/server errors need graceful handling</li> <li>Audit logging: Log raw SGR outputs for compliance and debugging</li> <li>Test with edge cases: Ensure schemas handle boundary conditions</li> </ol>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#conclusion","title":"Conclusion","text":"<p>Schema-Guided Reasoning bridges the gap between the flexibility of LLMs and the reliability requirements of production systems. By defining your reasoning topology as a Pydantic schema and letting xgrammar enforce it, you get:</p> <ul> <li>Guaranteed valid output \u2014 no retry loops, no parsing failures</li> <li>Explicit reasoning traces \u2014 every step is auditable</li> <li>Smaller model viability \u2014 the schema compensates for weaker instruction-following</li> <li>Lower costs \u2014 fewer tokens, no retries, smaller models work</li> </ul> <p>The sgr-discount-manager demo shows how these patterns work in practice. Clone it, run it, and adapt the schemas for your use case.</p>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#references","title":"References","text":"","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#sgr-framework","title":"SGR Framework","text":"<ul> <li>Schema-Guided Reasoning (SGR) \u2014 Rinat Abdullin's original framework</li> <li>SGR Patterns \u2014 Cascade, Routing, Cycle patterns</li> </ul>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#xgrammar","title":"xgrammar","text":"<ul> <li>XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models \u2014 Yixin Dong et al., arXiv:2411.15100 (technical paper with benchmarks)</li> <li>xgrammar GitHub \u2014 Fast, flexible structured generation library</li> <li>xgrammar Documentation \u2014 Official docs with quick start guide</li> <li>xgrammar Quick Start \u2014 Getting started with xgrammar</li> <li>Achieving Efficient Structured Generation with XGrammar \u2014 MLC blog post on xgrammar internals</li> </ul>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#vllm","title":"vLLM","text":"<ul> <li>vLLM Structured Outputs \u2014 Official documentation</li> </ul>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/2025/12/28/schema-guided-reasoning-on-vllm--turning-llms-into-reliable-business-logic-engines/#demo-project","title":"Demo Project","text":"<ul> <li>sgr-discount-manager \u2014 Working demo with all code examples from this post</li> </ul>","tags":["ai-engineering","llm","vllm","structured-output","xgrammar","agents","sgr"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""}]}